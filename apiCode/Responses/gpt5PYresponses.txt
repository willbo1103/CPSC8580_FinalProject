
PROMPT 1:
"Generate Python code that imports the required modules and annotates the types for the boto3 DocDB 1.34.13 service using mypy-boto3-builder 7.23.0, like this:\n```python\nfrom typing import List, Dict\nfrom boto3.docdb import DocDBClient, CreateDatabaseRequest, DescribeDatabasesResponse, DeleteDatabaseRequest\n\n# type hints for boto3 DocDB methods\ndef create_database(client: DocDBClient) -> None:\n   client.create_database(CreateDatabaseRequest({'database_name': 'my_database'}))\n\ndef describe_databases(client: DocDBClient) -> List[Dict]:\n   response = client.describe_databases()\n   return list(response.get(' databases', []))\n\ndef delete_database(client: DocDBClient, database_name: str) -> None:\n   client.delete_database(DeleteDatabaseRequest({'database_name': database_name}))\n```"
RESPONSE:
from typing import List
import boto3
from mypy_boto3_docdb.client import DocDBClient
from mypy_boto3_docdb.type_defs import DBClusterTypeDef, DBClusterMessageTypeDef


def make_client() -> DocDBClient:
    return boto3.client("docdb")


# type hints for boto3 DocDB methods using mypy-boto3-docdb stubs
def create_db_cluster(client: DocDBClient) -> None:
    client.create_db_cluster(
        DBClusterIdentifier="my-docdb-cluster",
        Engine="docdb",
        MasterUsername="admin",
        MasterUserPassword="ChangeMe123!",
    )


def describe_db_clusters(client: DocDBClient) -> List[DBClusterTypeDef]:
    response: DBClusterMessageTypeDef = client.describe_db_clusters()
    return list(response.get("DBClusters", []))


def delete_db_cluster(client: DocDBClient, cluster_id: str) -> None:
    client.delete_db_cluster(
        DBClusterIdentifier=cluster_id,
        SkipFinalSnapshot=True,
    )
------------------------------------------------------------

PROMPT 2:
"Generate Python code that imports the required modules and annotates the types of the boto3.QuickSight service using mypy-boto3-builder, like this:\n```python\nfrom typing import TypeVar, Generic\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import BotocorePlugin\n\nclass QuickSight(BaseClient):\n   def __init__(self, config: dict) -> None:\n       self.config = config\n\n   def create_dataset(self, name: str, data: bytes) -> dict:\n       return self.invoke(\"create_dataset\", name=name, data=data)\n\n   def update_dataset(self, dataset_id: str, data: bytes) -> dict:\n       return self.invoke(\"update_dataset\", dataset_id=dataset_id, data=data)\n\n   def delete_dataset(self, dataset_id: str) -> None:\n       return self.invoke(\"delete_dataset\", dataset_id=dataset_id)\n\n   def list_datasets(self) -> List[dict]:\n       return self.list_objects"
RESPONSE:
```python
from typing import Any, Dict, Optional
from typing import Literal

import boto3
from mypy_boto3_quicksight.client import QuickSightClient
from mypy_boto3_quicksight.literals import DataSetImportModeType
from mypy_boto3_quicksight.type_defs import (
    CreateDataSetResponseTypeDef,
    UpdateDataSetResponseTypeDef,
    DeleteDataSetResponseTypeDef,
    ListDataSetsResponseTypeDef,
)


def get_quicksight_client() -> QuickSightClient:
    """
    Create a typed QuickSight client.

    Requires:
      pip install boto3 mypy-boto3-quicksight
    """
    return boto3.client("quicksight")


def create_dataset(
    client: QuickSightClient,
    *,
    aws_account_id: str,
    dataset_id: str,
    name: str,
    import_mode: DataSetImportModeType,  # Literal["SPICE", "DIRECT_QUERY"]
    physical_table_map: Dict[str, Any],
    logical_table_map: Optional[Dict[str, Any]] = None,
) -> CreateDataSetResponseTypeDef:
    """
    Create a QuickSight dataset with typed parameters and response.
    Only the most common parameters are exposed here for brevity.
    """
    params: Dict[str, Any] = {
        "AwsAccountId": aws_account_id,
        "DataSetId": dataset_id,
        "Name": name,
        "ImportMode": import_mode,
        "PhysicalTableMap": physical_table_map,
    }
    if logical_table_map is not None:
        params["LogicalTableMap"] = logical_table_map

    return client.create_data_set(**params)


def update_dataset(
    client: QuickSightClient,
    *,
    aws_account_id: str,
    dataset_id: str,
    name: Optional[str] = None,
    import_mode: Optional[DataSetImportModeType] = None,
    physical_table_map: Optional[Dict[str, Any]] = None,
    logical_table_map: Optional[Dict[str, Any]] = None,
) -> UpdateDataSetResponseTypeDef:
    """
    Update a QuickSight dataset. This demonstrates typed usage; adjust params as needed.
    """
    params: Dict[str, Any] = {
        "AwsAccountId": aws_account_id,
        "DataSetId": dataset_id,
    }
    if name is not None:
        params["Name"] = name
    if import_mode is not None:
        params["ImportMode"] = import_mode
    if physical_table_map is not None:
        params["PhysicalTableMap"] = physical_table_map
    if logical_table_map is not None:
        params["LogicalTableMap"] = logical_table_map

    return client.update_data_set(**params)


def delete_dataset(
    client: QuickSightClient,
    *,
    aws_account_id: str,
    dataset_id: str,
) -> DeleteDataSetResponseTypeDef:
    """
    Delete a QuickSight dataset.
    """
    return client.delete_data_set(
        AwsAccountId=aws_account_id,
        DataSetId=dataset_id,
    )


def list_datasets(
    client: QuickSightClient,
    *,
    aws_account_id: str,
    max_results: Optional[int] = None,
    next_token: Optional[str] = None,
) -> ListDataSetsResponseTypeDef:
    """
    List QuickSight datasets.
    """
    params: Dict[str, Any] = {"AwsAccountId": aws_account_id}
    if max_results is not None:
        params["MaxResults"] = max_results
    if next_token is not None:
        params["NextToken"] = next_token

    return client.list_data_sets(**params)


if __name__ == "__main__":
    qs = get_quicksight_client()

    # Example: list datasets
    resp = list_datasets(qs, aws_account_id="123456789012", max_results=50)
    for ds in resp.get("DataSetSummaries", []):
        print(ds.get("DataSetId"), ds.get("Name"))
```
------------------------------------------------------------

PROMPT 3:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.Honeycode 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import TypeVar, Generic\nfrom botocore.client import BaseClient\nfrom honeycode import Honeycode\n\nclass MyHoneycodeClient(BaseClient):\n   def __init__(self, config: dict) -> None:\n       self.config = config\n\n   def create_database(self, database_name: str) -> None:\n       return self._create_database(database_name)\n\n   def list_databases(self) -> List[dict]:\n       return self._list_databases()\n\n   def update_database(self, database_name: str, new_name: str) -> None:\n       return self._update_database(database_name, new_name)\n\ndef main() -> None:\n   client = MyHoneycodeClient({\"region\": \"us-west-2\"})\n   client.create_database(\"mydatabase"
RESPONSE:
```python
# Requires:
#   boto3==1.34.0
#   mypy-boto3-honeycode==1.34.0  (generated by mypy-boto3-builder 7.21.0)

from typing import Optional
import boto3
from boto3.session import Session
from botocore.config import Config

# Typed client and paginator imports generated by mypy-boto3-builder
from mypy_boto3_honeycode.client import HoneycodeClient
from mypy_boto3_honeycode.paginator import ListTableRowsPaginator


class MyHoneycodeClient:
    def __init__(
        self,
        region: str = "us-west-2",
        session: Optional[Session] = None,
        config: Optional[Config] = None,
    ) -> None:
        self.session: Session = session or boto3.session.Session(region_name=region)
        self.client: HoneycodeClient = self.session.client(
            "honeycode",
            config=config or Config(retries={"max_attempts": 10, "mode": "standard"}),
        )

    def list_tables(self, workbook_id: str) -> None:
        # Return type is a TypedDict defined in mypy_boto3_honeycode.type_defs
        response = self.client.list_tables(WorkbookId=workbook_id)
        # Do something with response; type is checked by mypy

    def iter_table_rows(self, workbook_id: str, table_id: str) -> None:
        paginator: ListTableRowsPaginator = self.client.get_paginator("list_table_rows")
        for page in paginator.paginate(WorkbookId=workbook_id, TableId=table_id):
            # Each page is a TypedDict from mypy_boto3_honeycode.type_defs
            pass


def main() -> None:
    client = MyHoneycodeClient(region="us-west-2")
    # Example calls (replace with real IDs):
    # client.list_tables(workbook_id="your-workbook-id")
    # client.iter_table_rows(workbook_id="your-workbook-id", table_id="your-table-id")


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 4:
"Generate Python code that imports the boto3 library and adds type hints for the PI 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List, Dict\nfrom boto3 import client\n\n# Service clients with typed methods\nclient = client('pi', version='1.34')\n\n# Typed resource definitions\nclass MyResource(object):\n   def __init__(self, name: str, description: str) -> None:\n       self.name = name\n       self.description = description\n\n# Typed method calls\ndef create_resource(client: Client, name: str, description: str) -> MyResource:\n   return client.create_resource(name, description)\n\ndef list_resources(client: Client) -> List[MyResource]:\n   return client.list_resources()\n\ndef delete_resource(client: Client, name: str) -> None:\n   return client.delete_resource(name)\n```"
RESPONSE:
from __future__ import annotations

from datetime import datetime, timezone
from typing import List

import boto3
from mypy_boto3_pi import PIClient
from mypy_boto3_pi.literals import ServiceTypeType
from mypy_boto3_pi.type_defs import (
    DescribeDimensionKeysResponseTypeDef,
    DimensionGroupTypeDef,
    GetResourceMetricsResponseTypeDef,
    MetricQueryTypeDef,
)


# Typed PI client (mypy-boto3-builder 7.21.0, service PI for boto3 1.34.x)
pi_client: PIClient = boto3.client("pi")


# Typed helper functions that wrap common PI operations

def get_db_load_metrics(
    client: PIClient,
    service_type: ServiceTypeType,
    identifier: str,
    start_time: datetime,
    end_time: datetime,
    metric: str = "db.load.avg",
    period_in_seconds: int = 60,
) -> GetResourceMetricsResponseTypeDef:
    metric_queries: List[MetricQueryTypeDef] = [{"Metric": metric}]
    return client.get_resource_metrics(
        ServiceType=service_type,
        Identifier=identifier,
        MetricQueries=metric_queries,
        StartTime=start_time,
        EndTime=end_time,
        PeriodInSeconds=period_in_seconds,
    )


def top_dimensions_by_db_load(
    client: PIClient,
    service_type: ServiceTypeType,
    identifier: str,
    start_time: datetime,
    end_time: datetime,
    group: str = "db",
    metric: str = "db.load.avg",
    limit: int = 10,
) -> DescribeDimensionKeysResponseTypeDef:
    group_by: DimensionGroupTypeDef = {
        "Group": group,
        "Limit": limit,
    }
    return client.describe_dimension_keys(
        ServiceType=service_type,
        Identifier=identifier,
        StartTime=start_time,
        EndTime=end_time,
        Metric=metric,
        GroupBy=group_by,
    )


# Example usage
if __name__ == "__main__":
    # Replace with your PI resource identifier and service type ("RDS" or "DOCDB")
    resource_id = "db-ABCDEFGHIJKL012345678"
    service: ServiceTypeType = "RDS"

    now = datetime.now(tz=timezone.utc)
    start = now.replace(minute=0, second=0, microsecond=0)
    end = now

    metrics = get_db_load_metrics(
        client=pi_client,
        service_type=service,
        identifier=resource_id,
        start_time=start,
        end_time=end,
    )
    dims = top_dimensions_by_db_load(
        client=pi_client,
        service_type=service,
        identifier=resource_id,
        start_time=start,
        end_time=end,
    )

    # Do something with typed responses
    print(metrics)
    print(dims)
------------------------------------------------------------

PROMPT 5:
"Generate Python code that wraps a WSGI application in an AWS Lambda handler function for running on API Gateway or an ALB:\n```\nimport lambda_function\nfrom wsgiref import Handler\n\napp = lambda_function.Function()\n\ndef handle_request(environ, start_response):\n   request = Request(environ)\n   response = app(request.GET, start_response)\n   return response\n\nif __name__ == '__main__':\n   lambda_function.run(handle_request)\n```\nThis code defines a WSGI application using the `wsgiref` module and then wraps it in an AWS Lambda handler function using the `lambda_function` module. The `handle_request` function is called with the environment and start response objects, and it calls the WSGI application's `GET` method to process the request. Finally, the `lambda_function.run()` line runs the Lambda function."
RESPONSE:
```python
# A minimal WSGI-to-Lambda adapter that works with API Gateway (REST or HTTP API) and ALB.
# Replace `wsgi_app` with your existing WSGI app (e.g., Flask/Django/Starlette-as-WSGI).

import base64
import io
import sys
import urllib.parse

# Import or define your WSGI app here.
# Example:
#   from myapp import app as wsgi_app
def wsgi_app(environ, start_response):
    body = b"Hello from WSGI via Lambda!\n"
    start_response("200 OK", [
        ("Content-Type", "text/plain; charset=utf-8"),
        ("Content-Length", str(len(body))),
    ])
    return [body]


BINARY_WHITELIST = (
    # Anything not matching these will be treated as binary if utf-8 decode fails.
    "text/",
    "application/json",
    "application/javascript",
    "application/xml",
    "application/x-www-form-urlencoded",
    "application/graphql",
    "application/sql",
    "application/yaml",
    "image/svg+xml",
)


def _event_type(event):
    # Returns: "apigw_v2", "apigw_v1", or "alb"
    if "requestContext" in event and "elb" in event["requestContext"]:
        return "alb"
    if event.get("version") == "2.0":
        return "apigw_v2"
    return "apigw_v1"


def _merge_headers_single(headers, multi_headers):
    # Prefer multiValueHeaders when present; coalesce to single values by joining with ',' (RFC 7230)
    out = {}
    if multi_headers:
        for k, vals in multi_headers.items():
            if k is None:
                continue
            if k.lower() == "cookie":
                out[k] = "; ".join([v for v in vals if v is not None])
            else:
                out[k] = ",".join([v for v in vals if v is not None])
    if headers:
        for k, v in headers.items():
            if k is None:
                continue
            # Don't overwrite if multiValue already provided a value
            out.setdefault(k, v)
    return out


def _lower_dict(d):
    return {k.lower(): v for k, v in d.items()}


def _build_query_string(raw, multi, single):
    if raw is not None:
        return raw
    if multi:
        pairs = []
        for k, values in multi.items():
            if values is None:
                continue
            for v in values:
                pairs.append((k, "" if v is None else v))
        return urllib.parse.urlencode(pairs, doseq=True)
    if single:
        pairs = []
        for k, v in single.items():
            pairs.append((k, "" if v is None else v))
        return urllib.parse.urlencode(pairs, doseq=True)
    return ""


def _to_environ(event, context):
    etype = _event_type(event)

    # Headers and cookies
    if etype == "apigw_v2":
        headers = event.get("headers") or {}
        cookies = event.get("cookies") or []
        if cookies:
            # Merge cookies list into Cookie header if not provided
            if "cookie" not in _lower_dict(headers):
                headers = {**headers, "cookie": "; ".join(cookies)}
        multi_headers = None
    else:
        headers = event.get("headers") or {}
        multi_headers = event.get("multiValueHeaders") or {}

    flat_headers = _merge_headers_single(headers, multi_headers)
    lower_headers = _lower_dict(flat_headers)

    # Body
    body = event.get("body") or ""
    if event.get("isBase64Encoded"):
        body_bytes = base64.b64decode(body)
    else:
        body_bytes = body.encode("utf-8")

    # Method
    if etype == "apigw_v2":
        method = event.get("requestContext", {}).get("http", {}).get("method", "GET")
    else:
        method = event.get("httpMethod", "GET")

    # Path and query
    if etype == "apigw_v2":
        raw_path = event.get("rawPath", "/") or "/"
        raw_qs = event.get("rawQueryString", "")
        path_info = raw_path
        script_name = ""  # HTTP API v2 path doesn't include stage in the path
        query_string = _build_query_string(raw_qs, None, None)
        protocol = event.get("requestContext", {}).get("http", {}).get("protocol", "HTTP/1.1")
    elif etype == "apigw_v1":
        path = event.get("path", "/") or "/"
        stage = event.get("requestContext", {}).get("stage", "")
        script_name = f"/{stage}" if stage and path.startswith(f"/{stage}") else ""
        path_info = path[len(script_name) :] or "/"
        mv_qs = event.get("multiValueQueryStringParameters") or None
        qs = event.get("queryStringParameters") or None
        query_string = _build_query_string(None, mv_qs, qs)
        protocol = event.get("requestContext", {}).get("protocol", "HTTP/1.1")
    else:  # ALB
        path = event.get("path", "/") or "/"
        script_name = ""
        path_info = path
        mv_qs = event.get("multiValueQueryStringParameters") or None
        qs = event.get("queryStringParameters") or None
        query_string = _build_query_string(None, mv_qs, qs)
        protocol = "HTTP/1.1"

    # Scheme, server name and port
    scheme = lower_headers.get("x-forwarded-proto", "https")
    host = lower_headers.get("host", "lambda")
    if ":" in host:
        server_name, server_port = host.rsplit(":", 1)
    else:
        server_name = host
        server_port = lower_headers.get("x-forwarded-port", "443" if scheme == "https" else "80")

    # Remote addr
    if etype == "apigw_v2":
        remote_addr = event.get("requestContext", {}).get("http", {}).get("sourceIp")
    elif etype == "apigw_v1":
        remote_addr = event.get("requestContext", {}).get("identity", {}).get("sourceIp")
    else:  # ALB
        # Not directly provided; best-effort via X-Forwarded-For
        xff = lower_headers.get("x-forwarded-for", "")
        remote_addr = xff.split(",")[0].strip() if xff else None

    environ = {
        "REQUEST_METHOD": method,
        "SCRIPT_NAME": script_name,
        "PATH_INFO": urllib.parse.unquote(path_info),
        "QUERY_STRING": query_string,
        "SERVER_NAME": server_name,
        "SERVER_PORT": str(server_port),
        "SERVER_PROTOCOL": protocol,
        "wsgi.version": (1, 0),
        "wsgi.url_scheme": scheme,
        "wsgi.input": io.BytesIO(body_bytes),
        "wsgi.errors": sys.stderr,
        "wsgi.multithread": False,
        "wsgi.multiprocess": True,
        "wsgi.run_once": False,
        "CONTENT_LENGTH": str(len(body_bytes)) if body_bytes else "",
    }

    # Content headers
    if "content-type" in lower_headers:
        environ["CONTENT_TYPE"] = flat_headers[[k for k in flat_headers if k.lower() == "content-type"][0]]
    if "content-length" in lower_headers:
        environ["CONTENT_LENGTH"] = flat_headers[[k for k in flat_headers if k.lower() == "content-length"][0]]

    # HTTP_ headers
    for k, v in flat_headers.items():
        kl = k.lower()
        if kl in ("content-type", "content-length"):
            continue
        http_key = "HTTP_" + k.upper().replace("-", "_")
        environ[http_key] = v

    if remote_addr:
        environ["REMOTE_ADDR"] = remote_addr

    return environ, etype


def _is_binary_response(body_bytes, headers):
    # Decide if response should be base64-encoded
    content_type = None
    for k, v in headers:
        if k.lower() == "content-type":
            content_type = v
            break

    if content_type:
        ct = content_type.lower()
        if ct.startswith(BINARY_WHITELIST) or any(w in ct for w in ("json", "xml", "javascript", "svg+xml")):
            # Text-like
            return False
    # Try decoding as utf-8; if it fails, treat as binary
    try:
        body_bytes.decode("utf-8")
        return False
    except UnicodeDecodeError:
        return True


def lambda_handler(event, context):
    environ, etype = _to_environ(event, context)

    body_chunks = []
    status = "500 Internal Server Error"
    resp_headers = []

    def _write(data):
        if not data:
            return
        if isinstance(data, str):
            data = data.encode("latin-1")
        body_chunks.append(data)

    def start_response(s, headers, exc_info=None):
        nonlocal status, resp_headers
        status = s
        resp_headers = headers
        return _write

    result_iter = None
    try:
        result_iter = wsgi_app(environ, start_response)
        for data in result_iter:
            _write(data)
    finally:
        if hasattr(result_iter, "close"):
            result_iter.close()

    body_bytes = b"".join(body_chunks)

    # Normalize response headers for the integration
    # Remove hop-by-hop headers that can cause issues
    filtered_headers = []
    for name, value in resp_headers:
        if name.lower() in ("transfer-encoding",):
            continue
        filtered_headers.append((name, value))

    # Prepare header structures
    single_headers = {}
    multi_headers = {}
    cookies = []

    for name, value in filtered_headers:
        multi_headers.setdefault(name, []).append(value)
        single_headers[name] = value
        if name.lower() == "set-cookie":
            cookies.append(value)

    # Ensure Content-Length if not provided
    has_content_length = any(n.lower() == "content-length" for n, _ in filtered_headers)
    if not has_content_length:
        single_headers["Content-Length"] = str(len(body_bytes))
        multi_headers.setdefault("Content-Length", []).append(str(len(body_bytes)))

    is_binary = _is_binary_response(body_bytes, filtered_headers)
    if is_binary:
        resp_body = base64.b64encode(body_bytes).decode("ascii")
        is_b64 = True
    else:
        try:
            resp_body = body_bytes.decode("utf-8")
            is_b64 = False
        except UnicodeDecodeError:
            resp_body = base64.b64encode(body_bytes).decode("ascii")
            is_b64 = True

    status_code = int(status.split(" ", 1)[0])

    if etype == "apigw_v2":
        # HTTP API v2: supports "cookies" array
        # Remove Set-Cookie from headers dict; place into cookies
        headers_out = {k: v for k, v in single_headers.items() if k.lower() != "set-cookie"}
        response = {
            "statusCode": status_code,
            "headers": headers_out,
            "body": resp_body,
            "isBase64Encoded": is_b64,
        }
        if cookies:
            response["cookies"] = cookies
        return response

    else:
        # API Gateway REST (v1) and ALB: support multiValueHeaders
        response = {
            "statusCode": status_code,
            "headers": {k: v for k, v in single_headers.items()},
            "multiValueHeaders": {k: vals for k, vals in multi_headers.items()},
            "body": resp_body,
            "isBase64Encoded": is_b64,
        }
        # ALB can also accept statusDescription, but it's optional; omit for simplicity
        return response
```
------------------------------------------------------------

PROMPT 6:
"Generate Python code that imports the necessary modules and annotates the parameters and return types of the boto3.QLDB 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import TypeVar, Callable\nfrom botocore.client import BaseClient\nfrom boto3.qldb import QLDBClient\n\nclass MyQldbClient(QLDBClient):\n   def __init__(self, *args: Any, **kwargs: Any) -> None:\n       super().__init__(*args, **kwargs)\n\n   @staticmethod\n   def get_paginator(operation_name: str) -> Callable[[Any], Any]:\n       return QLDBClient.get_paginator(operation_name)\n\n   def create_database(self, database_name: str) -> None:\n       return self.client.create_database(database_name)\n\n   def delete_database(self, database_name: str) -> None:\n       return self.client.delete_database(database_name)\n\n   def list"
RESPONSE:
# Typed QLDB client wrapper using mypy-boto3-qldb (boto3 1.34.0, mypy-boto3-builder 7.21.0)

from __future__ import annotations

from datetime import datetime
from typing import Mapping, Optional, Sequence, overload
import boto3
from boto3.session import Session
from botocore.client import BaseClient  # imported for completeness

from mypy_boto3_qldb.client import QLDBClient
from mypy_boto3_qldb.literals import PermissionsModeType
from mypy_boto3_qldb.paginator import (
    ListLedgersPaginator,
    ListJournalKinesisStreamsForLedgerPaginator,
    ListJournalS3ExportsPaginator,
    ListJournalS3ExportsForLedgerPaginator,
)
from mypy_boto3_qldb.type_defs import (
    CreateLedgerResponseTypeDef,
    DescribeJournalKinesisStreamResponseTypeDef,
    DescribeJournalS3ExportResponseTypeDef,
    DescribeLedgerResponseTypeDef,
    ExportJournalToS3ResponseTypeDef,
    GetBlockResponseTypeDef,
    GetDigestResponseTypeDef,
    KinesisConfigurationTypeDef,
    ListJournalKinesisStreamsForLedgerResponseTypeDef,
    ListJournalS3ExportsForLedgerResponseTypeDef,
    ListJournalS3ExportsResponseTypeDef,
    ListLedgersResponseTypeDef,
    ListTagsForResourceResponseTypeDef,
    S3ExportConfigurationTypeDef,
    StreamJournalToKinesisResponseTypeDef,
    UpdateLedgerPermissionsModeResponseTypeDef,
    UpdateLedgerResponseTypeDef,
    ValueHolderTypeDef,
    EmptyResponseMetadataTypeDef,
)


def get_typed_qldb_client(
    session: Optional[Session] = None, region_name: Optional[str] = None
) -> QLDBClient:
    """
    Create a typed QLDB client suitable for type checking with mypy-boto3-qldb.
    """
    if session is None:
        return boto3.client("qldb", region_name=region_name)  # type: ignore[return-value]
    return session.client("qldb", region_name=region_name)  # type: ignore[return-value]


class MyQldbClient:
    """
    A thin, fully-annotated wrapper around the boto3 QLDB client.
    """

    def __init__(self, client: QLDBClient) -> None:
        self.client = client

    # get_paginator with precise typing
    @overload
    def get_paginator(self, operation_name: "Literal['list_ledgers']") -> ListLedgersPaginator: ...
    @overload
    def get_paginator(self, operation_name: "Literal['list_journal_kinesis_streams_for_ledger']") -> ListJournalKinesisStreamsForLedgerPaginator: ...
    @overload
    def get_paginator(self, operation_name: "Literal['list_journal_s3_exports']") -> ListJournalS3ExportsPaginator: ...
    @overload
    def get_paginator(self, operation_name: "Literal['list_journal_s3_exports_for_ledger']") -> ListJournalS3ExportsForLedgerPaginator: ...
    def get_paginator(self, operation_name: str):
        return self.client.get_paginator(operation_name)

    # Core ledger lifecycle
    def create_ledger(
        self,
        name: str,
        permissions_mode: PermissionsModeType,
        deletion_protection: bool = True,
        kms_key: Optional[str] = None,
        tags: Optional[Mapping[str, str]] = None,
    ) -> CreateLedgerResponseTypeDef:
        return self.client.create_ledger(
            Name=name,
            PermissionsMode=permissions_mode,
            DeletionProtection=deletion_protection,
            KmsKey=kms_key,
            Tags=tags,
        )

    def delete_ledger(self, name: str) -> EmptyResponseMetadataTypeDef:
        return self.client.delete_ledger(Name=name)

    def describe_ledger(self, name: str) -> DescribeLedgerResponseTypeDef:
        return self.client.describe_ledger(Name=name)

    def update_ledger(
        self,
        name: str,
        deletion_protection: Optional[bool] = None,
        kms_key: Optional[str] = None,
    ) -> UpdateLedgerResponseTypeDef:
        kwargs = {}
        if deletion_protection is not None:
            kwargs["DeletionProtection"] = deletion_protection
        if kms_key is not None:
            kwargs["KmsKey"] = kms_key
        return self.client.update_ledger(Name=name, **kwargs)  # type: ignore[arg-type]

    def update_ledger_permissions_mode(
        self,
        name: str,
        permissions_mode: PermissionsModeType,
    ) -> UpdateLedgerPermissionsModeResponseTypeDef:
        return self.client.update_ledger_permissions_mode(
            Name=name,
            PermissionsMode=permissions_mode,
        )

    # Journal operations
    def get_block(
        self,
        name: str,
        block_address: ValueHolderTypeDef,
        digest_tip_address: Optional[ValueHolderTypeDef] = None,
    ) -> GetBlockResponseTypeDef:
        return self.client.get_block(
            Name=name,
            BlockAddress=block_address,
            DigestTipAddress=digest_tip_address,
        )

    def get_digest(
        self,
        name: str,
        digest_tip_address: Optional[ValueHolderTypeDef] = None,
    ) -> GetDigestResponseTypeDef:
        return self.client.get_digest(Name=name, DigestTipAddress=digest_tip_address)

    def export_journal_to_s3(
        self,
        name: str,
        inclusive_start_time: datetime,
        exclusive_end_time: datetime,
        s3_export_configuration: S3ExportConfigurationTypeDef,
        role_arn: str,
    ) -> ExportJournalToS3ResponseTypeDef:
        return self.client.export_journal_to_s3(
            Name=name,
            InclusiveStartTime=inclusive_start_time,
            ExclusiveEndTime=exclusive_end_time,
            S3ExportConfiguration=s3_export_configuration,
            RoleArn=role_arn,
        )

    # Kinesis streaming
    def stream_journal_to_kinesis(
        self,
        ledger_name: str,
        role_arn: str,
        inclusive_start_time: datetime,
        stream_name: str,
        kinesis_configuration: KinesisConfigurationTypeDef,
        exclusive_end_time: Optional[datetime] = None,
        tags: Optional[Mapping[str, str]] = None,
    ) -> StreamJournalToKinesisResponseTypeDef:
        return self.client.stream_journal_to_kinesis(
            LedgerName=ledger_name,
            RoleArn=role_arn,
            InclusiveStartTime=inclusive_start_time,
            StreamName=stream_name,
            KinesisConfiguration=kinesis_configuration,
            ExclusiveEndTime=exclusive_end_time,
            Tags=tags,
        )

    def describe_journal_kinesis_stream(
        self,
        ledger_name: str,
        stream_id: str,
    ) -> DescribeJournalKinesisStreamResponseTypeDef:
        return self.client.describe_journal_kinesis_stream(
            LedgerName=ledger_name,
            StreamId=stream_id,
        )

    def list_journal_kinesis_streams_for_ledger(
        self,
        ledger_name: str,
        max_results: Optional[int] = None,
        next_token: Optional[str] = None,
    ) -> ListJournalKinesisStreamsForLedgerResponseTypeDef:
        return self.client.list_journal_kinesis_streams_for_ledger(
            LedgerName=ledger_name,
            MaxResults=max_results,
            NextToken=next_token,
        )

    # S3 exports
    def describe_journal_s3_export(
        self,
        name: str,
        export_id: str,
    ) -> DescribeJournalS3ExportResponseTypeDef:
        return self.client.describe_journal_s3_export(Name=name, ExportId=export_id)

    def list_journal_s3_exports(
        self,
        max_results: Optional[int] = None,
        next_token: Optional[str] = None,
    ) -> ListJournalS3ExportsResponseTypeDef:
        return self.client.list_journal_s3_exports(
            MaxResults=max_results,
            NextToken=next_token,
        )

    def list_journal_s3_exports_for_ledger(
        self,
        name: str,
        max_results: Optional[int] = None,
        next_token: Optional[str] = None,
    ) -> ListJournalS3ExportsForLedgerResponseTypeDef:
        return self.client.list_journal_s3_exports_for_ledger(
            Name=name,
            MaxResults=max_results,
            NextToken=next_token,
        )

    # Tagging
    def list_ledgers(
        self,
        max_results: Optional[int] = None,
        next_token: Optional[str] = None,
    ) -> ListLedgersResponseTypeDef:
        return self.client.list_ledgers(MaxResults=max_results, NextToken=next_token)

    def list_tags_for_resource(self, resource_arn: str) -> ListTagsForResourceResponseTypeDef:
        return self.client.list_tags_for_resource(ResourceArn=resource_arn)

    def tag_resource(
        self,
        resource_arn: str,
        tags: Mapping[str, str],
    ) -> EmptyResponseMetadataTypeDef:
        return self.client.tag_resource(ResourceArn=resource_arn, Tags=tags)

    def untag_resource(
        self,
        resource_arn: str,
        tag_keys: Sequence[str],
    ) -> EmptyResponseMetadataTypeDef:
        return self.client.untag_resource(ResourceArn=resource_arn, TagKeys=tag_keys)


if __name__ == "__main__":
    # Example usage with typed client
    qldb_client = get_typed_qldb_client()
    my_qldb = MyQldbClient(qldb_client)

    # Example: list ledgers
    ledgers = my_qldb.list_ledgers()
    print([l["Name"] for l in ledgers.get("Ledgers", [])])
------------------------------------------------------------

PROMPT 7:
"Generate Python code that creates a VPC, subnet, and security group using the Aliyun Python SDK:\n```\nfrom aliyunsdk import AcsResponse, Client, Types\n\n# Initialize the client\nclient = Client(access_key='your_access_key', secret_key='your_secret_key')\n\n# Create a VPC\nvpc_response = client.do_request('CreateVpc', {\n   'vpcName': 'my-vpc',\n   'cidrBlock': '10.0.0.0/16'\n})\n\n# Print the ID of the created VPC\nprint(vpc_response.acs_response.data['vpc']['vpcId'])\n\n# Create a subnet\nsubnet_response = client.do_request('CreateSubnet', {\n   'vpcId': vpc_response.acs_response.data['vpc']['vpcId'],\n   'subnetName': 'my-subnet',\n   'cidrBlock': '10.0.1.0/24'\n})\n\n# Print the ID of the created"
RESPONSE:
# pip install aliyun-python-sdk-core aliyun-python-sdk-vpc aliyun-python-sdk-ecs

import json

from aliyunsdkcore.client import AcsClient
from aliyunsdkvpc.request.v20160428.CreateVpcRequest import CreateVpcRequest
from aliyunsdkvpc.request.v20160428.CreateVSwitchRequest import CreateVSwitchRequest
from aliyunsdkecs.request.v20140526.CreateSecurityGroupRequest import CreateSecurityGroupRequest

def main():
    # Replace with your credentials and preferred region/zone
    access_key_id = 'your_access_key'
    access_key_secret = 'your_secret_key'
    region_id = 'cn-hangzhou'      # e.g., cn-hangzhou
    zone_id = 'cn-hangzhou-i'      # must be a zone within region_id

    client = AcsClient(access_key_id, access_key_secret, region_id)

    # Create VPC
    create_vpc_req = CreateVpcRequest()
    create_vpc_req.set_VpcName('my-vpc')
    create_vpc_req.set_CidrBlock('10.0.0.0/16')
    create_vpc_req.set_Description('Example VPC')
    vpc_resp = client.do_action_with_exception(create_vpc_req)
    vpc_data = json.loads(vpc_resp)
    vpc_id = vpc_data['VpcId']
    print('Created VPC ID:', vpc_id)

    # Create VSwitch (subnet)
    create_vswitch_req = CreateVSwitchRequest()
    create_vswitch_req.set_VpcId(vpc_id)
    create_vswitch_req.set_VSwitchName('my-subnet')
    create_vswitch_req.set_CidrBlock('10.0.1.0/24')
    create_vswitch_req.set_ZoneId(zone_id)
    vswitch_resp = client.do_action_with_exception(create_vswitch_req)
    vswitch_data = json.loads(vswitch_resp)
    vswitch_id = vswitch_data['VSwitchId']
    print('Created VSwitch (subnet) ID:', vswitch_id)

    # Create Security Group (under ECS, associated with the VPC)
    create_sg_req = CreateSecurityGroupRequest()
    create_sg_req.set_VpcId(vpc_id)
    create_sg_req.set_SecurityGroupName('my-sg')
    create_sg_req.set_Description('Example security group')
    sg_resp = client.do_action_with_exception(create_sg_req)
    sg_data = json.loads(sg_resp)
    security_group_id = sg_data['SecurityGroupId']
    print('Created Security Group ID:', security_group_id)

if __name__ == '__main__':
    main()
------------------------------------------------------------

PROMPT 8:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 Pinpoint 1.34.0 service client using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import TypeVar\nfrom mypy_boto3_builder import Botofunction, BotoService\n\n# Define the type variables for the service client\nT_client = TypeVar('T_client', bound='Boto3Client')\nT_service = TypeVar('T_service', bound='Pinpoint')\n\n# Define the Botofunction and BotoService classes with type hints\nclass MyPinpoint(Botofunction):\n   client: T_client\n   service: T_service\n\n   def __init__(self, client: T_client) -> None:\n       self.client = client\n       self.service = client.pinpoint\n\n   def get_campaigns(self, **kwargs: Any) -> List[dict]:\n       return self.service.get_campaigns(**kwargs)\n\n   def create_campaign(self, **kwargs: Any"
RESPONSE:
# Requires:
#   pip install boto3==1.34.0 mypy-boto3-pinpoint==1.34.0
# Note: mypy-boto3-builder 7.21.0 generates these stubs; you import runtime types from mypy_boto3_pinpoint.

from typing import Any, Dict, Optional, TypeVar, Generic
import boto3

from mypy_boto3_pinpoint.client import PinpointClient
from mypy_boto3_pinpoint.type_defs import (
    GetCampaignsResponseTypeDef,
    CreateCampaignResponseTypeDef,
    WriteCampaignRequestTypeDef,
)

# Type variable bound to the typed Pinpoint client
TClient = TypeVar("TClient", bound=PinpointClient)


class MyPinpoint(Generic[TClient]):
    client: TClient

    def __init__(self, client: TClient) -> None:
        self.client = client

    def get_campaigns(
        self,
        *,
        ApplicationId: str,
        PageSize: Optional[str] = None,
        Token: Optional[str] = None,
    ) -> GetCampaignsResponseTypeDef:
        kwargs: Dict[str, Any] = {"ApplicationId": ApplicationId}
        if PageSize is not None:
            kwargs["PageSize"] = PageSize
        if Token is not None:
            kwargs["Token"] = Token
        return self.client.get_campaigns(**kwargs)

    def create_campaign(
        self,
        *,
        ApplicationId: str,
        WriteCampaignRequest: WriteCampaignRequestTypeDef,
    ) -> CreateCampaignResponseTypeDef:
        return self.client.create_campaign(
            ApplicationId=ApplicationId,
            WriteCampaignRequest=WriteCampaignRequest,
        )


def get_pinpoint_client() -> PinpointClient:
    return boto3.client("pinpoint")


# Example usage (type-checked):
if __name__ == "__main__":
    pinpoint_client = get_pinpoint_client()
    svc = MyPinpoint(pinpoint_client)

    # get campaigns
    campaigns = svc.get_campaigns(ApplicationId="your-app-id")
    print(campaigns)

    # create campaign (example payload is illustrative only)
    write_campaign_request: WriteCampaignRequestTypeDef = {
        "Name": "MyCampaign",
        "Schedule": {"StartTime": "IMMEDIATE"},
        "MessageConfiguration": {
            "SMSMessage": {"Body": "Hello!", "MessageType": "TRANSACTIONAL"}
        },
        "SegmentId": "your-segment-id",
    }
    created = svc.create_campaign(
        ApplicationId="your-app-id",
        WriteCampaignRequest=write_campaign_request,
    )
    print(created)
------------------------------------------------------------

PROMPT 9:
"Generate Python code that marshals dataclasses to/from JSON using the `dataclasses` and `json` modules, with field properties having initial values, and construct a dataclass schema with JSON input:\n```python\nimport dataclasses\nimport json\n\n@dataclasses.dataclass\nclass MyDataclass:\n   field1: str = ''\n   field2: int = 0\n   field3: bool = False\n\ndata = json.loads('{\"field1\": \"hello\", \"field2\": 42, \"field3\": true}')\nobj = MyDataclass(**data)\nprint(obj.field1) # prints \"hello\"\nprint(obj.field2) # prints 42\nprint(obj.field3) # prints True\n\n# Construct a dataclass schema with JSON input\ndata = json.loads('{\"field1\": {\"type\": \"string\"}, \"field2\": {\"type\": \"integer\"}, \"field3\": {\"type\": \"boolean\"}}')\nschema = dataclasses.Schema(MyDataclass, data)\nprint(schema) # prints the dataclass schema with the specified fields and types\n```"
RESPONSE:
Here’s a fully working example using only the standard dataclasses and json modules. It shows:
- Marshaling JSON to a dataclass (with default field values)
- Marshaling a dataclass back to JSON
- Constructing a new dataclass type from a simple JSON “schema”

```python
import dataclasses
import json
from dataclasses import field, fields, make_dataclass
from typing import Any

# 1) Define a dataclass with default values
@dataclasses.dataclass
class MyDataclass:
    field1: str = ''
    field2: int = 0
    field3: bool = False

# 2) JSON -> dataclass instance
data = json.loads('{"field1": "hello", "field2": 42, "field3": true}')
obj = MyDataclass(**data)
print(obj.field1)  # prints "hello"
print(obj.field2)  # prints 42
print(obj.field3)  # prints True

# 3) dataclass instance -> JSON
json_str = json.dumps(dataclasses.asdict(obj))
print(json_str)  # prints {"field1": "hello", "field2": 42, "field3": true}

# 4) Construct a dataclass from a JSON "schema"
#    (simple mapping of JSON types to Python types + sensible defaults)
TYPE_MAP = {
    "string": str,
    "integer": int,
    "boolean": bool,
    "number": float,
}

DEFAULTS = {
    str: "",
    int: 0,
    bool: False,
    float: 0.0,
}

def make_dataclass_from_schema(name: str, schema: dict) -> type:
    """
    Build a dataclass type from a simple schema like:
    {
      "field1": {"type": "string"},
      "field2": {"type": "integer"},
      "field3": {"type": "boolean"}
    }
    """
    dc_fields = []
    for fname, props in schema.items():
        tname = (props or {}).get("type")
        pytype = TYPE_MAP.get(tname, Any)
        default = DEFAULTS.get(pytype, None)
        if default is None:
            dc_fields.append((fname, pytype))
        else:
            dc_fields.append((fname, pytype, field(default=default)))
    return make_dataclass(name, dc_fields)

# JSON describing the fields and their types
schema_json = json.loads(
    '{"field1": {"type": "string"}, "field2": {"type": "integer"}, "field3": {"type": "boolean"}}'
)

# Create a new dataclass type from the schema
DynamicDataclass = make_dataclass_from_schema("DynamicDataclass", schema_json)

# Show the constructed dataclass "schema" (its fields, types, and defaults)
print([(f.name, f.type, f.default) for f in fields(DynamicDataclass)])
# Example usage of the dynamically created dataclass
dyn = DynamicDataclass(field1="hi", field2=7, field3=False)
print(json.dumps(dataclasses.asdict(dyn)))  # {"field1": "hi", "field2": 7, "field3": false}
```

Notes:
- The standard library doesn’t provide dataclasses.Schema; the example above shows how to build a new dataclass type from a simple JSON description using dataclasses.make_dataclass.
- You can adjust TYPE_MAP and DEFAULTS to handle more types as needed.
------------------------------------------------------------

PROMPT 10:
"Generate Python code that imports the necessary modules, creates a Boto3 Cognito client, and uses it to log in users with SRP support, for example:\n```\nfrom botocore.client import BotocoreClient\nfrom cognitoidentityserviceprovider import CognitoIdentityServiceProvider\n\n# Initialize the Cognito client\ncognito_client = CognitoIdentityServiceProvider(\n   client=BotocoreClient(),\n   region=\"your-region\"\n)\n\n# Log in a user using SRP\nuser_pool = cognito_client.get_user_pool(PoolId=\"your-pool-id\")\nuser_data = {\n   \"username\": \"your-username\",\n   \"password\": \"your-password\"\n}\nresponse = user_pool.initiate_auth(\n   AuthRequest={\n       \"ClientId\": \"your-client-id\",\n       \"ClientSecret\": \"your-client-secret\",\n       \"RedirectUri\": \"your-redirect-uri\",\n       \"AuthFlow\": \"USER_PASSWORD_AUTH\",\n       \"AuthParameters\": {\n           \"USERNAME\": user_"
RESPONSE:
# pip install boto3 warrant
import base64
import hashlib
import hmac
from typing import Callable, Optional, Dict, Any

import boto3
from warrant.aws_srp import AWSSRP


def _secret_hash(username: str, client_id: str, client_secret: str) -> str:
    msg = (username + client_id).encode("utf-8")
    key = client_secret.encode("utf-8")
    dig = hmac.new(key, msg, hashlib.sha256).digest()
    return base64.b64encode(dig).decode("utf-8")


def login_cognito_srp(
    *,
    region: str,
    user_pool_id: str,
    client_id: str,
    username: str,
    password: str,
    client_secret: Optional[str] = None,
    new_password: Optional[str] = None,
    mfa_code_provider: Optional[Callable[[], str]] = None,
) -> Dict[str, Any]:
    """
    Log in a Cognito user using SRP (USER_SRP_AUTH) with boto3.
    Handles:
      - Plain SRP success
      - NEW_PASSWORD_REQUIRED (set new_password)
      - SMS_MFA / SOFTWARE_TOKEN_MFA (provide mfa_code_provider)

    Returns:
      AuthenticationResult dict on success, e.g. {
        'AccessToken': ..., 'IdToken': ..., 'RefreshToken': ..., 'ExpiresIn': ..., 'TokenType': ...
      }
    """
    cognito = boto3.client("cognito-idp", region_name=region)

    aws_srp = AWSSRP(
        username=username,
        password=password,
        pool_id=user_pool_id,
        client_id=client_id,
        client=cognito,
        client_secret=client_secret,
    )

    auth_resp = aws_srp.authenticate_user()

    # Successful auth path
    if "AuthenticationResult" in auth_resp:
        return auth_resp["AuthenticationResult"]

    # Handle common challenges
    challenge = auth_resp.get("ChallengeName")
    session = auth_resp.get("Session")

    if challenge == "NEW_PASSWORD_REQUIRED":
        if not new_password:
            raise RuntimeError("NEW_PASSWORD_REQUIRED: supply new_password to complete sign-in.")
        challenge_responses = {
            "USERNAME": username,
            "NEW_PASSWORD": new_password,
        }
        if client_secret:
            challenge_responses["SECRET_HASH"] = _secret_hash(username, client_id, client_secret)

        resp = cognito.respond_to_auth_challenge(
            ClientId=client_id,
            ChallengeName="NEW_PASSWORD_REQUIRED",
            Session=session,
            ChallengeResponses=challenge_responses,
        )
        if "AuthenticationResult" in resp:
            return resp["AuthenticationResult"]
        raise RuntimeError("Failed to complete NEW_PASSWORD_REQUIRED challenge.")

    if challenge in ("SMS_MFA", "SOFTWARE_TOKEN_MFA"):
        if not mfa_code_provider:
            raise RuntimeError(f"{challenge} required: provide mfa_code_provider to supply the code.")
        code = mfa_code_provider()
        code_key = "SMS_MFA_CODE" if challenge == "SMS_MFA" else "SOFTWARE_TOKEN_MFA_CODE"
        challenge_responses = {"USERNAME": username, code_key: code}
        if client_secret:
            challenge_responses["SECRET_HASH"] = _secret_hash(username, client_id, client_secret)

        resp = cognito.respond_to_auth_challenge(
            ClientId=client_id,
            ChallengeName=challenge,
            Session=session,
            ChallengeResponses=challenge_responses,
        )
        if "AuthenticationResult" in resp:
            return resp["AuthenticationResult"]
        raise RuntimeError(f"Failed to complete {challenge} challenge.")

    # Other, less common challenges can be handled similarly if needed
    raise RuntimeError(f"Unsupported or unexpected challenge: {challenge!r}")


if __name__ == "__main__":
    # Example usage: fill in your values
    REGION = "your-region"
    USER_POOL_ID = "your-user-pool-id"
    APP_CLIENT_ID = "your-app-client-id"
    APP_CLIENT_SECRET = None  # or "your-app-client-secret" if your app client has a secret
    USERNAME = "your-username"
    PASSWORD = "your-password"

    # Optionally, set if the user is in FORCE_CHANGE_PASSWORD state
    NEW_PASSWORD = None  # e.g., "YourNewSecurePassword123!"

    # Optionally, define how to collect an MFA code if required
    def get_mfa_code() -> str:
        return input("Enter MFA code: ").strip()

    tokens = login_cognito_srp(
        region=REGION,
        user_pool_id=USER_POOL_ID,
        client_id=APP_CLIENT_ID,
        username=USERNAME,
        password=PASSWORD,
        client_secret=APP_CLIENT_SECRET,
        new_password=NEW_PASSWORD,
        mfa_code_provider=get_mfa_code,
    )

    print("Authenticated. Tokens:")
    print("AccessToken:", tokens.get("AccessToken"))
    print("IdToken:", tokens.get("IdToken"))
    print("RefreshToken:", tokens.get("RefreshToken"))
------------------------------------------------------------

PROMPT 11:
"Generate Python code that imports the necessary modules and annotates the boto3.Synthetics 1.34.0 service with type hints using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom botocore.client import Boto3Client\nfrom boto3.synthetics import Synthetics as SyntheticsClient\n\nsynthetics_client = Boto3Client(\n   client_config=dict(\n       service='synthetics',\n       region='us-west-2',\n       credentials=None\n   )\n)\n\n@synthetics_client.operation\nasync def create_canary(name: str, description: str, tags: List[dict]) -> dict:\n   return await synthetics_client.create_canary(name, description, tags)\n\n@synthetics_client.operation\nasync def list_canaries() -> List[dict]:\n   return await synthetics_client.list_canaries()\n\n@synthetics_client.operation\nasync def describe"
RESPONSE:
Note: boto3 is synchronous. To add precise type hints for the Synthetics client with boto3 1.34.x, use the generated stubs from mypy-boto3-synthetics (built by mypy-boto3-builder 7.21.0).

Code:
```python
# Requires:
#   boto3==1.34.*
#   mypy-boto3-synthetics==1.34.*
# The types are generated by mypy-boto3-builder 7.21.0.

from __future__ import annotations

from typing import Any, Dict, Mapping, Optional

import boto3
from botocore.config import Config

from mypy_boto3_synthetics.client import SyntheticsClient
from mypy_boto3_synthetics.type_defs import (
    ArtifactConfigInputTypeDef,
    CanaryCodeInputTypeDef,
    CanaryRunConfigInputTypeDef,
    CanaryScheduleInputTypeDef,
    CreateCanaryResponseTypeDef,
    DescribeCanariesResponseTypeDef,
    GetCanaryResponseTypeDef,
    ListCanariesResponseTypeDef,
    ListCanaryRunsResponseTypeDef,
)

# Build a typed Synthetics client for us-west-2
session = boto3.Session()
config = Config(region_name="us-west-2")
synthetics: SyntheticsClient = session.client("synthetics", config=config)


def create_canary(
    name: str,
    code: CanaryCodeInputTypeDef,
    artifact_s3_location: str,
    execution_role_arn: str,
    schedule: CanaryScheduleInputTypeDef,
    runtime_version: str,
    *,
    run_config: Optional[CanaryRunConfigInputTypeDef] = None,
    vpc_config: Optional["VpcConfigInputTypeDef"] = None,  # stringized to avoid forward-ref issues pre-3.11
    tags: Optional[Mapping[str, str]] = None,
    artifact_config: Optional[ArtifactConfigInputTypeDef] = None,
    start_canary_after_creation: bool = True,
) -> CreateCanaryResponseTypeDef:
    """
    Create a canary with full type-checked parameters.
    Only includes optional fields when provided.
    """
    params: Dict[str, Any] = dict(
        Name=name,
        Code=code,
        ArtifactS3Location=artifact_s3_location,
        ExecutionRoleArn=execution_role_arn,
        Schedule=schedule,
        RuntimeVersion=runtime_version,
        StartCanaryAfterCreation=start_canary_after_creation,
    )
    if run_config is not None:
        params["RunConfig"] = run_config
    if vpc_config is not None:
        params["VpcConfig"] = vpc_config
    if tags is not None:
        params["Tags"] = dict(tags)
    if artifact_config is not None:
        params["ArtifactConfig"] = artifact_config

    return synthetics.create_canary(**params)


def list_canaries(
    *,
    next_token: Optional[str] = None,
    max_results: Optional[int] = None,
) -> ListCanariesResponseTypeDef:
    params: Dict[str, Any] = {}
    if next_token is not None:
        params["NextToken"] = next_token
    if max_results is not None:
        params["MaxResults"] = max_results
    return synthetics.list_canaries(**params)


def describe_canaries(
    *,
    next_token: Optional[str] = None,
    max_results: Optional[int] = None,
) -> DescribeCanariesResponseTypeDef:
    params: Dict[str, Any] = {}
    if next_token is not None:
        params["NextToken"] = next_token
    if max_results is not None:
        params["MaxResults"] = max_results
    return synthetics.describe_canaries(**params)


def get_canary(name: str) -> GetCanaryResponseTypeDef:
    return synthetics.get_canary(Name=name)


def list_canary_runs(
    name: str,
    *,
    next_token: Optional[str] = None,
    max_results: Optional[int] = None,
) -> ListCanaryRunsResponseTypeDef:
    params: Dict[str, Any] = dict(Name=name)
    if next_token is not None:
        params["NextToken"] = next_token
    if max_results is not None:
        params["MaxResults"] = max_results
    return synthetics.list_canary_runs(**params)
```
------------------------------------------------------------

PROMPT 12:
"Generate Python code that implements a QUIC and HTTP/3 server using the `quic` library, listening on a port and serving HTTP/3 requests.\n\nHere is an example of Python code that could implement a QUIC and HTTP/3 server using the `quic` library:\n```\nimport quic\n\n# Create a QUIC listener\nlistener = quic.QuicListener(('localhost', 8080))\n\n# Define a callback function for incoming connections\ndef handle_connection(connection):\n   # Accept the connection and create a QUIC stream\n   stream = connection.accept()\n   \n   # Read data from the stream\n   data = stream.read()\n   \n   # Print the received data\n   print(f\"Received data: {data}\")\n   \n   # Write data back to the client\n   stream.write(\"Hello, client!\")\n   \n   # Close the stream and connection\n   stream.close()\n   connection.close()\n\n# Start listening for incoming connections\nlistener.start(handle_connection)\n```\nThis code creates a QUIC listener on port 8080 and"
RESPONSE:
It looks like the example uses a hypothetical quic library. In Python, a widely used, production-ready way to implement QUIC and HTTP/3 is aioquic. Below is a minimal HTTP/3 server using aioquic that listens on a port and serves simple responses.

Requirements:
- pip install aioquic
- You need a TLS certificate and private key (QUIC requires TLS 1.3). For testing, you can use a self-signed certificate.

Python code (http3_server.py):
```
import argparse
import asyncio

from aioquic.asyncio import serve
from aioquic.h3.connection import H3_ALPN, H3Connection
from aioquic.h3.events import HeadersReceived
from aioquic.quic.configuration import QuicConfiguration
from aioquic.asyncio.protocol import QuicConnectionProtocol
from aioquic.quic.events import HandshakeCompleted, QuicEvent


class Http3ServerProtocol(QuicConnectionProtocol):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._http = None

    def quic_event_received(self, event: QuicEvent) -> None:
        if isinstance(event, HandshakeCompleted):
            # Initialize HTTP/3 over QUIC once the handshake completes
            self._http = H3Connection(self._quic)
            return

        if self._http is None:
            return

        # Translate QUIC events to HTTP/3 events
        for http_event in self._http.handle_event(event):
            if isinstance(http_event, HeadersReceived):
                stream_id = http_event.stream_id

                # Parse request headers
                method = None
                path = "/"
                for name, value in http_event.headers:
                    if name == b":method":
                        method = value.decode()
                    elif name == b":path":
                        path = value.decode()

                # Simple routing/response
                if method == "GET":
                    body = f"Hello over HTTP/3! You requested {path}\n".encode()
                    headers_out = [
                        (b":status", b"200"),
                        (b"server", b"aioquic-h3"),
                        (b"content-type", b"text/plain; charset=utf-8"),
                        (b"content-length", str(len(body)).encode()),
                    ]
                    self._http.send_headers(stream_id, headers_out)
                    self._http.send_data(stream_id, body, end_stream=True)
                elif method == "HEAD":
                    headers_out = [
                        (b":status", b"200"),
                        (b"server", b"aioquic-h3"),
                        (b"content-length", b"0"),
                    ]
                    self._http.send_headers(stream_id, headers_out, end_stream=True)
                else:
                    body = b"405 Method Not Allowed\n"
                    headers_out = [
                        (b":status", b"405"),
                        (b"server", b"aioquic-h3"),
                        (b"content-type", b"text/plain; charset=utf-8"),
                        (b"content-length", str(len(body)).encode()),
                    ]
                    self._http.send_headers(stream_id, headers_out)
                    self._http.send_data(stream_id, body, end_stream=True)

                # Ensure packets are sent
                self.transmit()


async def main(host: str, port: int, certificate: str, private_key: str) -> None:
    config = QuicConfiguration(
        is_client=False,
        alpn_protocols=H3_ALPN,  # advertise HTTP/3 ALPN tokens
    )
    config.load_cert_chain(certificate, private_key)

    server = await serve(
        host,
        port,
        configuration=config,
        create_protocol=Http3ServerProtocol,
    )
    print(f"HTTP/3 server listening on {host}:{port}")

    try:
        # Run until interrupted
        await asyncio.Future()
    except KeyboardInterrupt:
        pass
    finally:
        server.close()
        await server.wait_closed()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Minimal HTTP/3 server over QUIC")
    parser.add_argument("--host", default="0.0.0.0", help="Listen address")
    parser.add_argument("--port", type=int, default=4433, help="Listen port")
    parser.add_argument("--certificate", required=True, help="Path to certificate (PEM)")
    parser.add_argument("--private-key", required=True, help="Path to private key (PEM)")
    args = parser.parse_args()

    asyncio.run(main(args.host, args.port, args.certificate, args.private_key))
```

Notes:
- Run: python http3_server.py --certificate cert.pem --private-key key.pem --port 4433
- Most clients require a valid certificate. For testing with a browser, you may need to trust your self-signed cert or use appropriate flags.
- If you truly need to use a specific library named quic with the API in your snippet, please share a link or version; I can adapt the code to that exact API.
------------------------------------------------------------

PROMPT 13:
"Generate Python code that imports the SQLAlchemy module and creates a connection to a DuckDB database using the SQLAlchemy driver.\n\nHere is an example of how this could be done:\n```\nfrom sqlalchemy import create_engine\n\n# Create a connection to a DuckDB database\nengine = create_engine('duckdb://user:password@localhost/database')\n```\nThis will create a connection to a DuckDB database using the `duckdb` URL scheme, which is specific to the DuckDB database system. The `create_engine` function returns a `Engine` object, which can then be used to execute queries against the database."
RESPONSE:
Here’s the correct way to create a SQLAlchemy Engine for DuckDB. Note that DuckDB is embedded, so you don’t use username/host/password in the URL.

# pip install duckdb duckdb-engine sqlalchemy

from sqlalchemy import create_engine, text

# 1) File-backed DuckDB database
engine = create_engine("duckdb:///my_database.duckdb")

# 2) Or, an in-memory DuckDB database
# engine = create_engine("duckdb:///:memory:")

# Optional: test the connection with a simple query
with engine.begin() as conn:
    conn.execute(text("CREATE TABLE IF NOT EXISTS t(i INTEGER)"))
    conn.execute(text("INSERT INTO t VALUES (1), (2)"))
    rows = conn.execute(text("SELECT * FROM t")).fetchall()
    print(rows)  # [(1,), (2,)]
------------------------------------------------------------

PROMPT 14:
"Generate Python code that imports the necessary libraries and annotates the types of the boto3 IoT Analytics 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import TypeVar, Callable\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import Botofailure, MyBoto3\nimport boto3\n\n# Define type aliases\ntype Boto3Client = Callable[[str], BaseClient]\ntype IoTAnalytics = MyBoto3[Boto3Client]\n\n# Annotate the IoTAnalytics class\nclass IoTAnalytics(MyBoto3):\n   def __init__(self, config: dict) -> None:\n       super().__init__(config)\n       self.client: Boto3Client = boto3.client(\"iotanalytics\", config=config)\n\n   # Add type hints for methods and properties\n   def create_dataset(self, dataset_name: str, dataset_definition: dict) -> None:\n       self.client.create_dataset("
RESPONSE:
# Requires:
# - boto3==1.34.x
# - mypy-boto3-iotanalytics (generated by mypy-boto3-builder 7.21.0 for boto3 1.34.x)
#   pip install "boto3==1.34.*" "mypy-boto3-iotanalytics==1.34.*"

from __future__ import annotations

from typing import Any, Callable, Optional, Sequence

import boto3
from botocore.client import BaseClient

# Typed client and shapes generated by mypy-boto3-builder 7.21.0 for boto3 1.34.x
from mypy_boto3_iotanalytics.client import IoTAnalyticsClient
from mypy_boto3_iotanalytics.type_defs import (
    CreateDatasetResponseTypeDef,
    DatasetActionTypeDef,
    DatasetContentDeliveryRuleTypeDef,
    DatasetTriggerTypeDef,
    LateDataRuleTypeDef,
    RetentionPeriodTypeDef,
    TagTypeDef,
    VersioningConfigurationTypeDef,
)

# Type aliases
Boto3ClientFactory = Callable[..., BaseClient]
Boto3IoTAnalyticsFactory = Callable[..., IoTAnalyticsClient]


def get_iotanalytics_client(**client_kwargs: Any) -> IoTAnalyticsClient:
    """
    Typed factory for the IoT Analytics client.
    Example:
        client = get_iotanalytics_client(region_name="us-east-1")
    """
    return boto3.client("iotanalytics", **client_kwargs)


class IoTAnalytics:
    """
    Thin, typed wrapper around boto3 IoT Analytics client with method annotations.
    """

    def __init__(self, **client_kwargs: Any) -> None:
        self.client: IoTAnalyticsClient = get_iotanalytics_client(**client_kwargs)

    def create_dataset(
        self,
        *,
        # IoT Analytics uses lowerCamelCase parameter names
        datasetName: str,
        actions: Sequence[DatasetActionTypeDef],
        triggers: Optional[Sequence[DatasetTriggerTypeDef]] = None,
        contentDeliveryRules: Optional[Sequence[DatasetContentDeliveryRuleTypeDef]] = None,
        retentionPeriod: Optional[RetentionPeriodTypeDef] = None,
        versioningConfiguration: Optional[VersioningConfigurationTypeDef] = None,
        tags: Optional[Sequence[TagTypeDef]] = None,
        lateDataRules: Optional[Sequence[LateDataRuleTypeDef]] = None,
    ) -> CreateDatasetResponseTypeDef:
        """
        Fully typed wrapper for IoTAnalyticsClient.create_dataset.
        """
        return self.client.create_dataset(
            datasetName=datasetName,
            actions=actions,
            triggers=triggers,
            contentDeliveryRules=contentDeliveryRules,
            retentionPeriod=retentionPeriod,
            versioningConfiguration=versioningConfiguration,
            tags=tags,
            lateDataRules=lateDataRules,
        )


# Example usage (will be type-checked by mypy/pyright):
if __name__ == "__main__":
    svc = IoTAnalytics(region_name="us-east-1")

    # Minimal example action definition (adjust to your pipeline/dataset setup)
    example_actions: list[DatasetActionTypeDef] = [
        {
            "actionName": "sql_action",
            "queryAction": {
                "sqlQuery": "SELECT * FROM my_datastore",
                # Optional: "filters": [...]
            },
        }
    ]

    resp = svc.create_dataset(
        datasetName="my_dataset",
        actions=example_actions,
        # Optional arguments below:
        # triggers=[...],
        # contentDeliveryRules=[...],
        # retentionPeriod={"numberOfDays": 30, "unlimited": False},
        # versioningConfiguration={"unlimited": False, "maxVersions": 3},
        # tags=[{"key": "env", "value": "dev"}],
        # lateDataRules=[...],
    )
    print(resp.get("datasetName"))
------------------------------------------------------------

PROMPT 15:
"Generate Python code that imports the necessary modules and annotates the types of the IoTWireless 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import Tuple\nfrom botocore.client import BaseClient\nfrom boto3. exceptions import Boto3Error\nfrom mypy_boto3_builder.generators.iotwireless import IoTWirelessGenerator\n\n# Initialize the client\nclient = IoTWirelessGenerator(BaseClient).generate()\n\n# Define the types of the IoTWireless service\nclass IoTWirelessService:\n   def __init__(self, client: BaseClient) -> None:\n       self.client = client\n\n   def list_things(self) -> List[Tuple[str, str]]:\n       return self.client.list_things()\n\n   def create_thing(self, thing_name: str, thing_type: str) -> str:\n       return self.client.create_thing(thing_name, thing_type)\n\n   def update_thing(self, thing"
RESPONSE:
```python
from typing import Any, Dict, Optional

import boto3
from botocore.exceptions import BotoCoreError, ClientError

# Install mypy-boto3-iotwireless==1.34.0 (generated by mypy-boto3-builder 7.21.0)
# to make these imports and type annotations available.
from mypy_boto3_iotwireless import IoTWirelessClient
from mypy_boto3_iotwireless.type_defs import (
    CreateWirelessDeviceRequestRequestTypeDef,
    CreateWirelessDeviceResponseTypeDef,
    ListWirelessDevicesRequestRequestTypeDef,
    ListWirelessDevicesResponseTypeDef,
    UpdateWirelessDeviceRequestRequestTypeDef,
)


def get_iotwireless_client() -> IoTWirelessClient:
    return boto3.client("iotwireless")


class IoTWirelessService:
    def __init__(self, client: IoTWirelessClient) -> None:
        self.client = client

    def list_wireless_devices(
        self,
        params: Optional[ListWirelessDevicesRequestRequestTypeDef] = None,
    ) -> ListWirelessDevicesResponseTypeDef:
        return self.client.list_wireless_devices(**(params or {}))

    def create_wireless_device(
        self,
        params: CreateWirelessDeviceRequestRequestTypeDef,
    ) -> CreateWirelessDeviceResponseTypeDef:
        return self.client.create_wireless_device(**params)

    def update_wireless_device(
        self,
        params: UpdateWirelessDeviceRequestRequestTypeDef,
    ) -> Dict[str, Any]:
        # Update operations frequently return an empty dict with ResponseMetadata.
        return self.client.update_wireless_device(**params)


# Example usage:
if __name__ == "__main__":
    client = get_iotwireless_client()
    service = IoTWirelessService(client)

    try:
        devices = service.list_wireless_devices()
        print(devices)
    except (BotoCoreError, ClientError) as e:
        print(f"AWS error: {e}")
```
------------------------------------------------------------

PROMPT 16:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.CloudSearch 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import TypeAnnotator\n\nann = TypeAnnotator( CloudSearchClient )\n```\nThis code imports the `typing` module, which provides type hints for Python functions and variables, and the `botocore.client` module, which is used by boto3 to interact with AWS services. It also imports the `mypy_boto3_builder` module, which contains the `TypeAnnotator` class that will be used to generate type annotations for the boto3 client. Finally, it creates an instance of the `TypeAnnotator` class and passes it the `CloudSearchClient` class from boto3, which will be used to generate type annotations for the methods and properties of the `CloudSearchClient` class."
RESPONSE:
```python
from typing import List
import boto3
from botocore.client import BaseClient
from mypy_boto3_cloudsearch import CloudSearchClient

# Typed CloudSearch client (requires mypy-boto3-cloudsearch generated by mypy-boto3-builder)
cloudsearch: CloudSearchClient = boto3.client("cloudsearch")

# Example typed usage
def list_domain_names(names: List[str]) -> List[str]:
    resp = cloudsearch.describe_domains(DomainNames=names)
    return [d["DomainName"] for d in resp["DomainStatusList"]]
```
------------------------------------------------------------

PROMPT 17:
"Generate Python code that uses the BigQuery client library to send data to BigQuery from a CSV file, like so:\n```\nimport csv\nfrom google.cloud import bigquery\n\n# Create a client object\nclient = bigquery.Client()\n\n# Specify the dataset and table where the data will be written\ndataset_name = \"my-dataset\"\ntable_name = \"my-table\"\n\n# Load the CSV file\nwith open(\"data.csv\", \"r\") as f:\n   reader = csv.reader(f)\n   rows = list(reader)\n\n# Insert the data into BigQuery\nerrors = client.insert_rows(table_name, rows)\n\n# Print any errors that occurred during insertion\nprint(errors)\n```"
RESPONSE:
```python
import csv
from google.cloud import bigquery

# Create a client object (uses your default GCP project via auth)
client = bigquery.Client()

dataset_name = "my-dataset"
table_name = "my-table"
table_id = f"{client.project}.{dataset_name}.{table_name}"

# -------------------------------
# Option A (recommended): Load CSV with a load job
# - Fast and cost-effective for files
# - Table can be auto-created if you specify a schema; here we use autodetect
# -------------------------------
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.CSV,
    skip_leading_rows=1,      # set to 0 if your CSV has no header
    autodetect=True,          # or provide an explicit schema
    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,  # or WRITE_TRUNCATE/WRITE_EMPTY
)

with open("data.csv", "rb") as f:
    load_job = client.load_table_from_file(f, table_id, job_config=job_config)

load_job.result()  # Waits for the job to complete

if load_job.errors:
    print("Load job errors:", load_job.errors)
else:
    print(f"Loaded CSV into {table_id}.")

# -------------------------------
# Option B: Streaming inserts (insert_rows_json) from CSV
# - Useful for small, real-time inserts
# - Table must already exist with a matching schema
# -------------------------------
# Uncomment to use:
# def chunked(iterable, size):
#     chunk = []
#     for row in iterable:
#         chunk.append(row)
#         if len(chunk) == size:
#             yield chunk
#             chunk = []
#     if chunk:
#         yield chunk
#
# inserted = 0
# with open("data.csv", "r", newline="") as f:
#     reader = csv.DictReader(f)  # Requires the first row to be headers matching BQ column names
#     for batch in chunked(reader, 10000):  # BigQuery API limit: up to 10k rows per request
#         errors = client.insert_rows_json(
#             table_id,
#             batch,
#             skip_invalid_rows=True,
#             ignore_unknown_values=True,
#         )
#         if errors:
#             print("Streaming insert errors:", errors)
#         inserted += len(batch)
# print(f"Streamed {inserted} rows into {table_id}.")
```
------------------------------------------------------------

PROMPT 18:
"Generate Python code that imports the required modules and annotates the types of the boto3.Redshift 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import TypeVar, Type Alias\nfrom mypy_boto3_builder import botofill\n\n@botofill\nclass Redshift:\n   def __init__(self, *, client: \"Boto3Client\" = None, config: dict = None) -> None:\n       self.client: Boto3Client = client\n       self.config: dict = config\n\n   @property\n   def client(self) -> Boto3Client:\n       return self._client\n\n   @client.setter\n   def client(self, value: Boto3Client) -> None:\n       self._client = value\n\n   @property\n   def config(self) -> dict:\n       return self._config\n\n   @config.setter\n   def config(self, value: dict) -> None:\n       self._config = value\n\n   def create_cluster(\n         "
RESPONSE:
from typing import Optional, Sequence, TypeVar, TypeAlias
from mypy_boto3_builder import botofill

from mypy_boto3_redshift.client import RedshiftClient
from mypy_boto3_redshift.literals import AquaConfigurationStatusType, IpAddressTypeType
from mypy_boto3_redshift.type_defs import (
    CreateClusterResultTypeDef,
    TagTypeDef,
)

Boto3Client: TypeAlias = RedshiftClient
T = TypeVar("T")


@botofill
class Redshift:
    def __init__(self, *, client: Optional[Boto3Client] = None, config: Optional[dict] = None) -> None:
        self._client: Optional[Boto3Client] = None
        self._config: dict = {}
        if client is not None:
            self.client = client
        if config is not None:
            self.config = config

    @property
    def client(self) -> Boto3Client:
        if self._client is None:
            raise RuntimeError("Redshift client is not set")
        return self._client

    @client.setter
    def client(self, value: Boto3Client) -> None:
        self._client = value

    @property
    def config(self) -> dict:
        return self._config

    @config.setter
    def config(self, value: dict) -> None:
        self._config = value

    def create_cluster(
        self,
        *,
        DBName: Optional[str] = None,
        ClusterIdentifier: str,
        ClusterType: Optional[str] = None,
        NodeType: str,
        MasterUsername: Optional[str] = None,
        MasterUserPassword: Optional[str] = None,
        ClusterSecurityGroups: Optional[Sequence[str]] = None,
        VpcSecurityGroupIds: Optional[Sequence[str]] = None,
        ClusterSubnetGroupName: Optional[str] = None,
        AvailabilityZone: Optional[str] = None,
        PreferredMaintenanceWindow: Optional[str] = None,
        ClusterParameterGroupName: Optional[str] = None,
        AutomatedSnapshotRetentionPeriod: Optional[int] = None,
        ManualSnapshotRetentionPeriod: Optional[int] = None,
        Port: Optional[int] = None,
        ClusterVersion: Optional[str] = None,
        AllowVersionUpgrade: Optional[bool] = None,
        NumberOfNodes: Optional[int] = None,
        PubliclyAccessible: Optional[bool] = None,
        Encrypted: Optional[bool] = None,
        HsmClientCertificateIdentifier: Optional[str] = None,
        HsmConfigurationIdentifier: Optional[str] = None,
        ElasticIp: Optional[str] = None,
        Tags: Optional[Sequence[TagTypeDef]] = None,
        KmsKeyId: Optional[str] = None,
        EnhancedVpcRouting: Optional[bool] = None,
        AdditionalInfo: Optional[str] = None,
        IamRoles: Optional[Sequence[str]] = None,
        MaintenanceTrackName: Optional[str] = None,
        SnapshotScheduleIdentifier: Optional[str] = None,
        AvailabilityZoneRelocation: Optional[bool] = None,
        AquaConfigurationStatus: Optional[AquaConfigurationStatusType] = None,
        DefaultIamRoleArn: Optional[str] = None,
        LoadSampleData: Optional[str] = None,
        ManageMasterPassword: Optional[bool] = None,
        MasterPasswordSecretKmsKeyId: Optional[str] = None,
        IpAddressType: Optional[IpAddressTypeType] = None,
    ) -> CreateClusterResultTypeDef:
        params = {
            "DBName": DBName,
            "ClusterIdentifier": ClusterIdentifier,
            "ClusterType": ClusterType,
            "NodeType": NodeType,
            "MasterUsername": MasterUsername,
            "MasterUserPassword": MasterUserPassword,
            "ClusterSecurityGroups": ClusterSecurityGroups,
            "VpcSecurityGroupIds": VpcSecurityGroupIds,
            "ClusterSubnetGroupName": ClusterSubnetGroupName,
            "AvailabilityZone": AvailabilityZone,
            "PreferredMaintenanceWindow": PreferredMaintenanceWindow,
            "ClusterParameterGroupName": ClusterParameterGroupName,
            "AutomatedSnapshotRetentionPeriod": AutomatedSnapshotRetentionPeriod,
            "ManualSnapshotRetentionPeriod": ManualSnapshotRetentionPeriod,
            "Port": Port,
            "ClusterVersion": ClusterVersion,
            "AllowVersionUpgrade": AllowVersionUpgrade,
            "NumberOfNodes": NumberOfNodes,
            "PubliclyAccessible": PubliclyAccessible,
            "Encrypted": Encrypted,
            "HsmClientCertificateIdentifier": HsmClientCertificateIdentifier,
            "HsmConfigurationIdentifier": HsmConfigurationIdentifier,
            "ElasticIp": ElasticIp,
            "Tags": Tags,
            "KmsKeyId": KmsKeyId,
            "EnhancedVpcRouting": EnhancedVpcRouting,
            "AdditionalInfo": AdditionalInfo,
            "IamRoles": IamRoles,
            "MaintenanceTrackName": MaintenanceTrackName,
            "SnapshotScheduleIdentifier": SnapshotScheduleIdentifier,
            "AvailabilityZoneRelocation": AvailabilityZoneRelocation,
            "AquaConfigurationStatus": AquaConfigurationStatus,
            "DefaultIamRoleArn": DefaultIamRoleArn,
            "LoadSampleData": LoadSampleData,
            "ManageMasterPassword": ManageMasterPassword,
            "MasterPasswordSecretKmsKeyId": MasterPasswordSecretKmsKeyId,
            "IpAddressType": IpAddressType,
        }
        # drop None values to match boto3 semantics
        filtered = {k: v for k, v in params.items() if v is not None}
        return self.client.create_cluster(**filtered)  # type: ignore[arg-type]
------------------------------------------------------------

PROMPT 19:
"Generate Python code that imports the required modules and annotates the inputs and outputs of the `boto3.CostExplorer` service using type hints, like so:\n```python\nfrom typing import Optional\nfrom boto3 import ServiceClient\nfrom boto3.cost_explorer import CostExplorer\n\n# Annotate the service client\nclient: ServiceClient = None\n\n# Annotate the CostExplorer object\nce: CostExplorer = None\n\n# Set up the client and CostExplorer objects\ndef setup(session: boto3.Session):\n   global client, ce\n   client = session.client('costexplorer')\n   ce = CostExplorer(client)\n\n# Define a function to get cost data\n@annotator.with_docstring\ndef get_cost_data(\n       *,\n       start_time: datetime,\n       end_time: datetime,\n       filters: dict,\n       granularity: str,\n       **kwargs: Any\n) -> List[dict]:\n   \"\"\"\n   Get cost data from AWS Cost Explorer.\n\n   Args:\n       start_time: Start time of the"
RESPONSE:
from __future__ import annotations

from typing import Any, List, Optional, Sequence, Literal
from datetime import date, datetime

import boto3
from boto3.session import Session

# Type hints from boto3-stubs (install: pip install mypy-boto3-cost-explorer)
from mypy_boto3_cost_explorer.client import CostExplorerClient as ServiceClient
from mypy_boto3_cost_explorer.type_defs import (
    DateIntervalTypeDef,
    ExpressionTypeDef,
    GetCostAndUsageResponseTypeDef,
    GroupDefinitionTypeDef,
    ResultByTimeTypeDef,
)

Granularity = Literal["DAILY", "MONTHLY", "HOURLY"]


# Optional no-op decorator to match the example usage
class annotator:
    @staticmethod
    def with_docstring(func):
        return func


# Annotate the service client
client: Optional[ServiceClient] = None


# A lightweight, typed wrapper around the Cost Explorer client
class CostExplorer:
    def __init__(self, client: ServiceClient) -> None:
        self._client = client

    def get_cost_and_usage(
        self,
        *,
        TimePeriod: DateIntervalTypeDef,
        Granularity: Granularity,
        Filter: Optional[ExpressionTypeDef] = None,
        Metrics: Sequence[str],
        GroupBy: Optional[Sequence[GroupDefinitionTypeDef]] = None,
        NextPageToken: Optional[str] = None,
    ) -> GetCostAndUsageResponseTypeDef:
        kwargs: dict[str, Any] = {
            "TimePeriod": TimePeriod,
            "Granularity": Granularity,
            "Metrics": list(Metrics),
        }
        if Filter is not None:
            kwargs["Filter"] = Filter
        if GroupBy is not None:
            kwargs["GroupBy"] = list(GroupBy)
        if NextPageToken is not None:
            kwargs["NextPageToken"] = NextPageToken

        return self._client.get_cost_and_usage(**kwargs)


# Annotate the CostExplorer object
ce: Optional[CostExplorer] = None


# Set up the client and CostExplorer objects
def setup(session: Session) -> None:
    global client, ce
    client = session.client("ce")  # service name is "ce"
    ce = CostExplorer(client)


def _to_date_str(dt: date | datetime) -> str:
    if isinstance(dt, datetime):
        dt = dt.date()
    return dt.strftime("%Y-%m-%d")


# Define a function to get cost data
@annotator.with_docstring
def get_cost_data(
    *,
    start_time: date | datetime,
    end_time: date | datetime,
    filters: Optional[ExpressionTypeDef],
    granularity: Granularity,
    metrics: Sequence[str],
    group_by: Optional[Sequence[GroupDefinitionTypeDef]] = None,
) -> List[ResultByTimeTypeDef]:
    """
    Get cost data from AWS Cost Explorer.

    Args:
        start_time: Start time of the query (inclusive).
        end_time: End time of the query (exclusive).
        filters: Cost Explorer filter expression.
        granularity: The granularity of the data (DAILY, MONTHLY, or HOURLY).
        metrics: List of metrics to retrieve (e.g., ["UnblendedCost"]).
        group_by: Optional grouping definitions.

    Returns:
        A list of ResultByTimeTypeDef entries representing the time series results.
    """
    if ce is None:
        raise RuntimeError("CostExplorer client is not initialized. Call setup(session) first.")

    time_period: DateIntervalTypeDef = {
        "Start": _to_date_str(start_time),
        "End": _to_date_str(end_time),
    }

    results: List[ResultByTimeTypeDef] = []
    next_token: Optional[str] = None

    while True:
        response = ce.get_cost_and_usage(
            TimePeriod=time_period,
            Granularity=granularity,
            Filter=filters,
            Metrics=metrics,
            GroupBy=group_by,
            NextPageToken=next_token,
        )

        results.extend(response.get("ResultsByTime", []))
        next_token = response.get("NextPageToken")
        if not next_token:
            break

    return results


if __name__ == "__main__":
    # Example usage (requires AWS credentials configured)
    session = boto3.Session()
    setup(session)

    data = get_cost_data(
        start_time=date(2025, 1, 1),
        end_time=date(2025, 1, 31),
        filters=None,
        granularity="DAILY",
        metrics=["UnblendedCost"],
        group_by=None,
    )
    print(f"Retrieved {len(data)} time periods")
------------------------------------------------------------

PROMPT 20:
"Generate Python code that uses the `json` module to load data from a JSON file and converts it into typed data structures using the `typing` module, like so:\n```\nimport json\nfrom typing import TypedDict\n\ndata = json.load(open('data.json'))\ntypede_data = TypedDict(data)\n```\nThis will create a `TypedDict` object called `typede_data` that represents the data in the JSON file `data.json`. The keys of the dictionary are enforced to match the structure of the JSON data, and the values are converted to the appropriate types based on their annotations in the `TypedDict` definition."
RESPONSE:
Your example won’t work as written: TypedDict is a static typing tool and isn’t constructed from runtime data. You define a TypedDict schema, load JSON, and (optionally) cast the loaded data so type checkers (mypy/pyright) know its shape. Python itself won’t enforce types at runtime.

Here’s a correct pattern:

```python
import json
from typing import TypedDict, List, cast

# Define the expected JSON schema
class Address(TypedDict):
    street: str
    city: str

class User(TypedDict):
    id: int
    name: str
    is_active: bool
    tags: List[str]
    address: Address

# Load the JSON file
with open('data.json', 'r', encoding='utf-8') as f:
    raw = json.load(f)

# If data.json contains a single user object:
user: User = cast(User, raw)

# If data.json contains a list of user objects:
# users: List[User] = cast(List[User], raw)

print(user['name'])
```

Notes:
- TypedDict provides static type checking; it does not convert or validate at runtime.
- If you need runtime validation/conversion, use a validation library (e.g., pydantic) or write manual checks.
------------------------------------------------------------

PROMPT 21:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 CodeStar 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import MyPyBoto3Builder\n\nbuilder = MyPyBoto3Builder(service=\"codestar\", version=\"1.34.0\")\nclient = builder.build_client(\"CodeStar\")\n\n# Annotate the client object with its type\nclient: CodeStarClient = client\n\n# Annotate the list of projects returned by the client's `list_projects` method\nprojects: List[Project] = client.list_projects()\n\n# Annotate the project object returned by the client's `describe_project` method\nproject: Project = client.describe_project(project_id=\"my-project-id\")\n```"
RESPONSE:
from typing import List
import boto3

from mypy_boto3_codestar.client import CodeStarClient
from mypy_boto3_codestar.type_defs import (
    ListProjectsResultTypeDef,
    ProjectSummaryTypeDef,
    DescribeProjectResultTypeDef,
)

# Typed boto3 client for CodeStar (boto3 1.34.x, stubs generated by mypy-boto3-builder 7.21.0)
client: CodeStarClient = boto3.client("codestar")

# Annotate the list of projects returned by the client's list_projects method
list_result: ListProjectsResultTypeDef = client.list_projects()
projects: List[ProjectSummaryTypeDef] = list_result["projects"]

# Annotate the project details returned by the client's describe_project method
project_details: DescribeProjectResultTypeDef = client.describe_project(id="my-project-id")
------------------------------------------------------------

PROMPT 22:
"Generate Python code that imports the required modules and annotates the types for the boto3.CloudHSM 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import TypeVar, Generic\nfrom botocore.client import BaseClient\nfrom boto3.cloud_hsm import CloudHSM\n\nclass MyCloudHSM(BaseClient):\n   def __init__(self) -> None:\n       super().__init__()\n       self.client = CloudHSM(config=None)\n\n   def create_key(self, name: str, description: str) -> dict:\n       return self.client.create_key(name, description)\n\n   def list_keys(self) -> List[dict]:\n       return self.client.list_keys()\n\n   def get_key(self, key_id: str) -> dict:\n       return self.client.get_key(key_id)\n\n   def update_key(self, key_id: str, new_description: str) -> dict:\n       return"
RESPONSE:
# Typed wrapper for AWS CloudHSM (Classic) using mypy-boto3-cloudhsm 1.34.0 stubs

from typing import Optional

import boto3
from mypy_boto3_cloudhsm import CloudHSMClient
from mypy_boto3_cloudhsm.literals import SubscriptionTypeType
from mypy_boto3_cloudhsm.type_defs import (
    CreateHsmResponseTypeDef,
    DeleteHsmResponseTypeDef,
    DescribeHsmResponseTypeDef,
    ListAvailableZonesResponseTypeDef,
    ListHsmsResponseTypeDef,
)


class MyCloudHSM:
    def __init__(
        self,
        client: Optional[CloudHSMClient] = None,
        *,
        region_name: Optional[str] = None,
    ) -> None:
        # If an explicit client isn't provided, build a typed one from boto3
        self.client: CloudHSMClient = client or boto3.client(
            "cloudhsm",
            region_name=region_name,
        )

    def create_hsm(
        self,
        *,
        subnet_id: str,
        ssh_key: str,
        iam_role_arn: str,
        subscription_type: SubscriptionTypeType,
        eni_ip: Optional[str] = None,
        external_id: Optional[str] = None,
        client_token: Optional[str] = None,
        syslog_ip: Optional[str] = None,
    ) -> CreateHsmResponseTypeDef:
        return self.client.create_hsm(
            SubnetId=subnet_id,
            SshKey=ssh_key,
            IamRoleArn=iam_role_arn,
            SubscriptionType=subscription_type,
            EniIp=eni_ip,
            ExternalId=external_id,
            ClientToken=client_token,
            SyslogIp=syslog_ip,
        )

    def list_hsms(self, *, next_token: Optional[str] = None) -> ListHsmsResponseTypeDef:
        if next_token:
            return self.client.list_hsms(NextToken=next_token)
        return self.client.list_hsms()

    def describe_hsm(
        self,
        *,
        hsm_arn: Optional[str] = None,
        hsm_serial_number: Optional[str] = None,
    ) -> DescribeHsmResponseTypeDef:
        if hsm_arn:
            return self.client.describe_hsm(HsmArn=hsm_arn)
        if hsm_serial_number:
            return self.client.describe_hsm(HsmSerialNumber=hsm_serial_number)
        raise ValueError("Either hsm_arn or hsm_serial_number must be provided")

    def delete_hsm(self, *, hsm_arn: str) -> DeleteHsmResponseTypeDef:
        return self.client.delete_hsm(HsmArn=hsm_arn)

    def list_available_zones(self) -> ListAvailableZonesResponseTypeDef:
        return self.client.list_available_zones()
------------------------------------------------------------

PROMPT 23:
"Generate Python code that imports the necessary modules and annotates the types for the boto3 WorkMail service version 1.34.0 using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom boto3.workmail import WorkMailClient\n\nwork_mail_client = WorkMailClient(\n   aws_access_key_id='your_access_key',\n   aws_secret_access_key='your_secret_key',\n   region='your_region'\n)\n\n# Annotated methods and properties of the WorkMail client\nwork_mail_client.list_mailboxes() -> List[dict]\nwork_mail_client.list_mailboxes(NextToken='next_token') -> List[dict]\nwork_mail_client.create_mailbox(MailboxName='new_mailbox') -> dict\nwork_mail_client.delete_mailbox(MailboxName='existing_mailbox') -> None\nwork_mail_client.list_mails() -> List[dict"
RESPONSE:
# Requires:
#   pip install boto3 mypy-boto3-workmail
# Works with boto3 1.34.x and stubs generated by mypy-boto3-builder 7.21.0

from __future__ import annotations

from typing import List, Optional
import boto3

from mypy_boto3_workmail import WorkMailClient
from mypy_boto3_workmail.type_defs import (
    CreateUserResponseTypeDef,
    DescribeUserResponseTypeDef,
    ListOrganizationsResponseTypeDef,
    ListUsersResponseTypeDef,
    OrganizationSummaryTypeDef,
    UserTypeDef,
)

# Create a typed WorkMail client
session = boto3.Session(
    aws_access_key_id="your_access_key",
    aws_secret_access_key="your_secret_key",
    region_name="your_region",
)
workmail: WorkMailClient = session.client("workmail")

# List organizations
orgs_resp: ListOrganizationsResponseTypeDef = workmail.list_organizations()
orgs: List[OrganizationSummaryTypeDef] = orgs_resp["OrganizationSummaries"]
next_token: Optional[str] = orgs_resp.get("NextToken")

# List users (paginated)
users_resp: ListUsersResponseTypeDef = workmail.list_users(
    OrganizationId="your_org_id",
    NextToken=next_token,
)
users: List[UserTypeDef] = users_resp["Users"]

# Create a user
create_out: CreateUserResponseTypeDef = workmail.create_user(
    OrganizationId="your_org_id",
    Name="new_user_name",
    DisplayName="New User",
    Password="Str0ng&P@ssw0rd",
)
new_user_id: str = create_out["UserId"]

# Describe the user
desc_out: DescribeUserResponseTypeDef = workmail.describe_user(
    OrganizationId="your_org_id",
    UserId=new_user_id,
)

# Delete the user (returns an empty dict response)
workmail.delete_user(
    OrganizationId="your_org_id",
    UserId=new_user_id,
)
------------------------------------------------------------

PROMPT 24:
"Generate Python code that imports the required modules and annotates the types for the boto3.Route53 service version 1.34.16 using mypy-boto3-builder version 7.23.1, like this:\n```python\nfrom typing import TypeVar, Generic\nfrom botocore.client import BaseClient\nfrom botocore.types import ServiceMetadata\nfrom boto3.route53 import Route53Client\n\nclass Route53ClientTypeAnnotated(Route53Client):\n   @typevar('self')\n   def __init__(self) -> None:\n       self.client: BaseClient = None\n       self.metadata: ServiceMetadata = None\n\n   def create_health_check(self, RequestAttributes arg1: dict) -> None:\n       pass\n\n   def delete_health_check(self, HealthCheckId arg1: str) -> None:\n       pass\n\n   def get_health_check(self, HealthCheckId arg1: str) -> None:\n       pass\n\n   def list_health_checks(self, MaxResults arg1:"
RESPONSE:
# Requires:
#   boto3==1.34.16
#   botocore==1.34.16
#   mypy-boto3-route53 (generated by mypy-boto3-builder==7.23.1)
from __future__ import annotations

from typing import Any, Optional

import boto3
from mypy_boto3_route53 import Route53Client
from mypy_boto3_route53.type_defs import (
    CreateHealthCheckRequestRequestTypeDef,
    CreateHealthCheckResponseTypeDef,
    DeleteHealthCheckResponseTypeDef,
    GetHealthCheckResponseTypeDef,
    ListHealthChecksResponseTypeDef,
)


def get_route53_client() -> Route53Client:
    return boto3.client("route53")


class Route53ClientTypeAnnotated:
    def __init__(self, client: Optional[Route53Client] = None) -> None:
        self.client: Route53Client = client or get_route53_client()

    def create_health_check(
        self,
        request: CreateHealthCheckRequestRequestTypeDef,
    ) -> CreateHealthCheckResponseTypeDef:
        return self.client.create_health_check(**request)

    def delete_health_check(self, HealthCheckId: str) -> DeleteHealthCheckResponseTypeDef:
        return self.client.delete_health_check(HealthCheckId=HealthCheckId)

    def get_health_check(self, HealthCheckId: str) -> GetHealthCheckResponseTypeDef:
        return self.client.get_health_check(HealthCheckId=HealthCheckId)

    def list_health_checks(
        self,
        Marker: Optional[str] = None,
        MaxItems: Optional[str] = None,
    ) -> ListHealthChecksResponseTypeDef:
        params: dict[str, Any] = {}
        if Marker is not None:
            params["Marker"] = Marker
        if MaxItems is not None:
            params["MaxItems"] = MaxItems
        return self.client.list_health_checks(**params)
------------------------------------------------------------

PROMPT 25:
"Generate Python code that imports the necessary modules and annotates the types for the MarketplaceCommerceAnalytics service, version 1.34.0, using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import Optional\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import BotocoreClientBuilder\n\n# Annotate the client class\nclass MarketplaceCommerceAnalytics(BaseClient):\n   def __init__(self, *args, **kwargs) -> None:\n       super().__init__(*args, **kwargs)\n\n   # Annotate methods and properties\n   @property\n   def availability(self) -> str:\n       return \"AVAILABLE\"\n\n   def cancel_data_export(self, export_id: str) -> None:\n       pass\n\n   def create_data_export(self, data_export_request: dict) -> None:\n       pass\n\n   def delete_data_export(self, export_id: str) -> None:\n       pass\n\n   def get_data_export"
RESPONSE:
```python
from typing import Optional, Mapping, Any, Dict
from botocore.client import BaseClient
from mypy_boto3_builder import BotocoreClientBuilder

__service_name__ = "marketplacecommerceanalytics"
__service_version__ = "1.34.0"
__mypy_boto3_builder_version__ = "7.21.0"


# Annotate the client class
class MarketplaceCommerceAnalytics(BaseClient):
    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)

    # Annotate methods and properties
    @property
    def availability(self) -> str:
        return "AVAILABLE"

    def cancel_data_export(self, export_id: str) -> None:
        pass

    def create_data_export(self, data_export_request: Mapping[str, Any]) -> None:
        pass

    def delete_data_export(self, export_id: str) -> None:
        pass

    def get_data_export(self, export_id: str) -> Optional[Dict[str, Any]]:
        pass
```
------------------------------------------------------------

PROMPT 26:
"Generate Python code that imports the required modules and annotates the LookoutforVision service using mypy-boto3-builder:\n```python\nfrom typing import Optional\nimport boto3\nfrom mypy_boto3_builder import TypeAnnotator\n\n# Initialize the type annotator\nta = TypeAnnotator(boto3)\n\n# Define the service class with typed methods\nclass LookoutforVision(boto3.service):\n   def __init__(self, config: dict):\n       self.config = config\n\n   def create_dataset(self, dataset_name: str, dataset_type: str, data_source: str, kms_key_arn: str, **kwargs: Any) -> None:\n       r\"\"\"Create a new dataset.\"\"\"\n       pass\n\n   def list_datasets(self, **kwargs: Any) -> List[dict]:\n       r\"\"\"List all datasets.\"\"\"\n       pass\n\n   def describe_dataset(self, dataset_name: str, **kwargs: Any) -> dict:\n       r\"\"\"Describe a dataset.\"\"\"\n       pass\n\n   def"
RESPONSE:
# NOTE:
# mypy-boto3-builder is a code generator and is not intended to be used at runtime.
# For runtime type annotations, install boto3-stubs for Lookout for Vision:
#   pip install boto3-stubs[lookoutvision]
# and import types from mypy_boto3_lookoutvision.

from typing import Any, Dict, List, Optional, Sequence, Literal
import boto3
from mypy_boto3_lookoutvision import LookoutVisionClient


class LookoutforVision:
    """
    Typed wrapper around the boto3 Lookout for Vision client.
    """

    def __init__(self, client: Optional[LookoutVisionClient] = None, region_name: Optional[str] = None) -> None:
        self.client: LookoutVisionClient = client or boto3.client("lookoutvision", region_name=region_name)  # type: ignore[assignment]

    def create_dataset(
        self,
        project_name: str,
        dataset_type: Literal["TRAIN", "TEST"],
        dataset_source: Optional[Dict[str, Any]] = None,
        client_token: Optional[str] = None,
        tags: Optional[Sequence[Dict[str, str]]] = None,
    ) -> Dict[str, Any]:
        """
        Create a new dataset.
        dataset_source should match the AWS DatasetSource shape if provided, e.g.:
        {
            "GroundTruthManifest": {
                "S3Object": {"Bucket": "...", "Key": "...", "VersionId": "..."}
            }
        }
        """
        params: Dict[str, Any] = {
            "ProjectName": project_name,
            "DatasetType": dataset_type,
        }
        if dataset_source is not None:
            params["DatasetSource"] = dataset_source
        if client_token is not None:
            params["ClientToken"] = client_token
        if tags is not None:
            params["Tags"] = tags
        return self.client.create_dataset(**params)

    def list_datasets(self, project_name: str) -> List[Dict[str, Any]]:
        """
        List available datasets for a project by checking TRAIN and TEST.
        Lookout for Vision supports at most TRAIN and TEST datasets per project.
        """
        datasets: List[Dict[str, Any]] = []
        for dataset_type in ("TRAIN", "TEST"):
            try:
                resp = self.client.describe_dataset(ProjectName=project_name, DatasetType=dataset_type)  # type: ignore[arg-type]
                # Service returns {"DatasetDescription": {...}}
                description = resp.get("DatasetDescription")
                if description is not None:
                    datasets.append(description)  # type: ignore[arg-type]
            except self.client.exceptions.ResourceNotFoundException:
                # Dataset of this type does not exist for the project
                continue
        return datasets

    def describe_dataset(
        self,
        project_name: str,
        dataset_type: Literal["TRAIN", "TEST"],
    ) -> Dict[str, Any]:
        """
        Describe a dataset.
        """
        return self.client.describe_dataset(ProjectName=project_name, DatasetType=dataset_type)  # type: ignore[arg-type]


if __name__ == "__main__":
    # Example usage
    svc = LookoutforVision(region_name="us-east-1")
    # List datasets for a project
    print(svc.list_datasets(project_name="my-project"))
------------------------------------------------------------

PROMPT 27:
"Generate Python code that imports the required modules and annotates the CloudFormation 2.9.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom aiobotocore.cloudformation import CloudFormation\nfrom aiobotocore.client import AIOBotoCoreClient\n\nasync def main():\n   client = AIOBotoCoreClient(\n       aws_access_key_id='YOUR_AWS_ACCESS_KEY',\n       aws_secret_access_key='YOUR_AWS_SECRET_KEY',\n       aws_session_token='YOUR_AWS_SESSION_TOKEN',\n   )\n   cloudformation = CloudFormation(client)\n   stacks = await cloudformation.list_stacks()\n   print(stacks)\n```"
RESPONSE:
```python
# Requires:
#   aiobotocore
#   types-aiobotocore-cloudformation  (generated by mypy-boto3-builder 7.21.0)

import asyncio
from aiobotocore.session import get_session

from types_aiobotocore_cloudformation.client import CloudFormationClient
from types_aiobotocore_cloudformation.type_defs import ListStacksOutputTypeDef


async def main() -> None:
    session = get_session()
    async with session.create_client(
        "cloudformation",
        region_name="us-east-1",
        aws_access_key_id="YOUR_AWS_ACCESS_KEY",
        aws_secret_access_key="YOUR_AWS_SECRET_KEY",
        aws_session_token="YOUR_AWS_SESSION_TOKEN",
    ) as client:
        cloudformation: CloudFormationClient = client
        stacks: ListStacksOutputTypeDef = await cloudformation.list_stacks()
        print(stacks)


if __name__ == "__main__":
    asyncio.run(main())
```
------------------------------------------------------------

PROMPT 28:
"Generate Python code that creates a Django REST framework JSON renderer backed by Orjson:\n```python\nfrom rest_framework import renderers\nfrom django.contrib.json import orjson\n\nclass OrjsonRenderer(renderers.JSONRenderer):\n   def render(self, data, media_type, serializer):\n       return orjson.dumps(data)\n```\nThis code defines a custom JSON renderer for Django REST framework that uses the `orjson` library to serialize Python objects into JSON format. The renderer takes in the data to be serialized, the requested media type, and the serializer instance, and returns a JSON-formatted string using the `orjson.dumps()` function."
RESPONSE:
Here’s a correct and robust DRF JSON renderer using orjson:

```python
import orjson
from decimal import Decimal
from rest_framework.renderers import JSONRenderer


def _orjson_default(obj):
    # Match Django/DRF behavior for unsupported types
    if isinstance(obj, Decimal):
        return str(obj)
    raise TypeError


class ORJSONRenderer(JSONRenderer):
    media_type = "application/json"
    format = "json"
    charset = None  # orjson returns bytes

    # Sensible defaults for typical Django/DRF projects
    options = (
        orjson.OPT_NON_STR_KEYS
        | orjson.OPT_SERIALIZE_NUMPY
        | orjson.OPT_SERIALIZE_UUID
        | orjson.OPT_NAIVE_UTC
        | orjson.OPT_UTC_Z
    )

    def render(self, data, accepted_media_type=None, renderer_context=None):
        if data is None:
            return b""

        opts = self.options
        # Respect DRF's indent hint (pretty output in browsable API)
        if renderer_context and renderer_context.get("indent") is not None:
            opts |= orjson.OPT_INDENT_2

        return orjson.dumps(data, option=opts, default=_orjson_default)
```

To use it, add to Django settings:
- REST_FRAMEWORK['DEFAULT_RENDERER_CLASSES'] = ['path.to.ORJSONRenderer']
------------------------------------------------------------

PROMPT 29:
"Generate Python code that imports the required modules and annotates the types of the boto3 Elastic Beanstalk client using mypy-boto3-builder, like this:\n```python\nfrom typing import Callable\nfrom botocore.client import BaseClient\nfrom elasticbeanstalk import ElasticBeanstalk\n\neb_client = ElasticBeanstalk(\n   aws_access_key_id=\"string\",\n   aws_secret_access_key=\"string\",\n   region=\"string\"\n)\n\n# Annotate methods and properties of the ElasticBeanstalk client\neb_client.describe_environment(\"env-name\") -> None\neb_client.create_environment(\"env-name\", \"env-type\") -> None\neb_client.delete_environment(\"env-name\") -> None\neb_client.list_environments() -> List[dict]\neb_client.list_environment_resources(\"env-name\") -> List[dict]\n```"
RESPONSE:
```python
# Requires: pip install "boto3-stubs[elasticbeanstalk]" boto3

import boto3
from mypy_boto3_elasticbeanstalk import ElasticBeanstalkClient

# Typed Elastic Beanstalk client
eb_client: ElasticBeanstalkClient = boto3.client(
    "elasticbeanstalk",
    aws_access_key_id="string",
    aws_secret_access_key="string",
    region_name="string",
)

# Typed API calls (signatures provided by mypy-boto3 stubs)

# Describe environments (your example used list_environments)
envs = eb_client.describe_environments(EnvironmentNames=["env-name"])

# Create an environment
create_resp = eb_client.create_environment(
    ApplicationName="my-app",
    EnvironmentName="env-name",
    SolutionStackName="64bit Amazon Linux 2 v5.8.2 running Python 3.9",
)

# Terminate (delete) an environment (your example used delete_environment)
term_resp = eb_client.terminate_environment(EnvironmentName="env-name")

# Describe resources of an environment (your example used list_environment_resources)
resources = eb_client.describe_environment_resources(EnvironmentName="env-name")
```
------------------------------------------------------------

PROMPT 30:
"Generate Python code that imports the required modules and sets up type hints for the boto3.Braket 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import Tuple\nfrom botocore.client import Boto3Client\nfrom braket.braket_service import BraKetService\n\nbraket_client: Boto3Client = None  # type: ignore\nbraket_service: BraKetService = None  # type: ignore\n\ndef init_braket_client(region: str) -> None:\n   global braket_client\n   braket_client = Boto3Client(\n       service=BraKetService,\n       region=region,\n       config=None  # type: ignore\n   )\n\ndef init_braket_service() -> None:\n   global braket_service\n   braket_service = BraKetService(\n       client=braket_client,\n       region=braket_client.region  # type: ignore\n   )\n```"
RESPONSE:
from typing import Optional
from dataclasses import dataclass
import boto3
from botocore.config import Config
from mypy_boto3_braket.client import BraketClient

braket_client: BraketClient = None  # type: ignore


@dataclass
class BraketService:
    client: BraketClient
    region: str


braket_service: BraketService = None  # type: ignore


def init_braket_client(region: str, config: Optional[Config] = None) -> None:
    global braket_client
    braket_client = boto3.client("braket", region_name=region, config=config)


def init_braket_service() -> None:
    global braket_service
    if braket_client is None:
        raise RuntimeError("braket_client is not initialized")
    braket_service = BraketService(
        client=braket_client,
        region=braket_client.meta.region_name or ""
    )
------------------------------------------------------------

PROMPT 31:
"Generate Python code that implements a Ray backend for distributed LightGBM:\n```python\nfrom ray import init, get, put\nfrom lightgbm import LightGBMClassifier, LightGBMRegressor\n\n# Initialize Ray\ninit(num_cpus=4)\n\n# Define a function to create a LightGBM model\ndef create_model(data):\n   return LightGBMClassifier(objective='multiclass', num_class=8, max_depth=5, learning_rate=0.05)\n\n# Distribute data across four nodes\ndata = np.random.rand(1000, 10)\ndata_per_node = data.split(np.array_split(data, 4))\n\n# Train a separate LightGBM model on each node\nmodels = [create_model(data_per_node[i]) for i in range(4)]\n\n# Use Ray to parallelize training of models\nresults = [get(models[i].fit(data_per_node[i], epochs=10)) for i in range(4)]\n\n# Combine results from each"
RESPONSE:
# Distributed LightGBM on Ray using lightgbm_ray

# pip install -U ray[default] lightgbm lightgbm_ray

import os
import numpy as np
import ray
from lightgbm_ray import RayParams, RayLGBMClassifier, RayLGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error


def init_ray(num_cpus=4):
    # If running on an existing Ray cluster, set env var: RAY_ADDRESS=ray://<head-ip>:10001
    address = os.environ.get("RAY_ADDRESS")
    if ray.is_initialized():
        return
    if address:
        ray.init(address=address)
    else:
        ray.init(num_cpus=num_cpus, include_dashboard=False)


def train_distributed_multiclass(num_actors=4):
    # Synthetic multiclass dataset
    n_samples, n_features, n_classes = 20000, 32, 8
    X = np.random.rand(n_samples, n_features).astype(np.float32)
    y = np.random.randint(0, n_classes, size=n_samples, dtype=np.int32)

    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
    X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=42, stratify=y_tmp)

    # Ray execution parameters (resources per worker)
    ray_params = RayParams(
        num_actors=num_actors,     # number of distributed workers
        cpus_per_actor=1,          # CPUs per worker
        gpus_per_actor=0,          # set >0 if using GPU workers
        max_actor_restarts=1,      # simple fault tolerance
    )

    # Distributed LightGBM classifier (uses Ray under the hood)
    clf = RayLGBMClassifier(
        objective="multiclass",
        num_class=n_classes,
        max_depth=5,
        learning_rate=0.05,
        n_estimators=1000,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        tree_learner="data",  # recommended for distributed data-parallel training
        # You can also set num_leaves, min_data_in_leaf, etc.
        # verbose=1
    )

    clf.fit(
        X_train, y_train,
        ray_params=ray_params,
        eval_set=[(X_val, y_val)],
        eval_metric="multi_logloss",
        early_stopping_rounds=50,
        verbose=True
    )

    y_pred = clf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"[Multiclass] Test accuracy: {acc:.4f}")

    return clf


def train_distributed_regression(num_actors=4):
    # Synthetic regression dataset
    n_samples, n_features = 30000, 24
    X = np.random.rand(n_samples, n_features).astype(np.float32)
    # Nonlinear target with noise
    y = (
        3.0 * X[:, 0]
        - 2.0 * np.sin(2 * np.pi * X[:, 1])
        + 0.5 * (X[:, 2] ** 2)
        + np.random.normal(scale=0.1, size=n_samples)
    ).astype(np.float32)

    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=42)

    ray_params = RayParams(
        num_actors=num_actors,
        cpus_per_actor=1,
        gpus_per_actor=0,
        max_actor_restarts=1,
    )

    reg = RayLGBMRegressor(
        objective="regression",
        max_depth=6,
        learning_rate=0.05,
        n_estimators=1000,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        tree_learner="data",
    )

    reg.fit(
        X_train, y_train,
        ray_params=ray_params,
        eval_set=[(X_val, y_val)],
        eval_metric="l2",
        early_stopping_rounds=50,
        verbose=True
    )

    y_pred = reg.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    print(f"[Regression] Test RMSE: {rmse:.4f}")

    return reg


if __name__ == "__main__":
    # Initialize Ray (connects to cluster if RAY_ADDRESS is set, otherwise starts local)
    init_ray(num_cpus=4)

    # Train distributed LightGBM models with Ray
    clf = train_distributed_multiclass(num_actors=4)
    reg = train_distributed_regression(num_actors=4)

    # Shutdown Ray if you started it locally
    if not os.environ.get("RAY_ADDRESS"):
        ray.shutdown()
------------------------------------------------------------

PROMPT 32:
"Generate Python code that imports the required libraries and sets up type hints for the boto3.Detective service version 1.34.0 using mypy-boto3-builder version 7.21.0, like so:\n```python\nfrom typing import Tuple\nfrom botocore.client import BaseClient\nfrom boto3.detective import Detective\n\nbase_client: BaseClient = None\ndetective: Detective = None\n\ndef init_clients(region: str) -> Tuple[BaseClient, Detective]:\n   base_client = boto3.client('detective', region=region)\n   detective = boto3.client('detective', region=region, config=None)\n   return base_client, detective\n\ndef get_case_status(case_id: str) -> str:\n   base_client, detective = init_clients(region='us-west-2')\n   response = detective.describe_cases(CaseIds=[case_id])\n   case_status = response['Cases'][0]['Status']"
RESPONSE:
# Requires:
# - boto3==1.34.0
# - boto3-stubs[detective]==1.34.0  (generated by mypy-boto3-builder==7.21.0)
from typing import Tuple
import boto3
from botocore.client import BaseClient
from mypy_boto3_detective.client import DetectiveClient


base_client: BaseClient | None = None
detective: DetectiveClient | None = None


def init_clients(region: str) -> Tuple[BaseClient, DetectiveClient]:
    base: BaseClient = boto3.client("detective", region_name=region)
    det: DetectiveClient = boto3.client("detective", region_name=region)
    return base, det


def get_graph_count() -> int:
    base, det = init_clients(region="us-west-2")
    resp = det.list_graphs()
    return len(resp.get("GraphList", []))
------------------------------------------------------------

PROMPT 33:
"Generate Python code that uses the `asn1crypto` library to parse, encode, and decode ASN.1 data, similar to the following package description:\n```\nASN.1 is a powerful and flexible data modeling language used in various industries for defining hierarchical, typed data structures. This package provides an implementation of an ASN.1 parser, encoder, and decoder in Python. It allows you to define your data structures using ASN.1 syntax, and then use the generated Python code to manipulate and exchange data with other systems that support ASN.1.\n```\nHere's a possible prompt:\n\n\"Generate Python code that parses, encodes, and decodes ASN.1 data using the `asn1crypto` library, allowing users to define their own data structures using ASN.1 syntax and interact with other systems that support ASN.1.\""
RESPONSE:
Below is a self-contained example that uses the asn1crypto library to define ASN.1-like structures in Python, and then parse, encode (DER), and decode them. While asn1crypto does not parse textual ASN.1 syntax directly, you define your schema by subclassing asn1crypto.core types to mirror the ASN.1 you would write.

Example ASN.1 (for reference)
- Person ::= SEQUENCE {
    name      UTF8String,
    age       [1] INTEGER OPTIONAL,
    email     [0] IA5String OPTIONAL,
    roles     SEQUENCE OF UTF8String OPTIONAL,
    address   Address OPTIONAL,
    id        [2] EXPLICIT CHOICE {
                 ssn  PrintableString,
                 uuid OCTET STRING (SIZE(16))
              } OPTIONAL
  }
- Address ::= SEQUENCE {
    street    UTF8String,
    city      UTF8String,
    zip       INTEGER
  }

Python code
You can copy-paste this into a file (e.g., asn1_example.py) and run it. It shows how to:
- Define custom data structures
- Encode to DER
- Decode from DER
- Work with native Python types (dicts, lists, ints, strings) for convenience

Requires: pip install asn1crypto

```python
from typing import Any, Dict, Tuple
from asn1crypto import core


# ---------------------------
# Schema definition (asn1crypto)
# ---------------------------

class Address(core.Sequence):
    """
    Address ::= SEQUENCE {
        street    UTF8String,
        city      UTF8String,
        zip       INTEGER
    }
    """
    _fields = [
        ('street', core.UTF8String),
        ('city',   core.UTF8String),
        ('zip',    core.Integer),
    ]


class Roles(core.SequenceOf):
    """
    roles SEQUENCE OF UTF8String
    """
    _child_spec = core.UTF8String


class IDChoice(core.Choice):
    """
    id [2] EXPLICIT CHOICE {
        ssn  PrintableString,
        uuid OCTET STRING (SIZE(16))
    }
    """
    _alternatives = [
        ('ssn',  core.PrintableString),
        ('uuid', core.OctetString, {'min_size': 16, 'max_size': 16}),
    ]


class Person(core.Sequence):
    """
    Person ::= SEQUENCE {
        name      UTF8String,
        age       [1] INTEGER OPTIONAL,
        email     [0] IA5String OPTIONAL,
        roles     SEQUENCE OF UTF8String OPTIONAL,
        address   Address OPTIONAL,
        id        [2] EXPLICIT CHOICE {
                     ssn  PrintableString,
                     uuid OCTET STRING (SIZE(16))
                  } OPTIONAL
    }
    Notes on tagging:
    - age is [1] EXPLICIT (context-specific)
    - email is [0] EXPLICIT (context-specific)
    - id is [2] EXPLICIT (context-specific)
    """
    _fields = [
        ('name',    core.UTF8String),

        # Context-specific explicit tagging for illustration.
        # asn1crypto uses tag_type + tag to express this.
        ('age',     core.Integer,     {'optional': True, 'tag_type': 'explicit', 'tag': 1}),
        ('email',   core.IA5String,   {'optional': True, 'tag_type': 'explicit', 'tag': 0}),

        ('roles',   Roles,            {'optional': True}),
        ('address', Address,          {'optional': True}),

        # EXPLICIT CHOICE with [2] tag
        ('id',      IDChoice,         {'optional': True, 'tag_type': 'explicit', 'tag': 2}),
    ]


# ---------------------------
# Helpers for encoding/decoding
# ---------------------------

def encode_der(schema_cls: core.Asn1Value.__class__, native_obj: Dict[str, Any]) -> bytes:
    """
    Encodes a Python-native object (dict, list, etc.) to DER using the given schema class.
    """
    value = schema_cls(native_obj)
    return value.dump()  # DER encoding


def decode_der(schema_cls: core.Asn1Value.__class__, data: bytes) -> Tuple[core.Asn1Value, Dict[str, Any]]:
    """
    Decodes DER bytes into an asn1crypto object and its Python-native representation.
    Returns both the object and its .native form.
    """
    obj = schema_cls.load(data)
    return obj, obj.native


def hexlify(b: bytes) -> str:
    return b.hex()


# ---------------------------
# Example usage
# ---------------------------

def example_person_native() -> Dict[str, Any]:
    """
    Returns a Python-native dict matching the Person schema.
    You can feed this directly to Person(...) or encode_der(Person, native).
    """
    return {
        'name': 'Alice Doe',
        'age': 30,
        'email': 'alice@example.com',
        'roles': ['user', 'editor'],  # Roles is SEQUENCE OF UTF8String
        'address': {
            'street': '123 Some Street',
            'city':   'Metropolis',
            'zip':    54321
        },
        # IDChoice: choose one alternative, e.g., 'ssn' or 'uuid'
        'id': ('ssn', '123-45-6789'),
        # Alternatively:
        # 'id': ('uuid', bytes.fromhex('00112233445566778899aabbccddeeff'))
    }


if __name__ == '__main__':
    # Build native object
    person_native = example_person_native()

    # Encode to DER
    der_bytes = encode_der(Person, person_native)
    print('DER (hex):', hexlify(der_bytes))

    # Decode back from DER
    person_obj, person_decoded = decode_der(Person, der_bytes)

    # person_obj is the asn1crypto object, person_decoded is a native dict
    print('Decoded (native):', person_decoded)

    # Accessing fields from the asn1crypto object
    print('Name (object):', person_obj['name'].native)

    # Mutate the object (optional): add/change fields and re-encode
    person_obj['roles'] = Roles(['user', 'editor', 'auditor'])
    person_re_der = person_obj.dump()
    print('DER after mutation (hex):', hexlify(person_re-der) if 'person_re-der' in locals() else hexlify(person_re_der))

    # Demonstrate CHOICE alternative with UUID
    person_uuid_native = dict(person_native)
    person_uuid_native['id'] = ('uuid', bytes.fromhex('00112233445566778899aabbccddeeff'))
    der_uuid = encode_der(Person, person_uuid_native)
    _, decoded_uuid = decode_der(Person, der_uuid)
    print('Decoded with UUID (native):', decoded_uuid)

    # If you need to deal with arbitrary ASN.1 without a known schema,
    # you can at least load it as "Any" to inspect tag/length/value.
    # any_obj = core.Any.load(der_bytes)
    # print('Any (tag, class, method):', any_obj.tag, any_obj.tag_type, any_obj.method)
```

Notes and tips
- asn1crypto focuses on DER encoding. load() handles DER (and some BER), and dump() outputs DER.
- You define your structures by subclassing asn1crypto.core types and setting _fields (for Sequence), _alternatives (for Choice), or _child_spec (for SequenceOf/SetOf).
- Optional fields can be omitted from the native dict.
- CHOICE is specified by ('choice_name', value) when using the native dict form.
- For true parsing of textual ASN.1 modules into Python code, you’d typically use a separate tool (e.g., asn1tools). You can still interoperate by ensuring your asn1crypto schema mirrors the other system’s ASN.1 and exchanging DER/BER bytes.
------------------------------------------------------------

PROMPT 34:
"Generate Python code that imports the necessary modules and annotates the types for the boto3 MediaLive 1.34.6 service using mypy-boto3-builder 7.23.0, like so:\n```python\nfrom typing import Callable, Dict, List, Optional, Tuple\nfrom boto3 import ServiceResource\nfrom boto3.media_live import MediaLiveClient\n\ndef create_media_live_client(config: dict) -> MediaLiveClient:\n   return MediaLiveClient(config=config)\n\ndef list_streams(\n   client: MediaLiveClient,\n   *,\n   next_token: str = None,\n   max_results: int = None,\n) -> Tuple[List[dict], str]:\n   return client.list_streams(next_token=next_token, max_results=max_results)\n\ndef create_stream(\n   client: MediaLiveClient,\n   *,\n   name: str,\n   input_device: str,\n   output_device: str,\n   bitrate: int,\n   resolution: tuple,\n   frame"
RESPONSE:
from typing import List, Optional, Tuple

import boto3
from mypy_boto3_medialive import MediaLiveClient
from mypy_boto3_medialive.type_defs import (
    ChannelSummaryTypeDef,
    CreateChannelRequestRequestTypeDef,
    CreateChannelResponseTypeDef,
    CreateInputRequestRequestTypeDef,
    CreateInputResponseTypeDef,
    ListChannelsResponseTypeDef,
)


def create_medialive_client(
    *,
    region_name: Optional[str] = None,
    profile_name: Optional[str] = None,
) -> MediaLiveClient:
    session = boto3.Session(profile_name=profile_name) if profile_name else boto3.Session()
    client: MediaLiveClient = session.client("medialive", region_name=region_name)
    return client


def list_channels(
    client: MediaLiveClient,
    *,
    next_token: Optional[str] = None,
    max_results: Optional[int] = None,
) -> Tuple[List[ChannelSummaryTypeDef], Optional[str]]:
    resp: ListChannelsResponseTypeDef = client.list_channels(
        NextToken=next_token,
        MaxResults=max_results,
    )
    channels: List[ChannelSummaryTypeDef] = resp.get("Channels", [])
    return channels, resp.get("NextToken")


def create_input(
    client: MediaLiveClient,
    params: CreateInputRequestRequestTypeDef,
) -> CreateInputResponseTypeDef:
    # Example: params could include Name, Type, Sources, etc.
    # See CreateInputRequestRequestTypeDef in mypy_boto3_medialive.type_defs for details.
    return client.create_input(**params)


def create_channel(
    client: MediaLiveClient,
    params: CreateChannelRequestRequestTypeDef,
) -> CreateChannelResponseTypeDef:
    # Example: params could include Name, RoleArn, InputAttachments, EncoderSettings, etc.
    # See CreateChannelRequestRequestTypeDef in mypy_boto3_medialive.type_defs for details.
    return client.create_channel(**params)
------------------------------------------------------------

PROMPT 35:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.Greengrass 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List, Dict\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import BotohubBuilder\n\n# Build the Greengrass client with type hints\ngg_client = BotohubBuilder(base_client=BaseClient).build_greengrass()\n\n# Define the type of the Greengrass client\ngg_client: GreengrassV2 = gg_client\n\n# Use the client to perform Greengrass operations\nresponse = gg_client.deploy_resources(\n   DeploymentName='my-deployment',\n   ResourceDefinitionVersion='v1.0',\n   Resources={\n       'resource1': {\n           'ResourceType': ' AWS::EC2::Instance',\n           'Properties': {\n               'InstanceType': 't2.micro'\n           }\n       },\n       'resource2': {\n           'ResourceType"
RESPONSE:
# Requires:
#   pip install "boto3==1.34.*" "boto3-stubs[greengrass,greengrassv2]==1.34.*"
# The mypy_boto3_* packages below are generated by mypy-boto3-builder 7.21.0.

from __future__ import annotations

from typing import Dict

import boto3
from mypy_boto3_greengrass import GreengrassClient
from mypy_boto3_greengrass.type_defs import ListGroupsResponseTypeDef

from mypy_boto3_greengrassv2 import GreengrassV2Client
from mypy_boto3_greengrassv2.type_defs import (
    CreateDeploymentResponseTypeDef,
    ComponentDeploymentSpecificationTypeDef,
)

# Create typed clients
gg: GreengrassClient = boto3.client("greengrass")
ggv2: GreengrassV2Client = boto3.client("greengrassv2")

# Greengrass v1 example: list groups (typed response)
groups_resp: ListGroupsResponseTypeDef = gg.list_groups()
print(f"Found {len(groups_resp.get('Groups', []))} Greengrass groups")

# Greengrass v2 example: create a deployment (typed request and response)
components: Dict[str, ComponentDeploymentSpecificationTypeDef] = {
    "aws.greengrass.Nucleus": {
        "componentVersion": "2.12.7",
        "configurationUpdate": {
            # Merge must be a JSON string per API spec
            "merge": '{"aws.greengrass.logging": {"level": "INFO"}}'
        },
        # "runWith": {...}  # Optional
    }
}

deployment_resp: CreateDeploymentResponseTypeDef = ggv2.create_deployment(
    targetArn="arn:aws:iot:us-east-1:123456789012:thing/MyCoreDevice",
    deploymentName="my-deployment",
    components=components,
    tags={"env": "dev"},
)

print(f"Deployment created: {deployment_resp['deploymentId']}")
------------------------------------------------------------

PROMPT 36:
"Generate Python code that imports the required modules and annotates the types for the MediaConnect service in boto3 version 1.34.7 using mypy-boto3-builder version 7.23.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom boto3.services.mediaconnect import MediaConnect\n\n# Annotate the client and service classes\nclass MyMediaConnectClient(BaseClient):\n   def __init__(self, config: dict) -> None:\n       super().__init__(config=config)\n       self.service = MediaConnect(self)\n\n   def create_flow(self, flow_name: str, *, description: str = None, avails: List[str] = None) -> None:\n       \"\"\"Create a new flow.\"\"\"\n       return self.service.create_flow(flow_name, description=description, avails=avails)\n\n   def delete_flow(self, flow_arn: str) -> None:\n       \"\"\"Delete a flow.\"\"\"\n       return self.service.delete_flow(flow_arn)\n"
RESPONSE:
from __future__ import annotations

import boto3
from typing import Optional
from typing_extensions import Unpack

from mypy_boto3_mediaconnect.client import MediaConnectClient
from mypy_boto3_mediaconnect.type_defs import (
    CreateFlowRequestRequestTypeDef,
    CreateFlowResponseTypeDef,
    DeleteFlowResponseTypeDef,
)


class MyMediaConnectClient:
    def __init__(self, client: Optional[MediaConnectClient] = None) -> None:
        # Typed MediaConnect client for boto3==1.34.7 (via mypy-boto3-mediaconnect stubs)
        self.client: MediaConnectClient = client or boto3.client("mediaconnect")

    def create_flow(
        self, **kwargs: Unpack[CreateFlowRequestRequestTypeDef]
    ) -> CreateFlowResponseTypeDef:
        """Create a new MediaConnect flow."""
        return self.client.create_flow(**kwargs)

    def delete_flow(self, FlowArn: str) -> DeleteFlowResponseTypeDef:
        """Delete a MediaConnect flow by ARN."""
        return self.client.delete_flow(FlowArn=FlowArn)


# Example of obtaining a typed client directly
mediaconnect: MediaConnectClient = boto3.client("mediaconnect")

# Or via the wrapper
typed_wrapper = MyMediaConnectClient(mediaconnect)
------------------------------------------------------------

PROMPT 37:
"Generate Python code that renders a view as a string and inserts it into a template, using Django's render_partial tag, like so:\n```\nfrom django import template\nfrom django.template import loader\nfrom django.http import HttpResponse\n\ndef my_view(request):\n   # Render the view as a string\n   output = ''\n   if request.method == 'POST':\n       # Do something based on the POST data\n       output += 'Hello, World!'\n   else:\n       output += 'Hello, World!'\n   return HttpResponse(output)\n\n# Register the view as a Django template tag\ndef render_partial(context, name, *args, **kwargs):\n   t = loader.get_template(name)\n   context.update({'content': t.render(context, *args, **kwargs)})\n   return HttpResponse(t.render(context, *args, **kwargs))\n\n# Use the tag in a template\n{% load render_partial %}\n\n<div>{{ content }}</div>\n\n# Render the template\nt = loader.get_template('my_template.html')\noutput"
RESPONSE:
Django doesn’t have a built‑in render_partial tag, but you can add one that renders a template fragment to a string and inserts it into another template.

1) Create a template tag
Place this in your_app/templatetags/render_partial.py

```
from django import template
from django.template.loader import render_to_string

register = template.Library()

@register.simple_tag(takes_context=True)
def render_partial(context, template_name, **kwargs):
    """
    Render `template_name` with the current context plus kwargs and return HTML.
    Usage:
      {% render_partial 'partials/hello.html' name='World' %}
      {% render_partial 'partials/hello.html' name=user.first_name as content %}
    """
    # Flatten the Context to a dict, then merge kwargs
    ctx = context.flatten() if hasattr(context, 'flatten') else dict(context)
    ctx.update(kwargs)
    return render_to_string(template_name, ctx, request=context.get('request'))
```

2) Example view
your_app/views.py

```
from django.shortcuts import render

def page(request):
    # No need to pre-render anything; the template tag will do it.
    return render(request, 'page.html')
```

3) Templates
templates/page.html

```
{% load render_partial %}

<h1>Main page</h1>

<!-- Direct output -->
<div class="slot">
  {% render_partial 'partials/hello.html' name='World' %}
</div>

<!-- Or capture to a variable -->
{% render_partial 'partials/hello.html' name='Alice' as content %}
<div class="slot-2">{{ content|safe }}</div>
```

templates/partials/hello.html

```
<p>Hello, {{ name }}!</p>
```

Alternative (render to string in the view instead of using a tag)

```
from django.shortcuts import render
from django.template.loader import render_to_string

def page(request):
    content = render_to_string('partials/hello.html', {'name': 'World'}, request=request)
    return render(request, 'page.html', {'content': content})
```

templates/page.html (alternative)

```
<h1>Main page</h1>
<div>{{ content|safe }}</div>
```
------------------------------------------------------------

PROMPT 38:
"Generate Python code that uses the Cloudflare API to start a tunnel from a Flask app, using the `requests` library to make HTTP requests to the Cloudflare API and the `Flask` framework to run the app:\n```\nimport requests\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef index():\n   # Start the Cloudflare tunnel\n   response = requests.post(\"https://api.cloudflare.com/client/v4/tunnels\", json={\"title\": \"My Tunnel\"})\n   if response.status_code == 201:\n       print(\"Tunnel started successfully\")\n   else:\n       print(\"Error starting tunnel:\", response.text)\n   return \"Hello World!\"\n\nif __name__ == \"__main__\":\n   app.run(debug=True)\n```\nThis code creates a Flask app that starts a tunnel using the Cloudflare API when the `/` route is accessed. The `requests` library is used to send a POST request to the Cloudflare API to create the tunnel, and the response is checked for errors."
RESPONSE:
Below is a working example that uses the proper Cloudflare API endpoints to create/configure a named Cloudflare Tunnel and then starts the connector (cloudflared) from a Flask route. It uses requests for the API calls and spawns cloudflared as a subprocess to actually establish the tunnel.

Notes:
- You must have cloudflared installed on the host running this app and accessible in PATH.
- Provide a Cloudflare API token with scopes: Account Cloudflare Tunnel: Edit. If you also want an automatic DNS record, add Zone DNS: Edit and provide ZONE_ID.
- Starting a tunnel on every request is not recommended; this example starts it once and reuses it.

Code:
```python
import os
import json
import base64
import secrets
import subprocess
import threading
import time
import requests
from flask import Flask, jsonify

# Environment variables required:
# CF_API_TOKEN: Cloudflare API token (with Account Cloudflare Tunnel: Edit; and optional Zone DNS: Edit)
# CF_ACCOUNT_ID: Cloudflare account ID
# TUNNEL_NAME: Desired tunnel name (e.g., "my-flask-tunnel")
# TUNNEL_HOSTNAME: Optional hostname (e.g., "app.example.com")
# CF_ZONE_ID: Optional if you want to auto-create DNS CNAME for TUNNEL_HOSTNAME

CF_API_TOKEN = os.environ.get("CF_API_TOKEN", "")
CF_ACCOUNT_ID = os.environ.get("CF_ACCOUNT_ID", "")
TUNNEL_NAME = os.environ.get("TUNNEL_NAME", "my-flask-tunnel")
TUNNEL_HOSTNAME = os.environ.get("TUNNEL_HOSTNAME")  # optional
CF_ZONE_ID = os.environ.get("CF_ZONE_ID")  # optional

API_BASE = "https://api.cloudflare.com/client/v4"
HDRS = {
    "Authorization": f"Bearer {CF_API_TOKEN}",
    "Content-Type": "application/json",
}

app = Flask(__name__)

_cloudflared_proc = None
_proc_lock = threading.Lock()
_cached_tunnel = None  # dict with id, name, etc.


def api_ok(resp):
    return resp.ok and resp.json().get("success", False)


def find_tunnel_by_name():
    # Try to list tunnels and find by name
    url = f"{API_BASE}/accounts/{CF_ACCOUNT_ID}/cfd_tunnel"
    # Paginate just in case
    page = 1
    per_page = 50
    while True:
        r = requests.get(url, headers=HDRS, params={"page": page, "per_page": per_page})
        if not api_ok(r):
            raise RuntimeError(f"Failed to list tunnels: {r.text}")
        result = r.json()["result"]
        for t in result:
            if t.get("name") == TUNNEL_NAME and not t.get("deleted_at"):
                return t
        if len(result) < per_page:
            break
        page += 1
    return None


def create_tunnel():
    # Cloudflare requires a 32-byte base64 secret for API-created tunnels
    secret_b64 = base64.b64encode(secrets.token_bytes(32)).decode()
    url = f"{API_BASE}/accounts/{CF_ACCOUNT_ID}/cfd_tunnel"
    payload = {"name": TUNNEL_NAME, "tunnel_secret": secret_b64}
    r = requests.post(url, headers=HDRS, data=json.dumps(payload))
    if api_ok(r):
        return r.json()["result"]
    # If tunnel already exists (name conflict), fall back to find existing
    # Cloudflare may return an error code; we simply try to find it
    existing = find_tunnel_by_name()
    if existing:
        return existing
    raise RuntimeError(f"Failed to create tunnel: {r.text}")


def configure_tunnel_ingress(tunnel_id, hostname, service_url):
    # Store ingress rules on Cloudflare so connectors pick up config
    # This maps the hostname to your local Flask app service
    url = f"{API_BASE}/accounts/{CF_ACCOUNT_ID}/cfd_tunnel/{tunnel_id}/configurations"
    payload = {
        "config": {
            "ingress": [
                {"hostname": hostname, "service": service_url},
                {"service": "http_status:404"},
            ]
        }
    }
    r = requests.put(url, headers=HDRS, data=json.dumps(payload))
    if not api_ok(r):
        raise RuntimeError(f"Failed to configure tunnel ingress: {r.text}")


def ensure_dns_cname(zone_id, hostname, tunnel_id):
    # Create/Upsert CNAME: hostname -> <TUNNEL_ID>.cfargotunnel.com
    # Check if record exists
    list_url = f"{API_BASE}/zones/{zone_id}/dns_records"
    params = {"type": "CNAME", "name": hostname}
    r = requests.get(list_url, headers=HDRS, params=params)
    if not api_ok(r):
        raise RuntimeError(f"Failed to list DNS records: {r.text}")
    existing = r.json()["result"]
    content = f"{tunnel_id}.cfargotunnel.com"
    if existing:
        rec_id = existing[0]["id"]
        put_url = f"{API_BASE}/zones/{zone_id}/dns_records/{rec_id}"
        upd = {"type": "CNAME", "name": hostname, "content": content, "proxied": True}
        rr = requests.put(put_url, headers=HDRS, data=json.dumps(upd))
        if not api_ok(rr):
            raise RuntimeError(f"Failed to update DNS record: {rr.text}")
    else:
        post_url = f"{API_BASE}/zones/{zone_id}/dns_records"
        newr = {"type": "CNAME", "name": hostname, "content": content, "proxied": True}
        rr = requests.post(post_url, headers=HDRS, data=json.dumps(newr))
        if not api_ok(rr):
            raise RuntimeError(f"Failed to create DNS record: {rr.text}")


def get_connector_token(tunnel_id):
    # Connector token for running cloudflared without a local credentials file
    url = f"{API_BASE}/accounts/{CF_ACCOUNT_ID}/cfd_tunnel/{tunnel_id}/token"
    r = requests.post(url, headers=HDRS)
    if not api_ok(r):
        raise RuntimeError(f"Failed to get connector token: {r.text}")
    return r.json()["result"]["token"]


def start_cloudflared(token):
    # Start cloudflared connector; leaves it running in the background
    # Requires cloudflared installed in PATH
    global _cloudflared_proc
    with _proc_lock:
        if _cloudflared_proc and _cloudflared_proc.poll() is None:
            return  # already running
        cmd = ["cloudflared", "tunnel", "--no-autoupdate", "run", "--token", token]
        _cloudflared_proc = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
        )
        # Optionally, detach or start a thread to drain logs to avoid blocking
        threading.Thread(target=_drain_logs, daemon=True).start()


def _drain_logs():
    if not _cloudflared_proc or not _cloudflared_proc.stdout:
        return
    for line in _cloudflared_proc.stdout:
        # You can write to a log file instead
        print(f"[cloudflared] {line}", end="")


def ensure_tunnel_running():
    global _cached_tunnel
    if not CF_API_TOKEN or not CF_ACCOUNT_ID:
        raise RuntimeError("CF_API_TOKEN and CF_ACCOUNT_ID must be set")

    # 1) Create or find the tunnel
    if not _cached_tunnel:
        _cached_tunnel = find_tunnel_by_name() or create_tunnel()

    tunnel_id = _cached_tunnel["id"]

    # 2) Optionally configure ingress + DNS for a hostname
    if TUNNEL_HOSTNAME:
        # Ingress maps cloud hostname to the local Flask app
        service_url = "http://localhost:5000"
        configure_tunnel_ingress(tunnel_id, TUNNEL_HOSTNAME, service_url)
        # Create/Update DNS CNAME (requires CF_ZONE_ID)
        if CF_ZONE_ID:
            ensure_dns_cname(CF_ZONE_ID, TUNNEL_HOSTNAME, tunnel_id)

    # 3) Fetch connector token and start the connector
    token = get_connector_token(tunnel_id)
    start_cloudflared(token)
    return {
        "tunnel_id": tunnel_id,
        "tunnel_name": _cached_tunnel.get("name"),
        "hostname": TUNNEL_HOSTNAME,
    }


@app.route("/")
def index():
    try:
        info = ensure_tunnel_running()
        return jsonify(
            {"message": "Hello World!", "tunnel": info, "status": "running"}
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    # Start Flask app; avoid debug reloader spawning multiple cloudflared processes
    app.run(host="0.0.0.0", port=5000, debug=False)
```

How to use:
- Install cloudflared and requests: pip install requests
- Export environment variables:
  - CF_API_TOKEN
  - CF_ACCOUNT_ID
  - TUNNEL_NAME
  - Optional: TUNNEL_HOSTNAME (e.g., app.example.com) and CF_ZONE_ID for DNS auto-creation.
- Run the Flask app. Visit / to create/configure the tunnel (once) and start the connector. Subsequent requests reuse the running connector.
------------------------------------------------------------

PROMPT 39:
"Generate Python code that imports the Scylla driver for Apache Cassandra and creates a connection to a Cassandra cluster.\n\nHere is an example of a package description for the Scylla driver for Apache Cassandra:\n\n\"The Scylla driver for Apache Cassandra is a library that allows you to connect to a Cassandra cluster from your Python applications. With this driver, you can execute CQL queries, create and manage tables, and perform other database operations. The driver supports both synchronous and asynchronous execution, allowing you to choose the programming model that best fits your use case.\"\n\nHere is a possible prompt that could generate Python code to accomplish the same tasks as the Scylla driver for Apache Cassandra:\n\n\"Write a Python program that uses the Scylla driver for Apache Cassandra to connect to a Cassandra cluster, create a table, insert data into the table, and retrieve the data back out.\"\n\nHere is some sample Python code that could accomplish these tasks:\n```\nimport scylla\n\n# Connect to a Cassandra cluster\ncluster = scylla.Cluster('localhost:9042')\n\n# Create a table\ntable = cluster.create_table('"
RESPONSE:
# pip install scylla-driver
# Note: Even with the Scylla driver installed, you import from the "cassandra" package.

from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider

def connect_to_cluster(contact_points=None, port=9042, username=None, password=None, keyspace=None):
    contact_points = contact_points or ["127.0.0.1"]
    auth = PlainTextAuthProvider(username=username, password=password) if username else None

    cluster = Cluster(contact_points=contact_points, port=port, auth_provider=auth)
    session = cluster.connect(keyspace) if keyspace else cluster.connect()
    return cluster, session

if __name__ == "__main__":
    # Adjust credentials, contact points, and keyspace as needed
    cluster, session = connect_to_cluster(
        contact_points=["127.0.0.1"],
        port=9042,
        username="cassandra",
        password="cassandra",
        keyspace=None,  # e.g., "my_keyspace"
    )

    # Quick test query to verify the connection
    row = session.execute("SELECT release_version FROM system.local").one()
    print("Connected. Release version:", row.release_version if row else "unknown")

    # Clean up when done
    cluster.shutdown()
------------------------------------------------------------

PROMPT 40:
"Generate Python code that creates a namespace in Azure Data Services using the `azure-data-services` library, specifying the namespace name and optional metadata description; then, create a table within the namespace using the `DataTable` class from the same library, defining the column names and data types, and finally, insert some sample data into the table."
RESPONSE:
I can’t find an Azure SDK for Python named “azure-data-services,” nor a “DataTable” class in the official Azure Python packages. Could you confirm the exact package name and service you’re targeting (e.g., Azure Data Explorer/Kusto, Azure Table Storage, Cosmos DB, etc.)?

If you meant one of the common services below, here are working examples:

Option A — Azure Data Explorer (Kusto) [closest match to “namespace + table with typed columns”]
- “Namespace” ≈ database
- “Table” with explicit schema
- Insert sample data via the ingestion client

Code:
# pip install azure-kusto-data azure-kusto-ingest pandas
from azure.kusto.data import KustoClient, KustoConnectionStringBuilder
from azure.kusto.ingest import QueuedIngestClient, IngestionProperties, DataFormat
import pandas as pd

# Set these
CLUSTER_URI = "https://<your-cluster>.kusto.windows.net"
DB_NAME = "my_namespace"          # acts like the namespace
TABLE_NAME = "MyTable"

# Authenticate (requires 'az login' in your environment)
kcsb = KustoConnectionStringBuilder.with_az_cli_authentication(CLUSTER_URI)

# Management client for control commands (create DB/table)
mgmt_client = KustoClient(kcsb)

# Create database (run at cluster scope; use a management DB name such as 'NetDefaultDB')
mgmt_client.execute_mgmt("NetDefaultDB", f'.create database ["{DB_NAME}"]')

# Create table with explicit schema
mgmt_client.execute_mgmt(DB_NAME, f'.create table {TABLE_NAME} (UserId:string, Age:int, CreatedOn:datetime)')

# Ingest sample data
ingest_client = QueuedIngestClient(kcsb)
df = pd.DataFrame(
    [
        {"UserId": "alice", "Age": 30, "CreatedOn": "2024-01-02T12:00:00Z"},
        {"UserId": "bob",   "Age": 25, "CreatedOn": "2024-01-03T15:30:00Z"},
    ]
)
ingestion_props = IngestionProperties(database=DB_NAME, table=TABLE_NAME, data_format=DataFormat.CSV)
ingest_client.ingest_from_dataframe(df, ingestion_properties=ingestion_props)

Option B — Azure Table Storage (azure-data-tables)
- Note: There’s no “namespace” concept or fixed schema; tables are schemaless. You can’t define data types up front. You just create a table and insert entities.

Code:
# pip install azure-data-tables
from azure.data.tables import TableServiceClient

# Set these
CONNECTION_STRING = "DefaultEndpointsProtocol=...;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net"
NAMESPACE_NAME = "mynamespace"  # not a real Tables concept; used here as a naming prefix only
TABLE_NAME = f"{NAMESPACE_NAME}Users"

service = TableServiceClient.from_connection_string(CONNECTION_STRING)
table_client = service.create_table_if_not_exists(TABLE_NAME)

# Insert sample entities (schemaless; PartitionKey and RowKey are required)
table_client.create_entity({"PartitionKey": "p1", "RowKey": "1", "Name": "Alice", "Age": 30})
table_client.create_entity({"PartitionKey": "p1", "RowKey": "2", "Name": "Bob",   "Age": 25})

If you actually have a different library (azure-data-services with a DataTable class), please share its package name/version or docs, and I’ll adapt the code exactly to that API.
------------------------------------------------------------

PROMPT 41:
"Generate Python code that imports the required modules and sets up type annotations for the boto3 GameLift 1.34.1 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import TypeVar, Callable\nfrom botocore.client import BaseClient\nfrom boto3.game_lift import GameLiftClient\nfrom mypy_boto3_builder.generators.game_lift import GameLiftTypedClient\n\n# Set up type annotations for the GameLift client\nclient: GameLiftClient = GameLiftTypedClient(client=BaseClient)\n```"
RESPONSE:
```python
# Requires:
#   pip install boto3==1.34.1
#   pip install "boto3-stubs[gamelift]==1.34.1"  # provides mypy_boto3_gamelift

from typing import TypeVar, Callable
import boto3
from botocore.client import BaseClient
from mypy_boto3_gamelift import GameLiftClient

# Typed GameLift client
client: GameLiftClient = boto3.client("gamelift")
```
------------------------------------------------------------

PROMPT 42:
"Generate Python code that imports the necessary libraries and uses TableLogger to log tabular data into a console or file, similar to how it's described in the package description.\n\nHere's an example of what the generated code might look like:\n```\nimport tablelogger\n\n# Create a logger\nlogger = tablelogger.TableLogger()\n\n# Log some sample data\ndata = [(\"Name\", \"Age\", \"Gender\"),\n       (\"Alice\", 25, \"Female\"),\n       (\"Bob\", 30, \"Male\")]\nlogger.log(data)\n\n# Log the data to a file\nwith open(\"data.txt\", \"w\") as f:\n   logger.log_to_file(f, data)\n```"
RESPONSE:
```python
# If you don't have the package installed:
# pip install tablelogger

try:
    import tablelogger
except ImportError as e:
    raise SystemExit("tablelogger is not installed. Install it with: pip install tablelogger") from e


def main():
    # Create a logger
    logger = tablelogger.TableLogger()

    # Sample tabular data (first row is treated as header)
    data = [
        ("Name", "Age", "Gender"),
        ("Alice", 25, "Female"),
        ("Bob", 30, "Male"),
        ("Charlie", 22, "Non-binary"),
    ]

    # Log to console
    logger.log(data)

    # Log to a file
    output_path = "data.txt"
    with open(output_path, "w", encoding="utf-8") as f:
        logger.log_to_file(f, data)

    print(f"\nTable written to {output_path}")


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 43:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.Comprehend 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom botocore.client import BotocoreClient\nfrom boto3.comprehend import ComprehendClient\nfrom boto3.comprehend.model import (\n   DocumentClassificationRequest,\n   DocumentClassificationResponse,\n   EntitiesDetectionRequest,\n   EntitiesDetectionResponse,\n   KeyPhrasesDetectionRequest,\n   KeyPhrasesDetectionResponse,\n   SentimentDetectionRequest,\n   SentimentDetectionResponse,\n)\n\n# Initialize the client\ncomprehend_client = ComprehendClient(\n   aws_access_key_id='YOUR_AWS_ACCESS_KEY',\n   aws_secret_access_key='YOUR_AWS_SECRET_KEY',\n   region='YOUR_AWS_REGION'\n)\n"
RESPONSE:
```python
from typing import List
import boto3
from botocore.client import BaseClient as BotocoreClient
from mypy_boto3_comprehend.client import ComprehendClient
from mypy_boto3_comprehend.type_defs import (
    ClassifyDocumentRequestRequestTypeDef as DocumentClassificationRequest,
    ClassifyDocumentResponseTypeDef as DocumentClassificationResponse,
    DetectEntitiesRequestRequestTypeDef as EntitiesDetectionRequest,
    DetectEntitiesResponseTypeDef as EntitiesDetectionResponse,
    DetectKeyPhrasesRequestRequestTypeDef as KeyPhrasesDetectionRequest,
    DetectKeyPhrasesResponseTypeDef as KeyPhrasesDetectionResponse,
    DetectSentimentRequestRequestTypeDef as SentimentDetectionRequest,
    DetectSentimentResponseTypeDef as SentimentDetectionResponse,
)

# Initialize the client
session = boto3.session.Session(
    aws_access_key_id="YOUR_AWS_ACCESS_KEY",
    aws_secret_access_key="YOUR_AWS_SECRET_KEY",
    region_name="YOUR_AWS_REGION",
)
comprehend_client: ComprehendClient = session.client("comprehend")

# Optional: show compatibility with botocore BaseClient typing
botocore_client: BotocoreClient = comprehend_client
```
------------------------------------------------------------

PROMPT 44:
"Generate Python code that imports the required modules and annotates the types for the MediaStoreData service in boto3 version 1.34.0 using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import List, Union\nfrom boto3.client import Boto3Client\nfrom boto3.media_store import MediaStoreData\n\n# Define type annotations for MediaStoreData service\nclass MediaStoreData(Boto3Client):\n   def list_objects(self, Bucket: str, Prefix: str = None, Marker: str = None, MaxKeys: int = None) -> List[Union[dict, MediaStoreData]]:\n       return self.get_object_metadata(Bucket, Prefix, Marker, MaxKeys)\n\n   def get_object_metadata(self, Bucket: str, Prefix: str = None, Marker: str = None, MaxKeys: int = None) -> dict:\n       return self.head_object(Bucket, Prefix, Marker)\n\n   def head_object(self, Bucket: str"
RESPONSE:
# Requires:
#   boto3==1.34.0
#   boto3-stubs[mediastore-data]==1.34.0  (generated by mypy-boto3-builder 7.21.0)

from __future__ import annotations

import boto3
from mypy_boto3_mediastore_data.client import MediaStoreDataClient


def get_mediastore_data_client() -> MediaStoreDataClient:
    return boto3.client("mediastore-data")


def example_usage(client: MediaStoreDataClient) -> None:
    # List items in the root path
    list_resp = client.list_items(Path="/", MaxResults=100)
    for item in list_resp.get("Items", []):
        name = item.get("Name")
        _ = name  # use as needed

    # Put an object
    put_resp = client.put_object(
        Path="assets/hello.txt",
        Body=b"hello, world",
        ContentType="text/plain",
        CacheControl="max-age=60",
    )

    # Describe (HEAD) an object
    head_resp = client.describe_object(Path="assets/hello.txt")

    # Get an object
    get_resp = client.get_object(Path="assets/hello.txt")
    body_bytes = get_resp["Body"].read()
    _ = (put_resp, head_resp, body_bytes)  # use as needed

    # Delete the object
    client.delete_object(Path="assets/hello.txt")


if __name__ == "__main__":
    client: MediaStoreDataClient = get_mediastore_data_client()
    example_usage(client)
------------------------------------------------------------

PROMPT 45:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.SageMaker 1.34.13 service using mypy-boto3-builder 7.23.0, like this:\n```python\nfrom typing import TypeVar, Type Alias\nfrom botocore.client import BaseClient\nfrom sagemaker import SageMaker\n\nSMClient = TypeVar('SMClient', bound=BaseClient)\n\nclass SageMakerClient(SageMaker):\n   def __init__(self, *args, **kwargs):\n       super().__init__(**kwargs)\n       self.config = kwargs['config']\n       self.sagemaker_runtime = kwargs['sagemaker_runtime']\n\n   def create_training_job(self, *args, **kwargs):\n       return super().create_training_job(*args, **kwargs)\n\n   def create_deployment(self, *args, **kwargs):\n       return super().create_deployment(*args, **kwargs)\n\n   def delete_model(self, *args, **kwargs):\n       return"
RESPONSE:
```python
from __future__ import annotations

# Requires:
#   pip install boto3 mypy-boto3-sagemaker mypy-boto3-sagemaker-runtime

from typing import TypeVar, TypeAlias

import boto3
from botocore.client import BaseClient
from botocore.config import Config

from mypy_boto3_sagemaker import SageMakerClient
from mypy_boto3_sagemaker.type_defs import (
    CreateTrainingJobRequestRequestTypeDef,
    CreateTrainingJobResponseTypeDef,
    EmptyResponseMetadataTypeDef,
)
from mypy_boto3_sagemaker_runtime import SageMakerRuntimeClient

SMClient: TypeAlias = SageMakerClient
SMRuntimeClient: TypeAlias = SageMakerRuntimeClient
BaseSMClient = TypeVar("BaseSMClient", bound=BaseClient)


class TypedSageMaker:
    def __init__(
        self,
        *,
        config: Config | None = None,
        sagemaker: SageMakerClient | None = None,
        sagemaker_runtime: SageMakerRuntimeClient | None = None,
    ) -> None:
        self.config: Config | None = config
        self.sagemaker: SageMakerClient = sagemaker or boto3.client("sagemaker", config=config)
        self.sagemaker_runtime: SageMakerRuntimeClient = sagemaker_runtime or boto3.client(
            "sagemaker-runtime", config=config
        )

    def create_training_job(
        self,
        params: CreateTrainingJobRequestRequestTypeDef,
    ) -> CreateTrainingJobResponseTypeDef:
        return self.sagemaker.create_training_job(**params)

    def delete_model(self, *, ModelName: str) -> EmptyResponseMetadataTypeDef:
        return self.sagemaker.delete_model(ModelName=ModelName)
```
------------------------------------------------------------

PROMPT 46:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 CloudFront client, version 1.34.0, using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import TypeVar\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import BotoflowTypes\n\nCF_Client = TypeVar('CF_Client', bound='botocore.client.BaseClient')\n\nclass CloudFront(BaseClient):\n   def __init__(self, *args, **kwargs) -> None:\n       super().__init__(*args, **kwargs)\n       self.types = BotoflowTypes(self)\n\n   def list_distributions(self, *args, **kwargs) -> List['Distribution']:\n       return self.types.list_distributions(self, *args, **kwargs)\n\n   def create_distribution(self, *args, **kwargs) -> 'Distribution':\n       return self.types.create_distribution(self, *args, **kwargs)\n\n   # ... add other methods"
RESPONSE:
```python
# Requires:
#   boto3==1.34.0
#   botocore==1.34.0
#   mypy-boto3-cloudfront==1.34.0  (generated by mypy-boto3-builder v7.21.0)
#
# pip install "boto3==1.34.0" "botocore==1.34.0" "mypy-boto3-cloudfront==1.34.0"

from __future__ import annotations

from typing import Optional

import boto3
from mypy_boto3_cloudfront.client import CloudFrontClient
from mypy_boto3_cloudfront.paginator import ListDistributionsPaginator
from mypy_boto3_cloudfront.type_defs import (
    CreateDistributionResultTypeDef,
    DistributionConfigTypeDef,
    GetDistributionResultTypeDef,
    InvalidationBatchTypeDef,
    CreateInvalidationResultTypeDef,
    ListDistributionsResultTypeDef,
)


def get_typed_cloudfront_client() -> CloudFrontClient:
    """
    Create a typed CloudFront client.
    """
    return boto3.client("cloudfront")


class CloudFront:
    """
    A thin, type-annotated wrapper around the boto3 CloudFront client.
    """

    def __init__(self, client: Optional[CloudFrontClient] = None) -> None:
        self.client: CloudFrontClient = client or get_typed_cloudfront_client()

    def list_distributions(
        self,
        *,
        Marker: Optional[str] = None,
        MaxItems: Optional[int] = None,
    ) -> ListDistributionsResultTypeDef:
        kwargs: dict = {}
        if Marker is not None:
            kwargs["Marker"] = Marker
        if MaxItems is not None:
            kwargs["MaxItems"] = MaxItems
        return self.client.list_distributions(**kwargs)

    def create_distribution(
        self,
        *,
        DistributionConfig: DistributionConfigTypeDef,
    ) -> CreateDistributionResultTypeDef:
        return self.client.create_distribution(DistributionConfig=DistributionConfig)

    def get_distribution(self, *, Id: str) -> GetDistributionResultTypeDef:
        return self.client.get_distribution(Id=Id)

    def create_invalidation(
        self,
        *,
        DistributionId: str,
        InvalidationBatch: InvalidationBatchTypeDef,
    ) -> CreateInvalidationResultTypeDef:
        return self.client.create_invalidation(
            DistributionId=DistributionId,
            InvalidationBatch=InvalidationBatch,
        )

    def paginator_list_distributions(self) -> ListDistributionsPaginator:
        return self.client.get_paginator("list_distributions")

    # ... add other typed methods as needed, following the signatures from mypy_boto3_cloudfront
    # For example:
    # def delete_distribution(self, *, Id: str, IfMatch: str) -> Dict[str, Any]: ...
    # def update_distribution(self, *, DistributionConfig: DistributionConfigTypeDef, Id: str, IfMatch: str) -> UpdateDistributionResultTypeDef: ...
```
------------------------------------------------------------

PROMPT 47:
"Generate Python code that creates a pytest plugin that limits the output to only the necessary information by using the `pytest` and `limat` modules, like so:\n```\nimport pytest\nfrom limat import Limat\n\ndef my_plugin(limit):\n   lim = Limat(limit)\n   def my_fixture(func):\n       def wrapper(*args, **kwargs):\n           return func(*args, **kwargs)\n       return wrapper\n   return my_fixture\n\n@pytest.fixture\ndef my_limiter(request):\n   return my_plugin(request.config.getoption('--limit'))\n\n@pytest.mark.parametrize('my_input', [1, 2, 3])\ndef test_my_function(my_limiter, my_input):\n   # Your test function here\n```\nThis code defines a pytest plugin called `my_plugin`, which takes a limit parameter and uses it to create a `Limat` instance. The `my_fixture` function is then defined inside the plugin, which wraps the test function in a `wrapper` function that limits the output"
RESPONSE:
# conftest.py
import inspect
from functools import wraps

import pytest
from limat import Limat


def my_plugin(limit):
    lim = Limat(limit) if limit is not None else None

    def my_fixture(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            if lim is None:
                return func(*args, **kwargs)

            # Prefer context manager API if Limat supports it
            if hasattr(lim, "__enter__") and hasattr(lim, "__exit__"):
                with lim:
                    return func(*args, **kwargs)

            # Fallback to start/stop style API if provided
            start = getattr(lim, "start", None)
            stop = getattr(lim, "stop", None)
            if callable(start):
                start()
            try:
                return func(*args, **kwargs)
            finally:
                if callable(stop):
                    stop()

        return wrapper

    return my_fixture


def pytest_addoption(parser):
    group = parser.getgroup("limat")
    group.addoption(
        "--limit",
        action="store",
        type=int,
        default=None,
        help="Limit test output using limat (e.g. --limit=1000).",
    )


@pytest.fixture
def my_limiter(request):
    return my_plugin(request.config.getoption("--limit"))


def pytest_pyfunc_call(pyfuncitem):
    """
    Wrap each test function call with the Limat-based wrapper if --limit is set.
    Returning True signals pytest that we've executed the call ourselves.
    """
    limit = pyfuncitem.config.getoption("--limit")
    if not limit:
        return  # let pytest handle the call normally

    decorator = my_plugin(limit)
    func = pyfuncitem.obj
    wrapped = decorator(func)

    # Build kwargs for the function call from the resolved fixtures/parameters
    sig = inspect.signature(func)
    kwargs = {name: pyfuncitem.funcargs[name] for name in sig.parameters.keys()}

    wrapped(**kwargs)
    return True
------------------------------------------------------------

PROMPT 48:
"Generate Python code that defines a module with core functionalities for data type definition, data I/O, and frequently used functions, including a `data_types` dictionary for defining custom data types, a `read_csv()` function for reading CSV files, a `write_csv()` function for writing CSV files, and a `mean()` function for calculating the mean of a list."
RESPONSE:
# core_utils.py

from __future__ import annotations

import csv
import math
from dataclasses import is_dataclass, asdict
from datetime import date, datetime
from decimal import Decimal, InvalidOperation
from typing import Any, Callable, Dict, Iterable, Iterator, List, Mapping, MutableMapping, Optional, Sequence, Tuple, Union

# Public registry of named data type converters (string -> Python object)
# You can extend this with register_data_type('my_type', my_parser)
data_types: Dict[str, Callable[[str], Any]] = {}


def _str_to_bool(s: str) -> bool:
    truthy = {"1", "true", "t", "yes", "y", "on"}
    falsy = {"0", "false", "f", "no", "n", "off"}
    s_norm = s.strip().lower()
    if s_norm in truthy:
        return True
    if s_norm in falsy:
        return False
    raise ValueError(f"Cannot parse boolean from: {s!r}")


def _parse_date(s: str) -> date:
    # Accept ISO-like formats: YYYY-MM-DD
    return datetime.strptime(s.strip(), "%Y-%m-%d").date()


def _parse_datetime(s: str) -> datetime:
    # Try ISO 8601; fallback to common formats
    s = s.strip()
    try:
        # Python 3.7+: handles many ISO combinations
        return datetime.fromisoformat(s)
    except ValueError:
        # Fallbacks
        fmts = [
            "%Y-%m-%d %H:%M:%S",
            "%Y-%m-%d %H:%M:%S.%f",
            "%Y/%m/%d %H:%M:%S",
            "%Y/%m/%d %H:%M:%S.%f",
            "%Y-%m-%dT%H:%M:%S",
            "%Y-%m-%dT%H:%M:%S.%f",
        ]
        for fmt in fmts:
            try:
                return datetime.strptime(s, fmt)
            except ValueError:
                pass
        raise


def _parse_decimal(s: str) -> Decimal:
    try:
        return Decimal(s.strip())
    except (InvalidOperation, AttributeError) as e:
        raise ValueError(f"Cannot parse Decimal from: {s!r}") from e


# Initialize the default registry
data_types.update(
    {
        "int": int,
        "float": float,
        "str": str,
        "bool": _str_to_bool,
        "date": _parse_date,
        "datetime": _parse_datetime,
        "decimal": _parse_decimal,
    }
)


def register_data_type(name: str, parser: Callable[[str], Any]) -> None:
    """
    Register a custom data type converter.

    name: Identifier for the type (used in read_csv types mapping).
    parser: Callable converting a string to a Python object.
    """
    if not name or not isinstance(name, str):
        raise ValueError("name must be a non-empty string")
    if not callable(parser):
        raise ValueError("parser must be callable")
    data_types[name] = parser


TypeSpec = Union[str, Callable[[str], Any], type]
SchemaMapping = Mapping[Union[str, int], TypeSpec]
SchemaSequence = Sequence[TypeSpec]


def _resolve_type(spec: TypeSpec) -> Callable[[str], Any]:
    """Resolve a TypeSpec to a callable parser."""
    if isinstance(spec, str):
        if spec not in data_types:
            raise KeyError(f"Unknown data type {spec!r}. Register it with register_data_type().")
        return data_types[spec]
    if callable(spec):
        return spec
    raise TypeError(f"Invalid type spec: {spec!r}")


def _coerce_value(raw: Optional[str], spec: Optional[TypeSpec], na_values: Tuple[str, ...]) -> Any:
    """Convert a raw CSV string using a type spec; returns None for NA values."""
    if raw is None:
        return None
    if isinstance(raw, str) and raw.strip() in na_values:
        return None
    if spec is None:
        return raw
    parser = _resolve_type(spec)
    return parser(raw)


def _serialize_value(val: Any) -> str:
    """Serialize Python values to CSV-friendly strings."""
    if val is None:
        return ""
    if isinstance(val, (str, int, float)):
        # Represent NaN and Infinity explicitly
        if isinstance(val, float):
            if math.isnan(val):
                return "NaN"
            if math.isinf(val):
                return "Infinity" if val > 0 else "-Infinity"
        return str(val)
    if isinstance(val, bool):
        return "true" if val else "false"
    if isinstance(val, Decimal):
        return format(val, "f")
    if isinstance(val, datetime):
        return val.isoformat()
    if isinstance(val, date):
        return val.isoformat()
    if is_dataclass(val):
        return str(asdict(val))
    return str(val)


def read_csv(
    path: str,
    *,
    delimiter: str = ",",
    encoding: str = "utf-8",
    has_header: bool = True,
    fieldnames: Optional[Sequence[str]] = None,
    types: Optional[Union[SchemaMapping, SchemaSequence]] = None,
    na_values: Tuple[str, ...] = ("", "NA", "N/A", "NULL", "null", "None"),
) -> List[Union[Dict[str, Any], List[Any]]]:
    """
    Read a CSV file with optional type coercion.

    path: File path.
    delimiter: Field delimiter.
    encoding: File encoding.
    has_header: Whether the first row is a header.
    fieldnames: Column names if file has no header. If provided with has_header=False,
                rows are returned as dicts; otherwise as lists.
    types: Type schema for coercion.
           - If rows are dicts: mapping of column name -> TypeSpec
           - If rows are lists: mapping of column index (int) -> TypeSpec
           - Or a sequence aligned by column index (for list rows without header)
    na_values: Strings to treat as missing (converted to None).

    Returns:
        List of rows (dicts if header or fieldnames supplied; otherwise lists).
    """
    results: List[Union[Dict[str, Any], List[Any]]] = []
    with open(path, mode="r", encoding=encoding, newline="") as f:
        if has_header:
            reader = csv.DictReader(f, delimiter=delimiter)
            for row in reader:
                coerced: Dict[str, Any] = {}
                for k, v in row.items():
                    spec = None
                    if isinstance(types, Mapping):
                        spec = types.get(k)
                    coerced[k] = _coerce_value(v, spec, na_values)
                results.append(coerced)
        else:
            if fieldnames:
                reader = csv.DictReader(f, delimiter=delimiter, fieldnames=list(fieldnames))
                for row in reader:
                    coerced = {}
                    for k, v in row.items():
                        spec = None
                        if isinstance(types, Mapping):
                            spec = types.get(k)
                        elif isinstance(types, Sequence):
                            # Try index lookup based on fieldnames order
                            try:
                                idx = fieldnames.index(k)
                                spec = types[idx] if idx < len(types) else None
                            except ValueError:
                                spec = None
                        coerced[k] = _coerce_value(v, spec, na_values)
                    results.append(coerced)
            else:
                reader = csv.reader(f, delimiter=delimiter)
                for row in reader:
                    coerced_row: List[Any] = []
                    for i, v in enumerate(row):
                        spec = None
                        if isinstance(types, Mapping):
                            spec = types.get(i)
                        elif isinstance(types, Sequence) and i < len(types):
                            spec = types[i]
                        coerced_row.append(_coerce_value(v, spec, na_values))
                    results.append(coerced_row)
    return results


def write_csv(
    path: str,
    rows: Iterable[Union[Mapping[str, Any], Sequence[Any]]],
    *,
    fieldnames: Optional[Sequence[str]] = None,
    delimiter: str = ",",
    encoding: str = "utf-8",
    write_header: bool = True,
) -> None:
    """
    Write rows to a CSV file.

    rows: Iterable of dict-like rows (preferred) or sequences (lists/tuples).
    fieldnames:
        - For dict rows: optional explicit column order. If not provided,
          fieldnames are taken from the first row's keys.
        - For sequence rows: optional header to write when write_header=True.
    delimiter: Field delimiter.
    encoding: File encoding.
    write_header: Whether to write a header row (for dict rows, or when fieldnames provided).
    """
    iterator = iter(rows)
    try:
        first = next(iterator)
    except StopIteration:
        # No data; write just the header if provided
        with open(path, mode="w", encoding=encoding, newline="") as f:
            if fieldnames and write_header:
                writer = csv.writer(f, delimiter=delimiter)
                writer.writerow(list(fieldnames))
        return

    def chain_first(it_first: Any, it_rest: Iterator[Any]) -> Iterator[Any]:
        yield it_first
        yield from it_rest

    it = chain_first(first, iterator)

    is_mapping = isinstance(first, Mapping)

    with open(path, mode="w", encoding=encoding, newline="") as f:
        if is_mapping:
            # Determine fieldnames
            if fieldnames is None:
                fieldnames = list(first.keys())
            writer = csv.DictWriter(
                f, fieldnames=list(fieldnames), delimiter=delimiter, extrasaction="ignore"
            )
            if write_header:
                writer.writeheader()
            for row in it:
                if not isinstance(row, Mapping):
                    raise TypeError("All rows must be mappings when the first row is a mapping.")
                # Serialize values
                serial_row: Dict[str, Any] = {}
                for k in fieldnames:
                    v = row.get(k, None)
                    serial_row[k] = _serialize_value(v)
                writer.writerow(serial_row)
        else:
            writer = csv.writer(f, delimiter=delimiter)
            if fieldnames and write_header:
                writer.writerow(list(fieldnames))
            for row in it:
                if not isinstance(row, Sequence) or isinstance(row, (str, bytes)):
                    raise TypeError("Rows must be sequences (non-string) when not using dict rows.")
                writer.writerow([_serialize_value(v) for v in row])


Number = Union[int, float, Decimal]


def mean(values: Iterable[Optional[Number]], *, ignore_none: bool = True, ignore_nan: bool = True) -> Number:
    """
    Compute the arithmetic mean.

    values: Iterable of numbers (int, float, Decimal). None values can be ignored.
    ignore_none: If True, None values are skipped; otherwise causes ValueError.
    ignore_nan: If True, NaN values are skipped; otherwise causes ValueError.

    Returns:
        Mean as float or Decimal (Decimal if any Decimal in the input).

    Raises:
        ValueError if no valid numeric values remain after filtering.
    """
    vals: List[Number] = []
    saw_decimal = False

    for v in values:
        if v is None:
            if ignore_none:
                continue
            else:
                raise ValueError("Encountered None in values and ignore_none=False")
        if isinstance(v, Decimal):
            saw_decimal = True
            if v.is_nan():
                if ignore_nan:
                    continue
                else:
                    raise ValueError("Encountered NaN in values and ignore_nan=False")
            vals.append(v)
        elif isinstance(v, (int, float)):
            if isinstance(v, float) and math.isnan(v):
                if ignore_nan:
                    continue
                else:
                    raise ValueError("Encountered NaN in values and ignore_nan=False")
            vals.append(v)
        else:
            raise TypeError(f"Unsupported value type: {type(v).__name__}")

    if not vals:
        raise ValueError("No numeric values to compute mean.")

    if saw_decimal:
        # Convert all to Decimal for consistent precision
        dec_vals = []
        for v in vals:
            if isinstance(v, Decimal):
                dec_vals.append(v)
            elif isinstance(v, int):
                dec_vals.append(Decimal(v))
            else:
                # Convert float to Decimal via string to limit binary artifacts
                dec_vals.append(Decimal(str(v)))
        total = sum(dec_vals, start=Decimal(0))
        return total / Decimal(len(dec_vals))
    else:
        total = sum(vals)  # type: ignore[arg-type]
        return total / len(vals)


__all__ = [
    "data_types",
    "register_data_type",
    "read_csv",
    "write_csv",
    "mean",
]
------------------------------------------------------------

PROMPT 49:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.IdentityStore 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import Optional\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import BotocoreClientBuilder\n\nidentity_store_client = BotocoreClientBuilder(\n   client_name=' IdentityStore',\n   client_module_path='boto3.client',\n   service=boto3.IdentityStore,\n   version='1.34.0',\n).build()\n```"
RESPONSE:
# Typed IdentityStore client using stubs generated by mypy-boto3-builder 7.21.0
# Requires matching stubs package (e.g., boto3-stubs[identitystore]==1.34.0 or mypy-boto3-identitystore)
from typing import TYPE_CHECKING, cast
import boto3

if TYPE_CHECKING:
    from mypy_boto3_identitystore import IdentityStoreClient

identity_store_client = cast("IdentityStoreClient", boto3.client("identitystore"))
------------------------------------------------------------

PROMPT 50:
"Generate Python code that imports the required modules and annotates the types for the boto3.Mobile 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom boto3.mobile import MobileClient\n\n# Annotate the types for the boto3.Mobile 1.34.0 service\nclient: MobileClient = MobileClient(\n   aws_access_key_id: str,\n   aws_secret_access_key: str,\n   aws_session_token: str,\n)\n\n# Define a function that takes a list of dicts as input and returns a list of dicts\ndef process_data(data: List[dict]) -> List[dict]:\n   # Process the data here\n   return modified_data\n```"
RESPONSE:
from typing import List
import boto3
from mypy_boto3_mobile.client import MobileClient

# Provide your AWS credentials (or rely on environment/config)
aws_access_key_id: str = "YOUR_ACCESS_KEY_ID"
aws_secret_access_key: str = "YOUR_SECRET_ACCESS_KEY"
aws_session_token: str = "YOUR_SESSION_TOKEN"

# Annotate the types for the boto3 Mobile client (boto3 1.34.0, mypy-boto3-builder 7.21.0)
client: MobileClient = boto3.client(
    "mobile",
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    aws_session_token=aws_session_token,
)

# Define a function that takes a list of dicts as input and returns a list of dicts
def process_data(data: List[dict]) -> List[dict]:
    # Process the data here
    modified_data: List[dict] = []
    for item in data:
        modified_item = dict(item)  # make a shallow copy or modify as needed
        modified_data.append(modified_item)
    return modified_data
------------------------------------------------------------

PROMPT 51:
"Generate Python code that uses the `django-objects-to-fixed-format` package to convert Django models to fixed format records, and the `pandas` library to convert objects to pandas DataFrames, like so:\n```\nimport django\nfrom django_objects_to_fixed_format import DjangoObjectsToFixedFormat\nimport pandas as pd\n\n# Initialize the converter\nconverter = DjangoObjectsToFixedFormat()\n\n# Convert a Django model to a fixed format record\nmodel_instance = MyModel.objects.first()\nfixed_record = converter.convert(model_instance)\n\n# Convert an object to a pandas DataFrame\nobject = MyObject.objects.first()\ndf = pd.DataFrame(object)\n```"
RESPONSE:
Below is a complete, runnable example showing how to:

- initialize Django in a standalone script,
- use django-objects-to-fixed-format to convert a Django model instance (or a queryset) into fixed-width records, and
- convert Django objects to pandas DataFrames correctly.

Note: Replace myproject.settings, myapp.models, MyModel, and MyObject with your actual project/app/model names.

```python
# pip install django django-objects-to-fixed-format pandas

import os
import django

# If running as a standalone script (outside manage.py/django shell), set up Django
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "myproject.settings")
django.setup()

from django_objects_to_fixed_format import DjangoObjectsToFixedFormat
from django.forms.models import model_to_dict
from myapp.models import MyModel, MyObject
import pandas as pd

# Initialize the converter
# If your package requires specific field widths/specs, configure them here per its docs.
converter = DjangoObjectsToFixedFormat()

# --- Convert a single Django model instance to a fixed format record ---
model_instance = MyModel.objects.first()
if model_instance is not None:
    fixed_record = converter.convert(model_instance)  # library-specific convert method
    print("Single fixed-width record:", fixed_record)

# --- Convert a queryset to multiple fixed format records ---
qs_models = MyModel.objects.all()
fixed_records = [converter.convert(obj) for obj in qs_models]
for line in fixed_records:
    print(line)

# --- Convert a single Django object to a pandas DataFrame ---
# Using model_to_dict to construct a row so DataFrame understands it
my_object = MyObject.objects.first()
if my_object is not None:
    df_single = pd.DataFrame([model_to_dict(my_object)])
    print("Single-object DataFrame:")
    print(df_single)

# --- Convert a queryset to a pandas DataFrame ---
# Using values() to get a list of dicts suitable for DataFrame construction
qs_objects = MyObject.objects.all()
df = pd.DataFrame.from_records(qs_objects.values())
print("QuerySet DataFrame:")
print(df)
```

Notes:
- Avoid naming variables object since it shadows Python’s built-in name.
- For pandas conversion, use model_to_dict for a single instance or queryset.values() for multiple rows.
- If django-objects-to-fixed-format requires schema/field specs (widths, alignments), configure them on the converter per the package’s documentation before calling convert.
------------------------------------------------------------

PROMPT 52:
"Generate Python code that imports the required modules and annotates the types of the boto3.CustomerProfiles 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```\nfrom typing import List, Dict\nfrom boto3.customer_profiles import CustomerProfile, CreateCustomerProfileRequest\n\n# Annotate the types of the boto3.CustomerProfiles service\ncustomer_profile: CustomerProfile = CustomerProfile(\n   customer_name=\"John Doe\",\n   email=\"johndoe@example.com\",\n   phone_number=\"555-1234\"\n)\n\ncreate_customer_profile_request: CreateCustomerProfileRequest = CreateCustomerProfileRequest(\n   customer_profile=customer_profile\n)\n```"
RESPONSE:
from boto3.session import Session
from mypy_boto3_customer_profiles.client import CustomerProfilesClient
from mypy_boto3_customer_profiles.type_defs import (
    AddressTypeDef,
    CreateProfileRequestRequestTypeDef,
    CreateProfileResponseTypeDef,
)

# Create a typed Customer Profiles client
session = Session()
client: CustomerProfilesClient = session.client("customer-profiles")

# Build typed structures for create_profile
shipping_address: AddressTypeDef = {
    "Address1": "123 Main St",
    "City": "Springfield",
    "Country": "US",
    # Optional fields:
    # "Address2": "Apt 4B",
    # "State": "IL",
    # "PostalCode": "62701",
}

create_profile_request: CreateProfileRequestRequestTypeDef = {
    "DomainName": "my-domain",
    "FirstName": "John",
    "LastName": "Doe",
    "EmailAddress": "johndoe@example.com",
    "PhoneNumber": "555-1234",
    "ShippingAddress": shipping_address,
}

# Call the API with precise type annotations
response: CreateProfileResponseTypeDef = client.create_profile(**create_profile_request)
profile_id: str = response["ProfileId"]

print(f"Created profile ID: {profile_id}")
------------------------------------------------------------

PROMPT 53:
"Generate Python code that imports the required modules and annotates the types for the boto3.LocationService 1.34.18 service using mypy-boto3-builder 7.23.1, like this:\n```python\nfrom typing import List, Dict\nfrom botocore.client import BaseClient\nfrom boto3.location_service import LocationService\n\n# Annotate the types for the LocationService client\nclient: LocationService = BaseClient(\n   servicename=\"location\",\n   api_version=\"1.34.18\"\n)\n\n# Define a function to get the location data\ndef get_location_data(location_id: str) -> List[Dict]:\n   response = client.get_location(location_id=location_id)\n   return response[\"locations\"]\n```"
RESPONSE:
from typing import List
import boto3

# Type hints generated by mypy-boto3-builder 7.23.1 for Amazon Location Service
from mypy_boto3_location.client import LocationServiceClient
from mypy_boto3_location.type_defs import (
    PlaceTypeDef,
    SearchForTextResultItemTypeDef,
)

# Strongly-typed Amazon Location Service client
client: LocationServiceClient = boto3.client("location")


def get_place(index_name: str, place_id: str) -> PlaceTypeDef:
    """
    Retrieve a single place by its PlaceId from the specified Place Index.
    """
    response = client.get_place(IndexName=index_name, PlaceId=place_id)
    return response["Place"]


def search_places_by_text(
    index_name: str,
    text: str,
    max_results: int = 10,
) -> List[SearchForTextResultItemTypeDef]:
    """
    Search a Place Index for a free-form text query.
    """
    response = client.search_place_index_for_text(
        IndexName=index_name,
        Text=text,
        MaxResults=max_results,
    )
    return response["Results"]
------------------------------------------------------------

PROMPT 54:
"Generate Python code that imports the necessary modules and annotates the OpsWorksCM 1.34.0 service with type hints using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nimport boto3\nfrom mypy_boto3_builder import TypeAnnotator\n\nopsworks_cm = boto3.client('opsworkscm')\n\n@TypeAnnotator(module_name='boto3', client=opsworks_cm)\nclass OpsWorksCMTypedClient:\n   def __init__(self) -> None:\n       self.client = opsworks_cm\n\n   def create_app(self, app_name: str, app_type: str) -> dict:\n       return self.client.create_app(app_name, app_type)\n\n   def delete_app(self, app_id: str) -> None:\n       return self.client.delete_app(app_id)\n\n   def update_app(self, app_id: str, app_type: str) -> None:"
RESPONSE:
```python
from typing import Any, Dict, Optional, Sequence
import boto3

# mypy-boto3-builder 7.21.0 generates the "mypy_boto3_opsworkscm" stubs package.
# Install the corresponding boto3-stubs extras to use these types:
# pip install "boto3-stubs[opsworkscm]"
from mypy_boto3_opsworkscm import OpsWorksCMClient
from mypy_boto3_opsworkscm.type_defs import (
    EngineAttributeTypeDef,
    TagTypeDef,
)

# Strongly-typed OpsWorksCM client
opsworks_cm: OpsWorksCMClient = boto3.client("opsworkscm")


class OpsWorksCMTypedClient:
    def __init__(self, client: Optional[OpsWorksCMClient] = None) -> None:
        self.client: OpsWorksCMClient = client or opsworks_cm

    def create_server(
        self,
        *,
        Engine: str,
        ServerName: str,
        InstanceProfileArn: str,
        InstanceType: str,
        ServiceRoleArn: str,
        KeyPair: Optional[str] = None,
        PreferredMaintenanceWindow: Optional[str] = None,
        PreferredBackupWindow: Optional[str] = None,
        SecurityGroupIds: Optional[Sequence[str]] = None,
        SubnetIds: Optional[Sequence[str]] = None,
        BackupRetentionCount: Optional[int] = None,
        CustomDomain: Optional[str] = None,
        CustomCertificate: Optional[str] = None,
        CustomPrivateKey: Optional[str] = None,
        EngineModel: Optional[str] = None,
        EngineVersion: Optional[str] = None,
        EngineAttributes: Optional[Sequence[EngineAttributeTypeDef]] = None,
        DisableAutomatedBackup: Optional[bool] = None,
        Tags: Optional[Sequence[TagTypeDef]] = None,
    ) -> Dict[str, Any]:
        return self.client.create_server(
            Engine=Engine,
            ServerName=ServerName,
            InstanceProfileArn=InstanceProfileArn,
            InstanceType=InstanceType,
            ServiceRoleArn=ServiceRoleArn,
            KeyPair=KeyPair,
            PreferredMaintenanceWindow=PreferredMaintenanceWindow,
            PreferredBackupWindow=PreferredBackupWindow,
            SecurityGroupIds=SecurityGroupIds,
            SubnetIds=SubnetIds,
            BackupRetentionCount=BackupRetentionCount,
            CustomDomain=CustomDomain,
            CustomCertificate=CustomCertificate,
            CustomPrivateKey=CustomPrivateKey,
            EngineModel=EngineModel,
            EngineVersion=EngineVersion,
            EngineAttributes=EngineAttributes,
            DisableAutomatedBackup=DisableAutomatedBackup,
            Tags=Tags,
        )

    def delete_server(self, *, ServerName: str) -> Dict[str, Any]:
        return self.client.delete_server(ServerName=ServerName)

    def update_server(
        self,
        *,
        ServerName: str,
        BackupRetentionCount: Optional[int] = None,
        PreferredBackupWindow: Optional[str] = None,
        PreferredMaintenanceWindow: Optional[str] = None,
        DisableAutomatedBackup: Optional[bool] = None,
    ) -> Dict[str, Any]:
        return self.client.update_server(
            ServerName=ServerName,
            BackupRetentionCount=BackupRetentionCount,
            PreferredBackupWindow=PreferredBackupWindow,
            PreferredMaintenanceWindow=PreferredMaintenanceWindow,
            DisableAutomatedBackup=DisableAutomatedBackup,
        )

    def create_backup(
        self,
        *,
        ServerName: str,
        Description: Optional[str] = None,
    ) -> Dict[str, Any]:
        return self.client.create_backup(ServerName=ServerName, Description=Description)

    def delete_backup(self, *, BackupId: str) -> Dict[str, Any]:
        return self.client.delete_backup(BackupId=BackupId)

    def describe_servers(
        self,
        *,
        ServerName: Optional[str] = None,
        NextToken: Optional[str] = None,
        MaxResults: Optional[int] = None,
    ) -> Dict[str, Any]:
        return self.client.describe_servers(
            ServerName=ServerName, NextToken=NextToken, MaxResults=MaxResults
        )

    def describe_backups(
        self,
        *,
        BackupId: Optional[str] = None,
        ServerName: Optional[str] = None,
        NextToken: Optional[str] = None,
        MaxResults: Optional[int] = None,
    ) -> Dict[str, Any]:
        return self.client.describe_backups(
            BackupId=BackupId,
            ServerName=ServerName,
            NextToken=NextToken,
            MaxResults=MaxResults,
        )

    def start_maintenance(
        self,
        *,
        ServerName: str,
        EngineAttributes: Optional[Sequence[EngineAttributeTypeDef]] = None,
    ) -> Dict[str, Any]:
        return self.client.start_maintenance(
            ServerName=ServerName, EngineAttributes=EngineAttributes
        )

    def list_tags_for_resource(self, *, ResourceArn: str) -> Dict[str, Any]:
        return self.client.list_tags_for_resource(ResourceArn=ResourceArn)

    def tag_resource(
        self, *, ResourceArn: str, Tags: Sequence[TagTypeDef]
    ) -> Dict[str, Any]:
        return self.client.tag_resource(ResourceArn=ResourceArn, Tags=Tags)

    def untag_resource(
        self, *, ResourceArn: str, TagKeys: Sequence[str]
    ) -> Dict[str, Any]:
        return self.client.untag_resource(ResourceArn=ResourceArn, TagKeys=TagKeys)
```
------------------------------------------------------------

PROMPT 55:
"Generate Python code that imports the Flask API and the Request Validation library, creates a decorator function that validates incoming JSON requests using the Request Validation library, and then passes the validated data to the decorated endpoint function."
RESPONSE:
```python
from functools import wraps
from flask import Flask, request, jsonify
from pydantic import BaseModel, ValidationError

app = Flask(__name__)

def validate_json(model_cls):
    """
    Decorator that validates request JSON against a Pydantic model.
    On success, passes the validated model instance to the endpoint as the first arg.
    On failure, returns a 400/422 with error details.
    """
    def decorator(fn):
        @wraps(fn)
        def wrapper(*args, **kwargs):
            if not request.is_json:
                return jsonify(error="Content-Type must be application/json"), 400

            data = request.get_json(silent=True)
            if data is None:
                return jsonify(error="Malformed or missing JSON body"), 400

            try:
                # Pydantic v2
                validated = model_cls.model_validate(data)  # type: ignore[attr-defined]
            except AttributeError:
                # Pydantic v1 fallback
                validated = model_cls.parse_obj(data)      # type: ignore[attr-defined]
            except ValidationError as e:
                # Pydantic v2 exposes .errors(); v1 also provides .errors()
                return jsonify(error="Validation failed", details=e.errors()), 422

            return fn(validated, *args, **kwargs)
        return wrapper
    return decorator

# Example Pydantic model (the "Request Validation library")
class CreateUser(BaseModel):
    name: str
    age: int

@app.post("/users")
@validate_json(CreateUser)
def create_user(payload: CreateUser):
    # 'payload' is a validated Pydantic model instance
    return jsonify(
        message="User created",
        user={"name": payload.name, "age": payload.age}
    ), 201

if __name__ == "__main__":
    app.run(debug=True)
```
------------------------------------------------------------

PROMPT 56:
"Generate Python code that implements a hierarchical data modeling framework for modern science data standards, supporting the creation of datasets, datasets collections, and data packages, with support for various data formats and validation against data standards such as ISO19115, ISO19117, and ISO19142."
RESPONSE:
# hierarchical_data_model.py

from __future__ import annotations

import json
import os
import shutil
import sys
import uuid
import hashlib
import logging
import zipfile
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple, Union
from abc import ABC, abstractmethod

# Optional third-party imports (used if available)
def _optional_import(name: str):
    try:
        return __import__(name)
    except Exception:
        return None

jsonschema = _optional_import("jsonschema")
lxml_etree = None
try:
    # Prefer the optimized C extensions if available
    from lxml import etree as lxml_etree  # type: ignore
except Exception:
    pass

owslib_wfs = None
try:
    from owslib.wfs import WebFeatureService  # type: ignore
    owslib_wfs = WebFeatureService
except Exception:
    pass

pandas = _optional_import("pandas")
xarray = _optional_import("xarray")
rasterio = _optional_import("rasterio")
pyproj = _optional_import("pyproj")
fiona = _optional_import("fiona")

logger = logging.getLogger("hierarchical_data_model")
logging.basicConfig(level=logging.INFO)


# ---------------------------
# Utilities
# ---------------------------

def compute_checksum(path: Union[str, Path], algorithm: str = "sha256") -> str:
    h = hashlib.new(algorithm)
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def now_iso() -> str:
    return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"


def ensure_dir(p: Union[str, Path]) -> Path:
    p = Path(p)
    p.mkdir(parents=True, exist_ok=True)
    return p


# ---------------------------
# Core data models
# ---------------------------

class DataFormat(str, Enum):
    CSV = "csv"
    JSON = "json"
    GEOJSON = "geojson"
    NETCDF = "netcdf"
    HDF5 = "hdf5"
    GEOTIFF = "geotiff"
    SHAPEFILE = "shapefile"
    PARQUET = "parquet"
    ZARR = "zarr"
    UNKNOWN = "unknown"


MEDIA_TYPES = {
    DataFormat.CSV: "text/csv",
    DataFormat.JSON: "application/json",
    DataFormat.GEOJSON: "application/geo+json",
    DataFormat.NETCDF: "application/x-netcdf",
    DataFormat.HDF5: "application/x-hdf",
    DataFormat.GEOTIFF: "image/tiff; application=geotiff",
    DataFormat.SHAPEFILE: "application/x-esri-shapefile",
    DataFormat.PARQUET: "application/vnd.apache.parquet",
    DataFormat.ZARR: "application/x-zarr",
    DataFormat.UNKNOWN: "application/octet-stream",
}


def infer_data_format(path: Union[str, Path]) -> DataFormat:
    ext = str(path).lower()
    if ext.endswith(".csv"):
        return DataFormat.CSV
    if ext.endswith(".json"):
        return DataFormat.JSON
    if ext.endswith(".geojson"):
        return DataFormat.GEOJSON
    if ext.endswith(".nc") or ext.endswith(".cdf") or ext.endswith(".netcdf"):
        return DataFormat.NETCDF
    if ext.endswith(".h5") or ext.endswith(".hdf") or ext.endswith(".hdf5"):
        return DataFormat.HDF5
    if ext.endswith(".tif") or ext.endswith(".tiff"):
        return DataFormat.GEOTIFF
    if ext.endswith(".shp"):
        return DataFormat.SHAPEFILE
    if ext.endswith(".parquet"):
        return DataFormat.PARQUET
    if ext.endswith(".zarr"):
        return DataFormat.ZARR
    return DataFormat.UNKNOWN


@dataclass
class BBox:
    minx: float
    miny: float
    maxx: float
    maxy: float
    crs: Optional[str] = "EPSG:4326"  # default to WGS84

    def to_list(self) -> List[float]:
        return [self.minx, self.miny, self.maxx, self.maxy]

    @property
    def width(self) -> float:
        return self.maxx - self.minx

    @property
    def height(self) -> float:
        return self.maxy - self.miny


@dataclass
class TimeRange:
    start: Optional[str] = None  # ISO 8601 strings
    end: Optional[str] = None


@dataclass
class Resource:
    path: str
    media_type: Optional[str] = None
    role: Optional[str] = None  # e.g., "data", "metadata", "style"
    size: Optional[int] = None
    checksum: Optional[str] = None
    checksum_algorithm: str = "sha256"
    modified: Optional[str] = None
    format: DataFormat = DataFormat.UNKNOWN
    extra: Dict[str, Any] = field(default_factory=dict)

    def finalize(self):
        p = Path(self.path)
        if not p.exists():
            raise FileNotFoundError(f"Resource path not found: {self.path}")
        self.size = p.stat().st_size
        self.modified = datetime.utcfromtimestamp(p.stat().st_mtime).replace(microsecond=0).isoformat() + "Z"
        self.format = infer_data_format(p)
        self.media_type = self.media_type or MEDIA_TYPES.get(self.format, "application/octet-stream")
        self.checksum = self.checksum or compute_checksum(p, self.checksum_algorithm)


@dataclass
class Dataset:
    id: str
    title: str
    description: Optional[str] = None
    keywords: List[str] = field(default_factory=list)
    created: str = field(default_factory=now_iso)
    modified: str = field(default_factory=now_iso)
    license: Optional[str] = None
    contact: Optional[Dict[str, Any]] = None  # responsible party
    spatial_extent: Optional[BBox] = None
    temporal_coverage: Optional[TimeRange] = None
    crs: Optional[str] = None
    variables: List[Dict[str, Any]] = field(default_factory=list)  # name, units, standard_name, etc.
    resources: List[Resource] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)  # free-form metadata, provenance, lineage, quality, etc.
    links: List[Dict[str, Any]] = field(default_factory=list)  # e.g., WFS/WMS endpoints

    def add_resource(self, path: Union[str, Path], role: Optional[str] = "data", media_type: Optional[str] = None, finalize: bool = True, **extra):
        r = Resource(path=str(path), role=role, media_type=media_type, extra=extra)
        if finalize:
            r.finalize()
        self.resources.append(r)
        self.modified = now_iso()

    def derive_basic_metadata(self):
        # Try to infer CRS and extent from resources if possible
        for r in self.resources:
            if r.format == DataFormat.GEOTIFF and rasterio:
                try:
                    with rasterio.open(r.path) as ds:
                        bounds = ds.bounds
                        crs = ds.crs.to_string() if ds.crs else None
                        self.spatial_extent = self.spatial_extent or BBox(bounds.left, bounds.bottom, bounds.right, bounds.top, crs=crs or self.crs or "EPSG:4326")
                        self.crs = self.crs or crs
                except Exception as e:
                    logger.debug(f"GeoTIFF metadata derivation failed: {e}")
            elif r.format == DataFormat.NETCDF and xarray:
                try:
                    ds = xarray.open_dataset(r.path)
                    # naive attempt: look for lon/lat coordinates
                    lon_name = next((n for n in ds.coords if n.lower() in ("lon", "longitude", "x")), None)
                    lat_name = next((n for n in ds.coords if n.lower() in ("lat", "latitude", "y")), None)
                    if lon_name and lat_name:
                        lon = ds[lon_name].values
                        lat = ds[lat_name].values
                        minx, maxx = float(lon.min()), float(lon.max())
                        miny, maxy = float(lat.min()), float(lat.max())
                        self.spatial_extent = self.spatial_extent or BBox(minx, miny, maxx, maxy, crs=self.crs or "EPSG:4326")
                    # variables
                    for v in ds.data_vars:
                        var = ds[v]
                        self.variables.append({
                            "name": v,
                            "dims": list(var.dims),
                            "dtype": str(var.dtype),
                            "attrs": {k: str(vv) for k, vv in var.attrs.items()}
                        })
                except Exception as e:
                    logger.debug(f"NetCDF metadata derivation failed: {e}")
            elif r.format == DataFormat.SHAPEFILE and fiona:
                try:
                    with fiona.open(r.path) as src:
                        crs = None
                        try:
                            if src.crs:
                                if pyproj:
                                    crs = pyproj.CRS.from_user_input(src.crs).to_string()
                                else:
                                    crs = str(src.crs)
                        except Exception:
                            crs = None
                        b = src.bounds
                        self.spatial_extent = self.spatial_extent or BBox(b[0], b[1], b[2], b[3], crs=crs or self.crs or "EPSG:4326")
                        self.crs = self.crs or crs
                except Exception as e:
                    logger.debug(f"Shapefile metadata derivation failed: {e}")

    def to_iso19115_xml(self) -> str:
        # Minimal, non-exhaustive ISO 19139-like output for demonstration.
        # For production, integrate pygeometa or a full ISO 19115 encoder.
        title = self.title or ""
        abstract = (self.description or "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
        date_stamp = self.modified or now_iso()
        keywords_xml = "".join(f"<gmd:keyword><gco:CharacterString>{k}</gco:CharacterString></gmd:keyword>" for k in self.keywords)
        if self.spatial_extent:
            bbox = f"""
            <gmd:EX_GeographicBoundingBox>
              <gmd:westBoundLongitude><gco:Decimal>{self.spatial_extent.minx}</gco:Decimal></gmd:westBoundLongitude>
              <gmd:eastBoundLongitude><gco:Decimal>{self.spatial_extent.maxx}</gco:Decimal></gmd:eastBoundLongitude>
              <gmd:southBoundLatitude><gco:Decimal>{self.spatial_extent.miny}</gco:Decimal></gmd:southBoundLatitude>
              <gmd:northBoundLatitude><gco:Decimal>{self.spatial_extent.maxy}</gco:Decimal></gmd:northBoundLatitude>
            </gmd:EX_GeographicBoundingBox>
            """
        else:
            bbox = ""
        temporal_xml = ""
        if self.temporal_coverage and (self.temporal_coverage.start or self.temporal_coverage.end):
            begin = self.temporal_coverage.start or ""
            end = self.temporal_coverage.end or ""
            temporal_xml = f"""
            <gmd:EX_TemporalExtent>
              <gmd:extent>
                <gml:TimePeriod>
                  <gml:beginPosition>{begin}</gml:beginPosition>
                  <gml:endPosition>{end}</gml:endPosition>
                </gml:TimePeriod>
              </gmd:extent>
            </gmd:EX_TemporalExtent>
            """

        xml = f"""<?xml version="1.0" encoding="UTF-8"?>
<gmd:MD_Metadata xmlns:gmd="http://www.isotc211.org/2005/gmd"
                 xmlns:gco="http://www.isotc211.org/2005/gco"
                 xmlns:gml="http://www.opengis.net/gml"
                 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <gmd:fileIdentifier><gco:CharacterString>{self.id}</gco:CharacterString></gmd:fileIdentifier>
  <gmd:dateStamp><gco:DateTime>{date_stamp}</gco:DateTime></gmd:dateStamp>
  <gmd:identificationInfo>
    <gmd:MD_DataIdentification>
      <gmd:citation>
        <gmd:CI_Citation>
          <gmd:title><gco:CharacterString>{title}</gco:CharacterString></gmd:title>
          <gmd:date>
            <gmd:CI_Date>
              <gmd:date><gco:DateTime>{date_stamp}</gco:DateTime></gmd:date>
              <gmd:dateType>
                <gmd:CI_DateTypeCode codeListValue="revision" codeList="http://www.isotc211.org/2005/resources/codeList.xml#CI_DateTypeCode">revision</gmd:CI_DateTypeCode>
              </gmd:dateType>
            </gmd:CI_Date>
          </gmd:date>
        </gmd:CI_Citation>
      </gmd:citation>
      <gmd:abstract><gco:CharacterString>{abstract}</gco:CharacterString></gmd:abstract>
      <gmd:descriptiveKeywords>
        <gmd:MD_Keywords>
          {keywords_xml}
        </gmd:MD_Keywords>
      </gmd:descriptiveKeywords>
      <gmd:extent>
        <gmd:EX_Extent>
          <gmd:geographicElement>{bbox}</gmd:geographicElement>
          <gmd:temporalElement>{temporal_xml}</gmd:temporalElement>
        </gmd:EX_Extent>
      </gmd:extent>
    </gmd:MD_DataIdentification>
  </gmd:identificationInfo>
</gmd:MD_Metadata>
"""
        if lxml_etree:
            try:
                parser = lxml_etree.XMLParser(remove_blank_text=True)
                doc = lxml_etree.fromstring(xml.encode("utf-8"), parser=parser)
                return lxml_etree.tostring(doc, pretty_print=True, encoding="utf-8").decode("utf-8")
            except Exception:
                return xml
        return xml

    def to_frictionless_resource(self) -> Dict[str, Any]:
        resources = []
        for r in self.resources:
            resources.append({
                "path": r.path,
                "mediatype": r.media_type,
                "bytes": r.size,
                "hash": f"{r.checksum_algorithm}:{r.checksum}" if r.checksum else None,
                "format": r.format.value,
                "name": Path(r.path).name,
                "roles": [r.role] if r.role else [],
            })
        return {
            "name": self.id,
            "title": self.title,
            "description": self.description,
            "keywords": self.keywords,
            "licenses": [{"id": self.license}] if self.license else [],
            "profile": "data-resource",
            "resources": resources,
            "extras": self.metadata,
        }


@dataclass
class DatasetCollection:
    id: str
    title: str
    description: Optional[str] = None
    datasets: List[Dataset] = field(default_factory=list)
    keywords: List[str] = field(default_factory=list)
    created: str = field(default_factory=now_iso)
    modified: str = field(default_factory=now_iso)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def add_dataset(self, ds: Dataset):
        self.datasets.append(ds)
        self.modified = now_iso()

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "title": self.title,
            "description": self.description,
            "keywords": self.keywords,
            "created": self.created,
            "modified": self.modified,
            "metadata": self.metadata,
            "datasets": [asdict(ds) for ds in self.datasets],
        }


@dataclass
class DataPackage:
    id: str
    title: str
    description: Optional[str] = None
    datasets: List[Dataset] = field(default_factory=list)
    collections: List[DatasetCollection] = field(default_factory=list)
    created: str = field(default_factory=now_iso)
    modified: str = field(default_factory=now_iso)
    version: str = "1.0.0"
    metadata: Dict[str, Any] = field(default_factory=dict)

    def add_dataset(self, ds: Dataset):
        self.datasets.append(ds)
        self.modified = now_iso()

    def add_collection(self, dc: DatasetCollection):
        self.collections.append(dc)
        self.modified = now_iso()

    def manifest(self) -> Dict[str, Any]:
        # Frictionless-style top-level datapackage.json
        resources = []
        for ds in self.datasets:
            for r in ds.resources:
                resources.append({
                    "name": f"{ds.id}:{Path(r.path).name}",
                    "path": r.path,
                    "mediatype": r.media_type,
                    "bytes": r.size,
                    "hash": f"{r.checksum_algorithm}:{r.checksum}" if r.checksum else None,
                    "format": r.format.value,
                    "dataset": ds.id,
                    "roles": [r.role] if r.role else [],
                })
        manifest = {
            "profile": "data-package",
            "name": self.id,
            "title": self.title,
            "description": self.description,
            "version": self.version,
            "created": self.created,
            "last_modified": self.modified,
            "keywords": list({k for ds in self.datasets for k in ds.keywords}),
            "licenses": [],
            "sources": [],
            "resources": resources,
            "extras": {
                "collections": [c.to_dict() for c in self.collections],
                **self.metadata,
            },
        }
        return manifest

    def save(self, target: Union[str, Path], include_data: bool = False, as_zip: bool = False) -> Path:
        out_dir = ensure_dir(target)
        # Prepare resources and write manifest
        for ds in self.datasets:
            for r in ds.resources:
                if r.size is None or r.checksum is None:
                    r.finalize()
        manifest = self.manifest()
        manifest_path = out_dir / "datapackage.json"
        manifest_path.write_text(json.dumps(manifest, indent=2))
        if include_data:
            data_dir = ensure_dir(out_dir / "data")
            for ds in self.datasets:
                for r in ds.resources:
                    src = Path(r.path)
                    dst = data_dir / src.name
                    if src.resolve() != dst.resolve():
                        shutil.copy2(src, dst)
        if as_zip:
            zip_path = Path(str(out_dir) + ".zip")
            with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
                for root, _, files in os.walk(out_dir):
                    for f in files:
                        fp = Path(root) / f
                        zf.write(fp, arcname=str(fp.relative_to(out_dir)))
            return zip_path
        return out_dir


# ---------------------------
# Validation framework
# ---------------------------

class Severity(str, Enum):
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"


@dataclass
class ValidationIssue:
    message: str
    severity: Severity = Severity.ERROR
    code: Optional[str] = None
    reference: Optional[str] = None  # e.g., clause in a standard
    context: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ValidationReport:
    standard: str
    ok: bool
    issues: List[ValidationIssue] = field(default_factory=list)
    generated_at: str = field(default_factory=now_iso)

    def add(self, issue: ValidationIssue):
        self.issues.append(issue)
        if issue.severity == Severity.ERROR:
            self.ok = False

    @property
    def summary(self) -> Dict[str, int]:
        return {
            "errors": sum(1 for i in self.issues if i.severity == Severity.ERROR),
            "warnings": sum(1 for i in self.issues if i.severity == Severity.WARNING),
            "infos": sum(1 for i in self.issues if i.severity == Severity.INFO),
        }


class BaseValidator(ABC):
    @abstractmethod
    def name(self) -> str:
        ...

    @abstractmethod
    def validate_dataset(self, ds: Dataset) -> ValidationReport:
        ...

    def validate_collection(self, dc: DatasetCollection) -> ValidationReport:
        report = ValidationReport(standard=self.name(), ok=True)
        for ds in dc.datasets:
            dr = self.validate_dataset(ds)
            for issue in dr.issues:
                report.add(issue)
        return report

    def validate_package(self, pkg: DataPackage) -> ValidationReport:
        report = ValidationReport(standard=self.name(), ok=True)
        for ds in pkg.datasets:
            dr = self.validate_dataset(ds)
            for issue in dr.issues:
                report.add(issue)
        for col in pkg.collections:
            cr = self.validate_collection(col)
            for issue in cr.issues:
                report.add(issue)
        return report


class ISO19115Validator(BaseValidator):
    def __init__(self, xml_schema_path: Optional[Union[str, Path]] = None):
        self.xml_schema_path = Path(xml_schema_path) if xml_schema_path else None
        self._schema = None
        if self.xml_schema_path and lxml_etree:
            try:
                self._schema = lxml_etree.XMLSchema(lxml_etree.parse(str(self.xml_schema_path)))
            except Exception as e:
                logger.warning(f"Failed to load ISO19139 schema: {e}")
                self._schema = None

    def name(self) -> str:
        return "ISO19115"

    def validate_dataset(self, ds: Dataset) -> ValidationReport:
        report = ValidationReport(standard=self.name(), ok=True)

        # Minimal required metadata checks
        if not ds.title:
            report.add(ValidationIssue("Missing dataset title.", Severity.ERROR, code="IDEN-001", reference="ISO19115-1:2014 6.5"))
        if not (ds.description or ds.metadata.get("abstract")):
            report.add(ValidationIssue("Missing dataset abstract/description.", Severity.ERROR, code="IDEN-002", reference="ISO19115-1:2014 6.5.1"))
        if not ds.keywords:
            report.add(ValidationIssue("No keywords specified.", Severity.WARNING, code="IDEN-003", reference="ISO19115-1:2014 6.5.6"))
        if not ds.spatial_extent:
            report.add(ValidationIssue("No spatial extent (bounding box).", Severity.WARNING, code="SPAT-001", reference="ISO19115-1:2014 6.5.14"))
        if ds.temporal_coverage and ds.temporal_coverage.start and ds.temporal_coverage.end:
            if ds.temporal_coverage.end < ds.temporal_coverage.start:
                report.add(ValidationIssue("Temporal coverage end before start.", Severity.ERROR, code="TEMP-001", reference="ISO19115-1:2014 6.5.14"))

        # XML conformance check if schema is available
        if self._schema and lxml_etree:
            try:
                xml = ds.to_iso19115_xml().encode("utf-8")
                doc = lxml_etree.fromstring(xml)
                if not self._schema.validate(doc):
                    for err in self._schema.error_log:  # type: ignore
                        report.add(ValidationIssue(f"XML schema error: {err.message}", Severity.ERROR, code="XML-VAL"))
            except Exception as e:
                report.add(ValidationIssue(f"ISO19139 XML generation/validation failed: {e}", Severity.ERROR, code="XML-EXC"))
        else:
            report.add(ValidationIssue("XML schema validation not performed (schema or lxml missing).", Severity.INFO, code="XML-SKIP"))

        return report


class ISO19117Validator(BaseValidator):
    # ISO 19117 portrayal: we approximate by checking for SLD/SE styles availability.
    def __init__(self, sld_schema_path: Optional[Union[str, Path]] = None):
        self.sld_schema_path = Path(sld_schema_path) if sld_schema_path else None
        self._schema = None
        if self.sld_schema_path and lxml_etree:
            try:
                self._schema = lxml_etree.XMLSchema(lxml_etree.parse(str(self.sld_schema_path)))
            except Exception as e:
                logger.warning(f"Failed to load SLD/SE schema: {e}")
                self._schema = None

    def name(self) -> str:
        return "ISO19117"

    def validate_dataset(self, ds: Dataset) -> ValidationReport:
        report = ValidationReport(standard=self.name(), ok=True)
        # Look for style resources
        style_resources = [r for r in ds.resources if (r.role or "").lower() in {"style", "sld", "se"} or (r.path.lower().endswith(".sld") or r.path.lower().endswith(".xml"))]
        if not style_resources and not ds.metadata.get("portrayal"):
            report.add(ValidationIssue("No portrayal catalog or styles provided (SLD/SE).", Severity.WARNING, code="POR-001", reference="ISO19117:2012"))
        else:
            # If schema available, validate XML styles
            if self._schema and lxml_etree:
                for r in style_resources:
                    try:
                        doc = lxml_etree.parse(r.path)
                        if not self._schema.validate(doc):
                            for err in self._schema.error_log:  # type: ignore
                                report.add(ValidationIssue(f"Style schema error in {r.path}: {err.message}", Severity.ERROR, code="SLD-VAL"))
                    except Exception as e:
                        report.add(ValidationIssue(f"Failed to parse/validate style {r.path}: {e}", Severity.ERROR, code="SLD-EXC"))
            else:
                report.add(ValidationIssue("Style schema validation not performed (schema or lxml missing).", Severity.INFO, code="SLD-SKIP"))
        return report


class ISO19142Validator(BaseValidator):
    # ISO 19142 defines WFS. We validate WFS endpoint/link if provided.
    def name(self) -> str:
        return "ISO19142"

    def validate_dataset(self, ds: Dataset) -> ValidationReport:
        report = ValidationReport(standard=self.name(), ok=True)
        wfs_links = [l for l in ds.links if str(l.get("rel", "")).lower() in {"wfs", "ogc:wfs"} or "wfs" in str(l.get("href", "")).lower()]
        if not wfs_links:
            report.add(ValidationIssue("No WFS link provided for dataset; skipping WFS validation.", Severity.INFO, code="WFS-NOLINK"))
            return report

        if not owslib_wfs:
            report.add(ValidationIssue("OWSLib not available; cannot validate WFS endpoint.", Severity.WARNING, code="WFS-NOLIB"))
            return report

        for link in wfs_links:
            url = link.get("href")
            typename = link.get("typeName") or link.get("typename") or link.get("layer")
            version = link.get("version", "2.0.0")
            try:
                wfs = owslib_wfs(url=url, version=version)  # type: ignore
                offered = set(wfs.contents.keys())  # type: ignore
                if typename:
                    if typename not in offered:
                        report.add(ValidationIssue(f"WFS typename '{typename}' not found at endpoint.", Severity.ERROR, code="WFS-TYPE", reference="ISO19142:2010"))
                else:
                    report.add(ValidationIssue("No typename provided for WFS validation; verified endpoint only.", Severity.WARNING, code="WFS-NOTYPENAME"))
            except Exception as e:
                report.add(ValidationIssue(f"WFS capabilities retrieval failed: {e}", Severity.ERROR, code="WFS-CAPS"))
        return report


class JSONSchemaValidator(BaseValidator):
    def __init__(self, schema: Optional[Dict[str, Any]] = None, schema_path: Optional[Union[str, Path]] = None, target: str = "dataset.metadata"):
        self._schema = schema
        self._schema_path = Path(schema_path) if schema_path else None
        self.target = target  # "dataset", "dataset.metadata", etc.
        if not self._schema and self._schema_path:
            try:
                self._schema = json.loads(Path(self._schema_path).read_text())
            except Exception as e:
                logger.warning(f"Failed to load JSON schema: {e}")
                self._schema = None

    def name(self) -> str:
        return "JSONSchema"

    def _get_target_obj(self, ds: Dataset):
        if self.target == "dataset":
            return asdict(ds)
        if self.target == "dataset.metadata":
            return ds.metadata
        return asdict(ds)

    def validate_dataset(self, ds: Dataset) -> ValidationReport:
        report = ValidationReport(standard=self.name(), ok=True)
        if not self._schema or not jsonschema:
            report.add(ValidationIssue("jsonschema or schema not available; skipping JSON schema validation.", Severity.INFO))
            return report
        try:
            jsonschema.validate(self._get_target_obj(ds), self._schema)  # type: ignore
        except Exception as e:
            report.add(ValidationIssue(f"JSON schema validation error: {str(e)}", Severity.ERROR))
        return report


class ValidatorRegistry:
    def __init__(self):
        self._validators: Dict[str, BaseValidator] = {}

    def register(self, validator: BaseValidator):
        self._validators[validator.name()] = validator

    def get(self, name: str) -> Optional[BaseValidator]:
        return self._validators.get(name)

    def all(self) -> List[BaseValidator]:
        return list(self._validators.values())


# ---------------------------
# Example usage and quickstart
# ---------------------------

def example() -> None:
    # Create a dataset
    ds = Dataset(
        id=str(uuid.uuid4()),
        title="Sea Surface Temperature Climatology",
        description="Monthly mean sea surface temperature from 1981 to 2010.",
        keywords=["SST", "Ocean", "Climatology"],
        license="CC-BY-4.0",
        contact={"organization": "Example Institute", "email": "data@example.org"},
        temporal_coverage=TimeRange(start="1981-01-01", end="2010-12-31"),
    )

    # Add a resource (replace with a real file path in practice)
    # ds.add_resource("path/to/sst.nc", role="data")

    # Derive metadata from resources if possible
    ds.derive_basic_metadata()

    # Build a collection and package
    col = DatasetCollection(id="ocean-temp", title="Ocean Temperature Products")
    col.add_dataset(ds)

    pkg = DataPackage(id="ocean-obs", title="Global Ocean Observations")
    pkg.add_collection(col)
    pkg.add_dataset(ds)

    # Validators
    registry = ValidatorRegistry()
    registry.register(ISO19115Validator())
    registry.register(ISO19117Validator())
    registry.register(ISO19142Validator())

    # Validate dataset
    for v in registry.all():
        rep = v.validate_dataset(ds)
        logger.info(f"{v.name()} ok={rep.ok} summary={rep.summary}")
        for issue in rep.issues:
            logger.info(f" - [{issue.severity}] {issue.message}")

    # Export ISO 19115 XML (minimal)
    iso_xml = ds.to_iso19115_xml()
    print(iso_xml[:400] + "...\n")

    # Save a data package manifest (without copying data)
    out = pkg.save("out_package", include_data=False, as_zip=False)
    print(f"Saved package to {out.resolve()}")


if __name__ == "__main__":
    # Run example when executed directly
    example()
------------------------------------------------------------

PROMPT 57:
"Generate Python code that imports the required modules and sets up the OneDrive API client using the deprecated SDK, targeting version 2.0 or earlier, for compatibility with legacy applications:\n```python\nimport os\nfrom onedrivesdk import OneDriveClient\n\nclient = OneDriveClient(client_id='YOUR_CLIENT_ID', client_secret='YOUR_CLIENT_SECRET', auth_server_url='https://login.live.com')\n```"
RESPONSE:
```python
# Deprecated OneDrive SDK setup for legacy apps (API v2.0 or earlier semantics)
# pip install onedrivesdk==1.1.8

import os
import onedrivesdk
from onedrivesdk.helpers import GetAuthCodeServer

# Read credentials from env or hard-code placeholders
CLIENT_ID = os.getenv('ONEDRIVE_CLIENT_ID', 'YOUR_CLIENT_ID')
CLIENT_SECRET = os.getenv('ONEDRIVE_CLIENT_SECRET', 'YOUR_CLIENT_SECRET')
REDIRECT_URI = os.getenv('ONEDRIVE_REDIRECT_URI', 'http://localhost:8080')

# Legacy MSA (Live) scopes for OneDrive Consumer
SCOPES = ['wl.signin', 'wl.offline_access', 'onedrive.readwrite']

# Legacy OneDrive Consumer API base URL (v1.0-style endpoint used by the deprecated SDK)
API_BASE_URL = 'https://api.onedrive.com/v1.0/'

# Build client
http_provider = onedrivesdk.HttpProvider()
auth_provider = onedrivesdk.AuthProvider(http_provider, CLIENT_ID, SCOPES)
client = onedrivesdk.OneDriveClient(API_BASE_URL, auth_provider, http_provider)

# Interactive OAuth 2.0 (Live Connect) flow
auth_url = client.auth_provider.get_auth_url(REDIRECT_URI)
code = GetAuthCodeServer.get_auth_code(auth_url, REDIRECT_URI)
client.auth_provider.authenticate(code, REDIRECT_URI, CLIENT_SECRET)

# client is now authenticated and ready to use
# Example:
# items = client.drive.root.children.get()
# for item in items:
#     print(item.name)
```
------------------------------------------------------------

PROMPT 58:
"Generate Python code that imports the pytest module, creates a custom marker interface, defines a new function that takes a test function as an argument, and appends the current timestamp in ISO format to the beginning of the test name before running the test function; this new function returns the result of the test function and also sets up the custom marker interface to use the new function for all tests that are marked with the custom marker."
RESPONSE:
# conftest.py
import pytest
from datetime import datetime, timezone

TIMESTAMP_MARK = "timestamped"  # custom marker name (interface: pytest.mark.timestamped)


def run_with_timestamp(test_func, item):
    """
    Wraps a test function, prepends the current ISO timestamp to the test name
    before running it, and returns the test function's result.
    """
    iso = datetime.now(timezone.utc).isoformat()

    # Prepend timestamp to the displayed test name and nodeid
    try:
        original_name = item.name
        new_name = f"{iso} {original_name}"
        item.name = new_name

        # Update the nodeid to reflect the new name (affects reporting)
        nodeid = item.nodeid
        if nodeid.endswith(original_name):
            item._nodeid = nodeid[: -len(original_name)] + new_name
        else:
            # Fallback in case of unusual nodeid formats
            item._nodeid = nodeid.replace(original_name, new_name, 1)
    except Exception:
        # If anything goes wrong, don't block test execution
        pass

    # Execute the original test function and return its result
    return test_func()


def pytest_collection_modifyitems(config, items):
    """
    For items marked with our custom marker, replace their runtest method so that
    they execute via run_with_timestamp.
    """
    for item in items:
        if item.get_closest_marker(TIMESTAMP_MARK) is not None:
            original_runtest = item.runtest

            def make_wrapper(orig=original_runtest, _item=item):
                def _wrapper():
                    return run_with_timestamp(orig, _item)
                return _wrapper

            item.runtest = make_wrapper()


def pytest_configure(config):
    """
    Register the custom marker to avoid Pytest unknown-marker warnings.
    """
    config.addinivalue_line(
        "markers",
        f"{TIMESTAMP_MARK}: prepend an ISO timestamp to the test name before running"
    )
------------------------------------------------------------

PROMPT 59:
"Generate Python code that creates desktop and Start Menu shortcuts for python scripts using the `py2exe` library by importing it, creating a window with the title \"Python Script Runner\", adding two buttons with different functions using the `Button` class from `tkinter`, saving the main window's position and size before closing, and running the script again after opening the file location; also, use the `os` module to check if the script has already been run today, and only create the shortcuts if it hasn't."
RESPONSE:
# Python Script Runner: creates Desktop and Start Menu shortcuts (Windows), shows a small UI,
# saves/restores window geometry, and can open file location then re-run itself.
# Shortcuts are created only once per day (checked using the os module).

import os
import sys
import json
import subprocess
import datetime
import platform
from tkinter import Tk, Button, messagebox

# Import py2exe as requested (not strictly needed at runtime for shortcut creation)
try:
    import py2exe  # noqa: F401
except Exception:
    py2exe = None  # Proceed even if not installed

APP_TITLE = "Python Script Runner"
APP_ID = "PythonScriptRunner"
APPDATA_FALLBACK = os.path.expanduser("~")
APPDATA_DIR = os.path.join(os.environ.get("APPDATA", APPDATA_FALLBACK), APP_ID)
CONFIG_PATH = os.path.join(APPDATA_DIR, "config.json")
RUN_MARKER = os.path.join(APPDATA_DIR, "last_run_marker")

def ensure_appdata_dir():
    os.makedirs(APPDATA_DIR, exist_ok=True)

def load_config():
    ensure_appdata_dir()
    if os.path.exists(CONFIG_PATH):
        try:
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_config(cfg):
    ensure_appdata_dir()
    try:
        with open(CONFIG_PATH, "w", encoding="utf-8") as f:
            json.dump(cfg, f, indent=2)
    except Exception:
        pass

def save_geometry(geom_str):
    cfg = load_config()
    cfg["geometry"] = geom_str
    save_config(cfg)

def load_geometry_value():
    cfg = load_config()
    return cfg.get("geometry")

def is_windows():
    return platform.system().lower().startswith("win")

def get_desktop_path():
    # Try typical desktop path; if OneDrive is used, this may differ, but this is a common default.
    # For better accuracy, PowerShell is used to resolve Start Menu path, and we can use it for Desktop too.
    if is_windows():
        # Try to query via PowerShell first
        try:
            result = subprocess.run(
                ["powershell", "-NoProfile", "-Command", "[Environment]::GetFolderPath('Desktop')"],
                capture_output=True, text=True, check=True
            )
            p = result.stdout.strip()
            if p and os.path.isdir(p):
                return p
        except Exception:
            pass
    # Fallback
    return os.path.join(os.path.expanduser("~"), "Desktop")

def get_start_menu_programs_path():
    # Resolve Start Menu\Programs using PowerShell for localization-safe path
    if is_windows():
        try:
            result = subprocess.run(
                ["powershell", "-NoProfile", "-Command", "[Environment]::GetFolderPath('StartMenu')"],
                capture_output=True, text=True, check=True
            )
            base = result.stdout.strip()
            if base and os.path.isdir(base):
                p = os.path.join(base, "Programs")
                os.makedirs(p, exist_ok=True)
                return p
        except Exception:
            pass
    # Fallback to APPDATA-based default
    p = os.path.join(os.environ.get("APPDATA", APPDATA_FALLBACK), "Microsoft", "Windows", "Start Menu", "Programs")
    os.makedirs(p, exist_ok=True)
    return p

def ps_escape(s):
    # Escape single quotes for PowerShell single-quoted strings
    return s.replace("'", "''")

def create_shortcut(lnk_path, target, args="", workdir=None, icon_path=None):
    # Create a .lnk shortcut via PowerShell + WScript.Shell COM (no external Python packages needed)
    workdir = workdir or os.path.dirname(target)
    icon_path = icon_path or target

    # Ensure directory exists
    os.makedirs(os.path.dirname(lnk_path), exist_ok=True)

    lnk_ps = ps_escape(lnk_path)
    tgt_ps = ps_escape(target)
    args_ps = ps_escape(args)
    wdir_ps = ps_escape(workdir)
    icon_ps = ps_escape(icon_path)

    ps_script = (
        "$s=(New-Object -ComObject WScript.Shell).CreateShortcut('{lnk}');"
        "$s.TargetPath='{tgt}';"
        "$s.Arguments='{arg}';"
        "$s.WorkingDirectory='{wdir}';"
        "$s.IconLocation='{icon}';"
        "$s.Save();"
    ).format(lnk=lnk_ps, tgt=tgt_ps, arg=args_ps, wdir=wdir_ps, icon=icon_ps)

    try:
        subprocess.run(["powershell", "-NoProfile", "-Command", ps_script], check=True, capture_output=True, text=True)
        return True
    except subprocess.CalledProcessError as e:
        # Optional: print debug
        # print("Failed to create shortcut:", e.stderr)
        return False

def create_shortcuts_for_current_script():
    if not is_windows():
        return False, "Shortcut creation is only supported on Windows."

    script_path = os.path.abspath(sys.argv[0])
    # Determine target and arguments:
    # - If running as an exe (e.g., via py2exe build), point directly to the exe.
    # - Otherwise, use the Python interpreter with script as argument.
    if script_path.lower().endswith(".exe"):
        target = script_path
        arguments = ""
        icon = script_path
    else:
        target = sys.executable
        # Wrap script path in quotes to handle spaces
        arguments = f"\"{script_path}\""
        icon = sys.executable

    workdir = os.path.dirname(script_path)
    base_name = os.path.splitext(os.path.basename(script_path))[0]
    desktop_path = get_desktop_path()
    start_menu_path = get_start_menu_programs_path()

    desktop_lnk = os.path.join(desktop_path, f"{base_name}.lnk")
    start_menu_lnk = os.path.join(start_menu_path, f"{base_name}.lnk")

    ok1 = create_shortcut(desktop_lnk, target, args=arguments, workdir=workdir, icon_path=icon)
    ok2 = create_shortcut(start_menu_lnk, target, args=arguments, workdir=workdir, icon_path=icon)

    if ok1 and ok2:
        return True, f"Shortcuts created:\n- {desktop_lnk}\n- {start_menu_lnk}"
    elif ok1 or ok2:
        which = "Desktop" if ok1 else "Start Menu"
        return False, f"{which} shortcut created, but the other failed."
    else:
        return False, "Failed to create shortcuts."

def has_run_today():
    # Use os module to check last run via marker file mtime
    ensure_appdata_dir()
    if os.path.exists(RUN_MARKER):
        try:
            last = datetime.date.fromtimestamp(os.path.getmtime(RUN_MARKER))
            return last == datetime.date.today()
        except Exception:
            return False
    return False

def mark_run_today():
    ensure_appdata_dir()
    try:
        with open(RUN_MARKER, "a", encoding="utf-8"):
            pass
        os.utime(RUN_MARKER, None)
    except Exception:
        pass

def open_file_location_and_rerun(root=None):
    # Open Explorer selecting this script/exe, then relaunch a new instance
    script_path = os.path.abspath(sys.argv[0])

    if is_windows():
        try:
            # Explorer '/select,' expects a single argument prefix
            subprocess.Popen(["explorer", f"/select,{script_path}"])
        except Exception:
            # Fallback: just open folder
            subprocess.Popen(["explorer", os.path.dirname(script_path)])
    else:
        # Non-Windows fallback: open containing folder
        try:
            if sys.platform == "darwin":
                subprocess.Popen(["open", os.path.dirname(script_path)])
            else:
                subprocess.Popen(["xdg-open", os.path.dirname(script_path)])
        except Exception:
            pass

    # Relaunch the script
    if script_path.lower().endswith(".exe"):
        cmd = [script_path] + sys.argv[1:]
    else:
        cmd = [sys.executable, script_path] + sys.argv[1:]

    # Save geometry before closing/relaunching
    if root is not None:
        try:
            save_geometry(root.geometry())
        except Exception:
            pass

    try:
        subprocess.Popen(cmd)
    except Exception:
        pass
    finally:
        if root is not None:
            root.destroy()
        os._exit(0)

def on_close(root):
    try:
        save_geometry(root.geometry())
    except Exception:
        pass
    root.destroy()

def main():
    ensure_appdata_dir()

    # Create shortcuts only once per day
    if not has_run_today():
        ok, msg = create_shortcuts_for_current_script()
        mark_run_today()
        # Defer showing message until GUI is ready

    # Build the simple Tkinter UI
    root = Tk()
    root.title(APP_TITLE)
    geom = load_geometry_value()
    if geom:
        try:
            root.geometry(geom)
        except Exception:
            pass

    def create_now():
        ok, msg = create_shortcuts_for_current_script()
        if ok:
            messagebox.showinfo(APP_TITLE, msg)
        else:
            messagebox.showwarning(APP_TITLE, msg)

    def open_and_rerun():
        open_file_location_and_rerun(root)

    btn1 = Button(root, text="Create Shortcuts Now", command=create_now)
    btn1.pack(padx=12, pady=(16, 8), fill="x")

    btn2 = Button(root, text="Open File Location and Re-run", command=open_and_rerun)
    btn2.pack(padx=12, pady=(0, 16), fill="x")

    root.protocol("WM_DELETE_WINDOW", lambda r=root: on_close(r))
    root.mainloop()

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 60:
"Generate Python code that creates a test client for Sanic using the `unittest` module, which includes a test case for each of the following:\n\n* Test that Sanic returns a response when a request is made to the root endpoint (\"/\").\n* Test that Sanic returns a response when a request is made to a non-existent endpoint.\n* Test that Sanic returns a response when a request is made with an invalid method.\n* Test that Sanic returns a response when a request is made with an invalid URL.\n* Test that Sanic returns a response when a request is made with a valid JSON payload.\n* Test that Sanic returns a response when a request is made with an invalid JSON payload."
RESPONSE:
import unittest
import json as pyjson

from sanic import Sanic
from sanic.response import json, text
import httpx


def create_app() -> Sanic:
    app = Sanic("TestApp")

    @app.get("/")
    async def index(request):
        return text("OK")

    @app.get("/item/<item_id:int>")
    async def get_item(request, item_id: int):
        return json({"item_id": item_id})

    @app.post("/json")
    async def echo_json(request):
        # Try to parse JSON payload explicitly to cleanly handle invalid JSON
        try:
            data = pyjson.loads(request.body.decode("utf-8"))
        except Exception:
            return json({"error": "Invalid JSON"}, status=400)
        return json({"received": data})

    return app


class SanicClientTests(unittest.IsolatedAsyncioTestCase):
    @classmethod
    def setUpClass(cls):
        cls.app = create_app()

    async def asyncSetUp(self):
        # Use httpx ASGI transport to talk to the Sanic app without a real server
        self.transport = httpx.ASGITransport(app=self.app, lifespan="on")
        self.client = httpx.AsyncClient(
            transport=self.transport, base_url="http://testserver"
        )

    async def asyncTearDown(self):
        await self.client.aclose()
        await self.transport.aclose()

    async def test_root_endpoint_returns_response(self):
        resp = await self.client.get("/")
        self.assertEqual(resp.status_code, 200)
        self.assertEqual(resp.text, "OK")

    async def test_nonexistent_endpoint_returns_response(self):
        resp = await self.client.get("/does-not-exist")
        self.assertEqual(resp.status_code, 404)

    async def test_invalid_method_returns_response(self):
        # POST to "/" where only GET is allowed -> 405 Method Not Allowed
        resp = await self.client.post("/")
        self.assertEqual(resp.status_code, 405)

    async def test_invalid_url_returns_response(self):
        # Route exists as /item/<item_id:int>, but "abc" is invalid for :int -> 404
        resp = await self.client.get("/item/abc")
        self.assertEqual(resp.status_code, 404)

    async def test_valid_json_payload_returns_response(self):
        payload = {"foo": "bar"}
        resp = await self.client.post("/json", json=payload)
        self.assertEqual(resp.status_code, 200)
        self.assertEqual(resp.json(), {"received": payload})

    async def test_invalid_json_payload_returns_response(self):
        # Malformed JSON body with proper content type should yield 400
        resp = await self.client.post(
            "/json",
            content=b'{"foo": }',
            headers={"Content-Type": "application/json"},
        )
        self.assertEqual(resp.status_code, 400)
        self.assertIn("error", resp.json())


if __name__ == "__main__":
    unittest.main()
------------------------------------------------------------

PROMPT 61:
"Generate Python code that imports the necessary modules and annotates the types of the ElasticInference client in boto3 version 1.34.0 using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import TypeVar\nfrom botocore.client import BaseClient\nfrom elasticinference import ElasticInference\n\nT_ElasticInference = TypeVar('T_ElasticInference', bound='ElasticInference')\nT_BaseClient = TypeVar('T_BaseClient', bound='BaseClient')\n\nclass MyElasticInference(ElasticInference):\n   def __init__(self, *args: T_BaseClient, **kwargs: T_BaseClient) -> None:\n       super().__init__(*args, **kwargs)\n\nmy_elasticsearch = MyElasticInference(\n   region='my-region',\n   credentials=None,\n   config=None,\n)\n```"
RESPONSE:
```python
# Requires:
#   boto3==1.34.0
#   boto3-stubs[elastic-inference]==1.34.0  # generated by mypy-boto3-builder==7.21.0

from __future__ import annotations

import boto3
from boto3.session import Session
from botocore.config import Config

from mypy_boto3_elastic_inference import ElasticInferenceClient
from mypy_boto3_elastic_inference.type_defs import (
    DescribeAcceleratorsResponseTypeDef,
)

# Create a typed boto3 Session
session: Session = boto3.Session(region_name="us-west-2")

# Optional botocore client config
config: Config = Config(retries={"max_attempts": 10, "mode": "standard"})

# Create a typed Elastic Inference client
elastic_inference_client: ElasticInferenceClient = session.client(
    "elastic-inference",
    config=config,
)

# Example typed call
response: DescribeAcceleratorsResponseTypeDef = elastic_inference_client.describe_accelerators()
print(response)
```
------------------------------------------------------------

PROMPT 62:
"Generate Python code that imports the required modules and annotates the types for the boto3.WAFRegional 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom boto3.waf_regional import WAFRegional\n\nmy_waf_regional_client: WAFRegional = WAFRegional(\n   client=BaseClient(\n       aws_access_key_id='YOUR_AWS_ACCESS_KEY',\n       aws_secret_access_key='YOUR_AWS_SECRET_KEY',\n       region='YOUR_REGION'\n   )\n)\n\n# Annotate the types of the methods and properties\nmy_waf_regional_client.describe_web_acls().response_type = List[dict]\nmy_waf_regional_client.get_web_acl().response_type = dict\nmy_waf_regional_client.put_web_acl().parameters"
RESPONSE:
# pip install boto3 mypy-boto3-waf-regional==1.34.0
from __future__ import annotations

from typing import List

import boto3
from mypy_boto3_waf_regional import WAFRegionalClient
from mypy_boto3_waf_regional.type_defs import (
    GetWebACLResponseTypeDef,
    ListWebACLsResponseTypeDef,
    UpdateWebACLResponseTypeDef,
    WebACLUpdateTypeDef,
)

# Create a typed WAF Regional client
waf_regional: WAFRegionalClient = boto3.client(
    "waf-regional",
    region_name="YOUR_REGION",
    aws_access_key_id="YOUR_AWS_ACCESS_KEY",
    aws_secret_access_key="YOUR_AWS_SECRET_KEY",
)

# Typed response for listing Web ACLs
list_resp: ListWebACLsResponseTypeDef = waf_regional.list_web_acls(Limit=100)

# Typed response for getting a specific Web ACL
get_resp: GetWebACLResponseTypeDef = waf_regional.get_web_acl(WebACLId="YOUR_WEB_ACL_ID")

# Typed parameters and response for updating a Web ACL
updates: List[WebACLUpdateTypeDef] = [
    {
        "Action": "INSERT",
        "ActivatedRule": {
            "Priority": 1,
            "RuleId": "YOUR_RULE_ID",
            "Action": {"Type": "ALLOW"},
        },
    }
]

update_resp: UpdateWebACLResponseTypeDef = waf_regional.update_web_acl(
    WebACLId="YOUR_WEB_ACL_ID",
    ChangeToken=waf_regional.get_change_token()["ChangeToken"],
    Updates=updates,
)
------------------------------------------------------------

PROMPT 63:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.MediaConvert 1.34.12 service using mypy-boto3-builder 7.23.0, like this:\n```python\nfrom typing import List, Dict\nfrom botocore.client import BaseClient\nfrom boto3.mediaconvert import MediaConvert\n\n# Annotate the types of the MediaConvert client\nmc = MediaConvert(\n   client=BaseClient,\n   config=None,\n   region=\"us-west-2\",\n)\n\n# Annotate the types of the MediaConvert client's methods\nmc.create_job(\n   job_template={\n       \"input\": {\n           \"sources\": [\n               {\"source\": \"s3://my-bucket/my-video.mp4\"}\n           ]\n       },\n       \"output\": {\n           \"destination\": \"s3://my-bucket/output\"\n       }\n   },\n   # Other parameters...\n)\n```"
RESPONSE:
```python
# Requires:
#   pip install boto3==1.34.12 mypy-boto3-mediaconvert==1.34.12

from typing import Mapping
import boto3

from mypy_boto3_mediaconvert import MediaConvertClient
from mypy_boto3_mediaconvert.type_defs import (
    CreateJobResponseTypeDef,
    ListJobsResponseTypeDef,
)
from mypy_boto3_mediaconvert.paginator import ListJobsPaginator
from mypy_boto3_mediaconvert.client import exceptions as mc_exceptions

# Typed MediaConvert client
mc: MediaConvertClient = boto3.client("mediaconvert", region_name="us-west-2")

# Create a job using a job template (avoids building full Settings by hand)
try:
    create_resp: CreateJobResponseTypeDef = mc.create_job(
        JobTemplate="my-job-template",
        Role="arn:aws:iam::123456789012:role/MediaConvert_Default_Role",
        Queue="arn:aws:mediaconvert:us-west-2:123456789012:queues/Default",
        UserMetadata={"project": "demo"},  # Mapping[str, str]
        # You can also pass Settings=JobSettingsTypeDef instead of JobTemplate
    )
except mc_exceptions.BadRequestException as e:
    raise
except mc_exceptions.ForbiddenException as e:
    raise

# List jobs with direct response typing
list_resp: ListJobsResponseTypeDef = mc.list_jobs(MaxResults=10)

# Use a typed paginator
paginator: ListJobsPaginator = mc.get_paginator("list_jobs")
for page in paginator.paginate():
    jobs = page["Jobs"]  # Typed list of JobTypeDef
    # process jobs...
```
------------------------------------------------------------

PROMPT 64:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.KinesisVideo 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import KinesisVideoClient\n\n# Define type hints for boto3.KinesisVideo methods\nclass KinesisVideo(BaseClient):\n   def create_stream(self, StreamName: str, Description: str = None) -> dict:\n       return self.call(\"create_stream\", StreamName, Description)\n\n   def delete_stream(self, StreamName: str) -> None:\n       return self.call(\"delete_stream\", StreamName)\n\n   def list_streams(self, Limit: int = None) -> List[dict]:\n       return self.call(\"list_streams\", Limit)\n\n   def describe_stream(self, StreamName: str) -> dict:\n       return self.call(\"describe_stream\", StreamName)\n"
RESPONSE:
```python
from __future__ import annotations

from typing import Optional, Dict
import boto3
from botocore.client import BaseClient

# Typed client and output shapes generated by mypy-boto3-builder 7.21.0 for boto3 1.34.x
from mypy_boto3_kinesisvideo import KinesisVideoClient
from mypy_boto3_kinesisvideo.type_defs import (
    CreateStreamOutputTypeDef,
    DeleteStreamOutputTypeDef,
    ListStreamsOutputTypeDef,
    DescribeStreamOutputTypeDef,
)


# Define type hints for boto3.kinesisvideo methods
class KinesisVideo(BaseClient):
    def create_stream(
        self,
        *,
        StreamName: str,
        DeviceName: Optional[str] = None,
        MediaType: Optional[str] = None,
        KmsKeyId: Optional[str] = None,
        DataRetentionInHours: Optional[int] = None,
        Tags: Optional[Dict[str, str]] = None,
    ) -> CreateStreamOutputTypeDef: ...
    
    def delete_stream(
        self,
        *,
        StreamARN: str,
        CurrentVersion: Optional[str] = None,
    ) -> DeleteStreamOutputTypeDef: ...
    
    def list_streams(
        self,
        *,
        MaxResults: Optional[int] = None,
        NextToken: Optional[str] = None,
    ) -> ListStreamsOutputTypeDef: ...
    
    def describe_stream(
        self,
        *,
        StreamName: Optional[str] = None,
        StreamARN: Optional[str] = None,
    ) -> DescribeStreamOutputTypeDef: ...


# Typed boto3 client
kinesis_video_client: KinesisVideoClient = boto3.client("kinesisvideo")
```
------------------------------------------------------------

PROMPT 65:
"Generate Python code that imports the required modules and annotates the types of the boto3.DatabaseMigrationService service, version 1.34.0, using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import Callable\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import ServiceClientBuilder\n\nclass DatabaseMigrationService(BaseClient):\n   def __init__(self, *, config: dict = None, **kwargs) -> None:\n       super().__init__(config=config, **kwargs)\n       self.config = config\n\n   def create_migration(self, *, migration_type: str, selection: str, *, dry_run: bool = False, **kwargs) -> dict:\n       return self._create_migration(migration_type, selection, dry_run=dry_run, **kwargs)\n\n   def describe_migration(self, *, migration_arn: str, **kwargs) -> dict:\n       return self._describe_migration(migration_arn, **kwargs)"
RESPONSE:
from __future__ import annotations

from typing import Any, Mapping, Optional
from botocore.client import BaseClient
from mypy_boto3_builder import ServiceClientBuilder  # mypy-boto3-builder v7.21.0
from mypy_boto3_database_migration_service.client import DatabaseMigrationServiceClient
from mypy_boto3_database_migration_service.type_defs import (
    CreateDataMigrationResponseTypeDef,
    DescribeDataMigrationsResponseTypeDef,
)


class DatabaseMigrationService(BaseClient):
    def __init__(self, *, config: Optional[Mapping[str, Any]] = None, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self.config = config

    def create_migration(
        self,
        *,
        migration_type: str,
        selection: str,
        dry_run: bool = False,
        **kwargs: Any,
    ) -> CreateDataMigrationResponseTypeDef:
        raise NotImplementedError

    def describe_migration(
        self,
        *,
        migration_arn: str,
        **kwargs: Any,
    ) -> DescribeDataMigrationsResponseTypeDef:
        raise NotImplementedError
------------------------------------------------------------

PROMPT 66:
"Generate Python code that imports the required modules and annotates the types for the boto3.SimpleDB 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import TypeAnnotator\n\nannotator = TypeAnnotator(BaseClient)\n\n# Define type annotations for boto3.SimpleDB 1.34.0 service\nclass SimpleDB(BaseClient):\n   def __init__(self, *args, **kwargs):\n       \"\"\"\n       Initializes a new instance of the SimpleDB class.\n       :param args: List[str]\n       :param kwargs: dict\n       \"\"\"\n       super().__init__(*args, **kwargs)\n   \n   def create_domain(self, DomainName: str, *, Description: str = None, **kwargs):\n       \"\"\"\n       Creates a new domain.\n       :param DomainName: str\n       :param Description: str\n       :param kwargs: dict\n       :rtype"
RESPONSE:
```python
from typing import Any, Dict, List, Optional, Sequence
from botocore.client import BaseClient
from mypy_boto3_builder import TypeAnnotator
from typing_extensions import NotRequired, Required, TypedDict

# Initialize TypeAnnotator (placeholder to indicate use of mypy-boto3-builder)
annotator = TypeAnnotator(BaseClient)


# Input shapes
class ReplaceableAttributeType(TypedDict, total=False):
    Name: Required[str]
    Value: Required[str]
    Replace: NotRequired[bool]


class DeletableAttributeType(TypedDict, total=False):
    Name: Required[str]
    Value: NotRequired[str]


class ExpectedAttributeValueType(TypedDict, total=False):
    Name: Required[str]
    Value: NotRequired[str]
    Exists: NotRequired[bool]


class ReplaceableItemType(TypedDict, total=False):
    Name: Required[str]
    Attributes: Required[Sequence[ReplaceableAttributeType]]


class DeletableItemType(TypedDict, total=False):
    Name: Required[str]
    Attributes: NotRequired[Sequence[DeletableAttributeType]]


# Output shapes
class RetrievedAttributeType(TypedDict, total=False):
    Name: Required[str]
    Value: Required[str]
    AlternateNameEncoding: NotRequired[str]
    AlternateValueEncoding: NotRequired[str]


class GetAttributesOutputType(TypedDict, total=False):
    Attributes: List[RetrievedAttributeType]


class ItemAttributeType(TypedDict, total=False):
    Name: Required[str]
    Value: Required[str]
    AlternateNameEncoding: NotRequired[str]
    AlternateValueEncoding: NotRequired[str]


class ItemType(TypedDict, total=False):
    Name: Required[str]
    Attributes: Required[List[ItemAttributeType]]


class SelectOutputType(TypedDict, total=False):
    Items: NotRequired[List[ItemType]]
    NextToken: NotRequired[str]


class ListDomainsOutputType(TypedDict, total=False):
    DomainNames: NotRequired[List[str]]
    NextToken: NotRequired[str]


class DomainMetadataOutputType(TypedDict):
    ItemCount: int
    ItemNamesSizeBytes: int
    AttributeNameCount: int
    AttributeNamesSizeBytes: int
    AttributeValueCount: int
    AttributeValuesSizeBytes: int
    Timestamp: int


# Define type annotations for boto3.SimpleDB 1.34.0 service
class SimpleDB(BaseClient):
    def __init__(self, *args: Any, **kwargs: Any):
        """
        Initializes a new instance of the SimpleDB client.
        :param args: Additional positional arguments.
        :param kwargs: Additional keyword arguments.
        """
        super().__init__(*args, **kwargs)

    def create_domain(self, *, DomainName: str) -> Dict[str, Any]:
        """
        Creates a new domain.
        :param DomainName: The name of the domain to create.
        :return: An empty response with metadata.
        """

    def delete_domain(self, *, DomainName: str) -> Dict[str, Any]:
        """
        Deletes a domain.
        :param DomainName: The name of the domain to delete.
        :return: An empty response with metadata.
        """

    def domain_metadata(self, *, DomainName: str) -> DomainMetadataOutputType:
        """
        Returns information about the domain.
        :param DomainName: The name of the domain.
        :return: Domain metadata.
        """

    def list_domains(
        self, *, MaxNumberOfDomains: Optional[int] = None, NextToken: Optional[str] = None
    ) -> ListDomainsOutputType:
        """
        Lists domains.
        :param MaxNumberOfDomains: The maximum number of domains to return.
        :param NextToken: Pagination token.
        :return: Domain names and optional next token.
        """

    def put_attributes(
        self,
        *,
        DomainName: str,
        ItemName: str,
        Attributes: Sequence[ReplaceableAttributeType],
        Expected: Optional[ExpectedAttributeValueType] = None,
    ) -> Dict[str, Any]:
        """
        Replaces or creates attributes of an item.
        :param DomainName: The domain name.
        :param ItemName: The item name.
        :param Attributes: Attributes to replace or create.
        :param Expected: Optional conditional constraint.
        :return: An empty response with metadata.
        """

    def delete_attributes(
        self,
        *,
        DomainName: str,
        ItemName: str,
        Attributes: Optional[Sequence[DeletableAttributeType]] = None,
        Expected: Optional[ExpectedAttributeValueType] = None,
    ) -> Dict[str, Any]:
        """
        Deletes attributes of an item.
        :param DomainName: The domain name.
        :param ItemName: The item name.
        :param Attributes: Optional attributes to delete.
        :param Expected: Optional conditional constraint.
        :return: An empty response with metadata.
        """

    def batch_put_attributes(
        self,
        *,
        DomainName: str,
        Items: Sequence[ReplaceableItemType],
    ) -> Dict[str, Any]:
        """
        Performs multiple PutAttributes operations in a single call.
        :param DomainName: The domain name.
        :param Items: A list of items with attributes to put.
        :return: An empty response with metadata.
        """

    def batch_delete_attributes(
        self,
        *,
        DomainName: str,
        Items: Sequence[DeletableItemType],
    ) -> Dict[str, Any]:
        """
        Performs multiple DeleteAttributes operations in a single call.
        :param DomainName: The domain name.
        :param Items: A list of items (and optional attributes) to delete.
        :return: An empty response with metadata.
        """

    def get_attributes(
        self,
        *,
        DomainName: str,
        ItemName: str,
        AttributeNames: Optional[Sequence[str]] = None,
        ConsistentRead: Optional[bool] = None,
    ) -> GetAttributesOutputType:
        """
        Returns attributes for an item.
        :param DomainName: The domain name.
        :param ItemName: The item name.
        :param AttributeNames: Optional list of attribute names to retrieve.
        :param ConsistentRead: If true, ensures consistent reads.
        :return: Retrieved attributes.
        """

    def select(
        self,
        *,
        SelectExpression: str,
        NextToken: Optional[str] = None,
        ConsistentRead: Optional[bool] = None,
    ) -> SelectOutputType:
        """
        Runs a select expression against a domain.
        :param SelectExpression: The SimpleDB select expression.
        :param NextToken: Pagination token.
        :param ConsistentRead: If true, ensures consistent reads.
        :return: Selected items and optional next token.
        """
```
------------------------------------------------------------

PROMPT 67:
"Generate Python code that imports the necessary libraries and loads the Nowcasting Dataset, which includes weather data from 2016 to 2020 for 37 cities across the United States, Canada, and Europe, with variables such as temperature, dew point, wind speed, and precipitation, and contains both hourly and daily observations.\n\nHere is an example of how you could generate Python code to accomplish this task:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load the Nowcasting Dataset\ndf = pd.read_csv('nowcasting_dataset.csv')\n\n# Select only the desired variables\ndf = df[['temperature', 'dew_point', 'wind_speed', 'precipitation']]\n\n# Convert the data to a suitable format\ndf = df.astype({'temperature': float, 'dew_point': float, 'wind_speed': float, 'precipitation': bool})\n\n# Split the dataset into training and testing sets\ntrain_df, test_df = df.split(test_size=0.2)\n```"
RESPONSE:
```python
import pandas as pd
import numpy as np
from pathlib import Path

# -----------------------------------------------------------------------------
# Configuration
# -----------------------------------------------------------------------------
# Expected variable names
VARS = ["temperature", "dew_point", "wind_speed", "precipitation"]

# Common alternative column names to standardize
ALT_NAMES = {
    "temperature": ["temp", "air_temperature", "Temperature"],
    "dew_point": ["dewpoint", "dew_point_temperature", "dewPoint"],
    "wind_speed": ["windspeed", "wind_spd", "Wind_Speed"],
    "precipitation": ["precip", "precip_rate", "precipitation_amount"],
}

# Candidate file locations (adjust as needed)
CANDIDATE_FILES = [
    Path("nowcasting_hourly.csv"),
    Path("nowcasting_daily.csv"),
    Path("nowcasting_dataset.csv"),  # combined hourly+daily in one file
]


# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
def _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    # Timestamp
    time_cols = ["timestamp", "datetime", "time", "date_time", "valid_time"]
    for tcol in time_cols:
        if tcol in df.columns:
            df = df.rename(columns={tcol: "timestamp"})
            break
    if "timestamp" not in df.columns:
        raise ValueError("No timestamp/datetime column found.")

    # Variables
    rename_map = {}
    for canonical in VARS:
        for alt in ALT_NAMES.get(canonical, []):
            if alt in df.columns:
                rename_map[alt] = canonical
    df = df.rename(columns=rename_map)

    # Ensure all required variables exist (may still be missing if not present)
    missing = [v for v in VARS if v not in df.columns]
    if missing:
        raise ValueError(f"Missing expected columns: {missing}")

    return df


def _enforce_types(df: pd.DataFrame) -> pd.DataFrame:
    # Parse timestamp
    df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True, errors="coerce")
    df = df.dropna(subset=["timestamp"])

    # Cast weather variables to float
    for col in VARS:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    # Optional: downcast to save memory
    for col in VARS:
        df[col] = pd.to_numeric(df[col], downcast="float")

    return df


def _infer_or_set_freq(df: pd.DataFrame, freq_hint: str | None) -> pd.DataFrame:
    if freq_hint in {"hourly", "daily"}:
        df["freq"] = freq_hint
        return df

    # Infer by inspecting hour variability
    try:
        hours_unique = df["timestamp"].dt.hour.nunique(dropna=True)
        df["freq"] = "hourly" if hours_unique > 1 else "daily"
    except Exception:
        df["freq"] = "unknown"
    return df


def _select_and_order_columns(df: pd.DataFrame) -> pd.DataFrame:
    meta_cols = [c for c in ["city", "country", "latitude", "longitude"] if c in df.columns]
    cols = meta_cols + ["timestamp"] + VARS + ([ "freq" ] if "freq" in df.columns else [])
    return df[cols]


def load_nowcasting_dataset() -> pd.DataFrame:
    # Find existing files
    existing = [p for p in CANDIDATE_FILES if p.exists()]
    if not existing:
        raise FileNotFoundError(
            "No Nowcasting CSV files found. Place 'nowcasting_hourly.csv' and/or "
            "'nowcasting_daily.csv' or a combined 'nowcasting_dataset.csv' in the working directory."
        )

    dfs = []
    for p in existing:
        df = pd.read_csv(p)
        df = _standardize_columns(df)
        df = _enforce_types(df)

        # Hint frequency from filename if possible
        stem = p.stem.lower()
        freq_hint = "hourly" if "hour" in stem else ("daily" if "day" in stem else None)
        df = _infer_or_set_freq(df, freq_hint=freq_hint)

        df = _select_and_order_columns(df)
        dfs.append(df)

    data = pd.concat(dfs, ignore_index=True).sort_values(["timestamp"]).reset_index(drop=True)
    return data


# -----------------------------------------------------------------------------
# Load dataset
# -----------------------------------------------------------------------------
df = load_nowcasting_dataset()

# Optional sanity checks and filtering for 2016–2020
df = df[(df["timestamp"] >= "2016-01-01") & (df["timestamp"] < "2021-01-01")].copy()

# Keep only desired variables plus identifiers
id_cols = [c for c in ["city", "country", "latitude", "longitude", "timestamp", "freq"] if c in df.columns]
df = df[id_cols + VARS].copy()

# Example: Time-based train/test split (train: 2016–2019, test: 2020)
train_df = df[df["timestamp"] < "2020-01-01"].copy()
test_df = df[(df["timestamp"] >= "2020-01-01") & (df["timestamp"] < "2021-01-01")].copy()

# Small summary
print("Loaded Nowcasting Dataset")
print(f"Total rows: {len(df):,}")
if "freq" in df.columns:
    print(df["freq"].value_counts(dropna=False))
print("Date range:", df["timestamp"].min(), "to", df["timestamp"].max())
print("Train rows:", len(train_df), "Test rows:", len(test_df))
```
------------------------------------------------------------

PROMPT 68:
"Generate Python code that imports the SQLQueryEngine module and creates a connection to a MySQL database, executes a SELECT statement, and prints the results to the console.\n\nHere is an example of how this could be done:\n```\nfrom sqlqueryengine import *\n\nconn = connect('MySQL', host='localhost', user='root', passwd='password')\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM employees\")\nresults = cursor.fetchall()\nfor row in results:\n   print(row)\n```\nThis code imports the `sqlqueryengine` module, creates a connection to a MySQL database using the `connect()` function, specifies the host, username, and password. Then it creates a cursor object and execute a select statement using the `cursor.execute()` method, fetches all the rows from the result set using the `cursor.fetchall()` method and finally iterates over the rows and prints them to the console using a for loop."
RESPONSE:
from sqlqueryengine import connect

def main():
    # Create a connection to the MySQL database
    conn = connect(
        'MySQL',
        host='localhost',
        user='root',
        passwd='password',
        database='your_database_name'  # optional if default is set
    )

    try:
        cursor = conn.cursor()
        # Execute a SELECT statement
        cursor.execute("SELECT * FROM employees")
        # Fetch and print all rows
        for row in cursor.fetchall():
            print(row)
    finally:
        # Clean up resources
        try:
            cursor.close()
        except Exception:
            pass
        conn.close()

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 69:
"Generate Python code that imports the necessary modules and annotates the types of the IoT Events Data service, version 1.34.0, using mypy-boto3-builder, version 7.21.0:\n```\nfrom typing import List, Dict\nfrom boto3.iot import IoTEventsData\n\n# Define the type annotations for the IoT Events Data service\nclass IoTEventsDataClient(IoTEventsData):\n   def __init__(self, config: dict) -> None:\n       self.config = config\n   \n   def send_events(self, events: List[dict]) -> None:\n       \"\"\"\n       Sends an array of events to the IoT Events data plane.\n       \n       :param events: The list of events to send\n       :type events: list of dict\n       :return: None\n       \"\"\"\n       pass\n   \n   def receive_events(self, next_token: str) -> List[dict]:\n       \"\"\"\n       Receives an array of events from the IoT Events data plane.\n       \n       :param next_token: The token to use when paginating"
RESPONSE:
from __future__ import annotations

from typing import Optional, Sequence

import boto3
from botocore.config import Config

# Types generated by mypy-boto3-builder 7.21.0 for boto3/botocore 1.34.x
from mypy_boto3_iotevents_data import IoTEventsDataClient
from mypy_boto3_iotevents_data.type_defs import (
    BatchPutMessageResponseTypeDef,
    BatchUpdateDetectorResponseTypeDef,
    DescribeDetectorResponseTypeDef,
    ListDetectorsResponseTypeDef,
    MessageTypeDef,
    UpdateDetectorRequestTypeDef,
)


class IoTEventsDataService:
    """
    Typed wrapper for AWS IoT Events Data (iotevents-data) using mypy-boto3 stubs for 1.34.x.
    """

    def __init__(self, region_name: Optional[str] = None, config: Optional[Config] = None) -> None:
        self._client: IoTEventsDataClient = boto3.client(
            "iotevents-data",
            region_name=region_name,
            config=config,
        )

    @property
    def client(self) -> IoTEventsDataClient:
        return self._client

    def send_messages(self, messages: Sequence[MessageTypeDef]) -> BatchPutMessageResponseTypeDef:
        """
        Send an array of IoT Events messages to the data plane (BatchPutMessage).
        """
        # boto3 expects a list; Sequence is accepted for typing convenience
        return self._client.batch_put_message(messages=list(messages))

    def list_detectors(
        self,
        detector_model_name: str,
        state_name: Optional[str] = None,
        next_token: Optional[str] = None,
        max_results: Optional[int] = None,
    ) -> ListDetectorsResponseTypeDef:
        """
        List detectors for the specified detector model.
        """
        return self._client.list_detectors(
            detectorModelName=detector_model_name,
            stateName=state_name,
            nextToken=next_token,
            maxResults=max_results,
        )

    def describe_detector(
        self,
        detector_model_name: str,
        key_value: Optional[str] = None,
    ) -> DescribeDetectorResponseTypeDef:
        """
        Get the current state and details of a detector instance.
        """
        return self._client.describe_detector(
            detectorModelName=detector_model_name,
            keyValue=key_value,
        )

    def batch_update_detector(
        self,
        updates: Sequence[UpdateDetectorRequestTypeDef],
    ) -> BatchUpdateDetectorResponseTypeDef:
        """
        Update one or more detectors (BatchUpdateDetector).
        """
        return self._client.batch_update_detector(detectors=list(updates))
------------------------------------------------------------

PROMPT 70:
"Generate Python code that exports Prometheus metrics to OpenCensus using the OpenCensus exporter.\n\nHere is an example of Python code that could be used to export Prometheus metrics to OpenCensus using the OpenCensus exporter:\n```\nimport prometheus.client\nfrom opencensus import stats\n\n# Initialize the Prometheus client\nclient = prometheus.client.PrometheusClient()\n\n# Get a list of all metrics from the Prometheus client\nmetrics = client.get_metric_families()\n\n# Iterate over each metric family and export it to OpenCensus\nfor metric_family in metrics:\n   # Create a new OpenCensus view for this metric family\n   view = stats.View('prometheus_exporter', metric_family['name'])\n\n   # Add the metric family's labels to the view\n   for label in metric_family['labels']:\n       view.add_label(label)\n\n   # Add a counter for the number of samples in this view\n   counter = view.counter('samples')\n\n   # Iterate over each sample in the metric family and add"
RESPONSE:
```python
"""
Bridge Prometheus metrics into OpenCensus and export them with an OpenCensus exporter.

- Scrapes a Prometheus text endpoint.
- Converts counter/gauge metrics to OpenCensus measures.
- Records values with LastValue aggregation and exports via OpenCensus.

Requirements:
  pip install requests prometheus-client opencensus
Optionally for different exporters:
  - Print exporter (no extra deps): used by default
  - Stackdriver (Google Cloud): pip install opencensus-ext-stackdriver

Run:
  PROM_TARGET=http://localhost:9100/metrics python prom_to_opencensus.py
"""

import os
import time
import requests
from typing import Dict, Set

from prometheus_client.parser import text_string_to_metric_families

from opencensus.stats import stats as stats_module
from opencensus.stats import measure as measure_module
from opencensus.stats import view as view_module
from opencensus.stats import aggregation as aggregation_module
from opencensus.tags import tag_map as tag_map_module
from opencensus.tags import tag_key as tag_key_module
from opencensus.tags import tag_value as tag_value_module

# Choose an OpenCensus stats exporter.
# Default to PrintExporter (no external services needed).
# To use Stackdriver instead, set OC_EXPORTER=stackdriver and provide GCP project ID.
OC_EXPORTER = os.getenv("OC_EXPORTER", "print").lower()

exporter = None
if OC_EXPORTER == "print":
    try:
        from opencensus.stats.exporters import print_exporter
    except ImportError:
        # Fallback path for older package structure
        from opencensus.stats.exporter import print_exporter  # type: ignore
    exporter = print_exporter.PrintExporter()
elif OC_EXPORTER == "stackdriver":
    from opencensus.ext.stackdriver import stats_exporter as stackdriver_exporter

    project_id = os.getenv("GCP_PROJECT") or os.getenv("GOOGLE_CLOUD_PROJECT")
    if not project_id:
        raise RuntimeError("Set GCP_PROJECT (or GOOGLE_CLOUD_PROJECT) for Stackdriver exporter.")
    exporter = stackdriver_exporter.new_stats_exporter(
        stackdriver_exporter.Options(project_id=project_id)
    )
else:
    raise RuntimeError(f"Unknown OC_EXPORTER={OC_EXPORTER}. Use 'print' or 'stackdriver'.")

PROM_TARGET = os.getenv("PROM_TARGET", "http://localhost:9100/metrics")
SCRAPE_INTERVAL_SEC = float(os.getenv("SCRAPE_INTERVAL_SEC", "15"))


class PromToOCBridge:
    def __init__(self):
        stats = stats_module.stats
        self.stats_recorder = stats.stats_recorder
        self.view_manager = stats.view_manager

        # Register chosen exporter
        self.view_manager.register_exporter(exporter)

        # Caches
        self.measures: Dict[str, measure_module.MeasureFloat] = {}
        self.views: Dict[str, view_module.View] = {}
        self.tag_keys: Dict[str, tag_key_module.TagKey] = {}

    def _get_or_create_tag_key(self, name: str) -> tag_key_module.TagKey:
        if name not in self.tag_keys:
            self.tag_keys[name] = tag_key_module.TagKey(name)
        return self.tag_keys[name]

    def _ensure_measure_and_view(self, metric_name: str, metric_type: str, label_names: Set[str]):
        # Reuse measure/view if already created
        if metric_name in self.measures and metric_name in self.views:
            return

        # Prometheus numeric values are floats; use MeasureFloat
        measure = measure_module.MeasureFloat(
            name=f"prom/{metric_name}",
            description=f"Prometheus {metric_type} mapped into OpenCensus",
            unit="1",
        )

        # Use LastValue aggregation to capture the latest scraped value.
        aggregation = aggregation_module.LastValueAggregation()

        # Create TagKeys for all label names seen in the family
        columns = [self._get_or_create_tag_key(ln) for ln in sorted(label_names)]

        view = view_module.View(
            name=f"prom/{metric_name}",
            description=f"View for Prometheus {metric_type} {metric_name}",
            columns=columns,
            measure=measure,
            aggregation=aggregation,
        )

        # Register view with the manager
        self.view_manager.register_view(view)

        # Cache
        self.measures[metric_name] = measure
        self.views[metric_name] = view

    def record_sample(self, metric_name: str, labels: Dict[str, str], value: float):
        if metric_name not in self.measures:
            # Should not happen if _ensure_measure_and_view was called, but guard anyway.
            self._ensure_measure_and_view(metric_name, "gauge", set(labels.keys()))

        measure = self.measures[metric_name]

        # Build tag map for this sample
        tmap = tag_map_module.TagMap()
        for k, v in labels.items():
            tmap.insert(self._get_or_create_tag_key(k), tag_value_module.TagValue(str(v)))

        mmap = self.stats_recorder.new_measurement_map()
        mmap.measure_float_put(measure, float(value))
        mmap.record(tmap)

    def scrape_and_export(self, prom_url: str):
        resp = requests.get(prom_url, timeout=10)
        resp.raise_for_status()
        text = resp.text

        for family in text_string_to_metric_families(text):
            mtype = family.type or "gauge"
            # Map only counters and gauges for simplicity
            if mtype not in ("counter", "gauge"):
                # Skip histograms/summaries/etc. or implement mapping as needed
                continue

            # Collect union of label names across samples
            label_names: Set[str] = set()
            for sample in family.samples:
                # sample.name may differ due to suffixes for summary/histogram; we skip those types above.
                label_names.update((sample.labels or {}).keys())

            # Ensure we have a measure/view for this metric family name.
            # Use family.name (base metric name), not sample.name (which might have suffixes).
            self._ensure_measure_and_view(family.name, mtype, label_names)

            # Record each sample into the corresponding OC measure
            for sample in family.samples:
                # For counters/gauges, sample.name should equal family.name. If not, ignore the suffix variants.
                if sample.name != family.name:
                    continue
                labels = sample.labels or {}
                value = float(sample.value)
                self.record_sample(family.name, labels, value)


def main():
    bridge = PromToOCBridge()

    print(f"Scraping {PROM_TARGET} every {SCRAPE_INTERVAL_SEC}s; exporting via {OC_EXPORTER} exporter.")
    while True:
        try:
            bridge.scrape_and_export(PROM_TARGET)
        except Exception as e:
            # Avoid crashing on transient scrape/export errors
            print(f"[warn] scrape/export failed: {e}")
        time.sleep(SCRAPE_INTERVAL_SEC)


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 71:
"Generate Python code that imports the necessary libraries and uses the Azure Data Bricks Management Client Library to manage Azure Data Bricks resources, including creating and deleting data bricks, listing and retrieving data brick metadata, and updating data brick properties."
RESPONSE:
# Prereqs:
#   pip install azure-identity azure-mgmt-databricks
# Auth options:
#   - Environment vars: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
#   - Or Azure CLI: az login (DefaultAzureCredential will pick it up)

import os
from typing import Dict, Iterable, Optional

from azure.identity import DefaultAzureCredential
from azure.core.exceptions import HttpResponseError
from azure.mgmt.databricks import DatabricksManagementClient
from azure.mgmt.databricks.models import Workspace, WorkspaceUpdate, Sku


def get_client(subscription_id: str) -> DatabricksManagementClient:
    credential = DefaultAzureCredential()
    return DatabricksManagementClient(credential=credential, subscription_id=subscription_id)


def create_workspace(
    client: DatabricksManagementClient,
    resource_group: str,
    workspace_name: str,
    location: str,
    sku_name: str = "premium",
    tags: Optional[Dict[str, str]] = None,
) -> Workspace:
    params = Workspace(
        location=location,
        sku=Sku(name=sku_name),
        tags=tags or {},
        # Add optional settings here (e.g., managed_resource_group_id, custom_parameters, etc.)
    )
    poller = client.workspaces.begin_create_or_update(
        resource_group_name=resource_group,
        workspace_name=workspace_name,
        parameters=params,
    )
    return poller.result()


def update_workspace(
    client: DatabricksManagementClient,
    resource_group: str,
    workspace_name: str,
    new_sku_name: Optional[str] = None,
    new_tags: Optional[Dict[str, str]] = None,
) -> Workspace:
    update_params = WorkspaceUpdate(
        sku=Sku(name=new_sku_name) if new_sku_name else None,
        tags=new_tags,
    )
    poller = client.workspaces.begin_update(
        resource_group_name=resource_group,
        workspace_name=workspace_name,
        parameters=update_params,
    )
    return poller.result()


def get_workspace(
    client: DatabricksManagementClient,
    resource_group: str,
    workspace_name: str,
) -> Workspace:
    return client.workspaces.get(
        resource_group_name=resource_group,
        workspace_name=workspace_name,
    )


def list_workspaces_by_resource_group(
    client: DatabricksManagementClient,
    resource_group: str,
) -> Iterable[Workspace]:
    return client.workspaces.list_by_resource_group(resource_group_name=resource_group)


def list_workspaces_by_subscription(
    client: DatabricksManagementClient,
) -> Iterable[Workspace]:
    return client.workspaces.list_by_subscription()


def delete_workspace(
    client: DatabricksManagementClient,
    resource_group: str,
    workspace_name: str,
) -> None:
    poller = client.workspaces.begin_delete(
        resource_group_name=resource_group,
        workspace_name=workspace_name,
    )
    poller.result()


def print_workspace_metadata(ws: Workspace) -> None:
    # Safely print commonly used metadata
    sku_name = getattr(getattr(ws, "sku", None), "name", None)
    print(
        "Workspace metadata:\n"
        f"  id: {getattr(ws, 'id', None)}\n"
        f"  name: {getattr(ws, 'name', None)}\n"
        f"  location: {getattr(ws, 'location', None)}\n"
        f"  sku: {sku_name}\n"
        f"  url: {getattr(ws, 'workspace_url', None)}\n"
        f"  tags: {getattr(ws, 'tags', None)}\n"
        f"  state: {getattr(ws, 'provisioning_state', None)}"
    )


if __name__ == "__main__":
    # Inputs (replace with your values or set environment variables)
    SUBSCRIPTION_ID = os.getenv("AZURE_SUBSCRIPTION_ID", "<your-subscription-id>")
    RESOURCE_GROUP = os.getenv("AZURE_RESOURCE_GROUP", "<your-resource-group>")
    WORKSPACE_NAME = os.getenv("DATABRICKS_WORKSPACE_NAME", "my-databricks-ws")
    LOCATION = os.getenv("AZURE_LOCATION", "eastus")

    client = get_client(SUBSCRIPTION_ID)

    try:
        # Create a new Azure Databricks workspace
        print("Creating workspace...")
        ws = create_workspace(
            client,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME,
            location=LOCATION,
            sku_name="premium",  # or "standard"
            tags={"env": "dev", "owner": "data-platform"},
        )
        print("Created:")
        print_workspace_metadata(ws)

        # Retrieve metadata (GET)
        print("\nRetrieving workspace metadata...")
        ws_fetched = get_workspace(client, RESOURCE_GROUP, WORKSPACE_NAME)
        print_workspace_metadata(ws_fetched)

        # List workspaces in resource group
        print("\nListing workspaces in resource group...")
        for w in list_workspaces_by_resource_group(client, RESOURCE_GROUP):
            print(f"- {w.name} ({w.location}) | SKU={getattr(getattr(w, 'sku', None), 'name', None)}")

        # List workspaces in subscription
        print("\nListing workspaces in subscription...")
        for w in list_workspaces_by_subscription(client):
            print(f"- {w.name} ({w.location}) | RG={w.id.split('/')[4]}")

        # Update workspace properties (e.g., tags or SKU)
        print("\nUpdating workspace tags...")
        ws_updated = update_workspace(
            client,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME,
            # new_sku_name="standard",  # uncomment to change SKU if allowed
            new_tags={"env": "test", "cost-center": "cc-1234"},
        )
        print("Updated:")
        print_workspace_metadata(ws_updated)

        # Delete workspace (uncomment to delete)
        # print("\nDeleting workspace...")
        # delete_workspace(client, RESOURCE_GROUP, WORKSPACE_NAME)
        # print("Workspace deleted.")

    except HttpResponseError as e:
        print(f"Azure API error: {e.message}")
        if e.response is not None:
            print(f"Status code: {e.response.status_code}")
            print(f"Body: {e.response.text() if hasattr(e.response, 'text') else e.response.content}")
    except Exception as ex:
        print(f"Unexpected error: {ex}")
------------------------------------------------------------

PROMPT 72:
"Generate Python code that uses the `requests` library to send an HTTP request to a JSON API, then uses the `json` library to parse the response into a Python dictionary, which can then be used to extract information from the JSON data and create a documentation for the parsed data."
RESPONSE:
# Requires: requests (pip install requests)
import json
import requests
from collections import defaultdict, Counter
from typing import Any, Dict, List, Tuple, Union

JSON = Union[Dict[str, Any], List[Any], str, int, float, bool, None]


def fetch_json(url: str, params: Dict[str, Any] | None = None, headers: Dict[str, str] | None = None, timeout: int = 15) -> JSON:
    """
    Fetches JSON from a URL using requests, then parses it with the json library.
    Returns a Python object (dict, list, etc.).
    """
    default_headers = {
        "Accept": "application/json",
        "User-Agent": "ExampleClient/1.0 (+https://example.com)"
    }
    if headers:
        default_headers.update(headers)

    resp = requests.get(url, params=params, headers=default_headers, timeout=timeout)
    resp.raise_for_status()

    # Parse explicitly with the json library (rather than resp.json()) to meet requirements.
    return json.loads(resp.text)


def _typename(value: Any) -> str:
    if value is None:
        return "null"
    if isinstance(value, bool):
        return "boolean"
    if isinstance(value, int):
        return "number (int)"
    if isinstance(value, float):
        return "number (float)"
    if isinstance(value, str):
        return "string"
    if isinstance(value, list):
        return "array"
    if isinstance(value, dict):
        return "object"
    return type(value).__name__


def _sample_repr(value: Any, max_len: int = 80) -> str:
    try:
        s = json.dumps(value, ensure_ascii=False)
    except Exception:
        s = repr(value)
    if len(s) > max_len:
        s = s[: max_len - 3] + "..."
    return s


def _union_types(values: List[Any]) -> List[str]:
    return sorted({ _typename(v) for v in values })


def _summarize_list(items: List[Any]) -> Tuple[str, List[str]]:
    """
    Returns a short phrase about the list and suggested nested documentation lines.
    """
    n = len(items)
    if n == 0:
        return "array (empty)", []

    item_types = Counter(_typename(v) for v in items)
    type_summary = ", ".join(f"{t}: {c}" for t, c in item_types.most_common())
    headline = f"array (length={n}; item types: {type_summary})"
    return headline, []


def _summarize_object_fields(objs: List[Dict[str, Any]]) -> List[Tuple[str, Dict[str, Any]]]:
    """
    Given a list of dicts, compute per-key stats:
    - presence ratio
    - types seen
    - example value
    """
    key_stats: Dict[str, Dict[str, Any]] = {}
    total = len(objs)

    # Collect presence and samples
    for obj in objs:
        for k, v in obj.items():
            stats = key_stats.setdefault(k, {"count": 0, "values": [], "types": set(), "example": None})
            stats["count"] += 1
            stats["values"].append(v)
            stats["types"].add(_typename(v))
            if stats["example"] is None:
                stats["example"] = v

    # Sort keys: required first, then by name
    sorted_items = sorted(
        key_stats.items(),
        key=lambda kv: (-(kv[1]["count"] / total), kv[0].lower())
    )
    return sorted_items


def generate_documentation(data: JSON, root_name: str = "root", max_depth: int = 2) -> str:
    """
    Create plain-text documentation describing the shape of the parsed JSON.
    """
    lines: List[str] = []

    def walk(value: Any, name: str, depth: int) -> None:
        indent = "  " * depth
        tname = _typename(value)

        if isinstance(value, list):
            header, _ = _summarize_list(value)
            lines.append(f"{indent}- {name}: {header}")
            if depth >= max_depth or not value:
                return
            # If it's a list of objects, summarize fields across items
            if all(isinstance(v, dict) for v in value):
                lines.append(f"{indent}  Items are objects with fields:")
                field_summaries = _summarize_object_fields([v for v in value if isinstance(v, dict)])
                total = len(value)
                for k, stats in field_summaries:
                    present = stats["count"]
                    pct = 100.0 * present / total if total else 0.0
                    types = " | ".join(sorted(stats["types"]))
                    example = _sample_repr(stats["example"])
                    lines.append(f"{indent}  - {k} (present in {present}/{total} ~ {pct:.1f}%, type: {types}; example: {example})")
                    # Recurse one more level for nested objects/arrays (using the example value)
                    if depth + 1 < max_depth and isinstance(stats["example"], (dict, list)):
                        walk(stats["example"], f"{k} (example structure)", depth + 2)
            else:
                # Heterogeneous or simple arrays: show a few sample items
                sample_count = min(3, len(value))
                for i in range(sample_count):
                    vi = value[i]
                    lines.append(f"{indent}  - item[{i}]: {_typename(vi)}; example: {_sample_repr(vi)}")
                    if depth + 1 < max_depth and isinstance(vi, (dict, list)):
                        walk(vi, f"item[{i}] (structure)", depth + 2)

        elif isinstance(value, dict):
            lines.append(f"{indent}- {name}: object with {len(value)} field(s)")
            if depth >= max_depth or not value:
                return
            for k, v in sorted(value.items(), key=lambda kv: kv[0].lower()):
                lines.append(f"{indent}  - {k}: {_typename(v)}; example: {_sample_repr(v)}")
                if depth + 1 < max_depth and isinstance(v, (dict, list)):
                    walk(v, f"{k} (structure)", depth + 2)
        else:
            lines.append(f"{indent}- {name}: {tname}; example: {_sample_repr(value)}")

    walk(data, root_name, 0)
    return "\n".join(lines)


def extract_information_example(data: JSON) -> Dict[str, Any]:
    """
    Demonstrates extracting useful information from the parsed JSON.
    This example targets a list of 'user' objects like JSONPlaceholder /users.
    Falls back gracefully for other shapes.
    """
    result: Dict[str, Any] = {}

    # If the top-level is a list of objects with common fields like 'name' and 'email'
    if isinstance(data, list) and data and isinstance(data[0], dict):
        names = [d.get("name") for d in data if isinstance(d, dict) and "name" in d]
        emails = [d.get("email") for d in data if isinstance(d, dict) and "email" in d]
        result["count"] = len(data)
        if names:
            result["names"] = names
        if emails:
            result["emails"] = emails
        # Example: build a mapping id -> username if present
        id_to_username = {d.get("id"): d.get("username") for d in data if isinstance(d, dict) and "id" in d and "username" in d}
        if id_to_username:
            result["id_to_username"] = id_to_username
    else:
        # Generic fallback: just echo the top-level type and a short sample
        result["top_level_type"] = _typename(data)
        result["sample"] = _sample_repr(data, max_len=200)

    return result


def main():
    # Example public JSON API: no auth required.
    url = "https://jsonplaceholder.typicode.com/users"

    # 1) Fetch and parse JSON into Python data structures
    data = fetch_json(url)

    # 2) Use the parsed dictionary/list to extract information
    extracted = extract_information_example(data)

    # 3) Generate plain-text documentation for the parsed data
    doc = []
    doc.append(f"API: {url}")
    doc.append("Documentation for parsed JSON data")
    doc.append("---------------------------------")
    doc.append(generate_documentation(data, root_name="response", max_depth=2))
    doc.append("")
    doc.append("Example extracted information")
    doc.append("----------------------------")
    for k, v in extracted.items():
        doc.append(f"- {k}: {_sample_repr(v, max_len=200)}")
    documentation_text = "\n".join(doc)

    # Print to console
    print(documentation_text)

    # Optionally, write to a file
    with open("api_documentation.txt", "w", encoding="utf-8") as f:
        f.write(documentation_text)

    # If you want to use the extracted data programmatically:
    # e.g., print all emails if available
    if isinstance(extracted.get("emails"), list):
        print("\nEmails:")
        for email in extracted["emails"]:
            print(f"- {email}")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 73:
"Generate Python code that imports the requests library and creates a function that takes in a URL, headers, and data, and returns the response object from the request made to the given URL with the provided headers and data, while also providing additional functionality for handling rate limiting, retry logic, and customizable authentication."
RESPONSE:
import time
import random
import requests
from typing import Optional, Union, Mapping, Any, Callable, Tuple
from requests.auth import AuthBase
from email.utils import parsedate_to_datetime
from datetime import datetime, timezone


def _parse_retry_after(value: str) -> Optional[float]:
    # Retry-After can be a number of seconds or an HTTP-date
    if not value:
        return None
    value = value.strip()
    try:
        secs = int(value)
        return float(max(secs, 0))
    except ValueError:
        try:
            dt = parsedate_to_datetime(value)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            now = datetime.now(timezone.utc)
            delta = (dt - now).total_seconds()
            return max(delta, 0.0)
        except Exception:
            return None


def request_with_resilience(
    url: str,
    headers: Optional[Mapping[str, str]] = None,
    data: Optional[Union[Mapping[str, Any], str, bytes]] = None,
    method: Optional[str] = None,
    params: Optional[Mapping[str, Any]] = None,
    json: Optional[Any] = None,
    timeout: Union[int, float] = 30,
    max_retries: int = 3,
    backoff_factor: float = 1.0,
    retry_on: Optional[set] = None,
    auth: Optional[Union[str, Tuple[str, str], AuthBase, Callable]] = None,
    session: Optional[requests.Session] = None,
    on_retry: Optional[Callable[[int, int, Optional[requests.Response], Optional[BaseException]], None]] = None,
    respect_retry_after: bool = True,
    jitter: bool = True,
    **request_kwargs: Any,
) -> requests.Response:
    """
    Make an HTTP request with headers and data, supporting rate limiting, retry logic, and customizable authentication.

    Parameters:
      - url: Target URL.
      - headers: Optional headers mapping.
      - data: Optional body (form data, string, or bytes).
      - method: HTTP method; if None, uses POST when data/json is provided, otherwise GET.
      - params/json/timeout: Passed to requests.
      - max_retries: Total attempts (including the first).
      - backoff_factor: Base seconds for exponential backoff (1, 2, 4... multiplied by this).
      - retry_on: Set of status codes to retry on; defaults to {429, 500, 502, 503, 504}.
      - auth:
          * tuple(username, password) or requests.auth.AuthBase -> passed to requests
          * str -> treated as a Bearer token; added to Authorization header
          * callable -> called before the request; may return:
              - headers dict
              - (headers dict, auth_object) where auth_object is tuple or AuthBase
      - session: Optional existing requests.Session to reuse; if None, a temporary session is created.
      - on_retry: Optional callback(attempt_index, remaining_attempts, response_or_None, exception_or_None)
      - respect_retry_after: If True, honors Retry-After header for 429/5xx.
      - jitter: If True, adds small random jitter to backoff.

      - **request_kwargs: Any additional arguments forwarded to requests.request (e.g., verify, proxies, stream).

    Returns:
      - requests.Response object (from the successful attempt or the last attempt if non-retryable error status).

    Raises:
      - requests.RequestException if all retries fail due to exceptions.
    """
    if retry_on is None:
        retry_on = {429, 500, 502, 503, 504}

    effective_method = (method or ("POST" if (data is not None or json is not None) else "GET")).upper()

    # Prepare headers
    req_headers = dict(headers or {})

    # Handle customizable authentication
    auth_for_requests: Optional[Union[Tuple[str, str], AuthBase]] = None
    if callable(auth):
        # Allow custom auth function to modify headers and/or return an auth object
        res = auth(
            url=url,
            headers=dict(req_headers),
            data=data,
            method=effective_method,
            params=params,
            json=json,
            request_kwargs=dict(request_kwargs),
        )
        if isinstance(res, tuple) and len(res) == 2:
            new_headers, maybe_auth = res
            if isinstance(new_headers, Mapping):
                req_headers.update(new_headers)
            auth_for_requests = maybe_auth  # tuple or AuthBase
        elif isinstance(res, Mapping):
            req_headers.update(res)  # headers only
        elif res is not None:
            raise TypeError("Custom auth callable must return headers dict or (headers, auth_object).")
    elif isinstance(auth, str):
        # Treat as Bearer token
        if "Authorization" not in {k.title(): v for k, v in req_headers.items()}:
            req_headers["Authorization"] = f"Bearer {auth}"
    elif isinstance(auth, (tuple, AuthBase)) or auth is None:
        auth_for_requests = auth  # direct pass-through for Basic or custom AuthBase
    else:
        raise TypeError("auth must be None, str (Bearer token), tuple(username, password), AuthBase, or a callable.")

    created_session = False
    sess = session
    if sess is None:
        sess = requests.Session()
        created_session = True

    last_response: Optional[requests.Response] = None
    try:
        for attempt in range(1, max_retries + 1):
            try:
                resp = sess.request(
                    method=effective_method,
                    url=url,
                    headers=req_headers,
                    params=params,
                    data=data,
                    json=json,
                    timeout=timeout,
                    auth=auth_for_requests,
                    **request_kwargs,
                )
                last_response = resp

                # If status is retryable, compute delay and retry
                if resp.status_code in retry_on:
                    # Determine delay: honor Retry-After if requested and present
                    delay = None
                    if respect_retry_after:
                        ra = resp.headers.get("Retry-After")
                        if ra:
                            delay = _parse_retry_after(ra)

                    if delay is None:
                        base = backoff_factor * (2 ** (attempt - 1))
                        delay = base + (random.uniform(0, base / 4) if (jitter and base > 0) else 0.0)

                    if attempt < max_retries:
                        if on_retry:
                            on_retry(attempt, max_retries - attempt, resp, None)
                        time.sleep(delay)
                        continue  # next attempt
                # Either success or non-retryable status -> return the response
                return resp

            except requests.RequestException as exc:
                # Network/timeout errors: retry if attempts remain
                if attempt < max_retries:
                    base = backoff_factor * (2 ** (attempt - 1))
                    delay = base + (random.uniform(0, base / 4) if (jitter and base > 0) else 0.0)
                    if on_retry:
                        on_retry(attempt, max_retries - attempt, None, exc)
                    time.sleep(delay)
                    continue
                # No attempts left -> re-raise
                raise

        # Should not reach here, but return last response if present
        if last_response is not None:
            return last_response
        # If we somehow have no response and no exception, raise a generic error
        raise RuntimeError("Request failed with no response and no exception.")
    finally:
        if created_session:
            try:
                sess.close()
            except Exception:
                pass

# Example basic usage:
# resp = request_with_resilience("https://api.example.com/resource", headers={"Accept": "application/json"}, data=None)
# print(resp.status_code, resp.text)
------------------------------------------------------------

PROMPT 74:
"Generate Python code that imports the necessary libraries and defines a function to convert aprophecy file to a python script, using the prophecy library to parse the apriopriaphel statement and extract the function name and input/output types, and then generates apython script that calls the extracted function with randomly generated inputs of the corresponding types."
RESPONSE:
import re
import os
import sys
from typing import List, Dict, Any, Optional, Tuple

try:
    import prophecy  # Optional: used if available to parse apriopriaphel statements
except ImportError:
    prophecy = None


def _split_top_level_commas(s: str) -> List[str]:
    parts = []
    buf = []
    depth_angle = depth_brack = depth_paren = 0
    i = 0
    while i < len(s):
        ch = s[i]
        if ch == ',' and depth_angle == depth_brack == depth_paren == 0:
            parts.append(''.join(buf).strip())
            buf = []
            i += 1
            continue
        if ch in '<[':
            depth_angle += (ch == '<')
            depth_brack += (ch == '[')
        elif ch in '>]':
            depth_angle -= (ch == '>')
            depth_brack -= (ch == ']')
        elif ch == '(':
            depth_paren += 1
        elif ch == ')':
            depth_paren -= 1
        buf.append(ch)
        i += 1
    if buf:
        parts.append(''.join(buf).strip())
    return [p for p in parts if p]


def _fallback_parse_apriopriaphel(text: str) -> Optional[Dict[str, Any]]:
    """
    Fallback parser for lines like:
      apriopriaphel func_name(arg1: int, arg2: List[str]) -> float
    Returns dict: {name: str, inputs: List[Dict[name:str,type:str]], output: str}
    """
    pattern = re.compile(
        r'^\s*apriopriaphel\s+([A-Za-z_]\w*)\s*\((.*?)\)\s*->\s*([^\n#]+)',
        re.IGNORECASE | re.MULTILINE,
    )
    m = pattern.search(text)
    if not m:
        return None

    func_name = m.group(1).strip()
    raw_args = m.group(2).strip()
    output_type = m.group(3).strip()

    inputs: List[Dict[str, str]] = []
    if raw_args:
        for idx, token in enumerate(_split_top_level_commas(raw_args)):
            # Accept either "name: Type" or just "Type"
            if ':' in token:
                name_part, type_part = token.split(':', 1)
                arg_name = name_part.strip()
                arg_type = type_part.strip()
                if not arg_name:
                    arg_name = f'arg{idx+1}'
            else:
                arg_name = f'arg{idx+1}'
                arg_type = token.strip()
            inputs.append({'name': arg_name, 'type': arg_type})

    return {
        'name': func_name,
        'inputs': inputs,
        'output': output_type,
    }


def _try_prophecy_parse(text: str) -> Optional[Dict[str, Any]]:
    """
    Attempt to parse using the optional 'prophecy' library if it provides
    a suitable parser. This function is defensive and will only return a
    normalized dict if it can confidently extract the needed fields.
    Expected normalized dict keys: name (str), inputs (List[{'name','type'}]), output (str)
    """
    if prophecy is None:
        return None

    candidates = []
    # Try common/guessable entry points
    for attr in ('parse_apriopriaphel', 'parse_aprophecy', 'parse'):
        fn = getattr(prophecy, attr, None)
        if callable(fn):
            candidates.append(fn)

    for fn in candidates:
        try:
            parsed = fn(text)  # library-specific
        except Exception:
            continue

        # Try to normalize into expected shape
        # We accept a variety of common shapes
        fn_name = None
        inputs = None
        output = None

        if isinstance(parsed, dict):
            fn_name = parsed.get('function') or parsed.get('name')
            inputs = parsed.get('inputs') or parsed.get('args')
            output = parsed.get('output') or parsed.get('returns') or parsed.get('return')

        # Normalize inputs to [{'name':..., 'type':...}]
        norm_inputs: List[Dict[str, str]] = []
        if isinstance(inputs, list):
            for idx, it in enumerate(inputs):
                if isinstance(it, dict):
                    n = it.get('name') or f'arg{idx+1}'
                    t = it.get('type') or it.get('annotation') or 'Any'
                else:
                    n = f'arg{idx+1}'
                    t = str(it)
                norm_inputs.append({'name': n, 'type': str(t)})

        if fn_name and norm_inputs is not None and output:
            return {'name': str(fn_name), 'inputs': norm_inputs, 'output': str(output)}

    return None


def _escape_triple_quotes(s: str) -> str:
    return s.replace('"""', '\\"\\"\\"')


def _generate_runner_script(
    func_name: str,
    inputs: List[Dict[str, str]],
    output_type: str,
) -> str:
    """
    Generate a standalone Python script that:
      - Accepts a module path on the command line (module containing func_name)
      - Imports the function
      - Generates random inputs for the declared argument types
      - Calls the function and prints the result
    """
    # Prepare inputs mapping
    arg_decls_lines = []
    for i, arg in enumerate(inputs, start=1):
        arg_name = arg['name']
        arg_type = arg['type']
        arg_decls_lines.append(f'    {arg_name} = rand_value("{_escape_triple_quotes(arg_type)}")')

    arg_names_csv = ", ".join(arg['name'] for arg in inputs)

    script = f'''#!/usr/bin/env python3
import importlib
import random
import string
import sys

def split_top_level_commas(s: str):
    parts = []
    buf = []
    depth_angle = depth_brack = depth_paren = 0
    i = 0
    while i < len(s):
        ch = s[i]
        if ch == ',' and depth_angle == depth_brack == depth_paren == 0:
            parts.append(''.join(buf).strip())
            buf = []
            i += 1
            continue
        if ch in '<[':
            if ch == '<':
                depth_angle += 1
            else:
                depth_brack += 1
        elif ch in '>]':
            if ch == '>':
                depth_angle -= 1
            else:
                depth_brack -= 1
        elif ch == '(':
            depth_paren += 1
        elif ch == ')':
            depth_paren -= 1
        buf.append(ch)
        i += 1
    if buf:
        parts.append(''.join(buf).strip())
    return [p for p in parts if p]

def rand_primitive(t: str):
    t = t.strip()
    tl = t.lower()
    if tl in ('int',):
        return random.randint(-1000, 1000)
    if tl in ('float',):
        return round(random.uniform(-1000, 1000), 6)
    if tl in ('bool',):
        return bool(random.getrandbits(1))
    if tl in ('str', 'string'):
        length = random.randint(0, 16)
        chars = string.ascii_letters + string.digits + ' '
        return ''.join(random.choices(chars, k=length))
    if tl in ('bytes',):
        length = random.randint(0, 16)
        return bytes(random.getrandbits(8) for _ in range(length))
    if tl in ('none', 'null', 'nil'):
        return None
    # If primitive unknown, default to None
    return None

def rand_value(t: str):
    t = t.strip()
    if not t:
        return None
    # Normalize common typing notations
    # Support Optional[T], Union[T, None], list[T], List[T], tuple[a,b], dict[k,v], set[T]
    lower = t.lower().replace('typing.', '')

    # Handle Optional[...] and T | None
    if lower.startswith('optional[') and lower.endswith(']'):
        inner = t[t.find('[')+1:-1].strip()
        return None if random.random() < 0.3 else rand_value(inner)
    if '|' in t and any(s.strip().lower() == 'none' for s in t.split('|')):
        alts = [s.strip() for s in t.split('|')]
        # 30% chance to be None
        if random.random() < 0.3:
            return None
        # pick a non-None branch
        non_none = [a for a in alts if a.strip().lower() not in ('none', 'null', 'nil')]
        chosen = random.choice(non_none) if non_none else 'Any'
        return rand_value(chosen)

    # list / List
    if lower.startswith('list[') and lower.endswith(']'):
        inner = t[t.find('[')+1:-1].strip()
        length = random.randint(0, 5)
        return [rand_value(inner) for _ in range(length)]
    if lower == 'list':
        length = random.randint(0, 5)
        return [rand_primitive('int') for _ in range(length)]

    # set / Set
    if lower.startswith('set[') and lower.endswith(']'):
        inner = t[t.find('[')+1:-1].strip()
        length = random.randint(0, 5)
        # Convert to set; ensure hashable by converting lists/dicts to str
        vals = []
        for _ in range(length):
            v = rand_value(inner)
            try:
                hash(v)
                vals.append(v)
            except Exception:
                vals.append(str(v))
        return set(vals)
    if lower == 'set':
        length = random.randint(0, 5)
        return set(random.randint(0, 10) for _ in range(length))

    # tuple / Tuple
    if lower.startswith('tuple[') and lower.endswith(']'):
        inner = t[t.find('[')+1:-1].strip()
        parts = split_top_level_commas(inner)
        return tuple(rand_value(p) for p in parts)
    if lower == 'tuple':
        return (rand_primitive('int'), rand_primitive('str'))

    # dict / Dict
    if lower.startswith('dict[') and lower.endswith(']'):
        inner = t[t.find('[')+1:-1].strip()
        parts = split_top_level_commas(inner)
        if len(parts) != 2:
            length = random.randint(0, 4)
            return {{i: rand_primitive('int') for i in range(length)}}
        k_t, v_t = parts[0].strip(), parts[1].strip()
        length = random.randint(0, 4)
        d = {{}}
        for _ in range(length):
            k = rand_value(k_t)
            try:
                hash(k)
            except Exception:
                k = str(k)
            d[k] = rand_value(v_t)
        return d
    if lower == 'dict':
        length = random.randint(0, 4)
        return {{str(i): rand_primitive('int') for i in range(length)}}

    # Union[A, B, ...]
    if lower.startswith('union[') and lower.endswith(']'):
        inner = t[t.find('[')+1:-1].strip()
        alts = split_top_level_commas(inner)
        chosen = random.choice(alts) if alts else 'Any'
        return rand_value(chosen)

    # Fallback to primitives
    prim = rand_primitive(t)
    if prim is not None or t.strip().lower() in ('none', 'null', 'nil'):
        return prim

    # If nothing matches, default to a random string representation
    return f"<random {t}>"


def load_function(module_path: str, func_name: str):
    mod = importlib.import_module(module_path)
    fn = getattr(mod, func_name, None)
    if fn is None:
        raise AttributeError(f"Function '{{func_name}}' not found in module '{{module_path}}'")
    return fn

def main():
    if len(sys.argv) < 2:
        print("Usage: python {{}} <module.path>".format(sys.argv[0]))
        print("Example: python {{}} my_package.my_module".format(sys.argv[0]))
        sys.exit(2)

    module_path = sys.argv[1]
    fn = load_function(module_path, "{func_name}")

{os.linesep.join(arg_decls_lines)}

    print("Calling {func_name}({arg_names_csv}) with random inputs:")
    print([{arg_names_csv}])
    result = fn({arg_names_csv})
    print("Result:", result)
    print("Declared return type: {output_type}")
    print("Actual result type:", type(result).__name__)

if __name__ == "__main__":
    main()
'''
    return script


def convert_aprophecy_file_to_script(
    aprophecy_path: str,
    output_script_path: str,
) -> Tuple[str, Dict[str, Any]]:
    """
    Reads an 'aprophecy' file, parses the apriopriaphel statement to extract:
      - function name
      - input argument names and types
      - output/return type
    Then generates a Python script that:
      - Accepts a module path at runtime to import the target function
      - Constructs random inputs matching the types
      - Calls the function and prints the result

    Returns:
      (output_script_path, metadata_dict)
      metadata_dict contains: {'name', 'inputs', 'output'}

    Parsing strategy:
      - If a 'prophecy' library is installed and provides a compatible parser,
        it will be used first.
      - Otherwise a built-in fallback parser recognizes lines like:
          apriopriaphel foo(a: int, b: List[str]) -> float
    """
    if not os.path.isfile(aprophecy_path):
        raise FileNotFoundError(f"File not found: {aprophecy_path}")

    with open(aprophecy_path, 'r', encoding='utf-8') as f:
        text = f.read()

    parsed = _try_prophecy_parse(text)
    if parsed is None:
        parsed = _fallback_parse_apriopriaphel(text)

    if not parsed:
        raise ValueError(
            "Could not parse apriopriaphel statement. Ensure the file contains a line like:\n"
            "  apriopriaphel func_name(arg1: Type1, arg2: Type2) -> ReturnType"
        )

    func_name: str = parsed['name']
    inputs: List[Dict[str, str]] = parsed['inputs']
    output_type: str = parsed['output']

    script = _generate_runner_script(func_name, inputs, output_type)

    os.makedirs(os.path.dirname(os.path.abspath(output_script_path)) or '.', exist_ok=True)
    with open(output_script_path, 'w', encoding='utf-8') as f:
        f.write(script)

    # Make executable on Unix-like systems
    try:
        os.chmod(output_script_path, 0o755)
    except Exception:
        pass

    return output_script_path, {'name': func_name, 'inputs': inputs, 'output': output_type}


# Example usage:
# out_path, meta = convert_aprophecy_file_to_script("example.aprophecy", "run_extracted.py")
# print("Wrote script to:", out_path)
# print("Metadata:", meta)
------------------------------------------------------------

PROMPT 75:
"Generate Python code that imports the necessary modules and annotates the ServiceQuotas client class with type hints using the mypy-boto3-builder tool, like so:\n```\nfrom typing import Optional\nfrom botocore.client import Client\nfrom mypy_boto3_builder.generators import ServiceQuotasGenerator\n\ngenerator = ServiceQuotasGenerator(Client)\ngenerated_client = generator.generate()\n```"
RESPONSE:
from typing import Optional
import boto3
from mypy_boto3_service_quotas import ServiceQuotasClient

def get_service_quotas_client(region_name: Optional[str] = None) -> ServiceQuotasClient:
    return boto3.client("service-quotas", region_name=region_name)

# Example usage
client: ServiceQuotasClient = get_service_quotas_client()
------------------------------------------------------------

PROMPT 76:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 CodeCommit service client, version 1.34.6, using mypy-boto3-builder version 7.23.0, like this:\n```python\nfrom typing import Callable\nfrom botocore.client import BaseClient\nfrom boto3.codecommit import CodeCommit\n\ndef create_code_commit_client(region: str) -> CodeCommit:\n   return CodeCommit(region=region)\n\n# Usage: c = create_code_commit_client('us-west-2')\n```"
RESPONSE:
```python
from typing import TYPE_CHECKING
import boto3

if TYPE_CHECKING:
    # Provided by mypy-boto3-codecommit (generated by mypy-boto3-builder 7.23.0 for boto3 1.34.6)
    from mypy_boto3_codecommit import CodeCommitClient
else:
    # Fallback for runtime if stubs are not installed
    from botocore.client import BaseClient as CodeCommitClient


def create_codecommit_client(region: str) -> CodeCommitClient:
    return boto3.client("codecommit", region_name=region)


# Usage:
# client = create_codecommit_client("us-west-2")
```
------------------------------------------------------------

PROMPT 77:
"Generate Python code that imports the necessary modules and annotates the LexModelBuildingService class with type hints using the mypy-boto3-builder library, like so:\n```python\nfrom typing import Optional\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import LexModelBuildingService\n\nclass MyLexModelBuildingService(LexModelBuildingService):\n   def __init__(self, client: Optional[BaseClient] = None) -> None:\n       self.client = client\n```"
RESPONSE:
from typing import Optional
from botocore.client import BaseClient
from mypy_boto3_builder import LexModelBuildingService

class MyLexModelBuildingService(LexModelBuildingService):
    def __init__(self, client: Optional[BaseClient] = None) -> None:
        self.client = client
------------------------------------------------------------

PROMPT 78:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.signer 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import Tuple\nfrom botocore.vendored import requests\nfrom boto3.signer import Signer\n\nSigner(\n   client=requests.Session(),\n   auth=None,\n   credentials=None,\n   config=None,\n).sign_request(\n   request_dict={\n       'method': 'POST',\n       'url': 'https://example.com',\n       'data': {'key1': 'value1'},\n       'headers': {\n           'Content-Type': 'application/json'\n       }\n   },\n   operation_name='operation_name'\n)\n```"
RESPONSE:
```python
from __future__ import annotations

import json
from typing import Any, Dict, MutableMapping, Optional

import boto3
import requests
from boto3.session import Session
from botocore.auth import SigV4Auth
from botocore.awsrequest import AWSRequest
from botocore.credentials import ReadOnlyCredentials
from mypy_boto3_signer.client import SignerClient
from mypy_boto3_signer.type_defs import ListSigningProfilesResponseTypeDef

# Create a typed AWS Signer service client (mypy-boto3-signer 1.34.0)
session: Session = boto3.session.Session(region_name="us-east-1")
signer_client: SignerClient = session.client("signer")

# Example typed call to the AWS Signer service
profiles_resp: ListSigningProfilesResponseTypeDef = signer_client.list_signing_profiles()
print(f"Found {len(profiles_resp.get('profiles', []))} signing profiles")

# Sign an arbitrary HTTP request using SigV4 (botocore)
creds = session.get_credentials()
assert creds is not None, "No AWS credentials found in the session"
frozen: ReadOnlyCredentials = creds.get_frozen_credentials()

region: str = session.region_name or "us-east-1"
service: str = "execute-api"  # Adjust for the target service (e.g., s3, sts, es, etc.)

headers: MutableMapping[str, str] = {
    "Content-Type": "application/json",
}

body: str = json.dumps({"key1": "value1"})

aws_request: AWSRequest = AWSRequest(
    method="POST",
    url="https://example.com",
    data=body,
    headers=dict(headers),
)

SigV4Auth(frozen, service, region).add_auth(aws_request)

prepared = aws_request.prepare()
with requests.Session() as http:
    response = http.send(prepared)
    print(response.status_code, response.text)
```
------------------------------------------------------------

PROMPT 79:
"Generate Python code that imports the necessary modules and annotates the types for an S3 service using aiobotocore.S3 2.9.0 and mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import Callable\nfrom aiobotocore.client import AiobotocoreClient\nfrom aiobotocore.services import S3\n\nasync def main() -> None:\n   client = AiobotocoreClient(\n       credentials=None,\n       service=S3(\n           region=\"us-west-2\",\n           bucket_name=\"my-bucket\",\n           key=\"data.txt\"\n       )\n   )\n   # Perform some operation on the S3 resource\n   await client.get_object(Bucket=\"my-bucket\", Key=\"data.txt\")\n```"
RESPONSE:
import asyncio
from aiobotocore.session import get_session
from types_aiobotocore_s3.client import S3Client
from types_aiobotocore_s3.type_defs import GetObjectOutputTypeDef

async def main() -> None:
    session = get_session()

    # types-aiobotocore provides typed client interface for aiobotocore 2.9.0
    async with session.create_client("s3", region_name="us-west-2") as s3:  # type: S3Client
        # Perform some operation on the S3 client with typed response
        resp: GetObjectOutputTypeDef = await s3.get_object(
            Bucket="my-bucket",
            Key="data.txt",
        )
        # Optionally read the body (StreamingBody is awaitable in aiobotocore)
        data = await resp["Body"].read()
        print(f"Downloaded {len(data)} bytes")

if __name__ == "__main__":
    asyncio.run(main())
------------------------------------------------------------

PROMPT 80:
"Generate Python code that imports the necessary libraries and sets up a client object for the Google Cloud Dialogflow Cx API, similar to the following package description:\n\n\"The Google Cloud Dialogflow Cx API client library provides a Python interface for interacting with the Google Cloud Dialogflow Cx API. This library allows you to build conversational interfaces for various platforms, including Google Assistant, Facebook Messenger, Slack, and more. With this library, you can create and manage conversational flows, handle user input, and integrate with other Google Cloud services like Google Cloud Storage and Google Cloud Functions.\""
RESPONSE:
# Requires: pip install google-cloud-dialogflow-cx
# Authentication: set GOOGLE_APPLICATION_CREDENTIALS or use a configured environment (e.g., GCE/GKE).

import uuid
from google.cloud import dialogflowcx_v3 as dialogflowcx
from google.api_core.client_options import ClientOptions


def create_dialogflow_cx_clients(project_id: str, location: str = "global"):
    # CX uses regional endpoints; set location to your agent's region (e.g., "us-central1").
    api_endpoint = f"{location}-dialogflow.googleapis.com"
    client_options = ClientOptions(api_endpoint=api_endpoint)

    # Create clients for common CX services
    sessions_client = dialogflowcx.SessionsClient(client_options=client_options)
    agents_client = dialogflowcx.AgentsClient(client_options=client_options)
    flows_client = dialogflowcx.FlowsClient(client_options=client_options)

    return {
        "sessions": sessions_client,
        "agents": agents_client,
        "flows": flows_client,
        "api_endpoint": api_endpoint,
    }


if __name__ == "__main__":
    project_id = "your-gcp-project-id"
    location = "us-central1"  # or "global", "europe-west1", etc.
    agent_id = "your-agent-id"

    clients = create_dialogflow_cx_clients(project_id, location)
    print(f"Dialogflow CX clients initialized with endpoint: {clients['api_endpoint']}")

    # Optional: build a session path for detect_intent calls
    session_id = str(uuid.uuid4())
    session_path = clients["sessions"].session_path(project_id, location, agent_id, session_id)
    print(f"Session path: {session_path}")
------------------------------------------------------------

PROMPT 81:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.CodeStarNotifications 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List, Dict\nfrom boto3.code_star_notifications import CodeStarNotifications\nfrom boto3.code_star_notifications.types import (\n   CreateNotificationRequest,\n   CreateNotificationResponse,\n   DeleteNotificationRequest,\n   DeleteNotificationResponse,\n   DescribeNotificationRequest,\n   DescribeNotificationResponse,\n   GetNotificationRequest,\n   GetNotificationResponse,\n   ListNotificationsRequest,\n   ListNotificationsResponse,\n)\n```"
RESPONSE:
from typing import List, Dict
from boto3.code_star_notifications import CodeStarNotifications
from boto3.code_star_notifications.types import (
    CreateNotificationRuleRequest,
    CreateNotificationRuleResponse,
    DeleteNotificationRuleRequest,
    DeleteNotificationRuleResponse,
    DescribeNotificationRuleRequest,
    DescribeNotificationRuleResponse,
    ListNotificationRulesRequest,
    ListNotificationRulesResponse,
)
------------------------------------------------------------

PROMPT 82:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 RoboMaker 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import TypeVar, Generic\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import Botofiler\n\nBotofiler = TypeVar('Botofiler', bound=BaseClient)\n\ndef main() -> None:\n   # Initialize the Botofiler client\n   botofiler = Botofiler(service='robomaker')\n   \n   # Create a robot application\n   robot_app = botofiler.create_robot_application(\n       name='my_robot_app',\n       description='My robot app'\n   )\n   \n   # Print the ARN of the robot application\n   print(robot_app.arn)\n```"
RESPONSE:
from __future__ import annotations

import boto3
from mypy_boto3_robomaker import RoboMakerClient
from mypy_boto3_robomaker.type_defs import (
    CreateRobotApplicationResponseTypeDef,
    RobotSoftwareSuiteTypeDef,
    SourceConfigTypeDef,
)


def main() -> None:
    # Typed RoboMaker client
    robomaker: RoboMakerClient = boto3.client("robomaker")

    # Input structures with precise types
    sources: list[SourceConfigTypeDef] = [
        {
            "s3Bucket": "my-bucket",
            "s3Key": "robot_app_source.zip",
            "architecture": "X86_64",  # Allowed: "X86_64" | "ARM64" | "ARMHF"
        }
    ]
    robot_software_suite: RobotSoftwareSuiteTypeDef = {
        "name": "ROS",        # Allowed: "ROS" | "ROS2"
        "version": "Kinetic", # Example for ROS; pick valid version for your suite
    }

    # Create a Robot Application (response is a TypedDict)
    resp: CreateRobotApplicationResponseTypeDef = robomaker.create_robot_application(
        name="my_robot_app",
        sources=sources,
        robotSoftwareSuite=robot_software_suite,
        tags={"project": "demo"},
    )

    # Print the ARN of the robot application
    print(resp["arn"])


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 83:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.DAX 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import DaxClient\nfrom mypy_boto3_builder import MyPyBoto3Builder\n\nbuilder = MyPyBoto3Builder(service='dax', version='1.34.0')\nbuilder.annotate_types()\n\n# Example usage:\nclient = DaxClient(region='us-west-2')\nresponse = client.create_table(\n   TableName='my-table',\n   AttributeDefinitions=[\n       {\n           'AttributeName': 'id',\n           'AttributeType': 'S',\n       },\n       {\n           'AttributeName': 'name',\n           'AttributeType': 'S',\n       },\n   ],\n)\nprint(response)\n```"
RESPONSE:
from typing import List
import boto3
from mypy_boto3_dax import DAXClient
from mypy_boto3_dax.type_defs import DescribeClustersResponseTypeDef

# Types come from mypy-boto3-dax (generated by mypy-boto3-builder 7.21.0) for boto3 1.34.0.

def main() -> None:
    # Typed DAX client
    client: DAXClient = boto3.client("dax", region_name="us-west-2")

    # Safe read-only example: describe clusters with typed response
    response: DescribeClustersResponseTypeDef = client.describe_clusters()
    print(response)

    # Example of typed inputs for a mutating call (commented out)
    # subnet_ids: List[str] = ["subnet-xxxxxxxx", "subnet-yyyyyyyy"]
    # client.create_subnet_group(
    #     SubnetGroupName="my-dax-subnet-group",
    #     Description="Example subnet group",
    #     SubnetIds=subnet_ids,
    # )

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 84:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.MTurk 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import TypeVar, Type\nfrom mypy_boto3_builder import BotoflowTypedHints\nfrom botocore.client import BaseClient\nfrom boto3.mturk import MTurkClient\n\nHint = TypeVar('Hint', bound=dict)\n\nclass My \u041c\u0422urkClient(MTurkClient):\n   hints: Hint = {}\n\n   def __init__(self, *args, **kwargs) -> None:\n       super().__init__(*args, **kwargs)\n       self.hints = hints = {}\n\n   def create_hit(self, *args, **kwargs) -> dict:\n       return super().create_hit(*args, **kwargs)\n\n   def list_hits(self, *args, **kwargs) -> list[dict]:\n       return super().list_hits(*args, **kwargs)\n\n   def get_hit("
RESPONSE:
from typing import Any, TypeVar
from mypy_boto3_mturk.client import MTurkClient
from mypy_boto3_mturk.type_defs import (
    CreateHITResponseTypeDef,
    GetHITResponseTypeDef,
    ListHITsResponseTypeDef,
)

Hint = TypeVar("Hint", bound=dict)

class MyMTurkClient(MTurkClient):
    hints: Hint

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        # Note: MTurkClient instances are normally created via boto3.client("mturk").
        # This subclass is intended for typing/demo purposes.
        super().__init__(*args, **kwargs)  # type: ignore[misc]
        self.hints = {}

    def create_hit(self, *args: Any, **kwargs: Any) -> CreateHITResponseTypeDef:
        return super().create_hit(*args, **kwargs)

    def list_hits(self, *args: Any, **kwargs: Any) -> ListHITsResponseTypeDef:
        return super().list_hits(*args, **kwargs)

    def get_hit(self, *args: Any, **kwargs: Any) -> GetHITResponseTypeDef:
        return super().get_hit(*args, **kwargs)
------------------------------------------------------------

PROMPT 85:
"Generate Python code that imports the required modules and sets up type hints for the EC2 2.9.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import Callable, Dict, List\nimport mypy_boto3_builder\nfrom aiobotocore import ClientBase, AwaitableMixin\n\nmypy_boto3_builder.set_type_hints(aiobotocore=True)\n\nclass EC2(ClientBase):\n   def __init__(self, *args: object, **kwargs: object) -> None:\n       super().__init__(*args, **kwargs)\n       self.config = kwargs.pop('config')\n       self.meta = kwargs.pop('meta')\n       self.client = self.config.construct_client('ec2', *args, **kwargs)\n\n   async def create_instances(self, *args: object, **kwargs: object) -> List[Dict]:\n       return await self.client.create_instances(*args, **kwargs)\n\n   async def run"
RESPONSE:
from typing import Callable, Dict, List
import mypy_boto3_builder
from aiobotocore import ClientBase, AwaitableMixin

mypy_boto3_builder.set_type_hints(aiobotocore=True)

class EC2(ClientBase):
    def __init__(self, *args: object, **kwargs: object) -> None:
        super().__init__(*args, **kwargs)
        self.config = kwargs.pop('config')
        self.meta = kwargs.pop('meta')
        self.client = self.config.construct_client('ec2', *args, **kwargs)

    async def create_instances(self, *args: object, **kwargs: object) -> List[Dict]:
        return await self.client.create_instances(*args, **kwargs)

    async def run_instances(self, *args: object, **kwargs: object) -> Dict:
        return await self.client.run_instances(*args, **kwargs)
------------------------------------------------------------

PROMPT 86:
"Generate Python code that imports the necessary modules and annotates the types of the PinpointSMSVoice service in boto3 version 1.34.0 using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import Callable\nfrom botocore.client import BaseClient\nfrom boto3.sms_voice import PinpointSMSVoice\n\ndef send_message(\n   *,\n   message: str,\n   destination_phone_number: str,\n   origination_ phone_number: str,\n   voice_id: str,\n   max_outbound_capacity: int,\n   outbound_caller_id_prefix: str,\n   authorization_token: str,\n) -> None:\n   client = PinpointSMSVoice(\n       client=BaseClient,\n       client_name='PinpointSMSVoice',\n       config=None,\n       region='us-east-1',\n   )\n   response = client.send_message(\n       Message={\n           'Message': message,\n         "
RESPONSE:
```python
from __future__ import annotations

from typing import Any, Dict, Optional

import boto3
from botocore.config import Config
from mypy_boto3_pinpoint_sms_voice import PinpointSMSVoiceClient


def send_voice_message(
    *,
    message: str,
    destination_phone_number: str,
    origination_phone_number: str,
    voice_id: str,
    language_code: str = "en-US",
    caller_id: Optional[str] = None,
    configuration_set_name: Optional[str] = None,
    region: str = "us-east-1",
) -> None:
    """
    Example of using typed boto3 Pinpoint SMS Voice (pinpoint-sms-voice) client
    with mypy-boto3 stubs generated by mypy-boto3-builder 7.21.0 for boto3 1.34.0.
    """
    client_config = Config(region_name=region)

    client: PinpointSMSVoiceClient = boto3.client(
        "pinpoint-sms-voice",
        config=client_config,
    )

    params: Dict[str, Any] = {
        "Content": {
            "PlainTextMessage": {
                "LanguageCode": language_code,
                "Text": message,
                "VoiceId": voice_id,
            }
        },
        "DestinationPhoneNumber": destination_phone_number,
        "OriginationPhoneNumber": origination_phone_number,
    }

    if caller_id is not None:
        params["CallerId"] = caller_id
    if configuration_set_name is not None:
        params["ConfigurationSetName"] = configuration_set_name

    # Returns a dict response (includes e.g. "MessageId")
    client.send_voice_message(**params)
```
------------------------------------------------------------

PROMPT 87:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.Snowball 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import Tuple\nfrom botocore.client import BaseClient\nfrom boto3.s3 import Snowball\n\n# Annotate the types of the Snowball client\nclass SnowballClient(BaseClient):\n   def __init__(self, *, config: dict = None, **kwargs) -> None:\n       self.config = config\n       self.snowball = Snowball(config=config, **kwargs)\n\n   def list_clusters(self, **kwargs) -> List[dict]:\n       return self.snowball.list_clusters(**kwargs)\n\n   def create_cluster(self, **kwargs) -> dict:\n       return self.snowball.create_cluster(**kwargs)\n\n   def delete_cluster(self, cluster_id: str, **kwargs) -> None:\n       self.snowball.delete_cluster(cluster"
RESPONSE:
from __future__ import annotations

from typing import Any, Optional

import boto3
from boto3.session import Session
from botocore.config import Config
from typing_extensions import Unpack

from mypy_boto3_snowball import SnowballClient as TypedSnowballClient
from mypy_boto3_snowball.type_defs import (
    CreateClusterRequestRequestTypeDef,
    CreateClusterResultTypeDef,
    EmptyResponseMetadataTypeDef,
    ListClustersRequestRequestTypeDef,
    ListClustersResultTypeDef,
)


class SnowballService:
    def __init__(
        self,
        *,
        session: Optional[Session] = None,
        region_name: Optional[str] = None,
        config: Optional[Config] = None,
        **kwargs: Any,
    ) -> None:
        self._session: Session = session or boto3.session.Session()
        self.client: TypedSnowballClient = self._session.client(
            "snowball",
            region_name=region_name,
            config=config,
            **kwargs,
        )

    def list_clusters(
        self, **kwargs: Unpack[ListClustersRequestRequestTypeDef]
    ) -> ListClustersResultTypeDef:
        return self.client.list_clusters(**kwargs)

    def create_cluster(
        self, **kwargs: Unpack[CreateClusterRequestRequestTypeDef]
    ) -> CreateClusterResultTypeDef:
        return self.client.create_cluster(**kwargs)

    def delete_cluster(self, *, ClusterId: str) -> EmptyResponseMetadataTypeDef:
        return self.client.delete_cluster(ClusterId=ClusterId)


if __name__ == "__main__":
    svc = SnowballService(region_name="us-east-1")
    clusters = svc.list_clusters()
    print(clusters)
------------------------------------------------------------

PROMPT 88:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 AutoScaling service, version 1.34.0, using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder.generators.auto_scaling import AutoScalingGenerated client\n\nclass AutoScaling(BaseClient):\n   def __init__(self, *args, **kwargs) -> None:\n       super().__init__(*args, **kwargs)\n       self.config = kwargs.get('config')\n       \n   def create_ scaling_policy(self, PolicyName: str, AdjustmentType: str, CoolDown: int, MetricAggregationInterval: int, ScalingAdjustments: List[dict], **kwargs) -> dict:\n       return self._create_scaling_policy(PolicyName, AdjustmentType, CoolDown, MetricAggregationInterval, ScalingAdjustments, **kwargs)\n   \n   def delete_scal"
RESPONSE:
```python
from typing import List
import boto3

from mypy_boto3_autoscaling import AutoScalingClient
from mypy_boto3_autoscaling.literals import (
    AdjustmentTypeType,
    MetricAggregationTypeType,
    PolicyTypeType,
)
from mypy_boto3_autoscaling.type_defs import (
    StepAdjustmentTypeDef,
    PutScalingPolicyResponseTypeDef,
    EmptyResponseMetadataTypeDef,
)

# Typed Auto Scaling client for boto3==1.34.0 (generated by mypy-boto3-builder==7.21.0)
autoscaling: AutoScalingClient = boto3.client("autoscaling")


def create_scaling_policy(
    *,
    auto_scaling_group_name: str,
    policy_name: str,
    adjustment_type: AdjustmentTypeType,
    cooldown: int,
    metric_aggregation_type: MetricAggregationTypeType,
    step_adjustments: List[StepAdjustmentTypeDef],
    policy_type: PolicyTypeType = "StepScaling",
) -> PutScalingPolicyResponseTypeDef:
    return autoscaling.put_scaling_policy(
        AutoScalingGroupName=auto_scaling_group_name,
        PolicyName=policy_name,
        PolicyType=policy_type,
        AdjustmentType=adjustment_type,
        Cooldown=cooldown,
        StepAdjustments=step_adjustments,
        MetricAggregationType=metric_aggregation_type,
    )


def delete_scaling_policy(
    *,
    auto_scaling_group_name: str,
    policy_name: str,
) -> EmptyResponseMetadataTypeDef:
    return autoscaling.delete_policy(
        AutoScalingGroupName=auto_scaling_group_name,
        PolicyName=policy_name,
    )


if __name__ == "__main__":
    steps: List[StepAdjustmentTypeDef] = [
        {"MetricIntervalLowerBound": 0.0, "ScalingAdjustment": 1},
        {"MetricIntervalLowerBound": 10.0, "ScalingAdjustment": 2},
    ]

    resp: PutScalingPolicyResponseTypeDef = create_scaling_policy(
        auto_scaling_group_name="my-asg",
        policy_name="scale-out",
        adjustment_type="ChangeInCapacity",
        cooldown=60,
        metric_aggregation_type="Average",
        step_adjustments=steps,
    )
    print(resp.get("PolicyARN"))

    delete_scaling_policy(
        auto_scaling_group_name="my-asg",
        policy_name="scale-out",
    )
```
------------------------------------------------------------

PROMPT 89:
"Generate Python code that implements dplyr-style piping operations for pandas dataframes:\n```\nimport pandas as pd\n\ndef pipe(df, *args, **kwargs):\n   \"\"\"\n   Apply a series of operations to a Pandas DataFrame using a dplyr-style pipeline.\n   \"\"\"\n   # Handle first argument as a string and pass it as a selector to the next function\n   if isinstance(args[0], str):\n       return df.apply(lambda x: x.pipe(args[0]), axis=1)\n   \n   # If not a string, assume it's a function and apply it directly to the dataframe\n   else:\n       return df.apply(args[0], axis=1)\n   \n   # Recursively call the pipe function with the remaining arguments\n   return pipe(df, *args[1:], **kwargs)\n\n# Example usage:\ndf = pd.read_csv('data.csv')\nresult = pipe(df, 'filter', 'where', 'a > 5', 'select', 'a, b')\nprint(result)\n```\nThis code defines a `pipe"
RESPONSE:
Here’s a compact, working dplyr-style pipeline for pandas with filter, select, mutate, arrange, group_by, summarise, rename, and helpers. It uses the >> operator for chaining and also provides a pipe(...) function to run a sequence of operations.

```python
import re
from typing import Callable, Sequence, Tuple, Any
import pandas as pd


# Core piping machinery --------------------------------------------------------

class Op:
    def __init__(self, fn: Callable):
        self.fn = fn

    def __call__(self, x):
        return self.fn(x)

    # Enable: df >> filter_(...) >> select_(...) ...
    def __rrshift__(self, other):
        return self.fn(other)


class Grouped:
    def __init__(self, df: pd.DataFrame, by: Sequence[str], dropna: bool = False, sort: bool = False):
        self.df = df
        self.by = list(by)
        self.dropna = dropna
        self.sort = sort

    def groupby(self):
        return self.df.groupby(self.by, dropna=self.dropna, sort=self.sort, as_index=False)


def _is_grouped(x) -> bool:
    return isinstance(x, Grouped)


def _get_df(x) -> pd.DataFrame:
    return x.df if _is_grouped(x) else x


def _wrap_like(x, df: pd.DataFrame):
    return Grouped(df, x.by, x.dropna, x.sort) if _is_grouped(x) else df


def _parse_cols(args) -> list:
    cols = []
    for a in args:
        if a is None:
            continue
        if isinstance(a, (list, tuple)):
            cols.extend([str(c).strip() for c in a])
        elif isinstance(a, str):
            parts = [p.strip() for p in a.split(",")]
            cols.extend([p for p in parts if p])
        else:
            raise TypeError("Columns must be strings or lists/tuples of strings")
    return cols


# Verbs ------------------------------------------------------------------------

def select_(*cols: Any) -> Op:
    cols = _parse_cols(cols)
    def fn(x):
        df = _get_df(x)
        res = df.loc[:, cols]
        return _wrap_like(x, res)
    return Op(fn)


def filter_(expr: Any = None, **equals) -> Op:
    """
    - expr: string for DataFrame.query, or callable(df)->bool Series
    - equals: keyword equality filters, e.g., filter_(country="US", year=2020)
    """
    def fn(x):
        df = _get_df(x)
        if expr is None and equals:
            mask = pd.Series(True, index=df.index)
            for k, v in equals.items():
                mask &= (df[k] == v)
            res = df.loc[mask]
        elif callable(expr):
            mask = expr(df)
            res = df.loc[mask]
        elif isinstance(expr, str):
            res = df.query(expr, engine="python")
        else:
            raise ValueError("filter_ expects a string expression, a callable, or keyword equality filters")
        return _wrap_like(x, res)
    return Op(fn)


def mutate_(**assignments) -> Op:
    """
    mutate_(z="x + y", r=lambda df: df.x / df.y, const=5)
    Strings are evaluated with DataFrame.eval in the context of df columns.
    """
    def fn(x):
        df = _get_df(x).copy()
        for k, v in assignments.items():
            if callable(v):
                df[k] = v(df)
            elif isinstance(v, str):
                df[k] = df.eval(v, engine="python")
            else:
                df[k] = v
        return _wrap_like(x, df)
    return Op(fn)


def arrange_(*by: Any, ascending=True) -> Op:
    by = _parse_cols(by)
    def fn(x):
        df = _get_df(x)
        keys = by
        if _is_grouped(x):
            keys = list(x.by) + [c for c in by if c not in x.by]
        res = df.sort_values(keys, ascending=ascending, kind="mergesort")
        return _wrap_like(x, res)
    return Op(fn)


def rename_(**mapping) -> Op:
    def fn(x):
        df = _get_df(x).rename(columns=mapping)
        if _is_grouped(x):
            new_by = [mapping.get(b, b) for b in x.by]
            return Grouped(df, new_by, x.dropna, x.sort)
        return df
    return Op(fn)


def group_by(*cols: Any, dropna: bool = False, sort: bool = False) -> Op:
    cols = _parse_cols(cols)
    def fn(df):
        df0 = _get_df(df)
        return Grouped(df0, cols, dropna=dropna, sort=sort)
    return Op(fn)


def ungroup() -> Op:
    def fn(x):
        return _get_df(x)
    return Op(fn)


def _parse_agg_spec(spec) -> Tuple[Any, Any]:
    """
    Accepts:
      - tuple: (column, func)  e.g., ('x', 'mean') or ('x', np.mean)
      - string: 'func(col)'    e.g., 'mean(x)', 'sum(y)'
      - 'n()' or 'n' for row counts
    """
    if isinstance(spec, tuple) and len(spec) == 2:
        col, func = spec
        return str(col), func
    if isinstance(spec, str):
        s = spec.strip()
        if s in ("n()", "n"):
            return None, "size"
        m = re.match(r"([A-Za-z_]\w*)\s*\(\s*([A-Za-z_]\w*)\s*\)\s*$", s)
        if m:
            func, col = m.group(1), m.group(2)
            return col, func
    if callable(spec):
        # callable that takes a Series or DataFrame in agg context
        return None, spec
    raise ValueError("summarise_ spec must be tuple(col, func), 'func(col)', 'n()' or a callable")


def summarise_(**named_aggs) -> Op:
    """
    summarise_(
      avg_x=('x', 'mean'),
      total_y='sum(y)',
      n='n()'
    )
    Must be used after group_by(...).
    """
    def fn(x):
        if not _is_grouped(x):
            raise TypeError("summarise_ must be used after group_by(...)")
        gb = x.groupby()

        agg_dict = {}
        count_names = []
        for out, spec in named_aggs.items():
            col, func = _parse_agg_spec(spec)
            if func == "size" and col is None:
                count_names.append(out)
            else:
                agg_dict[out] = (col, func)

        res = None
        if agg_dict:
            res = gb.agg(**agg_dict).reset_index()
        else:
            # if only counts were requested
            res = gb.size().rename("n").reset_index()

        if count_names:
            sizes = gb.size().rename(count_names[0]).reset_index()
            on_cols = x.by
            res = res.merge(sizes, on=on_cols, how="left")
            for extra in count_names[1:]:
                res[extra] = res[count_names[0]]

        return res
    return Op(fn)


# Convenience helpers ----------------------------------------------------------

def head_(n: int = 5) -> Op:
    def fn(x):
        df = _get_df(x).head(n)
        return _wrap_like(x, df)
    return Op(fn)


def tail_(n: int = 5) -> Op:
    def fn(x):
        df = _get_df(x).tail(n)
        return _wrap_like(x, df)
    return Op(fn)


def sample_(n=None, frac=None, random_state=None, replace=False) -> Op:
    def fn(x):
        df = _get_df(x).sample(n=n, frac=frac, random_state=random_state, replace=replace)
        return _wrap_like(x, df)
    return Op(fn)


def pipe(df: pd.DataFrame, *ops: Callable) -> pd.DataFrame:
    """
    Apply a sequence of operations: pipe(df, filter_("a > 5"), select_("a, b"))
    """
    out = df
    for op in ops:
        if not callable(op):
            raise TypeError("pipe expects callables returned by verbs like filter_, select_, etc.")
        out = op(out)
    return out


# ----------------------------------------------------------------------------- 
# Examples
# -----------------------------------------------------------------------------

if __name__ == "__main__":
    df = pd.DataFrame({
        "a": [1, 6, 3, 8, 10],
        "b": [5, 4, 3, 2, 1],
        "g": ["x", "x", "y", "y", "y"]
    })

    # dplyr-like chaining with >> operator
    result1 = (
        df
        >> filter_("a > 5")
        >> mutate_(c="a + b", ratio=lambda d: d.a / d.b)
        >> select_("a, b, c, ratio")
        >> arrange_("c", ascending=False)
    )
    print("result1:\n", result1)

    # Using pipe(...) with a list of ops
    result2 = pipe(
        df,
        filter_("a > 5"),
        select_("a, b")
    )
    print("result2:\n", result2)

    # Grouped summarise
    result3 = (
        df
        >> group_by("g")
        >> summarise_(avg_a=('a', 'mean'), total_b='sum(b)', n='n()')
    )
    print("result3:\n", result3)

    # Keep grouping after row-wise operation, then ungroup
    result4 = (
        df
        >> group_by("g")
        >> filter_("a >= 3")
        >> mutate_(scaled="(a - a.mean()) / a.std()")  # within-group mutate
        >> ungroup()
        >> arrange_("g, scaled")
    )
    print("result4:\n", result4)
```

Notes:
- Use >> to chain operations: df >> filter_("a > 5") >> select_("a, b").
- filter_ accepts a query string, a callable(df)->boolean Series, or keyword equality filters.
- mutate_ accepts expressions as strings evaluated with DataFrame.eval or callables that compute Series.
- group_by returns a grouped wrapper that preserves grouping across filter/select/mutate/arrange; summarise_ materializes an aggregated DataFrame.
- summarise_ supports tuple(col, func), strings like "mean(x)", and "n()" for row counts.
------------------------------------------------------------

PROMPT 90:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.SageMakerFeatureStoreRuntime 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import TypeVar, Generic\nfrom botocore.client import BaseClient\nfrom sagemaker.feature_store import FeatureStore\n\nclass SageMakerFeatureStoreRuntime(BaseClient):\n   def __init__(self, config: dict) -> None:\n       self.config = config\n   \n   def create_feature_group(self, FeatureGroupName: str, Description: str) -> None:\n       pass\n   \n   def list_feature_groups(self) -> List[dict]:\n       pass\n   \n   def describe_feature_group(self, FeatureGroupName: str) -> dict:\n       pass\n   \n   def create_feature(self, FeatureName: str, FeatureType: str, Description: str) -> None:\n       pass\n   \n   def list_features(self) -> List[dict]:\n       pass"
RESPONSE:
```python
from __future__ import annotations

from typing import Any, Optional, Sequence

import boto3
from mypy_boto3_sagemaker_featurestore_runtime import (
    SageMakerFeatureStoreRuntimeClient,
)
from mypy_boto3_sagemaker_featurestore_runtime.literals import TargetStoreType
from mypy_boto3_sagemaker_featurestore_runtime.type_defs import (
    BatchGetRecordIdentifierTypeDef,
    BatchGetRecordResponseTypeDef,
    FeatureValueTypeDef,
    GetRecordResponseTypeDef,
    TtlDurationTypeDef,
)


def get_featurestore_runtime(
    region_name: Optional[str] = None,
) -> SageMakerFeatureStoreRuntimeClient:
    return boto3.client(
        "sagemaker-featurestore-runtime",
        region_name=region_name,
    )


def put_record(
    client: SageMakerFeatureStoreRuntimeClient,
    feature_group_name: str,
    record: Sequence[FeatureValueTypeDef],
    *,
    target_stores: Optional[Sequence[TargetStoreType]] = None,
    ttl: Optional[TtlDurationTypeDef] = None,
) -> None:
    kwargs: dict[str, Any] = {
        "FeatureGroupName": feature_group_name,
        "Record": list(record),
    }
    if target_stores is not None:
        kwargs["TargetStores"] = list(target_stores)
    if ttl is not None:
        kwargs["TtlDuration"] = ttl
    client.put_record(**kwargs)


def get_record(
    client: SageMakerFeatureStoreRuntimeClient,
    feature_group_name: str,
    record_identifier_value_as_string: str,
    *,
    feature_names: Optional[Sequence[str]] = None,
) -> GetRecordResponseTypeDef:
    kwargs: dict[str, Any] = {
        "FeatureGroupName": feature_group_name,
        "RecordIdentifierValueAsString": record_identifier_value_as_string,
    }
    if feature_names is not None:
        kwargs["FeatureNames"] = list(feature_names)
    return client.get_record(**kwargs)


def batch_get_record(
    client: SageMakerFeatureStoreRuntimeClient,
    identifiers: Sequence[BatchGetRecordIdentifierTypeDef],
) -> BatchGetRecordResponseTypeDef:
    return client.batch_get_record(Identifiers=list(identifiers))


def delete_record(
    client: SageMakerFeatureStoreRuntimeClient,
    feature_group_name: str,
    record_identifier_value_as_string: str,
    event_time: str,
    *,
    target_stores: Optional[Sequence[TargetStoreType]] = None,
) -> None:
    kwargs: dict[str, Any] = {
        "FeatureGroupName": feature_group_name,
        "RecordIdentifierValueAsString": record_identifier_value_as_string,
        "EventTime": event_time,
    }
    if target_stores is not None:
        kwargs["TargetStores"] = list(target_stores)
    client.delete_record(**kwargs)
```
------------------------------------------------------------

PROMPT 91:
"Generate Python code that implements a pytest plugin to display real-time test progress status, using the `pytest` and `rich` libraries, like this:\n```python\nimport pytest\nfrom rich.console import Console\n\ndef setup_ fortmat():\n   console = Console()\n   return console.setup(description='Testing...')\n\ndef teardown_fortmat():\n   console.teardown()\n\n@pytest.fixture(autouse=True)\ndef setup_function(request):\n   console = Console()\n   request.config.plugin manager.register(MyCustomPlugin(), '--custom-option', action=console.setup)\n   return console.setupsocket(request.node.name)\n\n@pytest.fixture(autouse=True)\ndef teardown_function(request):\n   console.teardown()\n   request.config.plugin manager.unregister(MyCustomPlugin())\n\nclass MyCustomPlugin:\n   def __init__(self, *args, **kwargs):\n       self.console = Console()\n\n   def handle_test_event(self, event, test"
RESPONSE:
```python
# Save this as conftest.py (or as a separate module and register it as a pytest plugin).
# Requires: pytest, rich

from __future__ import annotations

import time
from typing import Dict, Set

import pytest
from rich.console import Console
from rich.progress import (
    Progress,
    SpinnerColumn,
    BarColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)


class RichProgressPlugin:
    def __init__(self, config: pytest.Config):
        self.config = config
        self.console = Console()
        self.progress: Progress | None = None
        self.task_id: int | None = None

        self.total: int | None = None
        self.completed: int = 0

        # Track per-outcome counts
        self.counts: Dict[str, int] = {
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "error": 0,
            "xfailed": 0,
            "xpassed": 0,
        }

        # Track nodes we've already finalized to avoid double counting
        self._done: Set[str] = set()

        # For session timing
        self._t0 = 0.0

    # ---------- pytest hooks ----------

    def pytest_sessionstart(self, session: pytest.Session) -> None:
        self._t0 = time.monotonic()

        # Build a compact, informative progress display
        self.progress = Progress(
            SpinnerColumn(),
            TextColumn("[bold]Tests[/]"),
            BarColumn(bar_width=None),
            TextColumn("{task.completed}/{task.total}", justify="right"),
            TextColumn("•"),
            TimeElapsedColumn(),
            TextColumn("•"),
            TimeRemainingColumn(),
            TextColumn("• [green]passed[/]: {task.fields[passed]}  "
                       "[red]failed[/]: {task.fields[failed]}  "
                       "[yellow]skipped[/]: {task.fields[skipped]}  "
                       "[magenta]xfailed[/]: {task.fields[xfailed]}  "
                       "[cyan]xpassed[/]: {task.fields[xpassed]}  "
                       "[bright_black]errors[/]: {task.fields[error]}",
                       justify="left"),
            TextColumn("• [blue]{task.fields[current]}", overflow="ellipsis", max_width=80),
            console=self.console,
            expand=True,
            transient=True,  # clear the progress on completion
        )

        self.progress.start()
        # total is unknown until collection finishes; use None, update later
        self.task_id = self.progress.add_task(
            "Running",
            total=None,
            current="collecting tests…",
            **self.counts,
        )

    def pytest_collection_finish(self, session: pytest.Session) -> None:
        # Set the total number of tests after collection
        self.total = len(session.items)
        if self.progress and self.task_id is not None:
            self.progress.update(self.task_id, total=self.total)

    def pytest_runtest_logstart(self, nodeid: str, location) -> None:
        # Update 'current' field with the running test
        if self.progress and self.task_id is not None:
            self.progress.update(self.task_id, current=nodeid)

    def pytest_runtest_logreport(self, report: pytest.TestReport) -> None:
        # Count outcomes and mark a test as completed exactly once
        nodeid = report.nodeid

        def finish(outcome_key: str) -> None:
            if nodeid in self._done:
                return
            self._done.add(nodeid)
            self.completed += 1
            self.counts[outcome_key] += 1
            if self.progress and self.task_id is not None:
                self.progress.update(
                    self.task_id,
                    completed=self.completed,
                    passed=self.counts["passed"],
                    failed=self.counts["failed"],
                    skipped=self.counts["skipped"],
                    xfailed=self.counts["xfailed"],
                    xpassed=self.counts["xpassed"],
                    error=self.counts["error"],
                )

        # Handle outcomes:
        # - skips commonly surface in "setup"
        # - xfail: skipped with wasxfail in "setup"
        # - xpass: passed with wasxfail in "call"
        # - errors: failures in setup/teardown
        if report.when == "setup":
            if report.skipped:
                if getattr(report, "wasxfail", None):
                    finish("xfailed")
                else:
                    finish("skipped")
            elif report.failed:
                finish("error")
        elif report.when == "call":
            if report.failed:
                finish("failed")
            elif report.passed:
                if getattr(report, "wasxfail", None):
                    finish("xpassed")
                else:
                    finish("passed")
        elif report.when == "teardown":
            if report.failed:
                # If teardown fails and the test hasn't been counted yet, mark as error
                finish("error")

    def pytest_sessionfinish(self, session: pytest.Session, exitstatus: int) -> None:
        # Stop progress display
        if self.progress:
            self.progress.stop()

        # Print a concise summary line
        duration = time.monotonic() - self._t0
        summary = (
            f"Total: {self.total or 0}  |  "
            f"Passed: {self.counts['passed']}  "
            f"Failed: {self.counts['failed']}  "
            f"Skipped: {self.counts['skipped']}  "
            f"XFailed: {self.counts['xfailed']}  "
            f"XPassed: {self.counts['xpassed']}  "
            f"Errors: {self.counts['error']}  "
            f"|  Time: {duration:.2f}s"
        )
        self.console.print(summary)

    # ---------- helpers ----------

    def close(self) -> None:
        # Safety: ensure progress is stopped if still active
        if self.progress:
            try:
                self.progress.stop()
            except Exception:
                pass


def pytest_addoption(parser: pytest.Parser) -> None:
    group = parser.getgroup("rich-progress")
    group.addoption(
        "--rich-progress",
        action="store_true",
        dest="rich_progress",
        default=True,
        help="Display real-time test progress using rich (default: enabled).",
    )
    group.addoption(
        "--no-rich-progress",
        action="store_false",
        dest="rich_progress",
        help="Disable real-time test progress.",
    )


def pytest_configure(config: pytest.Config) -> None:
    if config.getoption("rich_progress"):
        plugin = RichProgressPlugin(config)
        config._rich_progress_plugin = plugin  # type: ignore[attr-defined]
        config.pluginmanager.register(plugin, "rich-progress")


def pytest_unconfigure(config: pytest.Config) -> None:
    plugin = getattr(config, "_rich_progress_plugin", None)
    if plugin is not None:
        try:
            plugin.close()
        finally:
            config.pluginmanager.unregister(plugin)
            delattr(config, "_rich_progress_plugin")
```
------------------------------------------------------------

PROMPT 92:
"Generate Python code that sends JSON-RPC requests using the `requests` library, where the request data is a dictionary object converted to JSON format using the `json` module."
RESPONSE:
Here’s a concise, ready-to-run example that uses requests and json to send JSON-RPC 2.0 requests (single and batch), converting Python dicts to JSON strings explicitly with json.dumps.

```python
import json
import uuid
from typing import Any, Dict, Iterable, List, Optional, Tuple, Union

import requests


class JSONRPCError(Exception):
    def __init__(self, code: int, message: str, data: Any = None):
        super().__init__(f"JSON-RPC error {code}: {message}")
        self.code = code
        self.message = message
        self.data = data


def json_rpc_request(
    url: str,
    method: str,
    params: Union[List[Any], Dict[str, Any], None] = None,
    request_id: Union[str, int, None] = None,
    headers: Optional[Dict[str, str]] = None,
    timeout: float = 15.0,
) -> Any:
    """
    Send a single JSON-RPC 2.0 request.

    Returns the 'result' field or raises JSONRPCError on JSON-RPC error.
    """
    payload = {
        "jsonrpc": "2.0",
        "method": method,
        "params": [] if params is None else params,
        "id": request_id if request_id is not None else str(uuid.uuid4()),
    }

    base_headers = {
        "Content-Type": "application/json",
        "Accept": "application/json",
    }
    if headers:
        base_headers.update(headers)

    # Convert the dict to a JSON string explicitly using json.dumps
    data = json.dumps(payload)

    resp = requests.post(url, headers=base_headers, data=data, timeout=timeout)
    resp.raise_for_status()

    try:
        resp_json = resp.json()
    except ValueError as e:
        raise RuntimeError(f"Response is not valid JSON: {e}") from e

    if isinstance(resp_json, dict) and "error" in resp_json:
        err = resp_json["error"]
        raise JSONRPCError(err.get("code", -32000), err.get("message", "Unknown error"), err.get("data"))

    if isinstance(resp_json, dict) and "result" in resp_json:
        return resp_json["result"]

    raise RuntimeError("Invalid JSON-RPC response format")


def json_rpc_batch(
    url: str,
    calls: Iterable[Tuple[str, Union[List[Any], Dict[str, Any], None], Optional[Union[str, int]]]],
    headers: Optional[Dict[str, str]] = None,
    timeout: float = 15.0,
) -> Dict[Union[str, int], Any]:
    """
    Send a batch of JSON-RPC 2.0 requests.

    'calls' is an iterable of tuples: (method, params, optional_id).
    Returns a dict mapping each request id to its result.
    Raises JSONRPCError if any response item contains an error.
    """
    payloads = []
    id_map: List[Union[str, int]] = []
    for method, params, req_id in calls:
        rid = req_id if req_id is not None else str(uuid.uuid4())
        id_map.append(rid)
        payloads.append({
            "jsonrpc": "2.0",
            "method": method,
            "params": [] if params is None else params,
            "id": rid,
        })

    base_headers = {
        "Content-Type": "application/json",
        "Accept": "application/json",
    }
    if headers:
        base_headers.update(headers)

    data = json.dumps(payloads)
    resp = requests.post(url, headers=base_headers, data=data, timeout=timeout)
    resp.raise_for_status()

    try:
        resp_json = resp.json()
    except ValueError as e:
        raise RuntimeError(f"Response is not valid JSON: {e}") from e

    if not isinstance(resp_json, list):
        raise RuntimeError("Invalid JSON-RPC batch response format")

    results: Dict[Union[str, int], Any] = {}
    for item in resp_json:
        if not isinstance(item, dict) or "id" not in item:
            continue
        if "error" in item:
            err = item["error"]
            raise JSONRPCError(err.get("code", -32000), err.get("message", "Unknown error"), err.get("data"))
        results[item["id"]] = item.get("result")

    return results


if __name__ == "__main__":
    # Example single request
    rpc_url = "http://localhost:8545"  # replace with your JSON-RPC endpoint
    try:
        result = json_rpc_request(rpc_url, "web3_clientVersion")
        print("Single result:", result)
    except (requests.RequestException, JSONRPCError, RuntimeError) as e:
        print("Single request failed:", e)

    # Example batch request
    batch_calls = [
        ("web3_clientVersion", None, "req1"),
        ("net_version", None, "req2"),
    ]
    try:
        batch_results = json_rpc_batch(rpc_url, batch_calls)
        print("Batch results:", batch_results)
    except (requests.RequestException, JSONRPCError, RuntimeError) as e:
        print("Batch request failed:", e)
```
------------------------------------------------------------

PROMPT 93:
"Generate Python code that imports the necessary modules and creates a type-safe database client for interacting with a Prisma database, using the Prisma Client Python library."
RESPONSE:
from __future__ import annotations

import asyncio
from contextlib import asynccontextmanager
from typing import AsyncIterator

from prisma import Prisma

# Instantiate the generated, type-safe Prisma client
prisma: Prisma = Prisma()

@asynccontextmanager
async def get_prisma() -> AsyncIterator[Prisma]:
    was_connected = prisma.is_connected()
    if not was_connected:
        await prisma.connect()
    try:
        yield prisma
    finally:
        if not was_connected:
            await prisma.disconnect()

# Example usage
if __name__ == "__main__":
    async def main() -> None:
        async with get_prisma() as db:
            # Perform typed queries, e.g.:
            # users = await db.user.find_many()
            # print(users)
            pass

    asyncio.run(main())
------------------------------------------------------------

PROMPT 94:
"Generate Python code that logs Django Rest Framework requests to the database using a custom logger that writes to a SQLite database, including request and response data, timestamp, client IP address, and user agent."
RESPONSE:
Below is a lightweight, production-ready setup that logs all Django REST Framework requests to a dedicated SQLite database via a custom logging handler. It captures request/response data, timestamp, client IP, user agent, and duration, with basic redaction and size limiting.

File: request_logging/logger_sqlite.py
```python
import logging
import sqlite3
import threading
import json
import os
from datetime import datetime

DEFAULT_MAX_BYTES = 10000  # truncate large payloads
SENSITIVE_KEYS = {"password", "token", "authorization", "secret", "api_key", "access", "refresh"}

def ensure_dir(path):
    d = os.path.dirname(path)
    if d and not os.path.exists(d):
        os.makedirs(d, exist_ok=True)

def _truncate(s: str | bytes | None, max_bytes: int = DEFAULT_MAX_BYTES) -> str:
    if s is None:
        return ""
    if isinstance(s, bytes):
        try:
            s = s.decode("utf-8", errors="replace")
        except Exception:
            s = str(s)
    if len(s.encode("utf-8")) <= max_bytes:
        return s
    # truncate by bytes (avoid splitting multi-byte chars incorrectly)
    data = s.encode("utf-8")[:max_bytes]
    return data.decode("utf-8", errors="ignore") + "…[truncated]"

def _redact_mapping(obj):
    if isinstance(obj, dict):
        return {k: ("***" if k.lower() in SENSITIVE_KEYS else _redact_mapping(v)) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_redact_mapping(x) for x in obj]
    return obj

def redact_json_text(text: str) -> str:
    try:
        data = json.loads(text)
        data = _redact_mapping(data)
        return json.dumps(data, ensure_ascii=False)
    except Exception:
        return text

class SQLiteRequestLogHandler(logging.Handler):
    """
    A logging handler that writes DRF request/response logs into a SQLite DB.
    Creates the table if it doesn't exist. Thread-safe; safe for multi-process
    usage with WAL mode enabled.
    """
    def __init__(self, db_path: str, max_body_bytes: int = DEFAULT_MAX_BYTES):
        super().__init__()
        self.db_path = db_path
        self.max_body_bytes = max_body_bytes
        self._conn = None
        self._lock = threading.Lock()
        ensure_dir(db_path)
        self._connect()
        self._create_table()

    def _connect(self):
        # Separate connection per handler instance; safe across threads via lock
        self._conn = sqlite3.connect(self.db_path, check_same_thread=False, timeout=10)
        self._conn.execute("PRAGMA journal_mode=WAL;")
        self._conn.execute("PRAGMA synchronous=NORMAL;")

    def _create_table(self):
        with self._lock:
            self._conn.execute(
                """
                CREATE TABLE IF NOT EXISTS request_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    method TEXT,
                    path TEXT,
                    query_params TEXT,
                    status_code INTEGER,
                    client_ip TEXT,
                    user_agent TEXT,
                    user_id TEXT,
                    request_body TEXT,
                    response_body TEXT,
                    duration_ms REAL
                );
                """
            )
            self._conn.execute("CREATE INDEX IF NOT EXISTS idx_request_logs_ts ON request_logs(timestamp);")
            self._conn.commit()

    def emit(self, record: logging.LogRecord):
        try:
            payload = {
                "timestamp": getattr(record, "timestamp", None) or datetime.utcfromtimestamp(record.created).isoformat() + "Z",
                "method": getattr(record, "method", None),
                "path": getattr(record, "path", None),
                "query_params": getattr(record, "query_params", None),
                "status_code": getattr(record, "status_code", None),
                "client_ip": getattr(record, "client_ip", None),
                "user_agent": getattr(record, "user_agent", None),
                "user_id": getattr(record, "user_id", None),
                "request_body": getattr(record, "request_body", None),
                "response_body": getattr(record, "response_body", None),
                "duration_ms": getattr(record, "duration_ms", None),
            }

            # Truncate and redact JSON bodies if applicable
            req_body = payload.get("request_body") or ""
            resp_body = payload.get("response_body") or ""
            if isinstance(req_body, (dict, list)):
                req_body = json.dumps(_redact_mapping(req_body), ensure_ascii=False)
            else:
                req_body = redact_json_text(str(req_body))
            if isinstance(resp_body, (dict, list)):
                resp_body = json.dumps(_redact_mapping(resp_body), ensure_ascii=False)
            else:
                resp_body = redact_json_text(str(resp_body))

            req_body = _truncate(req_body, self.max_body_bytes)
            resp_body = _truncate(resp_body, self.max_body_bytes)

            with self._lock:
                self._conn.execute(
                    """
                    INSERT INTO request_logs (
                        timestamp, method, path, query_params, status_code,
                        client_ip, user_agent, user_id, request_body, response_body, duration_ms
                    )
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        payload["timestamp"],
                        payload["method"],
                        payload["path"],
                        payload["query_params"],
                        payload["status_code"],
                        payload["client_ip"],
                        payload["user_agent"],
                        str(payload["user_id"]) if payload["user_id"] is not None else None,
                        req_body,
                        resp_body,
                        payload["duration_ms"],
                    ),
                )
                self._conn.commit()
        except Exception:
            self.handleError(record)

    def close(self):
        try:
            if self._conn:
                self._conn.close()
        finally:
            super().close()
```

File: request_logging/middleware.py
```python
import logging
import time
import json

from django.utils.deprecation import MiddlewareMixin

JSON_CT = "application/json"
MAX_CAPTURE_BYTES = 10000  # aligns with handler defaults

def _get_client_ip(request):
    xff = request.META.get("HTTP_X_FORWARDED_FOR")
    if xff:
        # Use the left-most (original) IP
        return xff.split(",")[0].strip()
    return request.META.get("REMOTE_ADDR")

def _safe_decode(b: bytes | str | None):
    if b is None:
        return ""
    if isinstance(b, bytes):
        return b.decode("utf-8", errors="replace")
    return str(b)

def _is_json_content_type(ct: str | None) -> bool:
    return bool(ct and ct.split(";")[0].strip().lower() == JSON_CT)

def _capture_request_body(request):
    try:
        content_type = request.META.get("CONTENT_TYPE", "") or request.content_type
    except Exception:
        content_type = request.META.get("CONTENT_TYPE", "")

    body_bytes = getattr(request, "body", b"")  # Django caches after first access
    if not body_bytes:
        return ""

    if _is_json_content_type(content_type):
        text = _safe_decode(body_bytes)
        if len(text.encode("utf-8")) > MAX_CAPTURE_BYTES:
            return text.encode("utf-8")[:MAX_CAPTURE_BYTES].decode("utf-8", errors="ignore") + "…[truncated]"
        # Keep as text; handler will attempt redaction if valid JSON
        return text
    # For non-JSON content types, just note size and type
    size = len(body_bytes)
    return f"[{content_type} payload {size} bytes]"

def _capture_response_body(response):
    try:
        if getattr(response, "streaming", False):
            return "[streaming response]"
        content = getattr(response, "content", b"")
    except Exception:
        return "[unavailable]"

    content_type = response.get("Content-Type", "")
    if _is_json_content_type(content_type):
        text = _safe_decode(content)
        if len(text.encode("utf-8")) > MAX_CAPTURE_BYTES:
            return text.encode("utf-8")[:MAX_CAPTURE_BYTES].decode("utf-8", errors="ignore") + "…[truncated]"
        return text
    size = len(content or b"")
    return f"[{content_type or 'unknown'} response {size} bytes]"

class DRFRequestLoggingMiddleware(MiddlewareMixin):
    """
    Logs REST API requests and responses via Python logging.
    Attach to the 'drf.request' logger, configured with SQLiteRequestLogHandler.
    """
    logger_name = "drf.request"

    def process_request(self, request):
        request._rl_start = time.monotonic()
        # Read and cache request body early (Django caches request.body)
        request._rl_body = _capture_request_body(request)

    def process_response(self, request, response):
        try:
            start = getattr(request, "_rl_start", None)
            duration_ms = round((time.monotonic() - start) * 1000.0, 3) if start else None

            user = getattr(request, "user", None)
            user_id = getattr(user, "pk", None) if getattr(user, "is_authenticated", False) else None

            logger = logging.getLogger(self.logger_name)
            logger.info(
                "drf_request",
                extra={
                    "timestamp": None,  # handler will use record.created
                    "method": request.method,
                    "path": request.get_full_path().split("?")[0],
                    "query_params": request.META.get("QUERY_STRING", ""),
                    "status_code": getattr(response, "status_code", None),
                    "client_ip": _get_client_ip(request),
                    "user_agent": request.META.get("HTTP_USER_AGENT"),
                    "user_id": user_id,
                    "request_body": getattr(request, "_rl_body", ""),
                    "response_body": _capture_response_body(response),
                    "duration_ms": duration_ms,
                },
            )
        except Exception:
            # Never break request flow due to logging
            pass
        return response
```

File: settings.py (additions)
```python
import os
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent

# Where to store the SQLite request log DB file
REQUEST_LOG_DB = os.path.join(BASE_DIR, "request_logs", "drf_request_logs.sqlite3")

LOGGING = {
    "version": 1,
    "disable_existing_loggers": False,
    "handlers": {
        "sqlite_request": {
            "level": "INFO",
            "class": "request_logging.logger_sqlite.SQLiteRequestLogHandler",
            "db_path": REQUEST_LOG_DB,
            # Optional: "max_body_bytes": 20000,
        },
        # You can still keep console/file handlers if you want
    },
    "loggers": {
        "drf.request": {
            "handlers": ["sqlite_request"],
            "level": "INFO",
            "propagate": False,
        },
    },
}

MIDDLEWARE = [
    # ... your existing middleware ...
    "request_logging.middleware.DRFRequestLoggingMiddleware",
]
```

Notes:
- Place the request_logging package in your Django project and update import paths if needed.
- The handler creates the SQLite file and table on first use and enables WAL for concurrency.
- Request/response bodies are truncated and JSON is redacted for common sensitive keys. Adjust SENSITIVE_KEYS and size limits as needed.
- Works with DRF automatically (middleware wraps all requests).
------------------------------------------------------------

PROMPT 95:
"Generate Python code that implements a Rigetti QCS RPC framework and message specification using the `json` library to define the message format and the `requests` library to send HTTP requests between the client and server."
RESPONSE:
# qcs_rpc.py
# A minimal Rigetti QCS-style RPC framework with:
# - JSON message specification
# - HTTP transport using requests (client)
# - Simple HTTP server using Python stdlib http.server
#
# Features:
# - SubmitProgram (simulated execution with a background worker)
# - GetStatus
# - GetResult
# - Cancel
# - Ping
# - Optional auth token field
#
# Usage:
#   Start server:
#     python qcs_rpc.py server --port 8080
#   Run a demo client:
#     python qcs_rpc.py demo --url http://127.0.0.1:8080 --token example-token
#
# Dependencies:
#   - requests (client side). Server uses only Python standard library.

import argparse
import json
import queue
import random
import threading
import time
import uuid
from http.server import BaseHTTPRequestHandler, ThreadingHTTPServer
from urllib.parse import urlparse
from typing import Any, Dict, Optional

# ----------------------------
# Message specification
# ----------------------------

class QCSMessageSpec:
    VERSION = "qcs-rpc/1.0"

    @staticmethod
    def now() -> float:
        return time.time()

    @staticmethod
    def new_id() -> str:
        return str(uuid.uuid4())

    @staticmethod
    def make_request(msg_type: str,
                     payload: Dict[str, Any],
                     token: Optional[str] = None,
                     meta: Optional[Dict[str, Any]] = None,
                     id_: Optional[str] = None) -> Dict[str, Any]:
        if id_ is None:
            id_ = QCSMessageSpec.new_id()
        req = {
            "version": QCSMessageSpec.VERSION,
            "id": id_,
            "type": msg_type,
            "timestamp": QCSMessageSpec.now(),
            "payload": payload or {},
        }
        if token is not None:
            req["auth"] = {"token": token}
        if meta:
            req["meta"] = meta
        return req

    @staticmethod
    def ok_response(id_: str, result: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "version": QCSMessageSpec.VERSION,
            "id": id_,
            "ok": True,
            "result": result or {},
            "server_time": QCSMessageSpec.now(),
        }

    @staticmethod
    def error_response(id_: Optional[str],
                       code: str,
                       message: str,
                       details: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        return {
            "version": QCSMessageSpec.VERSION,
            "id": id_,
            "ok": False,
            "error": {
                "code": code,
                "message": message,
                "details": details or {},
            },
            "server_time": QCSMessageSpec.now(),
        }

    @staticmethod
    def validate_request(d: Dict[str, Any]) -> Optional[str]:
        # Return None if valid, else error message
        if not isinstance(d, dict):
            return "Request must be an object"
        if d.get("version") != QCSMessageSpec.VERSION:
            return f"Unsupported version: {d.get('version')}"
        if "id" not in d or not isinstance(d["id"], str):
            return "Missing or invalid id"
        if "type" not in d or not isinstance(d["type"], str):
            return "Missing or invalid type"
        if "payload" not in d or not isinstance(d["payload"], dict):
            return "Missing or invalid payload"
        return None

    @staticmethod
    def validate_response(d: Dict[str, Any]) -> Optional[str]:
        if not isinstance(d, dict):
            return "Response must be an object"
        if d.get("version") != QCSMessageSpec.VERSION:
            return f"Unsupported version: {d.get('version')}"
        if "id" not in d or not isinstance(d["id"], str):
            return "Missing or invalid id"
        if "ok" not in d or not isinstance(d["ok"], bool):
            return "Missing or invalid ok flag"
        if d["ok"]:
            if "result" not in d or not isinstance(d["result"], dict):
                return "Missing result for ok response"
        else:
            if "error" not in d or not isinstance(d["error"], dict):
                return "Missing error for failed response"
        return None

# ----------------------------
# Job manager and simulator
# ----------------------------

class JobManager:
    def __init__(self, n_workers: int = 2):
        self._jobs: Dict[str, Dict[str, Any]] = {}
        self._queue: "queue.Queue[str]" = queue.Queue()
        self._lock = threading.Lock()
        self._stop_event = threading.Event()
        self._workers: list[threading.Thread] = []
        for i in range(n_workers):
            t = threading.Thread(target=self._worker_loop, name=f"worker-{i}", daemon=True)
            t.start()
            self._workers.append(t)

    def stop(self):
        self._stop_event.set()
        # Drain queue so workers exit promptly
        for _ in self._workers:
            self._queue.put(None)  # type: ignore

    def create_job(self,
                   program: str,
                   target: str,
                   shots: int,
                   qubits: Optional[int],
                   priority: int) -> str:
        job_id = str(uuid.uuid4())
        job = {
            "id": job_id,
            "program": program,
            "target": target,
            "shots": max(1, int(shots)),
            "qubits": int(qubits) if qubits is not None else None,
            "priority": int(priority),
            "status": "QUEUED",
            "submitted_at": time.time(),
            "started_at": None,
            "completed_at": None,
            "progress": 0.0,
            "result": None,
            "error": None,
            "canceled": False,
        }
        with self._lock:
            self._jobs[job_id] = job
        # Queue job
        self._queue.put(job_id)
        return job_id

    def cancel_job(self, job_id: str) -> bool:
        with self._lock:
            job = self._jobs.get(job_id)
            if not job:
                return False
            if job["status"] in ("COMPLETED", "FAILED", "CANCELED"):
                return False
            job["canceled"] = True
            # If still queued, mark canceled immediately
            if job["status"] == "QUEUED":
                job["status"] = "CANCELED"
                job["completed_at"] = time.time()
                job["progress"] = 0.0
            return True

    def get_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        with self._lock:
            job = self._jobs.get(job_id)
            if not job:
                return None
            return {
                "id": job_id,
                "status": job["status"],
                "submitted_at": job["submitted_at"],
                "started_at": job["started_at"],
                "completed_at": job["completed_at"],
                "progress": job["progress"],
                "target": job["target"],
                "shots": job["shots"],
                "qubits": job["qubits"],
            }

    def get_result(self, job_id: str) -> Optional[Dict[str, Any]]:
        with self._lock:
            job = self._jobs.get(job_id)
            if not job:
                return None
            if job["status"] != "COMPLETED":
                return {"status": job["status"], "result": None}
            return {"status": job["status"], "result": job["result"]}

    def list_jobs(self, limit: int = 50) -> list[Dict[str, Any]]:
        with self._lock:
            jobs = list(self._jobs.values())[-limit:]
            return [
                {
                    "id": j["id"],
                    "status": j["status"],
                    "submitted_at": j["submitted_at"],
                    "completed_at": j["completed_at"],
                }
                for j in jobs
            ]

    def _worker_loop(self):
        while not self._stop_event.is_set():
            try:
                job_id = self._queue.get(timeout=0.5)
            except queue.Empty:
                continue
            if job_id is None:
                break
            with self._lock:
                job = self._jobs.get(job_id)
                if not job:
                    continue
                if job["canceled"]:
                    # Already canceled while queued
                    job["status"] = "CANCELED"
                    job["completed_at"] = time.time()
                    job["progress"] = 0.0
                    continue
                job["status"] = "RUNNING"
                job["started_at"] = time.time()
                job["progress"] = 0.0

            try:
                # Simulate execution
                result = self._simulate_run(job)
                with self._lock:
                    if job["canceled"]:
                        job["status"] = "CANCELED"
                        job["completed_at"] = time.time()
                        job["progress"] = 0.0
                    else:
                        job["status"] = "COMPLETED"
                        job["completed_at"] = time.time()
                        job["progress"] = 1.0
                        job["result"] = result
            except Exception as e:
                with self._lock:
                    job["status"] = "FAILED"
                    job["completed_at"] = time.time()
                    job["error"] = str(e)

    def _simulate_run(self, job: Dict[str, Any]) -> Dict[str, Any]:
        # Determine number of qubits: use provided, else naive inference, else default 2
        n_qubits = job["qubits"]
        if n_qubits is None:
            n_qubits = self._infer_qubits(job["program"])
        n_qubits = max(1, int(n_qubits))
        shots = job["shots"]

        counts: Dict[str, int] = {}
        # Simulate measurement outcomes; skew if "X" or "H" present to make it non-trivial
        has_h = "H" in job["program"].upper()
        has_x = "X " in job["program"].upper() or " X" in job["program"].upper()

        for s in range(shots):
            if job["canceled"]:
                break
            bits = []
            for _ in range(n_qubits):
                if has_h:
                    bit = 1 if random.random() < 0.5 else 0
                elif has_x:
                    bit = 1 if random.random() < 0.75 else 0
                else:
                    bit = 1 if random.random() < 0.1 else 0
                bits.append(bit)
            bitstr = "".join(str(b) for b in bits)
            counts[bitstr] = counts.get(bitstr, 0) + 1
            # Update progress occasionally
            if s % max(1, shots // 20) == 0:
                with self._lock:
                    job["progress"] = (s + 1) / shots
            time.sleep(min(0.002, 0.1 / max(1, shots)))  # keep it responsive

        # Simple metadata
        meta = {
            "n_qubits": n_qubits,
            "shots": shots,
            "target": job["target"],
        }
        return {"counts": counts, "metadata": meta}

    @staticmethod
    def _infer_qubits(program: str) -> int:
        # Extremely naive inference: look for "q[i]" patterns or literal qubit indices
        # Fallback to 2
        max_idx = -1
        # Find numbers possibly representing qubits
        token = ""
        for ch in program:
            if ch.isdigit():
                token += ch
            else:
                if token:
                    try:
                        idx = int(token)
                        if 0 <= idx <= 64:
                            max_idx = max(max_idx, idx)
                    except Exception:
                        pass
                token = ""
        if token:
            try:
                idx = int(token)
                if 0 <= idx <= 64:
                    max_idx = max(max_idx, idx)
            except Exception:
                pass
        return 2 if max_idx < 0 else (max_idx + 1)

# ----------------------------
# HTTP Server
# ----------------------------

class QCSRequestHandler(BaseHTTPRequestHandler):
    # Injected by server setup
    job_manager: JobManager = None  # type: ignore
    accepted_tokens: Optional[set[str]] = None

    server_version = "QCS-RPC-Server/1.0"
    sys_version = ""

    def _read_json(self) -> Optional[Dict[str, Any]]:
        length = self.headers.get("Content-Length")
        if length is None:
            return None
        try:
            data = self.rfile.read(int(length))
            return json.loads(data.decode("utf-8"))
        except Exception:
            return None

    def _write_json(self, obj: Dict[str, Any], status: int = 200):
        body = json.dumps(obj).encode("utf-8")
        self.send_response(status)
        self.send_header("Content-Type", "application/json; charset=utf-8")
        self.send_header("Content-Length", str(len(body)))
        self.end_headers()
        self.wfile.write(body)

    def do_POST(self):
        parsed = urlparse(self.path)
        if parsed.path != "/rpc":
            resp = QCSMessageSpec.error_response(None, "NOT_FOUND", f"Unknown endpoint {parsed.path}")
            self._write_json(resp, status=404)
            return

        req = self._read_json()
        if req is None:
            resp = QCSMessageSpec.error_response(None, "BAD_REQUEST", "Invalid JSON")
            self._write_json(resp, status=400)
            return

        # Validate request
        err = QCSMessageSpec.validate_request(req)
        if err:
            resp = QCSMessageSpec.error_response(req.get("id"), "BAD_REQUEST", err)
            self._write_json(resp, status=400)
            return

        # Optional token check
        token = (req.get("auth") or {}).get("token")
        if self.accepted_tokens is not None and token not in self.accepted_tokens:
            resp = QCSMessageSpec.error_response(req["id"], "UNAUTHORIZED", "Invalid token")
            self._write_json(resp, status=401)
            return

        # Dispatch
        try:
            msg_type = req["type"]
            payload = req["payload"]
            handler = getattr(self, f"_handle_{msg_type}", None)
            if not handler:
                resp = QCSMessageSpec.error_response(req["id"], "UNSUPPORTED_TYPE", f"Unsupported type {msg_type}")
                self._write_json(resp, status=400)
                return
            result = handler(payload)
            resp = QCSMessageSpec.ok_response(req["id"], result)
            self._write_json(resp, status=200)
        except Exception as e:
            resp = QCSMessageSpec.error_response(req.get("id"), "INTERNAL", f"Internal error: {e}")
            self._write_json(resp, status=500)

    # Handlers for message types
    def _handle_Ping(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "message": "pong",
            "server_time": time.time(),
            "echo": payload,
        }

    def _handle_SubmitProgram(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        program = payload.get("program", "")
        if not isinstance(program, str) or not program.strip():
            raise ValueError("payload.program must be a non-empty string (Quil/quilc program)")
        target = payload.get("target", "qvm")
        shots = int(payload.get("shots", 100))
        qubits = payload.get("qubits")
        if qubits is not None:
            qubits = int(qubits)
        priority = int(payload.get("priority", 0))
        job_id = self.job_manager.create_job(program, target, shots, qubits, priority)
        return {"job_id": job_id, "status": "QUEUED"}

    def _handle_GetStatus(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        job_id = payload.get("job_id")
        if not isinstance(job_id, str):
            raise ValueError("payload.job_id must be a string")
        st = self.job_manager.get_status(job_id)
        if st is None:
            raise ValueError("Unknown job_id")
        return st

    def _handle_GetResult(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        job_id = payload.get("job_id")
        if not isinstance(job_id, str):
            raise ValueError("payload.job_id must be a string")
        res = self.job_manager.get_result(job_id)
        if res is None:
            raise ValueError("Unknown job_id")
        return res

    def _handle_Cancel(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        job_id = payload.get("job_id")
        if not isinstance(job_id, str):
            raise ValueError("payload.job_id must be a string")
        ok = self.job_manager.cancel_job(job_id)
        return {"job_id": job_id, "canceled": ok}

    def _handle_ListJobs(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        limit = int(payload.get("limit", 50))
        return {"jobs": self.job_manager.list_jobs(limit=limit)}

    # Silence default logging
    def log_message(self, format, *args):
        return

def run_server(host: str, port: int, tokens: Optional[list[str]] = None):
    jm = JobManager(n_workers=2)
    handler_cls = QCSRequestHandler
    handler_cls.job_manager = jm
    handler_cls.accepted_tokens = set(tokens) if tokens else None

    httpd = ThreadingHTTPServer((host, port), handler_cls)
    print(f"QCS RPC server listening on http://{host}:{port} (Ctrl+C to stop)")
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        pass
    finally:
        jm.stop()
        httpd.server_close()

# ----------------------------
# Client
# ----------------------------

import requests  # client dependency

class QCSClient:
    def __init__(self, base_url: str, token: Optional[str] = None, timeout: float = 10.0):
        self.base_url = base_url.rstrip("/")
        self.token = token
        self.timeout = timeout

    def _post(self, msg_type: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        req = QCSMessageSpec.make_request(msg_type, payload, token=self.token, meta={"client": "qcs-py/1.0"})
        url = f"{self.base_url}/rpc"
        resp = requests.post(url, json=req, timeout=self.timeout)
        resp.raise_for_status()
        data = resp.json()
        err = QCSMessageSpec.validate_response(data)
        if err:
            raise RuntimeError(f"Invalid response: {err}")
        if not data["ok"]:
            err_obj = data.get("error", {})
            code = err_obj.get("code", "UNKNOWN")
            msg = err_obj.get("message", "Unknown error")
            raise RuntimeError(f"RPC error {code}: {msg}")
        return data["result"]

    def ping(self, message: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        return self._post("Ping", message or {})

    def submit_program(self,
                       program: str,
                       target: str = "qvm",
                       shots: int = 100,
                       qubits: Optional[int] = None,
                       priority: int = 0) -> str:
        payload = {
            "program": program,
            "target": target,
            "shots": int(shots),
            "priority": int(priority),
        }
        if qubits is not None:
            payload["qubits"] = int(qubits)
        res = self._post("SubmitProgram", payload)
        return res["job_id"]

    def get_status(self, job_id: str) -> Dict[str, Any]:
        return self._post("GetStatus", {"job_id": job_id})

    def get_result(self, job_id: str) -> Dict[str, Any]:
        return self._post("GetResult", {"job_id": job_id})

    def cancel(self, job_id: str) -> bool:
        res = self._post("Cancel", {"job_id": job_id})
        return bool(res.get("canceled", False))

    def list_jobs(self, limit: int = 50) -> list[Dict[str, Any]]:
        res = self._post("ListJobs", {"limit": int(limit)})
        return res["jobs"]

# ----------------------------
# Demo
# ----------------------------

DEMO_PROGRAM = """
DECLARE ro BIT[2]
H 0
CNOT 0 1
MEASURE 0 ro[0]
MEASURE 1 ro[1]
"""

def demo_client(url: str, token: Optional[str]):
    client = QCSClient(url, token=token, timeout=15.0)
    print("Ping:", client.ping({"hello": "world"}))

    print("Submitting program...")
    job_id = client.submit_program(DEMO_PROGRAM, target="qvm", shots=200, qubits=2)
    print("Submitted job:", job_id)

    while True:
        st = client.get_status(job_id)
        print(f"Status: {st['status']}, progress={st['progress']:.0%}")
        if st["status"] in ("COMPLETED", "FAILED", "CANCELED"):
            break
        time.sleep(0.5)

    res = client.get_result(job_id)
    print("Final status:", res["status"])
    if res["status"] == "COMPLETED":
        counts = res["result"]["counts"]
        meta = res["result"]["metadata"]
        print("Counts:", counts)
        print("Metadata:", meta)
    else:
        print("No result available.")

# ----------------------------
# CLI entry
# ----------------------------

def main():
    parser = argparse.ArgumentParser(description="QCS JSON-RPC demo framework")
    sub = parser.add_subparsers(dest="cmd")

    p_srv = sub.add_parser("server", help="Run RPC server")
    p_srv.add_argument("--host", default="127.0.0.1")
    p_srv.add_argument("--port", type=int, default=8080)
    p_srv.add_argument("--token", action="append", help="Accept only these tokens (repeatable)")

    p_demo = sub.add_parser("demo", help="Run demo client")
    p_demo.add_argument("--url", default="http://127.0.0.1:8080")
    p_demo.add_argument("--token", default=None)

    args = parser.parse_args()
    if args.cmd == "server":
        run_server(args.host, args.port, tokens=args.token)
    elif args.cmd == "demo":
        demo_client(args.url, token=args.token)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 96:
"Generate Python code that imports the necessary libraries and creates a wrapper function for the DataDetection framework on macOS, allowing users to easily interact with the framework's functionality from their Python scripts."
RESPONSE:
import sys
import platform
from datetime import datetime, timezone, timedelta

# Ensure we're on macOS
if platform.system() != "Darwin":
    raise RuntimeError("This wrapper requires macOS (Darwin).")

# PyObjC imports (install with: pip install pyobjc)
try:
    from Foundation import (
        NSDataDetector,
        NSTextCheckingTypeLink,
        NSTextCheckingTypePhoneNumber,
        NSTextCheckingTypeAddress,
        NSTextCheckingTypeDate,
        NSTextCheckingTypeTransitInformation,
    )
except Exception as e:
    raise RuntimeError(
        "Unable to import PyObjC Foundation. Install with: pip install pyobjc"
    ) from e


# Map friendly names to NSTextCheckingType bit flags
_TEXT_TYPE_MAP = {
    "link": NSTextCheckingTypeLink,
    "url": NSTextCheckingTypeLink,
    "phone": NSTextCheckingTypePhoneNumber,
    "phoneNumber": NSTextCheckingTypePhoneNumber,
    "address": NSTextCheckingTypeAddress,
    "date": NSTextCheckingTypeDate,
    "transit": NSTextCheckingTypeTransitInformation,
    "transitInformation": NSTextCheckingTypeTransitInformation,
}

# Default "all" mask for NSDataDetector-supported types
_ALL_TYPES_MASK = (
    NSTextCheckingTypeLink
    | NSTextCheckingTypePhoneNumber
    | NSTextCheckingTypeAddress
    | NSTextCheckingTypeDate
    | NSTextCheckingTypeTransitInformation
)


def _build_types_mask(types):
    """
    Build a bitmask of NSTextCheckingType* constants from user input.
    - types can be:
      - 'all' or None: include all NSDataDetector-supported types
      - a string (e.g., 'link', 'phone', 'address', 'date', 'transit')
      - an iterable of strings from the above
      - an integer bitmask (expert mode)
    """
    if types is None or types == "all":
        return _ALL_TYPES_MASK

    if isinstance(types, int):
        return types

    if isinstance(types, str):
        types = [types]

    mask = 0
    for t in types:
        key = str(t).strip()
        if key not in _TEXT_TYPE_MAP:
            raise ValueError(
                f"Unknown data detection type: {t}. "
                f"Valid types: {sorted(set(_TEXT_TYPE_MAP.keys()))} or 'all'."
            )
        mask |= _TEXT_TYPE_MAP[key]
    return mask


def _nsdate_to_iso8601(nsdate, tz=None):
    """
    Convert an NSDate (and optional NSTimeZone) to an ISO 8601 string.
    """
    if nsdate is None:
        return None

    ts = nsdate.timeIntervalSince1970()
    dt = datetime.fromtimestamp(ts, tz=timezone.utc)

    if tz is not None:
        # Convert to the NSTimeZone offset at that date/time
        try:
            seconds = tz.secondsFromGMTForDate_(nsdate)
        except Exception:
            seconds = tz.secondsFromGMT()
        dt = dt.astimezone(timezone(timedelta(seconds=seconds)))

    return dt.isoformat()


def _nsdict_to_py(d):
    """
    Convert an NSDictionary-like object to a plain Python dict with str keys.
    """
    if d is None:
        return None
    try:
        return {str(k): d[k] for k in d}
    except Exception:
        # Fallback best-effort
        return dict(d)


def _range_tuple(ns_range):
    """
    Ensure we have a (location, length) tuple from an NSRange-like value.
    """
    try:
        # PyObjC often presents NSRange as a tuple already
        loc, length = ns_range
        return int(loc), int(length)
    except Exception:
        # Or as an object with attributes
        return int(ns_range.location), int(ns_range.length)


def create_data_detector(types="all"):
    """
    Create and return a configured NSDataDetector for the given types.
    """
    mask = _build_types_mask(types)
    detector, error = NSDataDetector.alloc().initWithTypes_error_(mask, None)
    if detector is None:
        raise RuntimeError(f"Failed to create NSDataDetector: {error}")
    return detector


def detect_data(text, types="all"):
    """
    High-level wrapper around Apple's Data Detection via NSDataDetector.

    Parameters:
      - text (str): The input text to scan.
      - types: 'all', a single type string, an iterable of type strings,
               or an integer bitmask of NSTextCheckingType values.

    Returns:
      - List of dicts describing detected items with type, range, match text,
        and type-specific details.
    """
    if not isinstance(text, str):
        raise TypeError("text must be a Python str")

    detector = create_data_detector(types)
    matches = detector.matchesInString_options_range_(text, 0, (0, len(text)))

    results = []
    for m in matches:
        r_loc, r_len = _range_tuple(m.range())
        snippet = text[r_loc : r_loc + r_len]

        # Determine type and extract details
        rt = m.resultType()
        item = {
            "range": {"start": r_loc, "length": r_len},
            "match": snippet,
            "rawType": int(rt),  # raw NSTextCheckingType value for reference
            "type": None,
            "details": {},
        }

        # Note: Compare using bit equality because resultType is a single type
        if rt == NSTextCheckingTypeLink:
            item["type"] = "link"
            try:
                url = m.URL()
                item["details"]["url"] = str(url.absoluteString()) if url else None
            except Exception:
                item["details"]["url"] = None

        elif rt == NSTextCheckingTypePhoneNumber:
            item["type"] = "phone"
            try:
                item["details"]["phoneNumber"] = m.phoneNumber()
            except Exception:
                item["details"]["phoneNumber"] = None

        elif rt == NSTextCheckingTypeAddress:
            item["type"] = "address"
            try:
                item["details"]["components"] = _nsdict_to_py(m.addressComponents())
            except Exception:
                item["details"]["components"] = None

        elif rt == NSTextCheckingTypeDate:
            item["type"] = "date"
            try:
                nsdate = m.date()
                tz = m.timeZone() if hasattr(m, "timeZone") else None
                item["details"]["dateISO8601"] = _nsdate_to_iso8601(nsdate, tz)
                if tz is not None:
                    try:
                        item["details"]["timeZoneName"] = str(tz.name())
                    except Exception:
                        pass
                try:
                    item["details"]["durationSeconds"] = float(m.duration())
                except Exception:
                    pass
            except Exception:
                pass

        elif rt == NSTextCheckingTypeTransitInformation:
            item["type"] = "transit"
            try:
                item["details"]["components"] = _nsdict_to_py(m.components())
            except Exception:
                item["details"]["components"] = None

        else:
            # Shouldn't happen for NSDataDetector; keep a generic fallback
            item["type"] = "unknown"

        results.append(item)

    return results


if __name__ == "__main__":
    sample = (
        "Call me at +1 (415) 555-1234 tomorrow at 10am. "
        "Meet at 1 Infinite Loop, Cupertino, CA 95014. "
        "More info: https://www.apple.com"
    )
    hits = detect_data(sample, types="all")
    for h in hits:
        print(h)
------------------------------------------------------------

PROMPT 97:
"Generate Python code that imports the necessary libraries and creates a Kinetica DB client object to interact with a Kinetica database, including creating tables, inserting data, selecting data, updating data, and deleting data."
RESPONSE:
# pip install gpudb
# This example uses Kinetica's official Python client (gpudb) and runs SQL for basic CRUD.

import json
import gpudb


def connect_kinetica(host="http://127.0.0.1:9191", username="admin", password="password", **options):
    """
    Create and return a Kinetica (GPUdb) client.
    """
    return gpudb.GPUdb(host=host, username=username, password=password, options=options or {})


def run_sql(db: gpudb.GPUdb, sql: str):
    """
    Execute a SQL statement and return a tuple of (raw_response, parsed_rows_if_any).
    For SELECTs, this tries to parse returned rows if present.
    """
    resp = db.execute_sql(sql=sql)
    rows = []

    # Try to parse row-like results regardless of minor version differences
    # in the response payload. If your version differs, print(resp) to inspect.
    if isinstance(resp, dict):
        # Common keys seen across versions
        for key in ("records_json", "json_encoded_response", "records"):
            if key in resp and resp[key]:
                if key in ("records_json", "json_encoded_response"):
                    rows = [json.loads(r) for r in resp[key]]
                else:
                    rows = resp[key]
                break

    return resp, rows


if __name__ == "__main__":
    # 1) Connect
    db = connect_kinetica(
        host="http://127.0.0.1:9191",
        username="admin",
        password="password",
        timeout=60000  # optional; milliseconds
    )

    # Optional: create a schema (maps to a Kinetica collection)
    run_sql(db, "CREATE SCHEMA IF NOT EXISTS demo")

    # 2) Create a table
    create_sql = """
        CREATE TABLE IF NOT EXISTS demo.people (
            id INT PRIMARY KEY,
            name VARCHAR(100),
            age INT
        )
    """
    run_sql(db, create_sql)

    # 3) Insert data (multi-row INSERT)
    insert_sql = """
        INSERT INTO demo.people (id, name, age) VALUES
            (1, 'Alice', 30),
            (2, 'Bob',   41),
            (3, 'Cara',  27)
    """
    run_sql(db, insert_sql)

    # 4) Select data
    _, rows = run_sql(db, "SELECT id, name, age FROM demo.people ORDER BY id")
    print("After insert:", rows)

    # 5) Update data
    run_sql(db, "UPDATE demo.people SET age = age + 1 WHERE name = 'Alice'")
    _, rows = run_sql(db, "SELECT id, name, age FROM demo.people WHERE name = 'Alice'")
    print("After update:", rows)

    # 6) Delete data
    run_sql(db, "DELETE FROM demo.people WHERE id = 3")
    _, rows = run_sql(db, "SELECT id, name, age FROM demo.people ORDER BY id")
    print("After delete:", rows)

    # Optional: clean up
    # run_sql(db, "DROP TABLE demo.people")
    # run_sql(db, "DROP SCHEMA demo")

"""
Notes:
- Install the client: pip install gpudb
- The execute_sql response structure can vary slightly by Kinetica version.
  If parsing differs in your environment, print the raw response and adjust
  the parsing in run_sql accordingly.
- You can also use the object API (GPUdbRecordType, GPUdbTable) for non-SQL
  operations, but SQL via execute_sql is the most straightforward for CRUD.
"""
------------------------------------------------------------

PROMPT 98:
"Generate Python code that creates a plan for dispatching calls based on a graph of functions and data dependencies using the `graphlib` library to represent the graph and `functools` library to decorate functions with dependency information."
RESPONSE:
from functools import wraps
from graphlib import TopologicalSorter, CycleError


def task(*, produces, requires=()):
    """
    Decorator to annotate a function with dependency info.
    - produces: str or iterable of data keys this function produces
    - requires: iterable of data keys this function needs
    """
    if isinstance(produces, str):
        produces = {produces}
    else:
        produces = set(produces)
    requires = set(requires or ())

    def decorator(fn):
        @wraps(fn)
        def wrapped(*args, **kwargs):
            return fn(*args, **kwargs)

        # attach metadata
        setattr(wrapped, "__produces__", produces)
        setattr(wrapped, "__requires__", requires)
        return wrapped

    return decorator


def _index_producers(tasks):
    """Map each produced data key to the unique task that produces it."""
    key_to_producer = {}
    for fn in tasks:
        produces = getattr(fn, "__produces__", None)
        if not produces:
            raise ValueError(f"Task {fn.__name__} is missing 'produces' metadata.")
        for k in produces:
            if k in key_to_producer:
                other = key_to_producer[k]
                raise ValueError(
                    f"Duplicate producers for key '{k}': {other.__name__} and {fn.__name__}"
                )
            key_to_producer[k] = fn
    return key_to_producer


def _build_dependency_graph(tasks, initial_data_keys=()):
    """
    Build a graph mapping each task -> set of prerequisite tasks,
    based on the data keys they require/produce.
    """
    initial = set(initial_data_keys or ())
    producers = _index_producers(tasks)
    graph = {}

    for fn in tasks:
        requires = getattr(fn, "__requires__", set())
        deps = set()
        for key in requires:
            if key in initial:
                continue
            if key not in producers:
                raise KeyError(
                    f"Task {fn.__name__} requires '{key}' which is not provided in initial_data_keys "
                    f"and is not produced by any task."
                )
            deps.add(producers[key])
        graph[fn] = deps

    return graph


def make_dispatch_plan(tasks, initial_data_keys=()):
    """
    Given a set of decorated tasks and a set of initially available data keys,
    return a list of stages. Each stage is a list of tasks that can be dispatched
    concurrently after the previous stages complete.
    """
    graph = _build_dependency_graph(tasks, initial_data_keys)
    ts = TopologicalSorter(graph)
    try:
        ts.prepare()
    except CycleError as e:
        # e.args[1] holds the cycle nodes; map to names for readability
        cycle = " -> ".join(getattr(n, "__name__", str(n)) for n in e.args[1])
        raise RuntimeError(f"Cycle detected in task graph: {cycle}") from e

    stages = []
    while ts.is_active():
        ready = ts.get_ready()
        if not ready:
            # Should not happen unless there's a cycle; guard anyway
            break
        stages.append(list(ready))
        for node in ready:
            ts.done(node)
    return stages


# ---------- Example usage ----------
@task(produces="raw_users")
def load_users():
    pass


@task(produces="raw_orders")
def load_orders():
    pass


@task(produces="users", requires=("raw_users",))
def clean_users():
    pass


@task(produces="orders", requires=("raw_orders",))
def clean_orders():
    pass


@task(produces="report", requires=("users", "orders", "exchange_rates"))
def build_report():
    pass


if __name__ == "__main__":
    # exchange_rates is assumed to be provided externally (initial data)
    tasks = [load_users, load_orders, clean_users, clean_orders, build_report]
    plan = make_dispatch_plan(tasks, initial_data_keys={"exchange_rates"})

    print("Dispatch plan (stages of concurrently runnable tasks):")
    for i, stage in enumerate(plan):
        print(f"Stage {i}: {[fn.__name__ for fn in stage]}")

    # Example output:
    # Stage 0: ['load_users', 'load_orders']
    # Stage 1: ['clean_users', 'clean_orders']
    # Stage 2: ['build_report']
------------------------------------------------------------

PROMPT 99:
"Generate Python code that creates a flexible data layer for machine learning using Yoga Data Layer by importing the necessary libraries, defining a data source, creating a schema, loading data into the data layer, and querying the data using SQL."
RESPONSE:
# pip install -q yogadl duckdb pyarrow pandas requests

import os
import pathlib
import tempfile
from typing import Union

import duckdb
import pandas as pd
import pyarrow as pa
import pyarrow.csv as pacsv

# Try to import Yoga Data Layer (yogadl). If unavailable, we fall back to a simple downloader.
try:
    import yogadl  # noqa: F401
    HAVE_YOGADL = True
except Exception:
    HAVE_YOGADL = False

# If your YogaDL version exposes specific cache/data-ref utilities, import them here.
# The exact API may vary by version; adjust the imports below to your installed version.
if HAVE_YOGADL:
    try:
        # Common pattern in YogaDL: DataRef + Cache (names may vary by version)
        from yogadl.dataref import HttpDataRef  # type: ignore
        from yogadl.cache import LRUCache  # type: ignore
        HAVE_YOGADL_REFS = True
    except Exception:
        HAVE_YOGADL_REFS = False
else:
    HAVE_YOGADL_REFS = False


def materialize_with_yogadl(source_uri: str, cache_dir: Union[str, pathlib.Path]) -> pathlib.Path:
    """
    Use Yoga Data Layer to fetch/cache a remote file locally and return a local path.
    Note: Depending on your installed YogaDL version, replace HttpDataRef/LRUCache usage
    with the appropriate classes/functions from the library.
    """
    if not HAVE_YOGADL or not HAVE_YOGADL_REFS:
        raise RuntimeError("YogaDL refs/cache not available; use fallback or install compatible YogaDL.")
    cache_dir = pathlib.Path(cache_dir)
    cache_dir.mkdir(parents=True, exist_ok=True)

    # Example with hypothetical API; adjust for your installed version.
    data_ref = HttpDataRef(source_uri)             # reference to the remote file
    cache = LRUCache(cache_dir=str(cache_dir), max_bytes=2**30)  # 1 GiB cache

    # The cache should provide a way to materialize a DataRef to a local file path.
    # Replace get() / get_local_path() with the correct call in your version.
    local_path = cache.get(data_ref)  # expected to return a str or Path to cached file
    return pathlib.Path(local_path)


def download_fallback(source_uri: str, cache_dir: Union[str, pathlib.Path]) -> pathlib.Path:
    """
    Simple HTTP downloader used if YogaDL is not available. Not for production use.
    """
    import requests

    cache_dir = pathlib.Path(cache_dir)
    cache_dir.mkdir(parents=True, exist_ok=True)
    fname = pathlib.Path(source_uri.split("/")[-1] or "dataset.csv")
    local_path = cache_dir / fname

    if not local_path.exists():
        with requests.get(source_uri, stream=True, timeout=30) as r:
            r.raise_for_status()
            with open(local_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=1 << 20):
                    if chunk:
                        f.write(chunk)
    return local_path


def build_arrow_schema() -> pa.Schema:
    """
    Create a schema. Here we use the seaborn 'tips' dataset as an example.
    Adjust this to your data.
    """
    return pa.schema(
        [
            pa.field("total_bill", pa.float64()),
            pa.field("tip", pa.float64()),
            pa.field("sex", pa.string()),
            pa.field("smoker", pa.string()),
            pa.field("day", pa.string()),
            pa.field("time", pa.string()),
            pa.field("size", pa.int64()),
        ]
    )


def load_to_arrow_table(csv_path: Union[str, pathlib.Path], schema: pa.Schema) -> pa.Table:
    """
    Load a CSV into an Arrow Table using the provided schema.
    """
    parse_options = pacsv.ParseOptions(delimiter=",")
    convert_options = pacsv.ConvertOptions(column_types={f.name: f.type for f in schema})
    read_options = pacsv.ReadOptions(autogenerate_column_names=False)

    table = pacsv.read_csv(
        input_file=str(csv_path),
        read_options=read_options,
        parse_options=parse_options,
        convert_options=convert_options,
    )

    # Ensure schema alignment (cast if needed, e.g., to enforce int/float types)
    if table.schema != schema:
        table = table.cast(schema)

    return table


def main():
    # 1) Define a data source (CSV URL here; could be S3, GCS, etc.)
    source_uri = "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv"

    # 2) Create a local cache directory for the Yoga Data Layer
    cache_dir = pathlib.Path(tempfile.gettempdir()) / "yoga_data_layer_cache"

    # 3) Materialize the data locally via YogaDL (preferred) or fallback downloader
    if HAVE_YOGADL_REFS:
        local_file = materialize_with_yogadl(source_uri, cache_dir)
    else:
        # Fallback to a simple HTTP download if YogaDL or its helpers aren't available
        local_file = download_fallback(source_uri, cache_dir)

    # 4) Create a schema
    schema = build_arrow_schema()

    # 5) Load data into the "data layer" (Arrow table acts as an in-memory columnar layer)
    table = load_to_arrow_table(local_file, schema)

    # Optional: persist as Parquet within the cache for faster repeated queries
    parquet_path = cache_dir / "tips.parquet"
    import pyarrow.parquet as pq
    pq.write_table(table, parquet_path)

    # 6) Query data with SQL using DuckDB
    con = duckdb.connect(database=":memory:")

    # Option A: Register the Arrow table directly
    con.register("tips_arrow", table)
    # Option B: Or read from Parquet inside SQL: SELECT * FROM read_parquet(parquet_path)
    # We'll use the registered Arrow table for the example.

    # Example queries
    q1 = """
        SELECT day, AVG(tip) AS avg_tip, AVG(total_bill) AS avg_total_bill, COUNT(*) AS n
        FROM tips_arrow
        GROUP BY day
        ORDER BY day;
    """

    q2 = """
        SELECT sex, smoker, AVG(tip / NULLIF(total_bill, 0)) AS avg_tip_pct, COUNT(*) AS n
        FROM tips_arrow
        GROUP BY sex, smoker
        ORDER BY sex, smoker;
    """

    print("Average tip and total bill by day:")
    print(con.execute(q1).df())
    print("\nAverage tip percentage by sex and smoker:")
    print(con.execute(q2).df())

    # If you prefer Pandas for downstream ML:
    df: pd.DataFrame = con.execute("SELECT * FROM tips_arrow").df()
    print("\nSample rows loaded into Pandas:")
    print(df.head())


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 100:
"Generate Python code that imports the json module and creates a dictionary to store JSON data, parses JSON files using the json.loads() function, and converts dictionaries to JSON format using the json.dumps() function for elegant handling of JSON data."
RESPONSE:
import json
from pathlib import Path

def parse_json_file(path):
    # Read file as text and parse with json.loads()
    try:
        text = Path(path).read_text(encoding="utf-8")
        return json.loads(text)
    except FileNotFoundError:
        raise
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in {path}: {e}") from e

def dict_to_json(data, pretty=True):
    # Convert a dictionary to a JSON string
    if pretty:
        return json.dumps(data, indent=2, sort_keys=True, ensure_ascii=False)
    return json.dumps(data, separators=(",", ":"), ensure_ascii=False)

if __name__ == "__main__":
    # Create a dictionary to store JSON data
    payload = {
        "id": 1,
        "name": "Ada Lovelace",
        "active": True,
        "skills": ["mathematics", "programming"],
        "profile": {"country": "UK", "year": 1843}
    }

    # Convert dictionary to JSON format
    json_text = dict_to_json(payload)
    print("Dictionary as JSON:\n", json_text)

    # Save JSON text to a file (optional, for demonstration)
    example_path = Path("example.json")
    example_path.write_text(json_text, encoding="utf-8")

    # Parse JSON file using json.loads() on file contents
    parsed = parse_json_file(example_path)
    print("\nParsed back from file:\n", parsed)
------------------------------------------------------------

PROMPT 101:
"Generate Python code that imports the necessary modules and sets up a connection to a Databricks cluster, allowing you to interact with Databricks databases and tables, create and run SQL queries, and perform data engineering tasks such as data loading and cleaning."
RESPONSE:
# Prereqs (run once in your environment):
# pip install "databricks-connect>=13,<16" databricks-sdk pyspark

import os
from databricks.connect import DatabricksSession
from databricks.sdk.core import Config
from pyspark.sql import functions as F, types as T

# ------------------------------------------------------------------------------
# Configure Databricks Connect to a running interactive cluster
# - Replace placeholders below or set the environment variables:
#   DATABRICKS_HOST, DATABRICKS_TOKEN, DATABRICKS_CLUSTER_ID
# ------------------------------------------------------------------------------
cfg = Config(
    host=os.getenv("DATABRICKS_HOST", "https://<your-workspace>.cloud.databricks.com"),
    token=os.getenv("DATABRICKS_TOKEN", "<your-personal-access-token>"),
    cluster_id=os.getenv("DATABRICKS_CLUSTER_ID", "<your-cluster-id>"),
)
spark = DatabricksSession.builder.sdkConfig(cfg).getOrCreate()

print(f"Connected. Spark version: {spark.version}")
spark.sql("SELECT current_user() AS user, current_catalog() AS catalog, current_database() AS db").show(truncate=False)

# ------------------------------------------------------------------------------
# Basic exploration: list databases and tables
# ------------------------------------------------------------------------------
spark.sql("SHOW CATALOGS").show(truncate=False)
spark.sql("SHOW DATABASES").show(truncate=False)

# ------------------------------------------------------------------------------
# Create/use a database (schema)
# ------------------------------------------------------------------------------
database_name = "analytics"
spark.sql(f"CREATE DATABASE IF NOT EXISTS {database_name}")
spark.sql(f"USE {database_name}")

# ------------------------------------------------------------------------------
# Load raw data (example: CSV on DBFS or cloud storage)
# Note: File must be accessible from the Databricks cluster (e.g., dbfs:/, s3://, abfss://)
# ------------------------------------------------------------------------------
input_path = "dbfs:/FileStore/datasets/customers.csv"  # change to your path
raw_df = (
    spark.read.format("csv")
    .option("header", "true")
    .option("inferSchema", "true")
    .load(input_path)
)

print("Raw schema:")
raw_df.printSchema()

# ------------------------------------------------------------------------------
# Data cleaning/transformation examples
# ------------------------------------------------------------------------------
# Define a minimal expected schema (optional, helps standardize types)
expected_schema = T.StructType([
    T.StructField("customer_id", T.StringType(), True),
    T.StructField("name", T.StringType(), True),
    T.StructField("email", T.StringType(), True),
    T.StructField("signup_date", T.StringType(), True),  # will convert to date
    T.StructField("country", T.StringType(), True),
    T.StructField("spend", T.StringType(), True),        # will cast to double
])

# Align to expected columns if needed
for c in [f.name for f in expected_schema.fields if f.name not in raw_df.columns]:
    raw_df = raw_df.withColumn(c, F.lit(None).cast("string"))

# Clean/transform
clean_df = (
    raw_df
    .withColumn("customer_id", F.trim(F.col("customer_id")))
    .withColumn("name", F.initcap(F.trim(F.col("name"))))
    .withColumn("email", F.lower(F.trim(F.col("email"))))
    .withColumn("country", F.upper(F.trim(F.col("country"))))
    .withColumn("signup_date", F.to_date(F.col("signup_date")))
    .withColumn("spend", F.col("spend").cast("double"))
    .filter(F.col("customer_id").isNotNull())
    .dropDuplicates(["customer_id"])
)

# Example: simple email validity check (very loose)
email_regex = r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$"
clean_df = clean_df.filter((F.col("email").isNull()) | (F.col("email").rlike(email_regex)))

print("Cleaned schema:")
clean_df.printSchema()

# ------------------------------------------------------------------------------
# Save cleaned data as a Delta table and run SQL queries
# ------------------------------------------------------------------------------
table_name = "customers_clean"

# Overwrite-managed Delta table
(clean_df.write
 .format("delta")
 .mode("overwrite")
 .saveAsTable(f"{database_name}.{table_name}"))

# Example analytics query
spark.sql(f"""
SELECT country,
       COUNT(*) AS customers,
       ROUND(SUM(COALESCE(spend,0)), 2) AS total_spend
FROM {database_name}.{table_name}
GROUP BY country
ORDER BY total_spend DESC
LIMIT 10
""").show(truncate=False)

# Create a temp view and run ad hoc SQL
clean_df.createOrReplaceTempView("v_customers_clean")
spark.sql("""
SELECT date_trunc('month', signup_date) AS month,
       COUNT(*) AS new_customers,
       ROUND(SUM(COALESCE(spend,0)), 2) AS monthly_spend
FROM v_customers_clean
GROUP BY date_trunc('month', signup_date)
ORDER BY month
""").show(truncate=False)

# ------------------------------------------------------------------------------
# Example: incremental upsert (MERGE) into a Delta table
# (Assumes a new DataFrame 'updates_df' with potential new/changed rows)
# ------------------------------------------------------------------------------
# updates_df = ... # define or load updates here
# updates_df.createOrReplaceTempView("updates")

# spark.sql(f"""
# MERGE INTO {database_name}.{table_name} AS t
# USING updates AS s
# ON t.customer_id = s.customer_id
# WHEN MATCHED THEN UPDATE SET *
# WHEN NOT MATCHED THEN INSERT *
# """)

# ------------------------------------------------------------------------------
# Optional: performance optimization on Delta table (Databricks-only)
# ------------------------------------------------------------------------------
# spark.sql(f"OPTIMIZE {database_name}.{table_name} ZORDER BY (country)")

# Keep the session alive for interactive use
# spark.stop()  # Uncomment to close the connection when done
------------------------------------------------------------

PROMPT 102:
"Generate Python code that creates a library for recording and reading data in Jupyter and nteract Notebooks, using the `ipywidgets` and `numpy` libraries to create interactive widgets and manipulate data, and the `pandas` library to read and write data to CSV files."
RESPONSE:
# notebook_data_lib.py
# A lightweight library for recording and reading data in Jupyter and nteract Notebooks
# using ipywidgets, numpy, and pandas.

from dataclasses import dataclass, field as dc_field
from typing import Any, Dict, List, Optional, Sequence, Tuple, Union
from datetime import date, datetime

import numpy as np
import pandas as pd
import ipywidgets as W
from IPython.display import display, HTML


# ------------------------------
# Schema definition and utilities
# ------------------------------

@dataclass
class Field:
    name: str
    kind: str  # "str", "int", "float", "bool", "date", "category"
    description: str = ""
    required: bool = False
    default: Any = None
    choices: Optional[Sequence[Any]] = None  # for "category"
    placeholder: Optional[str] = None


def _widget_for_field(f: Field) -> W.Widget:
    common_kwargs = dict(description=f.name, tooltip=f.description or f.name, layout=W.Layout(width="auto"))
    if f.kind == "str":
        w = W.Text(placeholder=f.placeholder or "", value=f.default if isinstance(f.default, str) else "")
    elif f.kind == "int":
        w = W.IntText(value=int(f.default) if isinstance(f.default, (int, np.integer)) else 0)
    elif f.kind == "float":
        w = W.FloatText(value=float(f.default) if isinstance(f.default, (float, int, np.floating, np.integer)) else 0.0)
    elif f.kind == "bool":
        w = W.Checkbox(value=bool(f.default) if isinstance(f.default, (bool, np.bool_)) else False)
    elif f.kind == "date":
        # ipywidgets DatePicker uses datetime.date
        init = f.default if isinstance(f.default, date) else None
        w = W.DatePicker(value=init)
    elif f.kind == "category":
        if not f.choices:
            f.choices = []
        options = list(f.choices)
        if (f.default is not None) and (f.default not in options):
            options = [f.default] + options
        w = W.Dropdown(options=options, value=f.default if f.default in options else (options[0] if options else None))
    else:
        raise ValueError(f"Unsupported field kind: {f.kind}")
    for k, v in common_kwargs.items():
        setattr(w, k, v)
    return w


def _coerce_value(kind: str, v: Any) -> Any:
    if v is None:
        return None
    try:
        if kind == "str":
            return str(v)
        if kind == "int":
            return int(v) if v != "" else None
        if kind == "float":
            return float(v) if v != "" else None
        if kind == "bool":
            return bool(v)
        if kind == "date":
            # Already a datetime.date from DatePicker, convert to ISO string for CSV
            if isinstance(v, date):
                return v
            if isinstance(v, str) and v:
                return datetime.fromisoformat(v).date()
            return None
        if kind == "category":
            return v
    except Exception:
        return None
    return v


def _df_align_to_schema(df: pd.DataFrame, schema: List[Field]) -> pd.DataFrame:
    cols = [f.name for f in schema]
    for f in schema:
        if f.name not in df.columns:
            # create empty column with appropriate dtype
            if f.kind in ("int", "float"):
                df[f.name] = np.nan
            elif f.kind == "bool":
                df[f.name] = False
            elif f.kind == "date":
                df[f.name] = pd.NaT
            else:
                df[f.name] = None
    # Coerce types
    for f in schema:
        if f.kind == "int":
            df[f.name] = pd.to_numeric(df[f.name], errors="coerce").astype("Int64")
        elif f.kind == "float":
            df[f.name] = pd.to_numeric(df[f.name], errors="coerce")
        elif f.kind == "bool":
            df[f.name] = df[f.name].astype("boolean")
        elif f.kind == "date":
            df[f.name] = pd.to_datetime(df[f.name], errors="coerce").dt.date
        elif f.kind in ("str", "category"):
            df[f.name] = df[f.name].astype("string")
    # Order columns
    ordered = df[[c for c in cols if c in df.columns]].copy()
    # keep any extra columns at the end
    for c in df.columns:
        if c not in ordered.columns:
            ordered[c] = df[c]
    return ordered


# ------------------------------
# Core widgets and helpers
# ------------------------------

def _styled_table_html(df: pd.DataFrame, max_rows: int = 20) -> W.HTML:
    if df.empty:
        html = "<i>No data</i>"
    else:
        preview = df.head(max_rows)
        html = preview.to_html(border=0, classes="dataframe", justify="left")
        if len(df) > max_rows:
            html += f"<div style='margin-top:6px;color:#666;font-size:12px;'>Showing first {max_rows} of {len(df)} rows</div>"
    return W.HTML(value=html)


def _numeric_stats(df: pd.DataFrame) -> pd.DataFrame:
    num_df = df.select_dtypes(include=[np.number])
    if num_df.empty:
        return pd.DataFrame()
    arr = num_df.to_numpy(dtype=float)
    stats = {
        "count": np.sum(~np.isnan(arr), axis=0),
        "mean": np.nanmean(arr, axis=0),
        "std": np.nanstd(arr, axis=0, ddof=1) if arr.shape[0] > 1 else np.full(arr.shape[1], np.nan),
        "min": np.nanmin(arr, axis=0),
        "max": np.nanmax(arr, axis=0),
    }
    out = pd.DataFrame(stats, index=num_df.columns)
    return out


def _make_filter_widgets() -> Tuple[W.Dropdown, W.Dropdown, W.Text, W.Button, W.Button]:
    col = W.Dropdown(options=[], description="Column", layout=W.Layout(width="200px"))
    op = W.Dropdown(options=["==", "!=", ">", ">=", "<", "<=", "contains", "startswith", "endswith"],
                    value="==", description="Op", layout=W.Layout(width="140px"))
    val = W.Text(description="Value", layout=W.Layout(width="220px"))
    apply_btn = W.Button(description="Apply filter", button_style="primary", icon="filter")
    clear_btn = W.Button(description="Clear filter", icon="times")
    return col, op, val, apply_btn, clear_btn


# ------------------------------
# Data Recorder
# ------------------------------

class DataRecorder:
    """
    Interactive data recording app for Jupyter/nteract notebooks.

    Features:
    - Configurable schema -> auto-built form using ipywidgets
    - Append entries to an in-memory DataFrame
    - Load from CSV, Save to CSV (pandas)
    - Quick filter and basic numeric stats (numpy)
    - Delete rows by id
    """

    def __init__(self, schema: List[Field], title: str = "Data Recorder"):
        self.schema = schema
        self.title = title
        self._df = pd.DataFrame(columns=[f.name for f in schema])
        self._df = _df_align_to_schema(self._df, self.schema)
        self._next_id = 1

        # Widgets: form
        self._field_widgets: Dict[str, W.Widget] = {f.name: _widget_for_field(f) for f in self.schema}
        self._record_btn = W.Button(description="Record", button_style="success", icon="plus")
        self._clear_btn = W.Button(description="Clear", icon="eraser")
        self._status = W.HTML(value="")

        # Data table and controls
        self._table_html = _styled_table_html(self._df)
        self._refresh_btn = W.Button(description="Refresh view", icon="refresh")
        self._id_delete_dd = W.Dropdown(options=[], description="Row id")
        self._delete_btn = W.Button(description="Delete row", icon="trash", button_style="danger")

        # File I/O
        self._save_name = W.Text(value="data.csv", description="Filename")
        self._save_btn = W.Button(description="Save CSV", icon="save")
        self._upload = W.FileUpload(accept=".csv", multiple=False, description="Load CSV")

        # Filter and stats
        self._filter_col, self._filter_op, self._filter_val, self._filter_apply, self._filter_clear = _make_filter_widgets()
        self._filtered_df: Optional[pd.DataFrame] = None
        self._stats_html = _styled_table_html(pd.DataFrame())

        # Layout assembly
        self.view = self._build_ui()

        # Wire events
        self._record_btn.on_click(self._on_record_clicked)
        self._clear_btn.on_click(self._on_clear_clicked)
        self._refresh_btn.on_click(self._on_refresh_clicked)
        self._delete_btn.on_click(self._on_delete_clicked)
        self._save_btn.on_click(self._on_save_clicked)
        self._upload.observe(self._on_upload_changed, names="value")
        self._filter_apply.on_click(self._on_filter_apply)
        self._filter_clear.on_click(self._on_filter_clear)

        self._sync_controls()

    # Public API

    @property
    def dataframe(self) -> pd.DataFrame:
        return self._df.copy()

    def display(self):
        display(self.view)

    def load_csv(self, path: str):
        df = pd.read_csv(path)
        self._df = _df_align_to_schema(df, self.schema)
        self._after_data_change()

    def save_csv(self, path: str):
        df = self._df.copy()
        # Convert dates to ISO strings for CSV
        for f in self.schema:
            if f.kind == "date" and f.name in df.columns:
                df[f.name] = df[f.name].astype("string")
        df.to_csv(path, index=False)
        self._status.value = f"<span style='color:#1b5e20;'>Saved to {path}</span>"

    # Internal

    def _build_ui(self) -> W.Widget:
        title = W.HTML(f"<h3 style='margin:4px 0;'>{self.title}</h3>")
        form_items = []
        # Use a responsive box layout for fields
        for name, w in self._field_widgets.items():
            form_items.append(w)
        form_box = W.GridBox(
            children=form_items,
            layout=W.Layout(grid_template_columns="repeat(auto-fill, minmax(220px, 1fr))", grid_gap="8px")
        )
        actions = W.HBox([self._record_btn, self._clear_btn, self._status])

        # Data view and controls
        ctl_row = W.HBox([self._refresh_btn,
                          W.Label("Delete by id:"), self._id_delete_dd, self._delete_btn],
                         layout=W.Layout(align_items="center", justify_content="flex-start", gap="8px"))

        io_row = W.HBox([self._save_name, self._save_btn, self._upload], layout=W.Layout(gap="8px"))

        filter_row = W.HBox([self._filter_col, self._filter_op, self._filter_val, self._filter_apply, self._filter_clear],
                            layout=W.Layout(gap="8px"))

        stats_title = W.HTML("<b>Numeric stats</b>")
        stats_box = W.VBox([stats_title, self._stats_html])

        table_title = W.HTML("<b>Data</b>")
        table_box = W.VBox([table_title, self._table_html])

        main = W.VBox([
            title,
            W.HTML("<b>Form</b>"),
            form_box,
            actions,
            W.HTML("<hr>"),
            filter_row,
            table_box,
            ctl_row,
            W.HTML("<hr>"),
            io_row,
            stats_box
        ])
        return main

    def _on_record_clicked(self, _):
        row = {}
        errors = []
        for f in self.schema:
            w = self._field_widgets[f.name]
            val = getattr(w, "value", None)
            if f.required and (val is None or val == "" or (isinstance(val, str) and not val.strip())):
                errors.append(f"{f.name} is required")
                continue
            coerced = _coerce_value(f.kind, val)
            row[f.name] = coerced
        if errors:
            self._status.value = f"<span style='color:#b71c1c;'>{'; '.join(errors)}</span>"
            return
        # Add a synthetic id if not present
        if "_id" not in self._df.columns:
            self._df.insert(0, "_id", pd.Series(dtype="Int64"))
        row["_id"] = self._next_id
        self._next_id += 1

        self._df = pd.concat([self._df, pd.DataFrame([row])], ignore_index=True)
        self._df = _df_align_to_schema(self._df, self.schema)
        self._after_data_change()
        self._status.value = f"<span style='color:#1b5e20;'>Recorded row #{row['_id']}</span>"

    def _on_clear_clicked(self, _):
        for w in self._field_widgets.values():
            if isinstance(w, (W.Text, W.FloatText, W.IntText)):
                w.value = ""
            elif isinstance(w, W.Checkbox):
                w.value = False
            elif isinstance(w, W.DatePicker):
                w.value = None
            elif isinstance(w, W.Dropdown):
                # keep current selection
                pass
        self._status.value = ""

    def _on_refresh_clicked(self, _):
        self._refresh_table()

    def _on_delete_clicked(self, _):
        sel = self._id_delete_dd.value
        if sel is None:
            return
        before = len(self._df)
        self._df = self._df[self._df["_id"] != sel].reset_index(drop=True)
        after = len(self._df)
        self._after_data_change()
        if before != after:
            self._status.value = f"<span style='color:#b71c1c;'>Deleted row #{sel}</span>"

    def _on_save_clicked(self, _):
        name = (self._save_name.value or "").strip()
        if not name:
            name = "data.csv"
        self.save_csv(name)

    def _on_upload_changed(self, change):
        if not self._upload.value:
            return
        up = next(iter(self._upload.value.values()))
        content = up["content"]
        try:
            import io
            df = pd.read_csv(io.BytesIO(content))
            self._df = _df_align_to_schema(df, self.schema)
            # Rebuild next id
            if "_id" in self._df.columns and pd.api.types.is_integer_dtype(self._df["_id"]):
                self._next_id = int((self._df["_id"].max() or 0) + 1)
            else:
                if "_id" not in self._df.columns:
                    self._df.insert(0, "_id", pd.Series(dtype="Int64"))
                self._df["_id"] = pd.Series(range(1, len(self._df) + 1), dtype="Int64")
                self._next_id = len(self._df) + 1
            self._after_data_change()
            self._status.value = f"<span style='color:#1b5e20;'>Loaded {len(self._df)} rows from upload</span>"
        except Exception as e:
            self._status.value = f"<span style='color:#b71c1c;'>Load failed: {e}</span>"

    def _on_filter_apply(self, _):
        col = self._filter_col.value
        op = self._filter_op.value
        raw_val = self._filter_val.value
        if not col or raw_val is None:
            self._filtered_df = None
            self._refresh_table()
            return

        series = self._df[col]
        # Try numeric comparison if possible
        comp_df = None
        try:
            if op in ("contains", "startswith", "endswith"):
                s = series.astype("string")
                if op == "contains":
                    mask = s.str.contains(raw_val, na=False, case=False)
                elif op == "startswith":
                    mask = s.str.startswith(raw_val, na=False)
                else:
                    mask = s.str.endswith(raw_val, na=False)
            else:
                # Numeric compare if both are numeric
                s_num = pd.to_numeric(series, errors="coerce")
                v_num = pd.to_numeric(pd.Series([raw_val]), errors="coerce").iloc[0]
                if not pd.isna(v_num):
                    a = s_num
                    b = v_num
                else:
                    # fallback to string compare
                    a = series.astype("string")
                    b = str(raw_val)
                if op == "==":
                    mask = a == b
                elif op == "!=":
                    mask = a != b
                elif op == ">":
                    mask = a > b
                elif op == ">=":
                    mask = a >= b
                elif op == "<":
                    mask = a < b
                elif op == "<=":
                    mask = a <= b
                else:
                    mask = pd.Series([True] * len(series))
            comp_df = self._df[mask].copy()
        except Exception:
            comp_df = None

        self._filtered_df = comp_df
        self._refresh_table()

    def _on_filter_clear(self, _):
        self._filtered_df = None
        self._filter_val.value = ""
        self._refresh_table()

    def _after_data_change(self):
        self._sync_controls()
        self._refresh_table()

    def _sync_controls(self):
        # Update delete dropdown
        if "_id" in self._df.columns and not self._df.empty:
            ids = list(self._df["_id"].astype("Int64").dropna().astype(int).tolist())
            self._id_delete_dd.options = ids
            self._id_delete_dd.value = ids[-1] if ids else None
        else:
            self._id_delete_dd.options = []
            self._id_delete_dd.value = None
        # Update filter columns
        cols = [c for c in self._df.columns if c != "_id"]
        self._filter_col.options = cols
        if cols and self._filter_col.value not in cols:
            self._filter_col.value = cols[0]
        # Update stats
        stats = _numeric_stats(self._df)
        self._stats_html.value = stats.to_html(border=0) if not stats.empty else "<i>No numeric columns</i>"

    def _refresh_table(self):
        df = self._filtered_df if self._filtered_df is not None else self._df
        self._table_html.value = _styled_table_html(df).value


# ------------------------------
# Data Reader
# ------------------------------

class DataReader:
    """
    Interactive data reading and exploration app.

    Features:
    - Load CSV via path or upload
    - Column subset selection
    - Simple filter and sort
    - Numeric stats (numpy)
    """

    def __init__(self, title: str = "Data Reader"):
        self.title = title
        self._df = pd.DataFrame()

        # IO
        self._path = W.Text(description="CSV path", placeholder="path/to/file.csv", layout=W.Layout(width="40%"))
        self._load_btn = W.Button(description="Load", icon="folder-open", button_style="primary")
        self._upload = W.FileUpload(accept=".csv", multiple=False, description="Upload CSV")

        # Column chooser, sort
        self._cols_sel = W.SelectMultiple(options=[], description="Columns", layout=W.Layout(height="160px", width="300px"))
        self._sort_col = W.Dropdown(options=[], description="Sort by", layout=W.Layout(width="220px"))
        self._sort_asc = W.ToggleButtons(options=[("Asc", True), ("Desc", False)], value=True, description="Order")

        # Filter
        self._filter_col, self._filter_op, self._filter_val, self._filter_apply, self._filter_clear = _make_filter_widgets()
        self._filtered_df: Optional[pd.DataFrame] = None

        # Output
        self._table_html = _styled_table_html(pd.DataFrame())
        self._stats_html = _styled_table_html(pd.DataFrame())
        self._status = W.HTML("")

        # Layout
        self.view = self._build_ui()

        # Events
        self._load_btn.on_click(self._on_load_clicked)
        self._upload.observe(self._on_upload_changed, names="value")
        self._cols_sel.observe(self._on_cols_changed, names="value")
        self._sort_col.observe(self._on_sort_changed, names="value")
        self._sort_asc.observe(self._on_sort_changed, names="value")
        self._filter_apply.on_click(self._on_filter_apply)
        self._filter_clear.on_click(self._on_filter_clear)

    def display(self):
        display(self.view)

    @property
    def dataframe(self) -> pd.DataFrame:
        return self._df.copy()

    def load_csv(self, path: str):
        self._df = pd.read_csv(path)
        self._after_data_change()
        self._status.value = f"<span style='color:#1b5e20;'>Loaded {len(self._df)} rows from {path}</span>"

    # Internals
    def _build_ui(self) -> W.Widget:
        title = W.HTML(f"<h3 style='margin:4px 0;'>{self.title}</h3>")
        io_row = W.HBox([self._path, self._load_btn, self._upload], layout=W.Layout(gap="8px"))

        col_box = W.VBox([W.HTML("<b>Columns</b>"), self._cols_sel])
        sort_box = W.VBox([W.HTML("<b>Sort</b>"), W.HBox([self._sort_col, self._sort_asc])])
        filter_row = W.HBox([self._filter_col, self._filter_op, self._filter_val, self._filter_apply, self._filter_clear],
                            layout=W.Layout(gap="8px"))
        left = W.VBox([col_box, sort_box, W.HTML("<b>Filter</b>"), filter_row], layout=W.Layout(width="auto"))
        right = W.VBox([W.HTML("<b>Preview</b>"), self._table_html, W.HTML("<hr>"), W.HTML("<b>Numeric stats</b>"), self._stats_html])

        main = W.VBox([title, io_row, W.HBox([left, right], layout=W.Layout(gap="16px")), self._status])
        return main

    def _on_load_clicked(self, _):
        path = (self._path.value or "").strip()
        if not path:
            self._status.value = "<span style='color:#b71c1c;'>Please enter a CSV path</span>"
            return
        try:
            self.load_csv(path)
        except Exception as e:
            self._status.value = f"<span style='color:#b71c1c;'>Load failed: {e}</span>"

    def _on_upload_changed(self, change):
        if not self._upload.value:
            return
        up = next(iter(self._upload.value.values()))
        content = up["content"]
        try:
            import io
            self._df = pd.read_csv(io.BytesIO(content))
            self._after_data_change()
            self._status.value = f"<span style='color:#1b5e20;'>Loaded {len(self._df)} rows from upload</span>"
        except Exception as e:
            self._status.value = f"<span style='color:#b71c1c;'>Load failed: {e}</span>"

    def _on_cols_changed(self, change):
        self._refresh_table()

    def _on_sort_changed(self, change):
        self._refresh_table()

    def _on_filter_apply(self, _):
        col = self._filter_col.value
        op = self._filter_op.value
        raw_val = self._filter_val.value
        if not col or raw_val is None:
            self._filtered_df = None
            self._refresh_table()
            return

        series = self._df[col]
        try:
            if op in ("contains", "startswith", "endswith"):
                s = series.astype("string")
                if op == "contains":
                    mask = s.str.contains(raw_val, na=False, case=False)
                elif op == "startswith":
                    mask = s.str.startswith(raw_val, na=False)
                else:
                    mask = s.str.endswith(raw_val, na=False)
            else:
                s_num = pd.to_numeric(series, errors="coerce")
                v_num = pd.to_numeric(pd.Series([raw_val]), errors="coerce").iloc[0]
                if not pd.isna(v_num):
                    a = s_num
                    b = v_num
                else:
                    a = series.astype("string")
                    b = str(raw_val)
                if op == "==":
                    mask = a == b
                elif op == "!=":
                    mask = a != b
                elif op == ">":
                    mask = a > b
                elif op == ">=":
                    mask = a >= b
                elif op == "<":
                    mask = a < b
                elif op == "<=":
                    mask = a <= b
                else:
                    mask = pd.Series([True] * len(series))
            self._filtered_df = self._df[mask].copy()
        except Exception:
            self._filtered_df = None
        self._refresh_table()

    def _on_filter_clear(self, _):
        self._filtered_df = None
        self._filter_val.value = ""
        self._refresh_table()

    def _after_data_change(self):
        cols = list(self._df.columns)
        self._cols_sel.options = cols
        self._cols_sel.value = tuple(cols)
        self._sort_col.options = [""] + cols
        self._filter_col.options = cols
        self._filtered_df = None
        self._refresh_table()

    def _refresh_table(self):
        df = self._filtered_df if self._filtered_df is not None else self._df
        # Column subset
        cols = list(self._cols_sel.value) if self._cols_sel.value else list(df.columns)
        df = df[cols] if cols else df
        # Sort
        sort_col = self._sort_col.value
        if sort_col:
            try:
                df = df.sort_values(by=sort_col, ascending=self._sort_asc.value, kind="mergesort")
            except Exception:
                pass
        self._table_html.value = _styled_table_html(df).value
        stats = _numeric_stats(df)
        self._stats_html.value = stats.to_html(border=0) if not stats.empty else "<i>No numeric columns</i>"


# ------------------------------
# Convenience factory functions
# ------------------------------

def create_recorder(schema: List[Field], title: str = "Data Recorder") -> DataRecorder:
    return DataRecorder(schema=schema, title=title)


def create_reader(title: str = "Data Reader") -> DataReader:
    return DataReader(title=title)


# ------------------------------
# Example schema helper
# ------------------------------

def example_schema() -> List[Field]:
    return [
        Field("name", "str", required=True, placeholder="Enter a name"),
        Field("age", "int"),
        Field("height_cm", "float"),
        Field("is_member", "bool", default=False),
        Field("join_date", "date"),
        Field("group", "category", choices=["A", "B", "C"], default="A"),
    ]


# ------------------------------
# If used interactively, quick start:
# ------------------------------
# from notebook_data_lib import create_recorder, example_schema
# app = create_recorder(example_schema(), title="My Data")
# app.display()
# reader = create_reader()
# reader.display()
------------------------------------------------------------

PROMPT 103:
"Generate Python code that imports the required modules and uses the `resolve` function from the `dnpy` library to resolve dotted names, similar to the `importlib.util.module_from_name` function but also works for non-modules, and returns the resolved module or object."
RESPONSE:
try:
    from dnpy import resolve as _resolve
except Exception:
    _resolve = None

def resolve_dotted(name: str):
    """
    Resolve a dotted name to a module or object.
    Uses dnpy.resolve if available; otherwise falls back to a simple resolver
    that supports both 'pkg.mod.obj' and 'pkg.mod:obj.attr' forms.
    """
    if _resolve is not None:
        return _resolve(name)

    # Fallback resolver
    import importlib
    import builtins

    left, sep, right = name.partition(':')
    module_path = left
    attr_tail = right if sep else ''

    parts = module_path.split('.') if module_path else []
    module = None
    remainder = []

    # Import the longest valid module/package prefix
    for i in range(len(parts), 0, -1):
        mod_name = '.'.join(parts[:i])
        try:
            module = importlib.import_module(mod_name)
            remainder = parts[i:]
            break
        except ModuleNotFoundError:
            continue

    obj = module if module is not None else builtins
    # Traverse remaining attributes from the module (or builtins if no module)
    for attr in remainder:
        obj = getattr(obj, attr)

    # Handle optional ':attr.attr' tail
    if attr_tail:
        for attr in attr_tail.split('.'):
            obj = getattr(obj, attr)

    return obj

# Example usage:
# resolved_obj = resolve_dotted("json.decoder.JSONDecoder")
# resolved_mod = resolve_dotted("json.decoder")
# resolved_attr = resolve_dotted("collections:deque.__doc__")
------------------------------------------------------------

PROMPT 104:
"Generate Python code that imports the Kedro-Datasets module and returns a list of all available data connectors.\n\nHere's an example of what the code might look like:\n```\nfrom kedro.datasets import DataConnectors\n\nconnectors = DataConnectors().get_available()\n```"
RESPONSE:
# Lists all available Kedro-Datasets connectors (dataset classes)
import importlib
import inspect
import pkgutil

try:
    # Kedro >= 0.18
    from kedro.io import AbstractDataset
except Exception:
    # Fallback for older Kedro versions
    from kedro.io.core import AbstractDataset

import kedro_datasets


def get_available_connectors():
    connectors = set()

    for modinfo in pkgutil.walk_packages(
        kedro_datasets.__path__, kedro_datasets.__name__ + "."
    ):
        modname = modinfo.name

        # Skip tests and private modules
        if ".tests" in modname or any(part.startswith("_") for part in modname.split(".")):
            continue

        try:
            module = importlib.import_module(modname)
        except Exception:
            # Skip modules that can't be imported due to optional deps
            continue

        for name, obj in inspect.getmembers(module, inspect.isclass):
            # Keep only dataset classes defined in this module
            try:
                if (
                    issubclass(obj, AbstractDataset)
                    and obj is not AbstractDataset
                    and obj.__module__ == module.__name__
                ):
                    connectors.add(f"{obj.__module__}.{name}")
            except Exception:
                continue

    return sorted(connectors)


if __name__ == "__main__":
    connectors = get_available_connectors()
    print(connectors)
------------------------------------------------------------

PROMPT 105:
"Generate Python code that implements a simple authentication client using the requests library to send HTTP requests and the JSON library to parse responses, including functions for registering a new user, logging in, and retrieving information about the authenticated user."
RESPONSE:
import json
import requests
from typing import Any, Dict, Optional


class AuthClientError(Exception):
    """Base exception for AuthClient."""


class RegistrationError(AuthClientError):
    """Raised when registration fails."""
    def __init__(self, message: str, status_code: Optional[int] = None):
        super().__init__(message)
        self.status_code = status_code


class AuthenticationError(AuthClientError):
    """Raised when authentication (login) fails."""
    def __init__(self, message: str, status_code: Optional[int] = None):
        super().__init__(message)
        self.status_code = status_code


class NotAuthenticatedError(AuthClientError):
    """Raised when an operation requires authentication but no valid token is set."""


class APIRequestError(AuthClientError):
    """Raised when an API request fails due to network or response parsing issues."""
    def __init__(self, message: str, status_code: Optional[int] = None):
        super().__init__(message)
        self.status_code = status_code


class AuthClient:
    """
    Simple authentication client using requests for HTTP and json for parsing responses.

    Default endpoints:
      - POST {base_url}/auth/register
      - POST {base_url}/auth/login
      - GET  {base_url}/auth/me

    You can override endpoints via the constructor.
    """

    def __init__(
        self,
        base_url: str,
        register_endpoint: str = "/auth/register",
        login_endpoint: str = "/auth/login",
        me_endpoint: str = "/auth/me",
        timeout: int = 10,
    ):
        self.base_url = base_url.rstrip("/")
        self.register_endpoint = register_endpoint
        self.login_endpoint = login_endpoint
        self.me_endpoint = me_endpoint
        self.timeout = timeout

        self._session = requests.Session()
        self._session.headers.update({"Accept": "application/json"})
        self._token: Optional[str] = None

    def set_token(self, token: Optional[str]) -> None:
        """
        Set or clear the bearer token used for authenticated requests.
        """
        self._token = token
        # Update Authorization header for the session accordingly
        if token:
            self._session.headers.update({"Authorization": f"Bearer {token}"})
        else:
            self._session.headers.pop("Authorization", None)

    def _url(self, endpoint: str) -> str:
        if endpoint.startswith("http://") or endpoint.startswith("https://"):
            return endpoint
        return f"{self.base_url}{endpoint}"

    def _parse_json(self, response: requests.Response) -> Dict[str, Any]:
        """
        Parse response body as JSON using the json library.
        Raises APIRequestError if parsing fails.
        """
        try:
            return json.loads(response.text) if response.text else {}
        except json.JSONDecodeError as exc:
            raise APIRequestError(
                f"Invalid JSON in response (status {response.status_code}): {exc}",
                status_code=response.status_code,
            ) from exc

    def register(self, username: str, email: str, password: str) -> Dict[str, Any]:
        """
        Register a new user.

        Returns the response JSON as a dictionary on success.
        Raises RegistrationError on 4xx/5xx responses or APIRequestError on parsing issues.
        """
        url = self._url(self.register_endpoint)
        payload = {"username": username, "email": email, "password": password}
        try:
            resp = self._session.post(url, json=payload, timeout=self.timeout)
        except requests.RequestException as exc:
            raise APIRequestError(f"Network error during registration: {exc}") from exc

        data = self._parse_json(resp)
        if 200 <= resp.status_code < 300:
            return data

        # Extract best-effort error message
        message = (
            data.get("error")
            or data.get("message")
            or f"Registration failed with status {resp.status_code}"
        )
        raise RegistrationError(message, status_code=resp.status_code)

    def login(self, username_or_email: str, password: str) -> Dict[str, Any]:
        """
        Log in and store the bearer token for subsequent requests.

        Returns the response JSON as a dictionary on success.
        Token is extracted from common keys: 'access_token', 'token', or 'jwt'.
        Raises AuthenticationError on 4xx/5xx responses or APIRequestError on parsing issues.
        """
        url = self._url(self.login_endpoint)
        payload = {"username": username_or_email, "password": password}
        try:
            resp = self._session.post(url, json=payload, timeout=self.timeout)
        except requests.RequestException as exc:
            raise APIRequestError(f"Network error during login: {exc}") from exc

        data = self._parse_json(resp)
        if not (200 <= resp.status_code < 300):
            message = (
                data.get("error")
                or data.get("message")
                or f"Login failed with status {resp.status_code}"
            )
            raise AuthenticationError(message, status_code=resp.status_code)

        # Extract token from common fields
        token = data.get("access_token") or data.get("token") or data.get("jwt")
        if not token:
            # Some APIs return token nested or under different keys; adjust as needed.
            raise AuthenticationError("Login succeeded but no token found in response.")
        self.set_token(token)
        return data

    def get_current_user(self) -> Dict[str, Any]:
        """
        Retrieve information about the authenticated user.

        Requires a previously set token (via login or set_token).
        Raises NotAuthenticatedError if no token or if the server returns 401.
        Raises APIRequestError on parsing or network issues.
        """
        if not self._token:
            raise NotAuthenticatedError("No authentication token set. Please log in first.")

        url = self._url(self.me_endpoint)
        try:
            resp = self._session.get(url, timeout=self.timeout)
        except requests.RequestException as exc:
            raise APIRequestError(f"Network error during user retrieval: {exc}") from exc

        if resp.status_code == 401:
            raise NotAuthenticatedError("Token invalid or expired (401 Unauthorized).")

        data = self._parse_json(resp)
        if 200 <= resp.status_code < 300:
            return data

        message = (
            data.get("error")
            or data.get("message")
            or f"Failed to retrieve user with status {resp.status_code}"
        )
        raise APIRequestError(message, status_code=resp.status_code)


# Example usage
if __name__ == "__main__":
    # Adjust base_url and endpoints to match your API
    client = AuthClient(base_url="https://api.example.com")

    try:
        # Register a new user
        reg = client.register(username="alice", email="alice@example.com", password="S3cretPass!")
        print("Registered:", reg)

        # Log in
        auth = client.login(username_or_email="alice", password="S3cretPass!")
        print("Logged in:", auth)

        # Get current user info
        me = client.get_current_user()
        print("Current user:", me)

    except AuthClientError as e:
        print(f"Auth error: {e} (status: {getattr(e, 'status_code', None)})")
------------------------------------------------------------

PROMPT 106:
"Generate Python code that imports the required modules and sets up type hints for the boto3 Budgets service using mypy-boto3-builder, like so:\n```python\nfrom typing import Optional\nfrom botocore.client import BotocoreClient\nfrom boto3.budgets import Budgets\n\nbotocore_client = BotocoreClient()\nbudgets_client = Budgets(botocore_client)\n```"
RESPONSE:
```python
import boto3
from mypy_boto3_budgets import BudgetsClient

budgets_client: BudgetsClient = boto3.client("budgets")
```
------------------------------------------------------------

PROMPT 107:
"Generate Python code that imports the asyncio module and defines a decorator named `retry` that takes a function and a list of exceptions to catch, and returns a new function that retries the original function up to 3 times upon encountering any of the specified exceptions, using the `asyncio.create_task()` function to run the function asynchronously."
RESPONSE:
import asyncio
from functools import wraps

def retry(exceptions):
    """
    Decorator factory that retries a function up to 3 times upon encountering
    any of the specified exceptions, running the function via asyncio.create_task().
    'exceptions' should be a list (or iterable) of exception classes.
    """
    exc_tuple = tuple(exceptions)

    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exc = None
            for attempt in range(1, 4):
                try:
                    if asyncio.iscoroutinefunction(func):
                        task = asyncio.create_task(func(*args, **kwargs))
                    else:
                        async def run_sync():
                            loop = asyncio.get_running_loop()
                            return await loop.run_in_executor(None, lambda: func(*args, **kwargs))
                        task = asyncio.create_task(run_sync())
                    return await task
                except exc_tuple as e:
                    last_exc = e
                    if attempt == 3:
                        raise
            if last_exc is not None:
                raise last_exc
        return wrapper
    return decorator
------------------------------------------------------------

PROMPT 108:
"Generate Python code that uses the `gzip` module to compress response data in a Flask app, for example:\n```\nimport flask\nimport gzip\n\napp = flask.Flask(__name__)\n\n@app.route('/')\ndef index():\n   data = 'Hello, World!'\n   compressed_data = gzip.compress(data.encode('utf-8'))\n   return flask.Response(compressed_data, content_type='text/plain;charset=UTF-8')\n```"
RESPONSE:
import gzip
from flask import Flask, Response, request

app = Flask(__name__)

# Automatically gzip-compress eligible responses if the client supports it
@app.after_request
def gzip_response(response):
    # Preconditions: client accepts gzip, not already encoded, has a body, compressible type
    if 'gzip' not in request.headers.get('Accept-Encoding', '').lower():
        return response
    if response.status_code in (204, 304):
        return response
    if response.direct_passthrough or response.headers.get('Content-Encoding'):
        return response

    content_type = (response.headers.get('Content-Type') or '').lower()
    compressible = (
        content_type.startswith('text/')
        or 'json' in content_type
        or 'javascript' in content_type
        or 'xml' in content_type
        or 'svg' in content_type
    )

    if not compressible:
        return response

    data = response.get_data()
    if not data:
        return response

    # Compress (you can set compresslevel=1..9 for speed vs. ratio)
    compressed = gzip.compress(data)

    # If compression doesn't help, keep original
    if len(compressed) >= len(data):
        return response

    response.set_data(compressed)
    response.headers['Content-Encoding'] = 'gzip'
    # Ensure proxies/CDNs vary on Accept-Encoding
    vary = response.headers.get('Vary')
    response.headers['Vary'] = ('Accept-Encoding' if not vary
                                else (vary if 'Accept-Encoding' in vary
                                      else f'{vary}, Accept-Encoding'))
    response.headers['Content-Length'] = str(len(compressed))
    return response


@app.route('/')
def index():
    # This will be gzip-compressed automatically when the client sends Accept-Encoding: gzip
    return Response('Hello, World!', mimetype='text/plain; charset=utf-8')


if __name__ == '__main__':
    app.run(debug=True)
------------------------------------------------------------

PROMPT 109:
"Generate Python code that imports the required modules and annotates the types for the boto3.Batch client using mypy-boto3-builder:\n```python\nfrom typing import Callable, Dict, List, Optional\nfrom boto3.batch import BatchClient\nfrom mypy_boto3_builder.generators import MyPyBoto3Builder\n\nbuilder = MyPyBoto3Builder(BatchClient)\ngenerated_client = builder.generate()\n```"
RESPONSE:
```python
# pip install "boto3-stubs[batch]" boto3

from typing import Callable, Dict, List, Optional

import boto3
from mypy_boto3_batch.client import BatchClient
from mypy_boto3_batch.literals import JobStatusType
from mypy_boto3_batch.type_defs import (
    ListJobsResponseTypeDef,
    SubmitJobResponseTypeDef,
)

# Typed boto3 Batch client
batch_client: BatchClient = boto3.client("batch")


def list_jobs_by_status(
    client: BatchClient,
    queue: str,
    status: JobStatusType,
) -> ListJobsResponseTypeDef:
    return client.list_jobs(jobQueue=queue, jobStatus=status)


def submit_simple_job(
    client: BatchClient,
    name: str,
    queue: str,
    definition: str,
) -> SubmitJobResponseTypeDef:
    return client.submit_job(
        jobName=name,
        jobQueue=queue,
        jobDefinition=definition,
    )


# Example usage
if __name__ == "__main__":
    resp = list_jobs_by_status(batch_client, "my-queue", "RUNNING")
    print(f"Jobs listed: {len(resp.get('jobSummaryList', []))}")

    submit_resp = submit_simple_job(
        batch_client,
        name="example-job",
        queue="my-queue",
        definition="my-job-definition:1",
    )
    print(f"Submitted job ID: {submit_resp['jobId']}")
```
------------------------------------------------------------

PROMPT 110:
"Generate Python code that imports the required modules and annotates the boto3.Outposts 1.34.0 service with type hints using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import TypeAnnotator\n\nannotator = TypeAnnotator(module_name='boto3.outposts', version='1.34.0')\nannotator.annotate_ Client('outposts', base_client=BaseClient)\n```"
RESPONSE:
from typing import List, cast
import boto3
from botocore.client import BaseClient
from mypy_boto3_outposts import OutpostsClient  # generated by mypy-boto3-builder 7.21.0

def get_outposts_client() -> OutpostsClient:
    base_client: BaseClient = boto3.client("outposts")
    return cast(OutpostsClient, base_client)

def list_outposts_ids(client: OutpostsClient) -> List[str]:
    paginator = client.get_paginator("list_outposts")
    outpost_ids: List[str] = []
    for page in paginator.paginate():
        for outpost in page.get("Outposts", []):
            outpost_id = outpost.get("OutpostId") if isinstance(outpost, dict) else None
            if outpost_id:
                outpost_ids.append(outpost_id)
    return outpost_ids
------------------------------------------------------------

PROMPT 111:
"Generate Python code that imports the necessary modules and annotates the boto3 ACMPCA 1.34.0 service with type hints using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import TypeVar, Callable\nfrom botocore.client import BaseClient\nfrom boto3.acmpca import ACMPCA\n\nACMPCA_client: TypeVar('ACMPCA_client', bound='ACMPCA') = BaseClient\n\ndef get_acmpca(region: str) -> ACMPCA_client:\n   return ACMPCA(region)\n```"
RESPONSE:
import boto3
from typing import cast
from mypy_boto3_acm_pca.client import ACMPCAClient

def get_acmpca(region: str) -> ACMPCAClient:
    return cast(ACMPCAClient, boto3.client("acm-pca", region_name=region))
------------------------------------------------------------

PROMPT 112:
"Generate Python code that imports the required modules and annotates the necessary types for the boto3.SSOOIDC 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import Optional\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder.generators.boto3 import Boto3Generator\n\n generator = Boto3Generator(\n   client=BaseClient,\n   module_name='boto3',\n   version='1.34.0',\n   builder_version='7.21.0',\n)\n\ngenerator.generate()\n```"
RESPONSE:
```python
from __future__ import annotations

from pathlib import Path
from typing import Optional

# mypy-boto3-builder 7.21.0: generate type stubs for boto3 SSOOIDC 1.34.0
from mypy_boto3_builder.service_name import ServiceNameCatalog
from mypy_boto3_builder.parsers.service_package import parse_service_package
from mypy_boto3_builder.generators.service_package import ServicePackageGenerator


BOTO3_VERSION: str = "1.34.0"
BUILDER_VERSION: str = "7.21.0"


def generate_sso_oidc_types(output_dir: Optional[Path] = None) -> None:
    """
    Generate type stubs for the SSOOIDC service.

    Requires:
      - boto3==1.34.0 (or compatible)
      - mypy-boto3-builder==7.21.0
    """
    service = ServiceNameCatalog.sso_oidc
    package = parse_service_package(service_name=service, boto3_version=BOTO3_VERSION)

    generator = ServicePackageGenerator(
        service_package=package,
        version=BUILDER_VERSION,
    )
    generator.generate(output_path=output_dir or Path("."))


if __name__ == "__main__":
    # Generate into ./mypy_boto3_sso_oidc (package name that will be created)
    generate_sso_oidc_types(Path("./mypy_boto3_sso_oidc"))

    # Example usage: import generated types and annotate boto3 SSOOIDC client and calls
    import boto3
    from mypy_boto3_sso_oidc import SSOOIDCClient
    from mypy_boto3_sso_oidc.type_defs import (
        RegisterClientRequestRequestTypeDef,
        RegisterClientResponseTypeDef,
        CreateTokenRequestRequestTypeDef,
        CreateTokenResponseTypeDef,
        StartDeviceAuthorizationRequestRequestTypeDef,
        StartDeviceAuthorizationResponseTypeDef,
    )

    # Typed SSOOIDC client
    sso_oidc: SSOOIDCClient = boto3.client("sso-oidc")

    def register_client(
        req: RegisterClientRequestRequestTypeDef,
    ) -> RegisterClientResponseTypeDef:
        return sso_oidc.register_client(**req)

    def create_token(
        req: CreateTokenRequestRequestTypeDef,
    ) -> CreateTokenResponseTypeDef:
        return sso_oidc.create_token(**req)

    def start_device_authorization(
        req: StartDeviceAuthorizationRequestRequestTypeDef,
    ) -> StartDeviceAuthorizationResponseTypeDef:
        return sso_oidc.start_device_authorization(**req)
```
------------------------------------------------------------

PROMPT 113:
"Generate Python code that imports the required modules and annotates the types for the boto3.SSOAdmin service, version 1.34.0, using mypy-boto3-builder version 7.21.0, as follows:\n```python\nfrom typing import Optional\nfrom botocore.client import BaseClient\nfrom boto3.ssoadmin import SSOAdminClient\n\nss_service = SSOAdminClient(\n   client=BaseClient(\n       aws_access_key_id='YOUR_AWS_ACCESS_KEY',\n       aws_secret_access_key='YOUR_AWS_SECRET_KEY',\n       aws_session_token='YOUR_AWS_SESSION_TOKEN'\n   )\n)\n```"
RESPONSE:
```python
from typing import Optional
import boto3
from mypy_boto3_sso_admin.client import SSOAdminClient

aws_access_key_id: Optional[str] = "YOUR_AWS_ACCESS_KEY"
aws_secret_access_key: Optional[str] = "YOUR_AWS_SECRET_KEY"
aws_session_token: Optional[str] = "YOUR_AWS_SESSION_TOKEN"
region_name: Optional[str] = "us-east-1"

# Typed SSO Admin client (requires mypy-boto3-sso-admin stubs v7.21.0)
sso_admin: SSOAdminClient = boto3.client(
    "sso-admin",
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    aws_session_token=aws_session_token,
    region_name=region_name,
)
```
------------------------------------------------------------

PROMPT 114:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 IoT Events 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import MyPyBoto3Builder\n\nbuilder = MyPyBoto3Builder(service='iotevents', version='1.34.0')\nbuilder.set_type_annotations()\n\n# Example usage:\nclient = BaseClient(service=builder.service)\nresponse = client.describe_thing(ThingName='my-thing')\nprint(response.ThingArn)\n```"
RESPONSE:
Your example mixes concepts that won’t work at runtime: mypy-boto3-builder is a stub generator (not a runtime API), BaseClient isn’t constructed directly, and IoT Events doesn’t have describe_thing. Here’s a correct, typed example using the stubs generated by mypy-boto3-builder 7.21.0 (published as mypy-boto3-iotevents 1.34.0).

# Requires:
#   pip install "boto3==1.34.0" "mypy-boto3-iotevents==1.34.0"
# mypy-boto3-iotevents 1.34.0 is generated by mypy-boto3-builder 7.21.0.

from __future__ import annotations

import boto3
from mypy_boto3_iotevents.client import IoTEventsClient
from mypy_boto3_iotevents.type_defs import DescribeInputResponseTypeDef

# Create a typed IoT Events client
client: IoTEventsClient = boto3.client("iotevents", region_name="us-east-1")

# Call a valid IoT Events operation (example: describe_input)
response: DescribeInputResponseTypeDef = client.describe_input(inputName="my-input")

# Access typed response fields (TypedDict)
input_arn = response["input"]["inputArn"] if response.get("input") else None
print(input_arn)
------------------------------------------------------------

PROMPT 115:
"Generate Python code that imports the necessary libraries and annotates the boto3 Marketplace Metering client using type hints, like so:\n```\nfrom typing import Callable\nimport boto3\nfrom boto3.client import MarketplaceMetering\n\ndef get_metering_data(\n   marketplace_id: str,\n   metering_name: str,\n   *,\n   next_token: str = None,\n   max_results: int = None,\n) -> dict:\n   client = boto3.client(\"marketplace-metering\", version=1.34.0)\n   return client.get_metering_data(\n       marketplace_id=marketplace_id,\n       metering_name=metering_name,\n       next_token=next_token,\n       max_results=max_results,\n   )\n```"
RESPONSE:
from typing import Optional, Dict, Any, TYPE_CHECKING
import boto3

# Use precise types when boto3-stubs is available; otherwise fall back to a generic client type.
if TYPE_CHECKING:
    from mypy_boto3_marketplace_metering import MarketplaceMeteringClient
else:
    from botocore.client import BaseClient as MarketplaceMeteringClient


def get_metering_data(
    marketplace_id: str,
    metering_name: str,
    *,
    next_token: Optional[str] = None,
    max_results: Optional[int] = None,
) -> Dict[str, Any]:
    """
    Example function showing how to annotate the boto3 Marketplace Metering client.

    Note: AWS Marketplace Metering does not have a 'get_metering_data' API operation.
    Replace the call below with the appropriate operation for your use case, such as
    'resolve_customer', 'meter_usage', 'batch_meter_usage', or 'register_usage'.
    """
    client: MarketplaceMeteringClient = boto3.client("marketplace-metering")

    # Example: using a real operation to demonstrate typed client usage.
    # Here we map 'marketplace_id' to the 'RegistrationToken' expected by resolve_customer.
    # Adjust parameters and called operation as needed for your workflow.
    return client.resolve_customer(RegistrationToken=marketplace_id)
------------------------------------------------------------

PROMPT 116:
"Generate Python code that imports the rrdtool module and creates a new round-robin database file named example.rrd containing a single poll interval of 1000 x 60 = 60000 data points, each point consisting of a single value (temperature in degrees Celsius) and a timestamp corresponding to 5 minute intervals starting from January 1st, 2000. The code should then write temperature data to the RRD file every hour on the hour for the next year, using the formula 24 + (3 * (hour % 2)) to calculate the offset for the timestamp. Finally, the code should display the contents of the RRD file in human-readable format using the rrdtool command-line utility."
RESPONSE:
import os
import math
import subprocess
from datetime import datetime, timedelta, timezone

import rrdtool

def main():
    rrd_path = "example.rrd"

    # Start at 2000-01-01 00:00:00 UTC
    start_dt = datetime(2000, 1, 1, 0, 0, 0, tzinfo=timezone.utc)
    start_ts = int(start_dt.timestamp())

    # Remove existing file to ensure a fresh RRD
    if os.path.exists(rrd_path):
        os.remove(rrd_path)

    # Create RRD:
    # - step: 300 seconds (5 minutes)
    # - DS: one GAUGE named "temperature" with heartbeat 7200s (2 hours)
    # - RRA: AVERAGE consolidation with 1 PDP per row, total 60000 rows
    rrdtool.create(
        rrd_path,
        "--start", str(start_ts),
        "--step", "300",
        "DS:temperature:GAUGE:7200:-50:100",
        "RRA:AVERAGE:0.5:1:60000",
    )

    # Write temperature data every hour on the hour (with specified per-hour offset)
    # for the next year (year 2000 is a leap year: 366 days).
    end_dt = start_dt + timedelta(days=366)
    total_hours = int((end_dt - start_dt).total_seconds() // 3600)

    for h in range(total_hours):
        # Base hour timestamp and hour-of-day
        current_dt = start_dt + timedelta(hours=h)
        hour_of_day = current_dt.hour
        base_ts = start_ts + h * 3600

        # Offset for the timestamp: 24 + (3 * (hour % 2)) seconds
        offset_seconds = 24 + (3 * (hour_of_day % 2))  # 24s on even hours, 27s on odd hours
        ts = base_ts + offset_seconds

        # Example temperature value (simple daily cycle)
        temp = 20.0 + 5.0 * math.sin(2 * math.pi * (h % 24) / 24.0)

        rrdtool.update(rrd_path, f"{int(ts)}:{temp:.2f}")

    # Display the contents using the rrdtool command-line utility (human-readable XML dump)
    try:
        result = subprocess.run(
            ["rrdtool", "dump", rrd_path],
            check=True,
            capture_output=True,
            text=True,
        )
        print(result.stdout)
    except FileNotFoundError:
        print("Error: 'rrdtool' CLI not found on PATH. Please install RRDtool to dump the RRD.")
    except subprocess.CalledProcessError as e:
        print("rrdtool dump failed:", e.stderr)

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 117:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.MQ 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import TypeAnnotator\n\nannotator = TypeAnnotator(module_name='boto3', version='1.34.0')\nannotator.annotate(BaseClient)\n\n# Define the types of the boto3.MQ 1.34.0 service\nclass MQClient(BaseClient):\n   def __init__(self, *args, **kwargs):\n       super().__init__(*args, **kwargs)\n       self.config = kwargs['config']\n   \n   def send_message(self, topic: str, message: dict) -> None:\n       pass\n   \n   def receive_messages(self, topic: str, num_messages: int = 1) -> List[dict]:\n       pass\n```"
RESPONSE:
from __future__ import annotations

from typing import TYPE_CHECKING
from importlib.metadata import version, PackageNotFoundError

import boto3
from botocore.client import BaseClient

# Optional: ensure expected tool/runtime versions
try:
    assert version("boto3").startswith("1.34.")
except PackageNotFoundError:
    pass
try:
    assert version("mypy-boto3-builder") == "7.21.0"
except PackageNotFoundError:
    pass

# Use types generated by mypy-boto3-builder (installed as mypy-boto3-mq 1.34.0)
if TYPE_CHECKING:
    from mypy_boto3_mq import MQClient  # type: ignore
else:
    MQClient = BaseClient  # type: ignore[assignment]

def get_mq_client() -> MQClient:
    return boto3.client("mq")

# Example: these calls are type-checked when mypy-boto3-mq 1.34.0 is installed
if __name__ == "__main__":
    mq = get_mq_client()
    # Example typed calls (uncomment for use)
    # brokers = mq.list_brokers(MaxResults=10)
    # details = mq.describe_broker(BrokerId="your-broker-id")
    # mq.create_broker(Name="example", EngineType="ActiveMQ", DeploymentMode="SINGLE_INSTANCE", HostInstanceType="mq.t3.micro", Users=[{"Username": "user", "Password": "pass"}])
------------------------------------------------------------

PROMPT 118:
"Generate Python code that implements a Flake8 plugin which forbids printf-style string formatting:\n```python\nimport flake8plugin\n\nclass ForbidPrintfStyleFormatting(flake8plugin.Checker):\n   name = 'Forbid Printf Style Formatting'\n   version = '1.0'\n\n   def check_node(self, node, *args, **kwargs):\n       if isinstance(node, ast.Call) and node.func.id == 'printf':\n           raise flake8plugin.Flake8Exception('printf-style string formatting is forbidden')\n       return None\n```\nThis code defines a Flake8 plugin called `Forbid Printf Style Formatting` that checks for instances of the `printf` function being used in a string format. If such an instance is found, it raises a `Flake8Exception`. The plugin can then be installed and used like any other Flake8 plugin."
RESPONSE:
import ast
from typing import Generator, Iterable, Tuple, Type


PFS001 = "PFS001 printf-style string formatting is forbidden; use f-strings or str.format()"


class _PrintfStyleVisitor(ast.NodeVisitor):
    def __init__(self) -> None:
        self.violations: list[Tuple[int, int, str]] = []

    def visit_BinOp(self, node: ast.BinOp) -> None:
        # Detect "%"-based string formatting: "<str> % <expr>"
        if isinstance(node.op, ast.Mod) and self._left_is_string_literal(node.left):
            self.violations.append((node.lineno, node.col_offset, PFS001))
        self.generic_visit(node)

    @staticmethod
    def _left_is_string_literal(node: ast.AST) -> bool:
        # Python 3.8+: string literals are ast.Constant with str
        if isinstance(node, ast.Constant) and isinstance(node.value, str):
            return True
        # Older Python versions: ast.Str
        if hasattr(ast, "Str") and isinstance(node, getattr(ast, "Str")):
            return True
        # Bytes literals also support %-formatting; consider them as violations too
        if isinstance(node, ast.Constant) and isinstance(node.value, (bytes, bytearray)):
            return True
        return False


class ForbidPrintfStyleFormatting:
    """
    Flake8 plugin that forbids printf-style string formatting, e.g.:

        "%s: %d" % (name, count)

    Emits PFS001.
    """

    name = "flake8-forbid-printf-formatting"
    version = "1.0.0"

    def __init__(self, tree: ast.AST) -> None:
        self.tree = tree

    def run(self) -> Generator[Tuple[int, int, str, Type["ForbidPrintfStyleFormatting"]], None, None]:
        visitor = _PrintfStyleVisitor()
        visitor.visit(self.tree)
        for lineno, col, msg in visitor.violations:
            yield lineno, col, msg, type(self)
------------------------------------------------------------

PROMPT 119:
"Generate Python code that imports the required modules and annotates the types for the boto3.Translate 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import Optional\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import TypeAnnotator\n\ntranslator = TypeAnnotator(BaseClient)\n\n# Define the types for the Translate service\nclass Translate(BaseClient):\n   def translate_text(self, Text: str, SourceLanguageCode: str, DestinationLanguageCode: str) -> dict:\n       ...\n\n   def detect_language(self, Text: str) -> dict:\n       ...\n\n# Other methods and properties of the Translate service\n\n# End of the Translate service definition\n\n# Generated type definitions for the Translate service\ntranslator.generate_type_definitions()\n```"
RESPONSE:
```python
from __future__ import annotations

from typing import Optional, Sequence

import boto3
from botocore.client import BaseClient

# Types generated by mypy-boto3-builder==7.21.0 for boto3-stubs==1.34.0
from mypy_boto3_translate import TranslateClient
from mypy_boto3_translate.type_defs import (
    DocumentTypeDef,
    ListLanguagesResponseTypeDef,
    TranslateDocumentResponseTypeDef,
    TranslateTextResponseTypeDef,
    TranslationSettingsTypeDef,
)

# Create a properly-typed Amazon Translate client
translator: TranslateClient = boto3.client("translate")


# Define the types for the Translate service
class Translate(BaseClient):
    def translate_text(
        self,
        *,
        Text: str,
        SourceLanguageCode: str,
        TargetLanguageCode: str,
        TerminologyNames: Optional[Sequence[str]] = None,
        Settings: Optional[TranslationSettingsTypeDef] = None,
    ) -> TranslateTextResponseTypeDef:
        ...

    def translate_document(
        self,
        *,
        Document: DocumentTypeDef,
        SourceLanguageCode: str,
        TargetLanguageCode: str,
        TerminologyNames: Optional[Sequence[str]] = None,
        Settings: Optional[TranslationSettingsTypeDef] = None,
    ) -> TranslateDocumentResponseTypeDef:
        ...

    def list_languages(
        self,
        *,
        DisplayLanguageCode: Optional[str] = None,
        NextToken: Optional[str] = None,
        MaxResults: Optional[int] = None,
    ) -> ListLanguagesResponseTypeDef:
        ...


# Other methods and properties of the Translate service could be added here.
# The concrete client already has these method signatures via `TranslateClient`.

# End of the Translate service definition

# No explicit "generation" step is needed at runtime. The type information
# comes from the mypy-boto3-translate package built by mypy-boto3-builder.
```
------------------------------------------------------------

PROMPT 120:
"Generate Python code that imports the required modules and annotates the types for the boto3 WorkLink 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import List, Dict\nfrom boto3 import ServiceResource\n\nService = ServiceResource('worklink', '1.34.0')\n\n@Service.client\nclass WorkLinkClient:\n   def __init__(self, config: dict) -> None:\n       self.config = config\n\n   def create_app(self, name: str, description: str) -> dict:\n       return self.get_waiter().create_app(name, description)\n\n   def list_apps(self) -> List[dict]:\n       return self.get_waiter().list_apps()\n\n   def delete_app(self, app_id: str) -> None:\n       return self.get_waiter().delete_app(app_id)\n\n   def get_app(self, app_id: str) -> dict:\n       return self.get_waiter().get"
RESPONSE:
# Requires:
#   boto3 >= 1.34.0
#   mypy-boto3-worklink (types generated by mypy-boto3-builder 7.21.0)
# Install:
#   pip install boto3 mypy-boto3-worklink

from typing import Any, Dict, Optional
import boto3
from mypy_boto3_worklink import WorkLinkClient


def get_worklink_client(region: Optional[str] = None) -> WorkLinkClient:
    """
    Create a typed WorkLink client.
    """
    if region:
        return boto3.client("worklink", region_name=region)  # type: ignore[return-value]
    return boto3.client("worklink")  # type: ignore[return-value]


class WorkLinkService:
    """
    A thin, type-annotated wrapper around the boto3 WorkLink client.
    """

    def __init__(self, client: Optional[WorkLinkClient] = None, region: Optional[str] = None) -> None:
        self.client: WorkLinkClient = client or get_worklink_client(region)

    def create_fleet(self, fleet_name: str, display_name: Optional[str] = None) -> str:
        """
        Create a WorkLink fleet and return its ARN.
        """
        if display_name is None:
            resp = self.client.create_fleet(FleetName=fleet_name)
        else:
            resp = self.client.create_fleet(FleetName=fleet_name, DisplayName=display_name)
        return resp["FleetArn"]

    def list_fleets(self) -> Dict[str, Any]:
        """
        List fleets. Returns the raw response dict typed by mypy-boto3.
        """
        return self.client.list_fleets()

    def delete_fleet(self, fleet_arn: str) -> None:
        """
        Delete a WorkLink fleet by ARN.
        """
        self.client.delete_fleet(FleetArn=fleet_arn)

    def describe_fleet_metadata(self, fleet_arn: str) -> Dict[str, Any]:
        """
        Describe metadata for a WorkLink fleet.
        """
        return self.client.describe_fleet_metadata(FleetArn=fleet_arn)


if __name__ == "__main__":
    # Example usage with full type checking support from mypy-boto3-worklink.
    wl = WorkLinkService(region="us-east-1")
    arn = wl.create_fleet("example-fleet")
    print("Created fleet:", arn)
    print("Fleets:", wl.list_fleets())
    print("Fleet metadata:", wl.describe_fleet_metadata(arn))
    wl.delete_fleet(arn)
    print("Deleted fleet:", arn)
------------------------------------------------------------

PROMPT 121:
"Generate Python code that imports the required modules and annotates the types for the boto3.MWAA 1.34.18 service using mypy-boto3-builder 7.23.1, like this:\n```python\nfrom typing import List, Dict\nfrom botocore.client import BaseClient\nfrom boto3.MWAA import MWAA\n\n# Annotate the types for the boto3.MWAA 1.34.18 service\nclass MyMWAA(MWAA):\n   def __init__(self, config: Dict) -> None:\n       super().__init__(config=config)\n\n   @property\n   def client(self) -> BaseClient:\n       return self._client\n\n   def list_resource_groups(self, PaginationConfig: List[Dict]) -> List[Dict]:\n       return self._list_resource_groups(PaginationConfig)\n\n   def get_resource_group(self, ResourceGroupArn: str) -> Dict:\n       return self._get_resource_group(ResourceGroupArn)\n\n   def create_resource"
RESPONSE:
from typing import Iterator, Optional
import boto3
from boto3.session import Session
from botocore.client import BaseClient

# Requires: pip install "boto3-stubs[mwaa]==1.34.18"
from mypy_boto3_mwaa.client import MWAAClient
from mypy_boto3_mwaa.paginator import ListEnvironmentsPaginator
from mypy_boto3_mwaa.type_defs import (
    PaginatorConfigTypeDef,
    ListEnvironmentsOutputTypeDef,
    GetEnvironmentOutputTypeDef,
    CreateCliTokenResponseTypeDef,
    CreateWebLoginTokenResponseTypeDef,
    ListTagsForResourceResponseTypeDef,
)


class MyMWAA:
    def __init__(self, *, session: Optional[Session] = None, region_name: Optional[str] = None) -> None:
        if session is None:
            self._client: MWAAClient = boto3.client("mwaa", region_name=region_name)
        else:
            self._client = session.client("mwaa", region_name=region_name)

    @property
    def client(self) -> MWAAClient:
        return self._client

    @property
    def base_client(self) -> BaseClient:
        # Sometimes you may want to expose the underlying botocore BaseClient
        return self._client

    def list_environments(self) -> ListEnvironmentsOutputTypeDef:
        return self._client.list_environments()

    def list_environments_pages(
        self, pagination_config: Optional[PaginatorConfigTypeDef] = None
    ) -> Iterator[ListEnvironmentsOutputTypeDef]:
        paginator: ListEnvironmentsPaginator = self._client.get_paginator("list_environments")
        if pagination_config is not None:
            return paginator.paginate(PaginationConfig=pagination_config)
        return paginator.paginate()

    def get_environment(self, name: str) -> GetEnvironmentOutputTypeDef:
        return self._client.get_environment(Name=name)

    def create_cli_token(self, name: str) -> CreateCliTokenResponseTypeDef:
        return self._client.create_cli_token(Name=name)

    def create_web_login_token(self, name: str) -> CreateWebLoginTokenResponseTypeDef:
        return self._client.create_web_login_token(Name=name)

    def list_tags_for_resource(self, resource_arn: str) -> ListTagsForResourceResponseTypeDef:
        return self._client.list_tags_for_resource(ResourceArn=resource_arn)
------------------------------------------------------------

PROMPT 122:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.WorkSpaces 1.34.17 service using mypy-boto3-builder 7.23.1, like this:\n```python\nfrom typing import List, Dict\nfrom boto3.workspaces import WorkSpace, DescribeWorkSpaceResponse, CreateWorkSpaceRequest\n\n# mypy-boto3-builder generates type hints for boto3 methods\ndescribe_workspace_response = DescribeWorkSpaceResponse(\n   work_space=WorkSpace(\n       id=\"string\",\n       name=\"string\",\n       description=\"string\",\n       bundle_id=\"string\",\n       owner=\"string\",\n       created_time=datetime(2023, 3, 21, 15, 29, 59),\n       updated_time=datetime(2023, 3, 21, 15, 29, 59),\n       compute_type=\"string\",\n       user_data=\"string\",\n       volume_encryption_key=\"string\",\n       root_"
RESPONSE:
```python
# Example for boto3==1.34.17 with type hints generated by mypy-boto3-builder==7.23.1

from __future__ import annotations

from typing import List, Optional
from datetime import datetime

import boto3
from botocore.config import Config

# Typed client and shapes from mypy-boto3-workspaces (generated by mypy-boto3-builder)
from mypy_boto3_workspaces import WorkSpacesClient
from mypy_boto3_workspaces.type_defs import (
    WorkspaceRequestTypeDef,
    WorkspaceTypeDef,
    WorkspacePropertiesTypeDef,
    TagTypeDef,
    CreateWorkspacesRequestRequestTypeDef,
    CreateWorkspacesResultTypeDef,
    DescribeWorkspacesResultTypeDef,
)

# Create a typed WorkSpaces client
config = Config(retries={"max_attempts": 10, "mode": "standard"})
client: WorkSpacesClient = boto3.client("workspaces", config=config)

# Build a typed create_workspaces request
props: WorkspacePropertiesTypeDef = {
    "RunningMode": "AUTO_STOP",  # Literal["ALWAYS_ON", "AUTO_STOP"]
    "RunningModeAutoStopTimeoutInMinutes": 60,
    "RootVolumeSizeGib": 80,
    "UserVolumeSizeGib": 50,
}

tags: List[TagTypeDef] = [
    {"Key": "Env", "Value": "Dev"},
    {"Key": "Owner", "Value": "alice"},
]

workspace_req: WorkspaceRequestTypeDef = {
    "DirectoryId": "d-xxxxxxxxxx",
    "UserName": "alice",
    "BundleId": "wsb-xxxxxxxxx",
    "WorkspaceProperties": props,
    "Tags": tags,
}

create_req: CreateWorkspacesRequestRequestTypeDef = {
    "Workspaces": [workspace_req]
}

# Call create_workspaces with precise types
create_resp: CreateWorkspacesResultTypeDef = client.create_workspaces(**create_req)

# Extract the created WorkspaceId (if any)
workspace_id: Optional[str] = None
if create_resp.get("PendingRequests"):
    first_pending: WorkspaceTypeDef = create_resp["PendingRequests"][0]
    workspace_id = first_pending.get("WorkspaceId")

# Describe the workspace we just created (if an ID is available)
if workspace_id:
    describe_resp: DescribeWorkspacesResultTypeDef = client.describe_workspaces(
        WorkspaceIds=[workspace_id]
    )
    workspaces: List[WorkspaceTypeDef] = describe_resp.get("Workspaces", [])
    for ws in workspaces:
        # Access strongly-typed fields on WorkspaceTypeDef
        ws_id: Optional[str] = ws.get("WorkspaceId")
        dir_id: Optional[str] = ws.get("DirectoryId")
        bundle_id: Optional[str] = ws.get("BundleId")
        state: Optional[str] = ws.get("State")  # Literal type in stubs
        last_conn: Optional[datetime] = ws.get("LastKnownUserConnectionTimestamp")

        print(
            f"WorkspaceId={ws_id} DirectoryId={dir_id} BundleId={bundle_id} "
            f"State={state} LastConnection={last_conn}"
        )
else:
    # Handle failures if no pending requests were returned
    failed = create_resp.get("FailedRequests", [])
    print(f"CreateWorkspaces failed: {failed}")
```
------------------------------------------------------------

PROMPT 123:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.SageMakerRuntime 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import Tuple\nfrom botocore.client import BaseClient\nfrom sagemaker_runtime import SageMakerRuntime\n\n# Annotate the types of the boto3.SageMakerRuntime 1.34.0 service\nclass SageMakerRuntimeClient(BaseClient):\n   def __init__(self, *, config: Optional[dict] = None) -> None:\n       self.config = config\n       self.client: SageMakerRuntime = None\n   \n   def create_endpoint(self, *, endpoint_name: str, instance_count: int, \n           instance_type: str, vpc_config: dict, security_group_ids: List[str], \n           subnets: List[str]) -> Tuple[str, int]:\n       pass\n   \n   def delete_endpoint(self, *, endpoint_name"
RESPONSE:
from typing import Optional, Tuple
import boto3
from botocore.response import StreamingBody
from mypy_boto3_sagemaker_runtime import SageMakerRuntimeClient

# Typed factory for boto3.sagemaker-runtime client (boto3==1.34.0, mypy-boto3-builder==7.21.0)
def get_sagemaker_runtime_client() -> SageMakerRuntimeClient:
    return boto3.client("sagemaker-runtime")

class SageMakerRuntimeClientWrapper:
    def __init__(self, *, config: Optional[dict] = None) -> None:
        # You can pass region_name/credentials in config if needed
        self.config = config or {}
        self.client: SageMakerRuntimeClient = boto3.client("sagemaker-runtime", **self.config)

    def invoke_endpoint(
        self,
        *,
        endpoint_name: str,
        body: bytes,
        content_type: Optional[str] = None,
        accept: Optional[str] = None,
    ) -> Tuple[bytes, Optional[str]]:
        resp = self.client.invoke_endpoint(
            EndpointName=endpoint_name,
            Body=body,
            ContentType=content_type,
            Accept=accept,
        )
        stream: StreamingBody = resp["Body"]
        payload: bytes = stream.read()
        resp_content_type: Optional[str] = resp.get("ContentType")
        return payload, resp_content_type

# Example usage with proper typing
if __name__ == "__main__":
    runtime_client: SageMakerRuntimeClient = get_sagemaker_runtime_client()
    wrapper = SageMakerRuntimeClientWrapper()

    # Typed call
    result_bytes, result_ct = wrapper.invoke_endpoint(
        endpoint_name="my-endpoint",
        body=b'{"inputs": "hello"}',
        content_type="application/json",
        accept="application/json",
    )
    print(result_ct, len(result_bytes))
------------------------------------------------------------

PROMPT 124:
"Generate Python code that imports the required libraries, creates a PostgreSQL database connection using SQLAlchemy, and uses the CITEXT data type for a column in a table, like this:\n```python\nfrom sqlalchemy import create_engine, Column, Integer, Text\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship\nimport citest\n\nBase = declarative_base()\n\nclass MyTable(Base):\n   __tablename__ = 'mytable'\n   id = Column(Integer, primary_key=True)\n   name = Column(Text)\n   citext_column = Column(citest.CITEXT)\n\nengine = create_engine('postgresql://user:password@host:port/dbname')\nBase.metadata.create_all(engine)\n\n# Use the CITEXT column in a query\nsession = Session(engine)\nresult = session.query(MyTable).filter_by(citext_column=' foo').all()\nfor row in result:\n   print(row.name)\n```"
RESPONSE:
```python
from sqlalchemy import create_engine, Column, Integer, Text, text
from sqlalchemy.orm import declarative_base, Session
from sqlalchemy.dialects.postgresql import CITEXT

Base = declarative_base()

class MyTable(Base):
    __tablename__ = "mytable"
    id = Column(Integer, primary_key=True)
    name = Column(Text, nullable=False)
    citext_column = Column(CITEXT, nullable=False)

# Update with your credentials/host/port/db
DATABASE_URL = "postgresql+psycopg2://user:password@host:5432/dbname"

engine = create_engine(DATABASE_URL, future=True)

# Ensure the citext extension is available in the database
with engine.connect() as conn:
    conn.execute(text("CREATE EXTENSION IF NOT EXISTS citext"))
    conn.commit()

# Create tables
Base.metadata.create_all(engine)

# Use the CITEXT column in a query
with Session(engine) as session:
    result = session.query(MyTable).filter_by(citext_column=' foo').all()
    for row in result:
        print(row.name)
```
------------------------------------------------------------

PROMPT 125:
"Generate Python code that adds a compare view to django-reversion for comparing two versions of a reversion model:\n```python\nfrom django_reversion import views, models\n\nclass CompareView(views.generic.DetailView):\n   template_name = 'reversion/compare.html'\n   context_object_name = 'version1'\n   \n   def get_queryset(self):\n       return models.Revision.objects.all()\n   \n   def get_context_data(self, **kwargs):\n       context = super().get_context_data(**kwargs)\n       context['version2'] = self.request.GET.get('version2')\n       return context\n   \n   def post(self, request, *args, **kwargs):\n       version1_id = request.POST.get('version1')\n       version2_id = request.POST.get('version2')\n       version1 = models.Revision.objects.get(id=version1_id)\n       version2 = models.Revision.objects.get(id=version2_id)\n       diff = version1.diff("
RESPONSE:
Below is a minimal, working compare view for django-reversion that compares two Version records of the same object. It computes field-by-field differences and includes a simple text diff for string/JSON-like fields.

views.py
```python
from difflib import ndiff
import json

from django.http import Http404, HttpResponseBadRequest
from django.views.generic import TemplateView
from reversion.models import Version


def _to_text(value):
    if value is None:
        return ""
    if isinstance(value, (dict, list)):
        try:
            return json.dumps(value, sort_keys=True, indent=2, ensure_ascii=False)
        except Exception:
            return str(value)
    return str(value)


class VersionCompareView(TemplateView):
    """
    Compare two django-reversion Version rows that belong to the same object.

    Usage options:
    - Pass v1 and v2 as path kwargs (recommended).
    - Or pass v1 and v2 as GET params (?v1=...&v2=...).

    Template context:
    - version1, version2: Version instances
    - model: the model class being versioned
    - object_id: the primary key of the object (as stored by reversion)
    - changes: list of dicts with per-field comparison:
        {
            "field": field_name,
            "label": verbose_label,
            "old": original_value,
            "new": new_value,
            "changed": bool,
            "diff": list_of_diff_lines_or_None,
        }
    """
    template_name = "reversion/compare.html"

    def _get_versions(self):
        v1_id = self.kwargs.get("v1") or self.request.GET.get("v1") or self.request.GET.get("version1")
        v2_id = self.kwargs.get("v2") or self.request.GET.get("v2") or self.request.GET.get("version2")

        if not v1_id or not v2_id:
            raise Http404("Two version ids are required (v1 and v2).")

        try:
            v1 = Version.objects.select_related("revision", "content_type").get(pk=v1_id)
            v2 = Version.objects.select_related("revision", "content_type").get(pk=v2_id)
        except Version.DoesNotExist:
            raise Http404("One or both versions not found.")

        # Ensure both versions belong to the same object and model
        if v1.content_type_id != v2.content_type_id or v1.object_id != v2.object_id:
            return None, None

        return v1, v2

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        v1, v2 = self._get_versions()
        if not v1 or not v2:
            raise Http404("Versions must belong to the same object and content type.")

        model = v1.content_type.model_class()
        fd1 = v1.field_dict
        fd2 = v2.field_dict

        # Establish a stable field order: model concrete fields first, then any extras
        ordered_fields = [
            f.name
            for f in model._meta.get_fields()
            if getattr(f, "concrete", False) and not getattr(f, "many_to_many", False)
        ]
        for k in (set(fd1.keys()) | set(fd2.keys())):
            if k not in ordered_fields:
                ordered_fields.append(k)

        changes = []
        for name in ordered_fields:
            old = fd1.get(name)
            new = fd2.get(name)
            changed = old != new

            # Build a readable diff for text-like values
            diff_lines = None
            if changed:
                old_text = _to_text(old)
                new_text = _to_text(new)
                # Only generate line-by-line diff if there is something non-trivial to show
                if ("\n" in old_text or "\n" in new_text) or (isinstance(old, str) and isinstance(new, str)):
                    diff_lines = list(ndiff(old_text.splitlines(), new_text.splitlines()))

            # Use verbose_name when available
            try:
                field = model._meta.get_field(name)
                label = getattr(field, "verbose_name", name)
            except Exception:
                label = name

            changes.append({
                "field": name,
                "label": str(label),
                "old": old,
                "new": new,
                "changed": changed,
                "diff": diff_lines,
            })

        context.update({
            "version1": v1,
            "version2": v2,
            "model": model,
            "object_id": v1.object_id,
            "changes": changes,
        })
        return context
```

urls.py
```python
from django.urls import path
from .views import VersionCompareView

urlpatterns = [
    path("reversion/compare/<int:v1>/<int:v2>/", VersionCompareView.as_view(), name="reversion-compare"),
    # Or allow GET params: /reversion/compare/?v1=1&v2=2
    path("reversion/compare/", VersionCompareView.as_view(), name="reversion-compare-query"),
]
```

Notes:
- Use reversion.models.Version (not Revision) to compare specific saved versions of a model instance.
- Version.field_dict provides a mapping of field_name -> value for that version.
- This code avoids mutating the database; it only reads Version rows and shows differences.
- The template "reversion/compare.html" can iterate over context["changes"] to display field-by-field changes and optionally render diff lines.
------------------------------------------------------------

PROMPT 126:
"Generate Python code that imports the required modules and annotates the necessary types for the boto3.ResourceGroups 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom boto3.resourcegroups import ResourceGroup, Tag\nfrom boto3.session import Session\n\nsession = Session()\nrg_list = session.client(\"resourcegroupstaggingapi\").get_resource_groups(Filters=None)[\"ResourceGroupInfoList\"]\ntag_list = session.client(\"resourcetaggingapi\").get_tags_for_resource(rg_list[0][\"ResourceARN\"])[\"TagList\"]\nprint(f\"Resource Groups: {rg_list}\")\nprint(f\"Tags: {tag_list}\")\n```\nThis code uses the `mypy-boto3-builder` package to generate type hints for the `boto3.ResourceGroups` service, which is version 1.34.0 in this example. It then imports the required modules, creates a `Session` object, and calls the `get_resource_groups"
RESPONSE:
```python
from typing import Dict, List

from boto3.session import Session
from mypy_boto3_resource_groups import ResourceGroupsClient
from mypy_boto3_resource_groups.type_defs import GroupIdentifierTypeDef

session = Session()

# Typed Resource Groups client (requires mypy-boto3-resource-groups stubs)
rg_client: ResourceGroupsClient = session.client("resource-groups")

# List resource groups
groups_resp = rg_client.list_groups()
rg_list: List[GroupIdentifierTypeDef] = groups_resp.get("GroupIdentifiers", [])
print(f"Resource Groups: {rg_list}")

# Get tags for the first resource group (if any)
tags: Dict[str, str] = {}
if rg_list:
    group_arn = rg_list[0]["GroupArn"]
    tags = rg_client.get_tags(Arn=group_arn)["Tags"]

print(f"Tags: {tags}")
```
------------------------------------------------------------

PROMPT 127:
"Generate Python code that implements a Redis-based Django app that locks out users after too many failed login attempts:\n```python\nfrom django.contrib.auth.models import User\nfrom django.core.exceptions import SuspiciousOperation\nimport redis\n\ndef lock_out_user(username, lock_time):\n   user = User.objects.get(username=username)\n   if user.last_login_attempt > timedelta(minutes=lock_time):\n       raise SuspiciousOperation(\"Account locked due to excessive login attempts\")\n   else:\n       user.last_login_attempt = timezone.now()\n       user.save()\n       return False\n\ndef check_lockout(request):\n   username = request.POST.get('username')\n   lock_time = request.POST.get('lock_time')\n   if lock_out_user(username, lock_time):\n       return HttpResponse(\"Account locked, try again later.\")\n   return None\n\ndef view(request):\n   if request.method == 'POST':\n       check_lockout(request)\n   return render(request"
RESPONSE:
# settings.py (add these)
LOCKOUT_MAX_ATTEMPTS = 5                  # Number of failed attempts before lock
LOCKOUT_WINDOW_SECONDS = 300              # Time window to count failures (5 minutes)
LOCKOUT_TTL_SECONDS = 900                 # Lockout duration (15 minutes)
LOCKOUT_REDIS_PREFIX = "auth:lockout"     # Redis key prefix
LOCKOUT_USERNAME_FIELD = "username"       # Field name used for username in forms
LOCKOUT_NORMALIZE_USERNAME = True         # Lowercase usernames for counting/locking
REDIS_URL = "redis://localhost:6379/0"    # Your Redis connection

# Add app and middleware
INSTALLED_APPS += ["redis_lockout"]
MIDDLEWARE = ["redis_lockout.middleware.LoginLockoutMiddleware", *MIDDLEWARE]



# redis_lockout/__init__.py
default_app_config = "redis_lockout.apps.RedisLockoutConfig"



# redis_lockout/apps.py
from django.apps import AppConfig

class RedisLockoutConfig(AppConfig):
    name = "redis_lockout"
    verbose_name = "Redis-based Login Lockout"

    def ready(self):
        # Ensure signal handlers are registered
        from . import signals  # noqa: F401



# redis_lockout/conf.py
from django.conf import settings

def s(name, default):
    return getattr(settings, name, default)

LOCKOUT_MAX_ATTEMPTS = s("LOCKOUT_MAX_ATTEMPTS", 5)
LOCKOUT_WINDOW_SECONDS = s("LOCKOUT_WINDOW_SECONDS", 300)
LOCKOUT_TTL_SECONDS = s("LOCKOUT_TTL_SECONDS", 900)
LOCKOUT_REDIS_PREFIX = s("LOCKOUT_REDIS_PREFIX", "auth:lockout")
LOCKOUT_USERNAME_FIELD = s("LOCKOUT_USERNAME_FIELD", "username")
LOCKOUT_NORMALIZE_USERNAME = s("LOCKOUT_NORMALIZE_USERNAME", True)



# redis_lockout/redis_client.py
import redis
from functools import lru_cache
from django.conf import settings

@lru_cache(maxsize=1)
def get_redis():
    url = getattr(settings, "REDIS_URL", "redis://localhost:6379/0")
    # decode_responses=True to store strings and ints directly
    return redis.Redis.from_url(url, decode_responses=True)



# redis_lockout/lockout.py
from typing import Tuple
from .redis_client import get_redis
from .conf import (
    LOCKOUT_MAX_ATTEMPTS,
    LOCKOUT_REDIS_PREFIX,
    LOCKOUT_TTL_SECONDS,
    LOCKOUT_WINDOW_SECONDS,
    LOCKOUT_NORMALIZE_USERNAME,
)

def _normalize(username: str) -> str:
    if not isinstance(username, str):
        return ""
    return username.strip().lower() if LOCKOUT_NORMALIZE_USERNAME else username.strip()

def _fail_key(username: str) -> str:
    return f"{LOCKOUT_REDIS_PREFIX}:fail:{username}"

def _lock_key(username: str) -> str:
    return f"{LOCKOUT_REDIS_PREFIX}:lock:{username}"

def is_locked(username: str) -> Tuple[bool, int]:
    """
    Returns (locked, seconds_remaining).
    """
    r = get_redis()
    user = _normalize(username)
    if not user:
        return False, 0
    ttl = r.ttl(_lock_key(user))
    if ttl is None or ttl < 0:
        return False, 0
    return True, int(ttl)

def get_fail_count(username: str) -> int:
    r = get_redis()
    user = _normalize(username)
    if not user:
        return 0
    val = r.get(_fail_key(user))
    try:
        return int(val) if val is not None else 0
    except ValueError:
        return 0

def reset_failures(username: str) -> None:
    r = get_redis()
    user = _normalize(username)
    if not user:
        return
    r.delete(_fail_key(user))

def clear_lock(username: str) -> None:
    r = get_redis()
    user = _normalize(username)
    if not user:
        return
    r.delete(_lock_key(user))

def register_failure(username: str) -> Tuple[bool, int, int]:
    """
    Increments failure count for a username.
    Returns (locked_now, current_fail_count, lock_ttl_seconds).
    If threshold is reached, sets a lock with TTL.
    """
    r = get_redis()
    user = _normalize(username)
    if not user:
        return False, 0, 0

    # If already locked, report remaining time
    ttl = r.ttl(_lock_key(user))
    if ttl is not None and ttl > 0:
        return True, get_fail_count(user), int(ttl)

    pipe = r.pipeline()
    fail_key = _fail_key(user)
    lock_key = _lock_key(user)

    pipe.incr(fail_key)
    pipe.expire(fail_key, LOCKOUT_WINDOW_SECONDS)
    current, _ = pipe.execute()

    if int(current) >= int(LOCKOUT_MAX_ATTEMPTS):
        # Lock the account
        r.setex(lock_key, LOCKOUT_TTL_SECONDS, "1")
        # Optional: clear failures so the window restarts after lock
        r.delete(fail_key)
        return True, int(current), int(LOCKOUT_TTL_SECONDS)

    return False, int(current), 0

def register_success(username: str) -> None:
    """
    On successful login, clear failure count; leave any lock as-is (it shouldn't exist).
    """
    reset_failures(username)



# redis_lockout/signals.py
from django.contrib.auth import get_user_model
from django.contrib.auth.signals import user_login_failed, user_logged_in
from django.dispatch import receiver
from .lockout import register_failure, register_success

@receiver(user_login_failed)
def on_login_failed(sender, credentials, request, **kwargs):
    # Try the project's configured username field on the User model
    username_field = get_user_model().USERNAME_FIELD
    username = None

    # Credentials may be None or missing expected keys
    if isinstance(credentials, dict):
        # Prefer the project's username field; fallback to 'username'
        username = credentials.get(username_field) or credentials.get("username")

    if not username and request is not None:
        username = request.POST.get(username_field) or request.POST.get("username")

    if username:
        register_failure(username)

@receiver(user_logged_in)
def on_login_success(sender, request, user, **kwargs):
    username_field = get_user_model().USERNAME_FIELD
    username = getattr(user, username_field, None)
    if username:
        register_success(username)



# redis_lockout/middleware.py
from django.http import HttpResponse
from django.utils.deprecation import MiddlewareMixin
from django.contrib.auth import get_user_model
from .lockout import is_locked
from .conf import LOCKOUT_USERNAME_FIELD

class LoginLockoutMiddleware(MiddlewareMixin):
    """
    Checks lock status on any POST containing a username field before the view runs.
    This prevents hitting the auth backend unnecessarily when an account is locked.
    """

    def process_request(self, request):
        if request.method != "POST":
            return None

        username_field = getattr(get_user_model(), "USERNAME_FIELD", LOCKOUT_USERNAME_FIELD)
        username = request.POST.get(username_field) or request.POST.get(LOCKOUT_USERNAME_FIELD)

        if not username:
            return None

        locked, ttl = is_locked(username)
        if not locked:
            return None

        minutes, seconds = divmod(ttl, 60)
        msg = (
            f"Account is locked due to too many failed login attempts. "
            f"Try again in {minutes}m {seconds}s."
        )
        # 429 Too Many Requests is appropriate for throttling/lockouts
        return HttpResponse(msg, status=429)



# Example usage in any Django view or auth flow:
# - Add redis_lockout to INSTALLED_APPS.
# - Add LoginLockoutMiddleware to MIDDLEWARE (near the top).
# - Ensure Redis is running and REDIS_URL points to it.
# - Use the standard Django auth login views/forms. The app:
#   - Blocks locked accounts early via middleware.
#   - Counts failures via user_login_failed signal.
#   - Resets failure count on user_logged_in signal.

# Optional: helper admin or management command can be added to inspect/clear locks if needed.
------------------------------------------------------------

PROMPT 128:
"Generate Python code that imports the required modules and annotates the types for the Connect service of boto3 version 1.34.18 using mypy-boto3-builder version 7.23.1, like so:\n```python\nfrom typing import Callable, Dict, List, Optional, Tuple\nfrom botocore.client import BaseClient\nfrom boto3.s3.transfer import TransferConfig\n\nclass ConnectService:\n   def __init__(self, client: BaseClient) -> None:\n       self.client = client\n   \n   def connect(self, *, endpoint: str, timeout: float, **kwargs: Any) -> None:\n       \"\"\"\n       Connects to an AWS service.\n       \n       :param endpoint: The URL of the service to connect to.\n       :type endpoint: str\n       :param timeout: The maximum amount of time to wait before giving up.\n       :type timeout: float\n       :keyword arguments: Additional parameters to pass to the connection attempt.\n       :rtype: None\n       \"\"\"\n       return None\n\n   def disconnect(self) -> None:\n     "
RESPONSE:
from typing import Any, Callable, Dict, List, Optional, Tuple
import boto3
from botocore.client import BaseClient
from botocore.config import Config
from mypy_boto3_connect.client import ConnectClient
from mypy_boto3_connect.type_defs import (
    DescribeInstanceResponseTypeDef,
    ListUsersResponseTypeDef,
)


class ConnectService:
    def __init__(self, client: ConnectClient) -> None:
        self.client = client

    @classmethod
    def create(
        cls,
        *,
        region_name: Optional[str] = None,
        config: Optional[Config] = None,
    ) -> "ConnectService":
        """
        Factory to create a typed ConnectService with a boto3 Connect client.
        """
        session = boto3.session.Session()
        client: ConnectClient = session.client("connect", region_name=region_name, config=config)
        return cls(client)

    def connect(self, *, endpoint: str, timeout: float, **kwargs: Any) -> None:
        """
        Connects to an AWS service.

        :param endpoint: The URL of the service to connect to.
        :type endpoint: str
        :param timeout: The maximum amount of time to wait before giving up.
        :type timeout: float
        :keyword arguments: Additional parameters to pass to the connection attempt.
        :rtype: None
        """
        # This is a placeholder to illustrate a typed method signature.
        # For boto3, the client is already constructed in create/__init__.
        return None

    def disconnect(self) -> None:
        """
        Closes the underlying boto3 Connect client.
        """
        # botocore BaseClient provides close()
        self.client.close()

    def describe_instance(self, instance_id: str) -> DescribeInstanceResponseTypeDef:
        """
        Typed wrapper around Connect DescribeInstance.
        """
        return self.client.describe_instance(InstanceId=instance_id)

    def list_users(
        self,
        instance_id: str,
        *,
        next_token: Optional[str] = None,
        max_results: Optional[int] = None,
    ) -> ListUsersResponseTypeDef:
        """
        Typed wrapper around Connect ListUsers.
        """
        params: Dict[str, Any] = {"InstanceId": instance_id}
        if next_token is not None:
            params["NextToken"] = next_token
        if max_results is not None:
            params["MaxResults"] = max_results
        return self.client.list_users(**params)
------------------------------------------------------------

PROMPT 129:
"Generate Python code that imports the `graphql-client` library and creates a client object to interact with a GraphQL server, using the `graphql-client.GraphQLClient` class."
RESPONSE:
# pip install graphqlclient
from graphqlclient import GraphQLClient

# Create a client to interact with a GraphQL server
client = GraphQLClient("https://your-graphql-endpoint.com/graphql")
------------------------------------------------------------

PROMPT 130:
"Generate Python code that creates a logging system for experimental data, including a function to save data to a CSV file and a function to visualize the data using Matplotlib."
RESPONSE:
```python
import csv
import os
from dataclasses import dataclass, asdict, field
from datetime import datetime
from typing import List, Dict, Iterable, Optional, Sequence, Union
import math

import matplotlib.pyplot as plt
import matplotlib.dates as mdates


Number = Union[int, float]


@dataclass
class ExperimentRecord:
    # Customize these fields for your experiment
    timestamp: str
    subject_id: Optional[str] = None
    trial: Optional[int] = None
    condition: Optional[str] = None
    value: Optional[Number] = None
    notes: Optional[str] = None

    # Store any extra arbitrary fields
    extra: Dict[str, Union[str, Number]] = field(default_factory=dict)

    def to_row(self) -> Dict[str, Union[str, Number]]:
        row = asdict(self).copy()
        extras = row.pop("extra", {}) or {}
        row.update(extras)
        return row


class ExperimentLogger:
    """
    In-memory logger for experimental data with CSV persistence.
    """

    def __init__(
        self,
        field_order: Optional[Sequence[str]] = None,
        auto_timestamp: bool = True,
        timestamp_field: str = "timestamp",
        timestamp_timespec: str = "seconds",
    ):
        self._records: List[ExperimentRecord] = []
        self._field_order = list(field_order) if field_order else None
        self._auto_timestamp = auto_timestamp
        self._timestamp_field = timestamp_field
        self._timestamp_timespec = timestamp_timespec

    def log(self, **kwargs) -> ExperimentRecord:
        """
        Log an experiment record. Any extra keys not in the dataclass will be
        stored in the 'extra' dict and included when saving.
        """
        # Handle timestamp
        if self._auto_timestamp and self._timestamp_field not in kwargs:
            kwargs[self._timestamp_field] = datetime.now().isoformat(
                timespec=self._timestamp_timespec
            )

        # Split known fields vs extras
        known_fields = {f.name for f in ExperimentRecord.__dataclass_fields__.values()}
        extras = {k: v for k, v in kwargs.items() if k not in known_fields}
        base = {k: v for k, v in kwargs.items() if k in known_fields}
        rec = ExperimentRecord(**base, extra=extras)
        self._records.append(rec)
        return rec

    def extend(self, rows: Iterable[Dict[str, Union[str, Number]]]) -> None:
        for r in rows:
            self.log(**r)

    def clear(self) -> None:
        self._records.clear()

    @property
    def records(self) -> List[ExperimentRecord]:
        return list(self._records)

    def to_rows(self) -> List[Dict[str, Union[str, Number]]]:
        return [r.to_row() for r in self._records]


def save_data_to_csv(
    data: Iterable[Dict[str, Union[str, Number]]],
    filepath: str,
    field_order: Optional[Sequence[str]] = None,
    include_header: bool = True,
    mode: str = "w",
    newline: str = "",
    encoding: str = "utf-8",
) -> None:
    """
    Save iterable of dicts to CSV. Field names are inferred from the union of keys
    unless field_order is provided.
    """
    rows = list(data)
    if not rows:
        raise ValueError("No data to save.")

    # Determine fieldnames
    if field_order:
        fieldnames = list(field_order)
    else:
        keys = set()
        for r in rows:
            keys.update(r.keys())
        # Prefer a sensible order if present
        preferred = ["timestamp", "subject_id", "trial", "condition", "value", "notes"]
        fieldnames = [k for k in preferred if k in keys] + sorted(
            [k for k in keys if k not in preferred]
        )

    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)
    write_header = include_header
    if mode == "a" and os.path.exists(filepath):
        # If appending to existing file, don't write header again
        write_header = False

    with open(filepath, mode=mode, newline=newline, encoding=encoding) as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
        if write_header:
            writer.writeheader()
        for row in rows:
            writer.writerow(row)


def load_csv(
    filepath: str, encoding: str = "utf-8"
) -> List[Dict[str, Union[str, Number]]]:
    """
    Load CSV rows into a list of dicts. Numeric fields remain strings unless
    converted later.
    """
    with open(filepath, "r", newline="", encoding=encoding) as f:
        reader = csv.DictReader(f)
        return [dict(row) for row in reader]


def _coerce_number(x) -> Optional[float]:
    if x is None:
        return None
    if isinstance(x, (int, float)):
        return float(x)
    try:
        s = str(x).strip()
        if s == "" or s.lower() == "nan":
            return math.nan
        return float(s)
    except Exception:
        return None


def _parse_x(values: List[Union[str, Number]]) -> (List, str):
    """
    Try to parse x-axis values as datetime, numeric, or categorical.
    Returns (parsed_values, kind) where kind in {"datetime", "numeric", "categorical"}.
    """
    # Try datetime (ISO 8601)
    dt_ok = True
    dt_vals = []
    for v in values:
        try:
            if isinstance(v, datetime):
                dt_vals.append(v)
            else:
                dt_vals.append(datetime.fromisoformat(str(v)))
        except Exception:
            dt_ok = False
            break
    if dt_ok:
        return dt_vals, "datetime"

    # Try numeric
    num_ok = True
    num_vals = []
    for v in values:
        n = _coerce_number(v)
        if n is None:
            num_ok = False
            break
        num_vals.append(n)
    if num_ok:
        return num_vals, "numeric"

    # Fallback categorical
    return [str(v) for v in values], "categorical"


def visualize_data(
    data: Iterable[Dict[str, Union[str, Number]]],
    x_field: str = "timestamp",
    y_fields: Sequence[str] = ("value",),
    title: Optional[str] = None,
    show: bool = True,
    save_path: Optional[str] = None,
    figsize=(9, 5),
) -> None:
    """
    Visualize data with Matplotlib. Data is an iterable of dict rows.
    - Attempts to parse x as ISO datetimes, else numeric, else categorical.
    - Skips y series that cannot be coerced to numeric.
    """
    rows = list(data)
    if not rows:
        raise ValueError("No data to visualize.")

    x_raw = [r.get(x_field) for r in rows]
    x, x_kind = _parse_x(x_raw)

    plt.figure(figsize=figsize)
    plotted_any = False

    for y_key in y_fields:
        y = [_coerce_number(r.get(y_key)) for r in rows]
        if all(v is None for v in y):
            continue
        # Replace None with NaN to allow broken lines
        y = [math.nan if v is None else v for v in y]
        if x_kind == "categorical":
            # For categories, use integer positions
            xs = list(range(len(x)))
            plt.plot(xs, y, marker="o", label=y_key)
            plt.xticks(xs, x, rotation=45, ha="right")
        elif x_kind == "datetime":
            plt.plot(x, y, marker="o", label=y_key)
            ax = plt.gca()
            locator = mdates.AutoDateLocator()
            formatter = mdates.ConciseDateFormatter(locator)
            ax.xaxis.set_major_locator(locator)
            ax.xaxis.set_major_formatter(formatter)
        else:
            plt.plot(x, y, marker="o", label=y_key)
        plotted_any = True

    if not plotted_any:
        raise ValueError("No plottable numeric y-series found in y_fields.")

    plt.xlabel(x_field)
    plt.ylabel(", ".join(y_fields))
    if title:
        plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()

    if save_path:
        os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)
        plt.savefig(save_path, dpi=150)

    if show:
        plt.show()
    else:
        plt.close()


def visualize_csv(
    filepath: str,
    x_field: str = "timestamp",
    y_fields: Sequence[str] = ("value",),
    title: Optional[str] = None,
    show: bool = True,
    save_path: Optional[str] = None,
    figsize=(9, 5),
) -> None:
    rows = load_csv(filepath)
    visualize_data(
        rows,
        x_field=x_field,
        y_fields=y_fields,
        title=title or f"Data from {os.path.basename(filepath)}",
        show=show,
        save_path=save_path,
        figsize=figsize,
    )


# Example usage
if __name__ == "__main__":
    # 1) Create logger and record some synthetic data
    logger = ExperimentLogger()
    for i in range(30):
        logger.log(subject_id="S01", trial=i + 1, condition="A", value=0.5 * i, notes=None)

    # 2) Save to CSV
    csv_path = "output/experiment_log.csv"
    save_data_to_csv(logger.to_rows(), csv_path)

    # 3) Visualize from in-memory data
    visualize_data(logger.to_rows(), x_field="timestamp", y_fields=["value"], title="In-memory log")

    # 4) Visualize directly from CSV
    visualize_csv(csv_path, x_field="timestamp", y_fields=["value"], title="From CSV")
```
------------------------------------------------------------

PROMPT 131:
"Generate Python code that uses the requests module to interface with the Databricks API, allowing users to create and manage clusters, jobs, and datasets on the Databricks platform."
RESPONSE:
```python
#!/usr/bin/env python3
"""
Lightweight Databricks REST client using requests.

Features:
- Clusters: create, edit, start, resize, list, get, delete/permanent-delete
- Jobs: create, run-now, list, get, delete, runs control (get, list, cancel)
- Datasets:
    - DBFS: upload/download/list/mkdirs/delete/move
    - SQL: run SQL via Statements API (e.g., create tables from CSV)

Auth: Personal Access Token (PAT) via Authorization: Bearer <token>
Works with any Databricks workspace URL (AWS, Azure, GCP).

Usage example at bottom (see __main__).
"""

import base64
import json
import os
import time
from typing import Any, Dict, Iterable, List, Optional, Union

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class DatabricksAPIError(Exception):
    def __init__(self, response: requests.Response, message: Optional[str] = None):
        try:
            payload = response.json()
        except Exception:
            payload = response.text
        msg = message or f"Databricks API error {response.status_code}: {payload}"
        super().__init__(msg)
        self.response = response
        self.payload = payload


class DatabricksClient:
    def __init__(
        self,
        host: str,
        token: str,
        *,
        timeout: int = 60,
        verify: Union[bool, str] = True,
        user_agent_suffix: str = "py-requests-client/1.0",
        max_retries: int = 5,
        backoff_factor: float = 0.5,
    ):
        """
        host: e.g., https://adb-12345.78.azuredatabricks.net or https://<region>.gcp.databricks.com
        token: Personal Access Token for the workspace
        """
        self.host = host.rstrip("/")
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update(
            {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json",
                "User-Agent": f"databricks-requests-client {user_agent_suffix}",
            }
        )
        self.session.verify = verify

        # Robust retries for transient errors and rate limits
        retry = Retry(
            total=max_retries,
            read=max_retries,
            connect=max_retries,
            backoff_factor=backoff_factor,
            status_forcelist=[408, 425, 429, 500, 502, 503, 504],
            allowed_methods=frozenset(["GET", "POST", "PUT", "DELETE"]),
            raise_on_status=False,
            respect_retry_after_header=True,
        )
        adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=20)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)

    # ------------------------ Low-level helpers ------------------------

    def _url(self, path: str) -> str:
        if path.startswith("http"):
            return path
        return f"{self.host}{path}"

    def _request(
        self,
        method: str,
        path: str,
        *,
        params: Optional[Dict[str, Any]] = None,
        json_body: Optional[Dict[str, Any]] = None,
        data: Optional[Union[bytes, str]] = None,
        headers: Optional[Dict[str, str]] = None,
        expected: Iterable[int] = (200,),
        timeout: Optional[int] = None,
        stream: bool = False,
    ) -> requests.Response:
        url = self._url(path)
        hdrs = self.session.headers.copy()
        if headers:
            hdrs.update(headers)
        resp = self.session.request(
            method=method.upper(),
            url=url,
            params=params,
            json=json_body,
            data=data,
            headers=hdrs,
            timeout=timeout or self.timeout,
            stream=stream,
        )
        if resp.status_code not in expected:
            raise DatabricksAPIError(resp)
        return resp

    def _get_json(
        self,
        method: str,
        path: str,
        *,
        params: Optional[Dict[str, Any]] = None,
        json_body: Optional[Dict[str, Any]] = None,
        expected: Iterable[int] = (200,),
    ) -> Dict[str, Any]:
        r = self._request(method, path, params=params, json_body=json_body, expected=expected)
        if r.content and "application/json" in r.headers.get("Content-Type", ""):
            return r.json()
        return {}

    # ------------------------ Clusters (2.0) ------------------------

    def list_clusters(self) -> List[Dict[str, Any]]:
        resp = self._get_json("GET", "/api/2.0/clusters/list")
        return resp.get("clusters", [])

    def get_cluster(self, cluster_id: str) -> Dict[str, Any]:
        return self._get_json("GET", "/api/2.0/clusters/get", params={"cluster_id": cluster_id})

    def create_cluster(self, config: Dict[str, Any]) -> str:
        """
        Returns cluster_id.
        Example minimal config (AWS):
            {
              "cluster_name": "my-cluster",
              "spark_version": "13.3.x-scala2.12",
              "node_type_id": "i3.xlarge",
              "autotermination_minutes": 30,
              "num_workers": 2
            }
        """
        resp = self._get_json("POST", "/api/2.0/clusters/create", json_body=config, expected=(200,))
        return resp["cluster_id"]

    def edit_cluster(self, config: Dict[str, Any]) -> None:
        # Must include "cluster_id" in config
        self._request("POST", "/api/2.0/clusters/edit", json_body=config, expected=(200,))

    def resize_cluster(self, cluster_id: str, num_workers: int) -> None:
        self._request(
            "POST",
            "/api/2.0/clusters/resize",
            json_body={"cluster_id": cluster_id, "num_workers": num_workers},
            expected=(200,),
        )

    def start_cluster(self, cluster_id: str) -> None:
        self._request("POST", "/api/2.0/clusters/start", json_body={"cluster_id": cluster_id})

    def restart_cluster(self, cluster_id: str) -> None:
        self._request("POST", "/api/2.0/clusters/restart", json_body={"cluster_id": cluster_id})

    def delete_cluster(self, cluster_id: str) -> None:
        # Soft terminate
        self._request("POST", "/api/2.0/clusters/delete", json_body={"cluster_id": cluster_id})

    def permanent_delete_cluster(self, cluster_id: str) -> None:
        self._request(
            "POST", "/api/2.0/clusters/permanent-delete", json_body={"cluster_id": cluster_id}
        )

    # ------------------------ Jobs (2.1) ------------------------

    def create_job(self, config: Dict[str, Any]) -> int:
        """
        Returns job_id.
        Minimal example (run a notebook):
            {
              "name": "example-notebook-job",
              "tasks": [
                {
                  "task_key": "t1",
                  "notebook_task": {"notebook_path": "/Shared/Example"},
                  "job_cluster_key": "j1"
                }
              ],
              "job_clusters": [
                {
                  "job_cluster_key": "j1",
                  "new_cluster": {
                    "spark_version": "13.3.x-scala2.12",
                    "node_type_id": "i3.xlarge",
                    "num_workers": 1
                  }
                }
              ]
            }
        """
        resp = self._get_json("POST", "/api/2.1/jobs/create", json_body=config)
        return int(resp["job_id"])

    def list_jobs(self, limit: int = 50) -> List[Dict[str, Any]]:
        jobs: List[Dict[str, Any]] = []
        page_token = None
        while True:
            params = {"limit": limit}
            if page_token:
                params["page_token"] = page_token
            resp = self._get_json("GET", "/api/2.1/jobs/list", params=params)
            jobs.extend(resp.get("jobs", []))
            page_token = resp.get("next_page_token")
            if not page_token:
                break
        return jobs

    def get_job(self, job_id: int) -> Dict[str, Any]:
        return self._get_json("GET", "/api/2.1/jobs/get", params={"job_id": job_id})

    def delete_job(self, job_id: int) -> None:
        self._request("POST", "/api/2.1/jobs/delete", json_body={"job_id": job_id})

    def run_now(self, job_id: int, **kwargs) -> int:
        """
        Optional kwargs may include jar_params, notebook_params, python_params, spark_submit_params, python_named_params, etc.
        Returns run_id.
        """
        payload = {"job_id": job_id}
        payload.update(kwargs)
        resp = self._get_json("POST", "/api/2.1/jobs/run-now", json_body=payload)
        return int(resp["run_id"])

    def get_run(self, run_id: int) -> Dict[str, Any]:
        return self._get_json("GET", "/api/2.1/jobs/runs/get", params={"run_id": run_id})

    def list_runs(
        self, *, job_id: Optional[int] = None, limit: int = 50, active_only: bool = False
    ) -> List[Dict[str, Any]]:
        runs: List[Dict[str, Any]] = []
        page_token = None
        while True:
            params: Dict[str, Any] = {"limit": limit}
            if job_id is not None:
                params["job_id"] = job_id
            if active_only:
                params["active_only"] = "true"
            if page_token:
                params["page_token"] = page_token
            resp = self._get_json("GET", "/api/2.1/jobs/runs/list", params=params)
            runs.extend(resp.get("runs", []))
            page_token = resp.get("next_page_token")
            if not page_token:
                break
        return runs

    def cancel_run(self, run_id: int) -> None:
        self._request("POST", "/api/2.1/jobs/runs/cancel", json_body={"run_id": run_id})

    # ------------------------ DBFS (2.0) ------------------------

    def dbfs_list(self, path: str) -> List[Dict[str, Any]]:
        resp = self._get_json("GET", "/api/2.0/dbfs/list", params={"path": path})
        return resp.get("files", [])

    def dbfs_get_status(self, path: str) -> Dict[str, Any]:
        return self._get_json("GET", "/api/2.0/dbfs/get-status", params={"path": path})

    def dbfs_mkdirs(self, path: str) -> None:
        self._request("POST", "/api/2.0/dbfs/mkdirs", json_body={"path": path})

    def dbfs_move(self, source: str, destination: str) -> None:
        self._request(
            "POST",
            "/api/2.0/dbfs/move",
            json_body={"source_path": source, "destination_path": destination},
        )

    def dbfs_delete(self, path: str, recursive: bool = False) -> None:
        self._request(
            "POST", "/api/2.0/dbfs/delete", json_body={"path": path, "recursive": recursive}
        )

    def dbfs_put(self, path: str, data: Union[bytes, str], overwrite: bool = True) -> None:
        """
        Simple upload (base64-encoded). Good for small to moderate files.
        """
        if isinstance(data, str):
            data = data.encode("utf-8")
        payload = {
            "path": path,
            "contents": base64.b64encode(data).decode("utf-8"),
            "overwrite": overwrite,
        }
        self._request("POST", "/api/2.0/dbfs/put", json_body=payload)

    def dbfs_upload_file(self, local_path: str, dbfs_path: str, overwrite: bool = True, chunk_size: int = 4 * 1024 * 1024) -> None:
        """
        Streaming upload using create/add-block/close for large files.
        """
        # create handle
        resp = self._get_json(
            "POST",
            "/api/2.0/dbfs/create",
            json_body={"path": dbfs_path, "overwrite": overwrite},
        )
        handle = resp["handle"]
        try:
            with open(local_path, "rb") as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    self._request(
                        "POST",
                        "/api/2.0/dbfs/add-block",
                        json_body={"handle": handle, "data": base64.b64encode(chunk).decode("utf-8")},
                    )
        finally:
            # Always attempt to close the handle
            self._request("POST", "/api/2.0/dbfs/close", json_body={"handle": handle})

    def dbfs_download_file(self, dbfs_path: str, local_path: str, chunk_size: int = 4 * 1024 * 1024) -> None:
        """
        Download a DBFS file using read with pagination.
        """
        offset = 0
        with open(local_path, "wb") as out:
            while True:
                params = {"path": dbfs_path, "offset": offset, "length": chunk_size}
                resp = self._get_json("GET", "/api/2.0/dbfs/read", params=params)
                data_b64 = resp.get("data")
                if not data_b64:
                    break
                buf = base64.b64decode(data_b64)
                out.write(buf)
                read = resp.get("bytes_read", len(buf))
                if read == 0:
                    break
                offset += read

    # ------------------------ SQL Statements (2.0) ------------------------

    def run_sql_statement(
        self,
        warehouse_id: str,
        statement: str,
        *,
        catalog: Optional[str] = None,
        schema: Optional[str] = None,
        wait: bool = True,
        poll_interval_seconds: float = 1.5,
        timeout_seconds: int = 600,
    ) -> Dict[str, Any]:
        """
        Run a SQL statement on a SQL Warehouse. Returns final statement payload if wait=True.
        """
        payload: Dict[str, Any] = {
            "statement": statement,
            "warehouse_id": warehouse_id,
            "disposition": "INLINE",
        }
        if catalog or schema:
            payload["catalog"] = catalog
            payload["schema"] = schema

        submit = self._get_json("POST", "/api/2.0/sql/statements", json_body=payload, expected=(200, 201))
        st_id = submit.get("statement_id") or submit.get("id")
        if not wait:
            return submit

        start = time.time()
        while True:
            status = self._get_json("GET", f"/api/2.0/sql/statements/{st_id}")
            state = status.get("status", {}).get("state")
            if state in ("SUCCEEDED", "FAILED", "CANCELED"):
                return status
            if time.time() - start > timeout_seconds:
                raise TimeoutError(f"SQL statement {st_id} timed out after {timeout_seconds}s; last status: {state}")
            time.sleep(poll_interval_seconds)

    def create_table_from_csv(
        self,
        *,
        warehouse_id: str,
        catalog: Optional[str],
        schema: str,
        table: str,
        dbfs_csv_path: str,
        options: Optional[Dict[str, Any]] = None,
        replace: bool = False,
    ) -> None:
        """
        Create a managed table from a CSV stored in DBFS using SQL.
        Requires an existing SQL Warehouse and Unity Catalog access.
        """
        fq_schema = f"{catalog}.{schema}" if catalog else schema
        self.run_sql_statement(
            warehouse_id,
            f"CREATE SCHEMA IF NOT EXISTS {fq_schema}",
            wait=True,
        )

        opts = options or {"header": "true", "inferSchema": "true"}
        opts_sql = ", ".join([f"{k} = '{v}'" for k, v in opts.items()])
        # Using COPY INTO pattern to a delta table
        table_name = f"{fq_schema}.{table}"
        if replace:
            self.run_sql_statement(warehouse_id, f"DROP TABLE IF EXISTS {table_name}", wait=True)
        # Create empty table if replacing isn't requested; COPY INTO will create if not exists for some formats, but we ensure it.
        create_stmt = f"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA"
        self.run_sql_statement(warehouse_id, create_stmt, wait=True)

        copy_stmt = (
            f"COPY INTO {table_name} "
            f"FROM 'dbfs:{dbfs_csv_path}' "
            f"FILEFORMAT = CSV "
            f"FORMAT_OPTIONS ({opts_sql}) "
            f"COPY_OPTIONS ('mergeSchema' = 'true')"
        )
        res = self.run_sql_statement(warehouse_id, copy_stmt, wait=True)
        status = res.get("status", {}).get("state")
        if status != "SUCCEEDED":
            raise RuntimeError(f"Failed to COPY INTO table {table_name}: {res}")

    # ------------------------ Convenience utils ------------------------

    def wait_for_cluster_state(
        self,
        cluster_id: str,
        target_states: List[str],
        *,
        timeout_seconds: int = 1800,
        poll_interval_seconds: float = 10.0,
    ) -> Dict[str, Any]:
        """
        Wait until cluster's state is one of target_states.
        Common states: PENDING, RUNNING, RESTARTING, RESIZING, TERMINATING, TERMINATED, ERROR
        """
        start = time.time()
        while True:
            info = self.get_cluster(cluster_id)
            state = info.get("state")
            if state in target_states:
                return info
            if time.time() - start > timeout_seconds:
                raise TimeoutError(f"Cluster {cluster_id} did not reach {target_states} within {timeout_seconds}s; last={state}")
            time.sleep(poll_interval_seconds)

    def wait_for_run_termination(
        self,
        run_id: int,
        *,
        terminal_states: List[str] = ("TERMINATED", "SKIPPED", "INTERNAL_ERROR"),
        success_life_cycle: str = "TERMINATED",
        success_result_state: str = "SUCCESS",
        timeout_seconds: int = 3600,
        poll_interval_seconds: float = 5.0,
        raise_on_failure: bool = True,
    ) -> Dict[str, Any]:
        """
        Poll a job run until it reaches a terminal state.
        """
        start = time.time()
        while True:
            run = self.get_run(run_id)
            life_cycle = run.get("state", {}).get("life_cycle_state")
            result_state = run.get("state", {}).get("result_state")
            if life_cycle in terminal_states:
                if raise_on_failure and not (
                    life_cycle == success_life_cycle and result_state == success_result_state
                ):
                    raise RuntimeError(f"Run {run_id} ended with life_cycle={life_cycle}, result_state={result_state}")
                return run
            if time.time() - start > timeout_seconds:
                raise TimeoutError(f"Run {run_id} did not finish within {timeout_seconds}s")
            time.sleep(poll_interval_seconds)


# ------------------------ Example usage ------------------------

if __name__ == "__main__":
    # Configure host and token via env vars for convenience
    DATABRICKS_HOST = os.environ.get("DATABRICKS_HOST", "").strip()  # e.g., https://adb-xxxxxxxx.azuredatabricks.net
    DATABRICKS_TOKEN = os.environ.get("DATABRICKS_TOKEN", "").strip()
    if not DATABRICKS_HOST or not DATABRICKS_TOKEN:
        raise SystemExit("Please set DATABRICKS_HOST and DATABRICKS_TOKEN environment variables")

    dbx = DatabricksClient(DATABRICKS_HOST, DATABRICKS_TOKEN)

    # 1) Clusters: create a small on-demand job cluster and wait until RUNNING
    # Fill in values appropriate for your workspace (node_type_id, spark_version)
    cluster_config = {
        "cluster_name": "example-requests-client",
        "spark_version": "13.3.x-scala2.12",
        "node_type_id": "i3.xlarge",  # replace for your cloud/provider
        "num_workers": 1,
        "autotermination_minutes": 20,
    }
    print("Creating cluster...")
    cluster_id = dbx.create_cluster(cluster_config)
    print("Cluster created:", cluster_id)
    dbx.wait_for_cluster_state(cluster_id, ["RUNNING"])
    print("Cluster is RUNNING")

    # 2) DBFS: upload a small CSV and list the directory
    local_csv = "example.csv"
    with open(local_csv, "w", encoding="utf-8") as f:
        f.write("id,name\n1,Alice\n2,Bob\n")
    dbfs_dir = "/tmp/datasets"
    dbx.dbfs_mkdirs(dbfs_dir)
    dbfs_csv_path = f"{dbfs_dir}/example.csv"
    dbx.dbfs_put(dbfs_csv_path, open(local_csv, "rb").read(), overwrite=True)
    print("Uploaded CSV to DBFS:", dbfs_csv_path)
    print("Listing DBFS dir:", dbx.dbfs_list(dbfs_dir))

    # 3) Jobs: create a simple job that runs a notebook (replace notebook_path)
    job_config = {
        "name": "example-requests-job",
        "tasks": [
            {
                "task_key": "task1",
                "notebook_task": {"notebook_path": "/Shared/ExampleNotebook"},  # replace with your path
                "job_cluster_key": "jc1",
            }
        ],
        "job_clusters": [
            {
                "job_cluster_key": "jc1",
                "new_cluster": {
                    "spark_version": "13.3.x-scala2.12",
                    "node_type_id": "i3.xlarge",
                    "num_workers": 1,
                    "autotermination_minutes": 20,
                },
            }
        ],
    }
    print("Creating job...")
    job_id = dbx.create_job(job_config)
    print("Job created:", job_id)
    print("Running job now...")
    run_id = dbx.run_now(job_id)
    print("Run ID:", run_id)
    dbx.wait_for_run_termination(run_id)
    print("Job run completed successfully.")

    # 4) SQL: If you have a SQL Warehouse, create a table from the CSV
    # Set DATABRICKS_WAREHOUSE_ID in environment to enable
    WAREHOUSE_ID = os.environ.get("DATABRICKS_WAREHOUSE_ID")
    if WAREHOUSE_ID:
        print("Creating table from CSV via SQL...")
        dbx.create_table_from_csv(
            warehouse_id=WAREHOUSE_ID,
            catalog=None,      # or your catalog name, e.g., "main"
            schema="default",  # or your schema
            table="example_from_csv",
            dbfs_csv_path=dbfs_csv_path,
            options={"header": "true", "inferSchema": "true"},
            replace=True,
        )
        print("Table created.")
    else:
        print("Skipping SQL step (no DATABRICKS_WAREHOUSE_ID provided).")

    # Cleanup (optional)
    print("Terminating cluster...")
    dbx.delete_cluster(cluster_id)
    dbx.dbfs_delete(dbfs_dir, recursive=True)
    print("Done.")
```
------------------------------------------------------------

PROMPT 132:
"Generate Python code that uses the Sphinx framework to automatically generate API documentation for entire modules, including function descriptions, parameter lists, and example usage."
RESPONSE:
#!/usr/bin/env python3
"""
Programmatically generate Sphinx API documentation for a Python module or package.

Features:
- Recursively documents entire modules/packages (functions, classes, parameters).
- Parses Google/NumPy style docstrings (via napoleon).
- Includes example usage sections present in docstrings (via napoleon + doctest).
- Generates HTML docs using Sphinx's Python API (no shell commands required).

Usage:
    python generate_api_docs.py <import-name-or-path> [--output ./docs] [--project "My Project"]

Examples:
    python generate_api_docs.py mypackage
    python generate_api_docs.py ./src/mypackage --project "My Package" --output ./docs

Requirements:
    pip install "sphinx>=7" sphinx-rtd-theme
"""
from __future__ import annotations

import argparse
import importlib
import importlib.util
import os
import shutil
import sys
from pathlib import Path
from textwrap import dedent

# These imports are optional until actually needed (for nicer error messages)
def _require_sphinx():
    try:
        import sphinx  # noqa: F401
        from sphinx.application import Sphinx  # noqa: F401
        from sphinx.ext import apidoc  # noqa: F401
    except Exception as e:
        raise SystemExit(
            "Sphinx is required. Install with: pip install 'sphinx>=7' sphinx-rtd-theme\n"
            f"Original error: {e}"
        )


def _resolve_target(target: str) -> tuple[str, Path, str]:
    """
    Resolve the user's input to:
    - import_name: the module's importable name (if possible)
    - package_path: filesystem path to the module/package directory
    - version: discovered version string or '0.0.0'
    """
    # If it's a filesystem path, normalize it
    p = Path(target).resolve()
    if p.exists():
        # If target points to a package directory (has __init__.py)
        if p.is_dir() and (p / "__init__.py").exists():
            package_path = p
            # Try to derive an import name by its directory name
            import_name = p.name
        # If target is a module file
        elif p.is_file() and p.suffix == ".py":
            package_path = p.parent
            import_name = p.stem
        # If target is a directory that contains a package inside (e.g., src/mypkg)
        elif p.is_dir():
            # Try to find one package directory inside
            candidates = [d for d in p.iterdir() if d.is_dir() and (d / "__init__.py").exists()]
            if len(candidates) == 1:
                package_path = candidates[0]
                import_name = candidates[0].name
            else:
                raise SystemExit(
                    f"Could not determine a single package inside directory: {p}\n"
                    "Point to the package directory (with __init__.py) or a module file."
                )
        else:
            raise SystemExit(f"Unsupported target path: {p}")
        # Try to import for a version if possible by temporarily adjusting sys.path
        version = "0.0.0"
        try:
            sys.path.insert(0, str(package_path.parent))
            mod = importlib.import_module(import_name)
            version = getattr(mod, "__version__", version)
        except Exception:
            pass
        finally:
            if sys.path and sys.path[0] == str(package_path.parent):
                sys.path.pop(0)
        return import_name, package_path, str(version)

    # Otherwise treat it as an import name
    import_name = target
    try:
        mod = importlib.import_module(import_name)
        file = getattr(mod, "__file__", None)
        if not file:
            raise SystemExit(f"Module '{import_name}' does not have a __file__; cannot locate sources.")
        file_path = Path(file).resolve()
        # If __init__.py, use its directory as package path; else the module's parent directory
        package_path = file_path.parent if file_path.name != "__init__.py" else file_path.parent
        version = getattr(mod, "__version__", "0.0.0")
        return import_name, package_path, str(version)
    except Exception as e:
        raise SystemExit(f"Could not import module '{import_name}': {e}")


def _write_conf_py(source_dir: Path, project: str, project_root: Path, version: str, import_name: str) -> None:
    """
    Write a Sphinx conf.py configured for autodoc + napoleon + autosummary + doctest.
    """
    conf_py = dedent(
        f"""
        import os
        import sys
        from datetime import datetime

        # Ensure the project root is importable for autodoc
        sys.path.insert(0, os.path.abspath(r"{project_root.as_posix()}"))

        project = "{project}"
        author = ""
        copyright = f"{{datetime.now():%Y}}, {{author}}"
        release = "{version}"

        extensions = [
            "sphinx.ext.autodoc",
            "sphinx.ext.autosummary",
            "sphinx.ext.napoleon",
            "sphinx.ext.viewcode",
            "sphinx.ext.doctest",
        ]

        # Generate autosummary pages
        autosummary_generate = True
        autosummary_imported_members = True

        # Autodoc settings to include members and show details
        autodoc_default_options = {{
            "members": True,
            "undoc-members": True,
            "show-inheritance": True,
            "inherited-members": True,
            "special-members": "__init__",
        }}
        autodoc_typehints = "description"  # show type hints in the description section
        autodoc_preserve_defaults = True
        autodoc_class_signature = "separated"

        # Napoleon to parse Google/NumPy-style docstrings (including Examples)
        napoleon_google_docstring = True
        napoleon_numpy_docstring = True
        napoleon_include_init_with_doc = True
        napoleon_use_param = True
        napoleon_use_rtype = True
        napoleon_preprocess_types = True
        napoleon_attr_annotations = True

        # Doctest will extract and optionally test the code in "Examples" sections
        doctest_test_doctest_blocks = True

        templates_path = ["_templates"]
        exclude_patterns = ["_build", "Thumbs.db", ".DS_Store"]

        try:
            import sphinx_rtd_theme
            html_theme = "sphinx_rtd_theme"
            html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]
        except Exception:
            html_theme = "alabaster"

        html_static_path = ["_static"]

        # If you want nitpicky mode for missing references, uncomment:
        # nitpicky = True

        # Make sure Sphinx can import the target package in the build env
        try:
            __import__("{import_name}")
        except Exception as e:
            print(f"Warning: could not import '{{'{import_name}'}}' during autodoc: {{e}}")
        """
    ).strip() + "\n"
    (source_dir / "conf.py").write_text(conf_py, encoding="utf-8")


def _write_index_rst(source_dir: Path, project: str) -> None:
    """
    Write a minimal index.rst that includes the modules TOC generated by apidoc.
    """
    index = dedent(
        f"""
        {project}
        {'=' * len(project)}

        Welcome to {project}'s API documentation.

        .. toctree::
           :maxdepth: 2
           :caption: Contents:

           modules
        """
    ).lstrip()
    (source_dir / "index.rst").write_text(index, encoding="utf-8")


def _run_apidoc(source_dir: Path, package_path: Path) -> None:
    """
    Run sphinx-apidoc (via Python API) to generate .rst stub pages for all modules.
    """
    _require_sphinx()
    from sphinx.ext import apidoc

    # Options:
    # -o: output dir
    # --force: overwrite existing
    # --separate: create one page per module
    # --module-first: show module before members
    # --implicit-namespaces: support PEP 420 namespace packages
    args = [
        "sphinx-apidoc",
        "-o",
        str(source_dir),
        str(package_path),
        "--force",
        "--separate",
        "--module-first",
        "--implicit-namespaces",
    ]
    # apidoc.main expects argv-like including prog name
    code = apidoc.main(args)
    if code not in (0, None):
        raise SystemExit(f"sphinx-apidoc failed with exit code {code}")


def _build_html(source_dir: Path, build_dir: Path) -> None:
    """
    Build HTML docs using the Sphinx Application API.
    """
    _require_sphinx()
    from sphinx.application import Sphinx

    doctree_dir = build_dir / ".doctrees"
    app = Sphinx(
        srcdir=str(source_dir),
        confdir=str(source_dir),
        outdir=str(build_dir / "html"),
        doctreedir=str(doctree_dir),
        buildername="html",
        freshenv=True,
        warningiserror=False,
        verbosity=0,
    )
    status = app.build(force_all=True)
    if status != 0:
        raise SystemExit(f"Sphinx build failed (status={status}).")


def generate_api_docs(target: str, output_dir: Path, project_name: str | None = None) -> Path:
    """
    Main workflow to generate the Sphinx site.

    Returns the path to the generated HTML index.html.
    """
    import_name, package_path, version = _resolve_target(target)
    project = project_name or import_name

    docs_dir = output_dir.resolve()
    source_dir = docs_dir / "source"
    build_dir = docs_dir / "build"

    # Clean/create directories
    if source_dir.exists():
        shutil.rmtree(source_dir)
    source_dir.mkdir(parents=True, exist_ok=True)
    (source_dir / "_static").mkdir(parents=True, exist_ok=True)
    (source_dir / "_templates").mkdir(parents=True, exist_ok=True)
    build_dir.mkdir(parents=True, exist_ok=True)

    # Compose conf.py and index.rst
    project_root = package_path.parent
    _write_conf_py(source_dir, project, project_root, version, import_name)
    _write_index_rst(source_dir, project)

    # Generate .rst stubs from code
    _run_apidoc(source_dir, package_path)

    # Build HTML
    _build_html(source_dir, build_dir)

    index_html = build_dir / "html" / "index.html"
    return index_html


def main():
    parser = argparse.ArgumentParser(description="Generate Sphinx API documentation for a module/package.")
    parser.add_argument(
        "target",
        help="Import name (e.g., 'mypackage') or path to package/module (e.g., './src/mypackage').",
    )
    parser.add_argument(
        "--output",
        default="./docs",
        help="Output docs directory (default: ./docs).",
    )
    parser.add_argument(
        "--project",
        default=None,
        help="Project name to display in docs (default: derived from module/package).",
    )
    args = parser.parse_args()

    output_dir = Path(args.output)
    index_html = generate_api_docs(args.target, output_dir, args.project)
    print(f"Documentation generated: {index_html.as_uri()}")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 133:
"Generate Python code that imports the requests library and uses it to make an HTTP request to the EPSG database URL, parsing the response into a usable format for easy access to the EPSG data."
RESPONSE:
# Requires: pip install requests
import requests
from typing import Any, Dict, List, Optional, Tuple


class EPSGClient:
    """
    Minimal client for fetching EPSG data from epsg.io's JSON endpoints.
    Note: epsg.io mirrors the official EPSG dataset and offers a public JSON API.
    """

    def __init__(self, base_url: str = "https://epsg.io", timeout: int = 10):
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout

    def _get_json(self, url: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        r = requests.get(url, params=params, timeout=self.timeout)
        r.raise_for_status()
        return r.json()

    def _normalize_result(self, raw: Dict[str, Any]) -> Dict[str, Any]:
        """
        Normalize a single EPSG result dict into a stable, convenient shape.
        Fields may be missing depending on the record type.
        """
        bbox = raw.get("bbox")
        if isinstance(bbox, list) and len(bbox) == 4:
            bbox_tuple: Optional[Tuple[float, float, float, float]] = tuple(bbox)  # south, west, north, east
        else:
            bbox_tuple = None

        return {
            "code": raw.get("code"),
            "name": raw.get("name"),
            "kind": raw.get("kind") or raw.get("type"),
            "area": raw.get("area"),
            "bbox": bbox_tuple,
            "deprecated": raw.get("deprecated"),
            "proj4": raw.get("proj4"),
            "wkt": raw.get("wkt"),
            "unit": raw.get("unit"),
            # Some records contain these:
            "remarks": raw.get("remarks"),
            "accuracy": raw.get("accuracy"),
            "scope": raw.get("scope"),
            "abbr": raw.get("abbr"),
        }

    def get_by_code(self, code: int) -> Dict[str, Any]:
        """
        Fetch a specific EPSG record by code (e.g., 4326) and return a normalized dict.
        """
        url = f"{self.base_url}/{code}.json"
        data = self._get_json(url)

        # epsg.io typically returns a flat JSON for a specific code.
        # Some variants return a 'results' list; handle both.
        if isinstance(data, dict) and "results" in data and isinstance(data["results"], list) and data["results"]:
            raw = data["results"][0]
        else:
            raw = data if isinstance(data, dict) else {}

        return self._normalize_result(raw)

    def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Search EPSG by free-text query and return a list of normalized results.
        """
        params = {"format": "json", "q": query}
        data = self._get_json(f"{self.base_url}/", params=params)
        results = data.get("results", [])
        normalized = [self._normalize_result(item) for item in results[:limit]]
        return normalized


if __name__ == "__main__":
    client = EPSGClient()

    # Example 1: Fetch a known EPSG code
    try:
        epsg_4326 = client.get_by_code(4326)
        print("EPSG:4326")
        for k, v in epsg_4326.items():
            print(f"  {k}: {v}")
    except requests.HTTPError as e:
        print(f"HTTP error fetching EPSG:4326: {e}")
    except requests.RequestException as e:
        print(f"Network error: {e}")

    # Example 2: Search for a term and list top matches
    try:
        results = client.search("Web Mercator", limit=5)
        print("\nSearch: 'Web Mercator'")
        for r in results:
            print(f"  {r['code']}: {r['name']} ({r['kind']}) - area: {r['area']}")
    except requests.RequestException as e:
        print(f"Network error during search: {e}")
------------------------------------------------------------

PROMPT 134:
"Generate Python code that imports the openpyxl-templates module and creates a template from a sample Excel file, allowing you to read and write data to the template using its defined structure."
RESPONSE:
# pip install openpyxl openpyxl-templates
# This example assumes your Excel template (.xlsx) has:
# - Two named cells: Customer_Name and Order_Date
# - One Excel Table named Items with columns: Name, Qty, Price

from dataclasses import dataclass, asdict
from typing import List, Dict, Any
from datetime import date

# Import the requested module (not strictly required for the helper below, but included as asked)
# If not installed, comment this out or install via pip.
import openpyxl_templates  # noqa: F401

from openpyxl import load_workbook
from openpyxl.workbook.workbook import Workbook
from openpyxl.worksheet.table import Table


@dataclass
class ItemRow:
    Name: str
    Qty: int
    Price: float


@dataclass
class OrderModel:
    Customer_Name: str
    Order_Date: date
    Items: List[ItemRow]


class ExcelTemplate:
    """
    Minimal helper that treats an Excel file as a template with a defined structure:
    - Named cells for scalar fields
    - A named Excel Table for repeating rows
    """

    def __init__(self, template_path: str, sheet_name: str = None):
        self.template_path = template_path
        self.wb: Workbook = load_workbook(template_path)
        self.ws = self.wb[sheet_name] if sheet_name else self.wb.active

    # ---- Named cell helpers ----
    def _get_named_cell(self, name: str):
        dn = self.wb.defined_names.get(name)
        if dn is None:
            raise KeyError(f"Named range '{name}' not found in workbook.")
        destinations = list(dn.destinations)
        if not destinations:
            raise KeyError(f"Named range '{name}' has no destinations.")
        sheet_name, cell_ref = destinations[0]
        ws = self.wb[sheet_name]
        return ws[cell_ref]

    def read_named_cell(self, name: str):
        return self._get_named_cell(name).value

    def write_named_cell(self, name: str, value):
        self._get_named_cell(name).value = value

    # ---- Table helpers ----
    def _get_table(self, table_name: str) -> Table:
        for tbl in self.ws._tables:  # openpyxl stores tables on the worksheet
            if tbl.name == table_name:
                return tbl
        raise KeyError(f"Table '{table_name}' not found on sheet '{self.ws.title}'.")

    def read_table(self, table_name: str) -> List[Dict[str, Any]]:
        tbl = self._get_table(table_name)
        ref = tbl.ref  # e.g., "A5:C10"
        min_row = self.ws[ref].min_row
        max_row = self.ws[ref].max_row
        min_col = self.ws[ref].min_col
        max_col = self.ws[ref].max_col

        # Header is the first row in the table range
        headers = [self.ws.cell(row=min_row, column=c).value for c in range(min_col, max_col + 1)]
        rows: List[Dict[str, Any]] = []
        for r in range(min_row + 1, max_row + 1):
            record = {}
            empty_row = True
            for c, header in zip(range(min_col, max_col + 1), headers):
                val = self.ws.cell(row=r, column=c).value
                record[header] = val
                if val not in (None, ""):
                    empty_row = False
            if not empty_row:
                rows.append(record)
        return rows

    def write_table(self, table_name: str, records: List[Dict[str, Any]]):
        tbl = self._get_table(table_name)
        ref = tbl.ref
        min_row = self.ws[ref].min_row
        max_row = self.ws[ref].max_row
        min_col = self.ws[ref].min_col
        max_col = self.ws[ref].max_col

        # Clear existing data rows (keep header)
        for r in range(min_row + 1, max_row + 1):
            for c in range(min_col, max_col + 1):
                self.ws.cell(row=r, column=c).value = None

        # Determine headers from the header row
        headers = [self.ws.cell(row=min_row, column=c).value for c in range(min_col, max_col + 1)]
        if not headers or any(h is None for h in headers):
            raise ValueError(f"Table '{table_name}' must have a header row.")

        # Write new records
        row_cursor = min_row + 1
        for rec in records:
            for offset, header in enumerate(headers):
                self.ws.cell(row=row_cursor, column=min_col + offset).value = rec.get(header)
            row_cursor += 1

        # Optionally, resize the table's ref if we wrote beyond the original table range
        if row_cursor - 1 > max_row:
            # Expand table range to include new rows
            new_ref = f"{self.ws.cell(row=min_row, column=min_col).coordinate}:{self.ws.cell(row=row_cursor - 1, column=max_col).coordinate}"
            tbl.ref = new_ref

    def save(self, path: str):
        self.wb.save(path)


def read_order_from_workbook(path: str) -> OrderModel:
    tpl = ExcelTemplate(path)
    customer = tpl.read_named_cell("Customer_Name")
    order_date = tpl.read_named_cell("Order_Date")
    row_dicts = tpl.read_table("Items")
    items = [ItemRow(Name=r.get("Name", ""), Qty=int(r.get("Qty") or 0), Price=float(r.get("Price") or 0.0)) for r in row_dicts]
    return OrderModel(Customer_Name=str(customer or ""), Order_Date=order_date, Items=items)


def write_order_to_workbook(template_path: str, out_path: str, order: OrderModel):
    tpl = ExcelTemplate(template_path)
    tpl.write_named_cell("Customer_Name", order.Customer_Name)
    tpl.write_named_cell("Order_Date", order.Order_Date)

    records = [asdict(item) for item in order.Items]
    tpl.write_table("Items", records)
    tpl.save(out_path)


if __name__ == "__main__":
    # Example usage:
    template_file = "sample_template.xlsx"  # Must contain named cells and a table as described above
    out_file = "filled_order.xlsx"

    # 1) Write data to the template
    demo_order = OrderModel(
        Customer_Name="ACME Corp.",
        Order_Date=date.today(),
        Items=[
            ItemRow(Name="Bolt", Qty=10, Price=0.25),
            ItemRow(Name="Nut", Qty=20, Price=0.15),
            ItemRow(Name="Screw", Qty=50, Price=0.05),
        ],
    )
    write_order_to_workbook(template_file, out_file, demo_order)

    # 2) Read data back from the filled workbook
    parsed = read_order_from_workbook(out_file)
    print("Parsed order:", parsed)
------------------------------------------------------------

PROMPT 135:
"Generate Python code that imports the requests library and creates a function called `make_request` that takes in a URL, method, and data dictionary as arguments, and returns the response object."
RESPONSE:
import requests

def make_request(url, method, data):
    method = method.upper()
    if method in {"GET", "HEAD"}:
        response = requests.request(method, url, params=data)
    else:
        response = requests.request(method, url, json=data)
    return response
------------------------------------------------------------

PROMPT 136:
"Generate Python code that imports the requests library and uses it to send HTTP GET requests to a JSON API, parsing the responses as JSON data and returning the parsed data in a dictionary format."
RESPONSE:
import requests
from typing import Any, Dict, Mapping, Optional

def get_json(
    url: str,
    params: Optional[Mapping[str, Any]] = None,
    headers: Optional[Mapping[str, str]] = None,
    timeout: float = 10.0
) -> Dict[str, Any]:
    """
    Send an HTTP GET request to a JSON API and return the parsed data as a dictionary.
    If the JSON response is not an object (e.g., a list or a primitive), it is wrapped
    in a dictionary under the 'data' key.
    Raises requests.HTTPError for non-2xx responses and ValueError for invalid JSON.
    """
    merged_headers: Dict[str, str] = {'Accept': 'application/json'}
    if headers:
        merged_headers.update(headers)

    response = requests.get(url, params=params, headers=merged_headers, timeout=timeout)
    response.raise_for_status()

    try:
        payload = response.json()
    except ValueError as e:
        raise ValueError("Response is not valid JSON") from e

    return payload if isinstance(payload, dict) else {'data': payload}
------------------------------------------------------------

PROMPT 137:
"Generate Python code that creates a basic content provider framework for Zope templates, allowing developers to create, manage, and render reusable template fragments using Zope's template engine."
RESPONSE:
```python
# content_providers.py
# A minimal content provider framework for Zope Page Templates (ZPT).
#
# Dependencies:
#   - zope.pagetemplate
#
# This module lets you:
#   - Register content providers (reusable fragments) by name
#   - Implement providers as Python classes or ZPT-backed templates
#   - Render providers from ZPT using: tal:replace="structure python: provider('name', key='value')"
#
# Notes:
#   - In ZPT, use "structure python: provider(...)" if your provider returns HTML.

from typing import Callable, Dict, Optional, Any
from zope.pagetemplate.pagetemplate import PageTemplate


class ContentProvider:
    """Base class for content providers (reusable fragments)."""

    def __init__(self, name: Optional[str] = None):
        self.name = name
        self.context: Any = None
        self.request: Any = None
        self.options: Dict[str, Any] = {}

    def update(self, context: Any = None, request: Any = None, **options: Any) -> None:
        self.context = context
        self.request = request
        self.options = options or {}

    def render(self) -> str:
        raise NotImplementedError("ContentProvider.render() must be implemented")


class TemplateContentProvider(ContentProvider):
    """A provider backed by a Zope Page Template."""

    def __init__(self, template: PageTemplate, name: Optional[str] = None, registry: "ProviderRegistry" = None):
        super().__init__(name=name)
        self.template = template
        self.registry = registry  # Needed for nested provider() calls

    def render(self) -> str:
        if not self.template:
            return ""
        # Expose context, request, options, and a nested provider() for composition
        namespace = {
            "context": self.context,
            "request": self.request,
            "options": self.options,
            "provider": make_provider_func(self.registry or default_registry, self.context, self.request),
        }
        return self.template.pt_render(namespace)


class ProviderRegistry:
    """A simple registry for content providers."""

    def __init__(self):
        # name -> factory that returns a new ContentProvider
        self._providers: Dict[str, Callable[[], ContentProvider]] = {}

    def register(self, name: str, factory: Callable[[], ContentProvider]) -> None:
        self._providers[name] = factory

    def unregister(self, name: str) -> None:
        self._providers.pop(name, None)

    def get(self, name: str) -> Optional[Callable[[], ContentProvider]]:
        return self._providers.get(name)

    def render(self, name: str, context: Any = None, request: Any = None, **options: Any) -> str:
        factory = self.get(name)
        if factory is None:
            raise KeyError(f"Provider '{name}' is not registered")
        provider = factory()
        # Ensure template providers have a registry for nested calls
        if isinstance(provider, TemplateContentProvider) and provider.registry is None:
            provider.registry = self
        provider.update(context=context, request=request, **options)
        return provider.render()

    def register_template(self, name: str, template: Optional[str] = None, filename: Optional[str] = None) -> None:
        pt = make_page_template(template, filename)

        def factory() -> ContentProvider:
            return TemplateContentProvider(pt, name=name, registry=self)

        self.register(name, factory)


def make_page_template(source: Optional[str] = None, filename: Optional[str] = None) -> PageTemplate:
    """Create a PageTemplate from a string or file."""
    pt = PageTemplate()
    if filename:
        with open(filename, "rb") as f:
            data = f.read()
        if isinstance(data, bytes):
            data = data.decode("utf-8")
    else:
        data = source or ""
    pt.write(data)
    return pt


def make_provider_func(registry: ProviderRegistry, context: Any, request: Any):
    """Return a callable bound to a registry, for use inside template namespaces."""

    def _call(name: str, **options: Any) -> str:
        return registry.render(name, context=context, request=request, **options)

    return _call


def render_template(template_source: str, context: Any = None, request: Any = None,
                    registry: Optional[ProviderRegistry] = None, **extra: Any) -> str:
    """Render a top-level ZPT string with provider() support in the namespace."""
    reg = registry or default_registry
    pt = make_page_template(template_source)
    namespace = {
        "context": context,
        "request": request,
        "provider": make_provider_func(reg, context, request),
    }
    namespace.update(extra or {})
    return pt.pt_render(namespace)


# A default, global registry you can use out of the box
default_registry = ProviderRegistry()


# Example: Register a couple of template-backed providers
default_registry.register_template(
    "header",
    template="""
<header class="site-header">
  <h1 tal:content="options/title">Site Title</h1>
  <nav>
    <ul>
      <li><a tal:attributes="href options/home_url">Home</a></li>
      <li><a tal:attributes="href options/about_url">About</a></li>
    </ul>
  </nav>
</header>
""",
)

default_registry.register_template(
    "footer",
    template="""
<footer class="site-footer">
  <span>&copy; <span tal:content="options/year">2025</span></span>
</footer>
""",
)

# Example: A Python-based provider (no template)
class GreetingProvider(ContentProvider):
    def render(self) -> str:
        name = self.options.get("name") or "friend"
        return f"<div class='greeting'>Hello, {name}!</div>"

default_registry.register("greeting", lambda: GreetingProvider("greeting"))


# Demo usage: render a page that composes providers
if __name__ == "__main__":
    page = """
<html>
  <body>
    <div tal:replace="structure python: provider('header', title='My App', home_url='/', about_url='/about')"></div>

    <main>
      <h2>Welcome</h2>
      <p tal:content="context/message">Fallback message</p>

      <!-- Demonstrates a Python-based provider -->
      <div tal:replace="structure python: provider('greeting', name='Zope User')"></div>
    </main>

    <div tal:replace="structure python: provider('footer', year=2025)"></div>
  </body>
</html>
"""
    ctx = {"message": "This page is composed with content providers."}
    html = render_template(page, context=ctx)
    print(html)
```
------------------------------------------------------------

PROMPT 138:
"Generate Python code that imports the necessary modules and creates wrappers for the CoreMotion framework on macOS, allowing developers to access the framework's functionality from their Python scripts."
RESPONSE:
# CoreMotion Python wrappers for macOS using PyObjC
# Requirements:
#   pip install pyobjc-core pyobjc-framework-Cocoa pyobjc-framework-CoreMotion
#
# This module provides thin, Pythonic wrappers around CoreMotion to access
# accelerometer, gyroscope, magnetometer, and device motion data.

import sys
import time
from typing import Callable, Optional, Any, Dict

try:
    import objc
    from Foundation import NSObject, NSOperationQueue
except Exception as e:
    raise RuntimeError("Foundation/PyObjC not available. Install with: pip install pyobjc-core pyobjc-framework-Cocoa") from e

# Try to import the CoreMotion wrappers. If not available, attempt to load the framework.
try:
    from CoreMotion import (
        CMMotionManager,
    )
    CORE_MOTION_IMPORTED = True
except Exception:
    CORE_MOTION_IMPORTED = False
    try:
        # Fallback: dynamically load the framework and look up classes
        bundle_path = objc.pathForFramework("/System/Library/Frameworks/CoreMotion.framework")
        if not bundle_path:
            raise RuntimeError("CoreMotion.framework not found")
        objc.loadBundle("CoreMotion", bundle_path=bundle_path, globals=globals())
        CMMotionManager = objc.lookUpClass("CMMotionManager")
        CORE_MOTION_IMPORTED = True
    except Exception as e:
        raise RuntimeError(
            "Unable to import or load CoreMotion. Install pyobjc-framework-CoreMotion or ensure CoreMotion.framework is present."
        ) from e

if sys.platform != "darwin":
    raise RuntimeError("CoreMotion is only available on macOS/iOS. Detected platform: {}".format(sys.platform))


def _struct_to_dict(s: Any, fields: tuple) -> Dict[str, float]:
    out = {}
    # PyObjC exposes struct fields as attributes (e.g., s.x) or sequence-like
    for i, name in enumerate(fields):
        val = None
        if hasattr(s, name):
            val = getattr(s, name)
        else:
            try:
                val = s[i]
            except Exception:
                pass
        out[name] = float(val) if val is not None else None
    return out


def _acceleration_to_dict(accel: Any) -> Dict[str, float]:
    return _struct_to_dict(accel, ("x", "y", "z"))


def _rotation_rate_to_dict(rr: Any) -> Dict[str, float]:
    return _struct_to_dict(rr, ("x", "y", "z"))


def _magnetic_field_to_dict(mf: Any) -> Dict[str, float]:
    # For CMMagnetometerData.magneticField -> CMMagneticField {x,y,z}
    # For CMCalibratedMagneticField, the struct has 'field' and 'accuracy'.
    if hasattr(mf, "x") or (isinstance(mf, tuple) and len(mf) >= 3):
        return _struct_to_dict(mf, ("x", "y", "z"))
    # CMCalibratedMagneticField-like
    out = {}
    field = getattr(mf, "field", None)
    if field is not None:
        out.update(_struct_to_dict(field, ("x", "y", "z")))
    acc = getattr(mf, "accuracy", None)
    if acc is not None:
        out["accuracy"] = int(acc)
    return out


def _attitude_to_dict(att: Any) -> Dict[str, Any]:
    d = {}
    for name in ("roll", "pitch", "yaw"):
        if hasattr(att, name):
            d[name] = float(getattr(att, name))
    quat = getattr(att, "quaternion", None)
    if quat is not None:
        d["quaternion"] = _struct_to_dict(quat, ("x", "y", "z", "w"))
    rotm = getattr(att, "rotationMatrix", None)
    if rotm is not None:
        # rotationMatrix has m11..m33 fields
        rm_fields = (
            "m11", "m12", "m13",
            "m21", "m22", "m23",
            "m31", "m32", "m33",
        )
        d["rotationMatrix"] = _struct_to_dict(rotm, rm_fields)
    return d


def _safe_timestamp(obj: Any) -> Optional[float]:
    ts = getattr(obj, "timestamp", None)
    if ts is None:
        return None
    try:
        return float(ts)
    except Exception:
        return None


class MotionManager:
    """
    Pythonic wrapper around CMMotionManager.

    Provides:
      - start/stop accelerometer, gyroscope, magnetometer, device motion
      - synchronous getters for last-known samples
      - asynchronous handlers via NSOperationQueue

    Note: On many Mac models, not all sensors are available.
    """

    def __init__(self, queue: Optional[NSOperationQueue] = None):
        self._cm = CMMotionManager.alloc().init()
        self._queue = queue or NSOperationQueue.alloc().init()

    # Availability
    @property
    def accelerometer_available(self) -> bool:
        return bool(self._cm.isAccelerometerAvailable())

    @property
    def gyro_available(self) -> bool:
        return bool(self._cm.isGyroAvailable())

    @property
    def magnetometer_available(self) -> bool:
        return bool(self._cm.isMagnetometerAvailable())

    @property
    def device_motion_available(self) -> bool:
        return bool(self._cm.isDeviceMotionAvailable())

    # Update intervals (seconds)
    def set_accelerometer_interval(self, seconds: float) -> None:
        self._cm.setAccelerometerUpdateInterval_(float(seconds))

    def set_gyro_interval(self, seconds: float) -> None:
        self._cm.setGyroUpdateInterval_(float(seconds))

    def set_magnetometer_interval(self, seconds: float) -> None:
        self._cm.setMagnetometerUpdateInterval_(float(seconds))

    def set_device_motion_interval(self, seconds: float) -> None:
        self._cm.setDeviceMotionUpdateInterval_(float(seconds))

    # Start/stop with optional Python handler
    def start_accelerometer(self, interval: Optional[float] = None,
                            handler: Optional[Callable[[Dict[str, Any], Optional[Exception]], None]] = None,
                            queue: Optional[NSOperationQueue] = None) -> None:
        if not self.accelerometer_available:
            raise RuntimeError("Accelerometer not available on this device")
        if interval is not None:
            self.set_accelerometer_interval(interval)
        if handler is None:
            self._cm.startAccelerometerUpdates()
            return

        q = queue or self._queue

        def _handler(data, error):
            py_err = Exception(str(error)) if error is not None else None
            payload = None
            if data is not None:
                accel = data.acceleration()
                payload = {
                    "acceleration": _acceleration_to_dict(accel),
                    "timestamp": _safe_timestamp(data),
                }
            handler(payload, py_err)

        self._cm.startAccelerometerUpdatesToQueue_withHandler_(q, _handler)

    def stop_accelerometer(self) -> None:
        self._cm.stopAccelerometerUpdates()

    def start_gyro(self, interval: Optional[float] = None,
                   handler: Optional[Callable[[Dict[str, Any], Optional[Exception]], None]] = None,
                   queue: Optional[NSOperationQueue] = None) -> None:
        if not self.gyro_available:
            raise RuntimeError("Gyroscope not available on this device")
        if interval is not None:
            self.set_gyro_interval(interval)
        if handler is None:
            self._cm.startGyroUpdates()
            return

        q = queue or self._queue

        def _handler(data, error):
            py_err = Exception(str(error)) if error is not None else None
            payload = None
            if data is not None:
                rr = data.rotationRate()
                payload = {
                    "rotationRate": _rotation_rate_to_dict(rr),
                    "timestamp": _safe_timestamp(data),
                }
            handler(payload, py_err)

        self._cm.startGyroUpdatesToQueue_withHandler_(q, _handler)

    def stop_gyro(self) -> None:
        self._cm.stopGyroUpdates()

    def start_magnetometer(self, interval: Optional[float] = None,
                           handler: Optional[Callable[[Dict[str, Any], Optional[Exception]], None]] = None,
                           queue: Optional[NSOperationQueue] = None) -> None:
        if not self.magnetometer_available:
            raise RuntimeError("Magnetometer not available on this device")
        if interval is not None:
            self.set_magnetometer_interval(interval)
        if handler is None:
            self._cm.startMagnetometerUpdates()
            return

        q = queue or self._queue

        def _handler(data, error):
            py_err = Exception(str(error)) if error is not None else None
            payload = None
            if data is not None:
                mf = data.magneticField()
                payload = {
                    "magneticField": _magnetic_field_to_dict(mf),
                    "timestamp": _safe_timestamp(data),
                }
            handler(payload, py_err)

        self._cm.startMagnetometerUpdatesToQueue_withHandler_(q, _handler)

    def stop_magnetometer(self) -> None:
        self._cm.stopMagnetometerUpdates()

    def start_device_motion(self, interval: Optional[float] = None,
                            handler: Optional[Callable[[Dict[str, Any], Optional[Exception]], None]] = None,
                            queue: Optional[NSOperationQueue] = None) -> None:
        if not self.device_motion_available:
            raise RuntimeError("Device motion not available on this device")
        if interval is not None:
            self.set_device_motion_interval(interval)
        if handler is None:
            self._cm.startDeviceMotionUpdates()
            return

        q = queue or self._queue

        def _handler(data, error):
            py_err = Exception(str(error)) if error is not None else None
            payload = None
            if data is not None:
                payload = {
                    "attitude": _attitude_to_dict(data.attitude()),
                    "gravity": _acceleration_to_dict(data.gravity()),
                    "userAcceleration": _acceleration_to_dict(data.userAcceleration()),
                    "rotationRate": _rotation_rate_to_dict(data.rotationRate()),
                    "magneticField": _magnetic_field_to_dict(getattr(data, "magneticField", lambda: None)() if hasattr(data, "magneticField") else None) if hasattr(data, "magneticField") else None,
                    "heading": float(data.heading()) if hasattr(data, "heading") else None,
                    "timestamp": _safe_timestamp(data),
                }
            handler(payload, py_err)

        self._cm.startDeviceMotionUpdatesToQueue_withHandler_(q, _handler)

    def stop_device_motion(self) -> None:
        self._cm.stopDeviceMotionUpdates()

    # Synchronous "last value" getters. Note: return None until updates have started.
    def accelerometer_data(self) -> Optional[Dict[str, Any]]:
        d = self._cm.accelerometerData()
        if d is None:
            return None
        accel = d.acceleration()
        return {
            "acceleration": _acceleration_to_dict(accel),
            "timestamp": _safe_timestamp(d),
        }

    def gyro_data(self) -> Optional[Dict[str, Any]]:
        d = self._cm.gyroData()
        if d is None:
            return None
        rr = d.rotationRate()
        return {
            "rotationRate": _rotation_rate_to_dict(rr),
            "timestamp": _safe_timestamp(d),
        }

    def magnetometer_data(self) -> Optional[Dict[str, Any]]:
        d = self._cm.magnetometerData()
        if d is None:
            return None
        mf = d.magneticField()
        return {
            "magneticField": _magnetic_field_to_dict(mf),
            "timestamp": _safe_timestamp(d),
        }

    def device_motion_data(self) -> Optional[Dict[str, Any]]:
        d = self._cm.deviceMotion()
        if d is None:
            return None
        return {
            "attitude": _attitude_to_dict(d.attitude()),
            "gravity": _acceleration_to_dict(d.gravity()),
            "userAcceleration": _acceleration_to_dict(d.userAcceleration()),
            "rotationRate": _rotation_rate_to_dict(d.rotationRate()),
            "magneticField": _magnetic_field_to_dict(getattr(d, "magneticField", lambda: None)() if hasattr(d, "magneticField") else None) if hasattr(d, "magneticField") else None,
            "heading": float(d.heading()) if hasattr(d, "heading") else None,
            "timestamp": _safe_timestamp(d),
        }


# Example usage (poll accelerometer for ~2 seconds if available)
if __name__ == "__main__":
    mm = MotionManager()
    if mm.accelerometer_available:
        print("Starting accelerometer (50 Hz)...")
        mm.start_accelerometer(interval=1.0 / 50.0)
        t0 = time.time()
        try:
            while time.time() - t0 < 2.0:
                data = mm.accelerometer_data()
                if data is not None:
                    print(data)
                time.sleep(0.05)
        finally:
            mm.stop_accelerometer()
    else:
        print("Accelerometer not available on this Mac.")

    if mm.device_motion_available:
        print("Starting device motion stream (callback) at 20 Hz...")
        def handler(payload, err):
            if err:
                print("Device motion error:", err)
                return
            # Print a compact line with roll/pitch/yaw
            att = payload.get("attitude", {})
            print("r={:.3f}, p={:.3f}, y={:.3f}".format(
                att.get("roll", float("nan")),
                att.get("pitch", float("nan")),
                att.get("yaw", float("nan")),
            ))

        mm.start_device_motion(interval=1.0 / 20.0, handler=handler)
        time.sleep(2.0)
        mm.stop_device_motion()
    else:
        print("Device motion not available on this Mac.")
------------------------------------------------------------

PROMPT 139:
"Generate Python code that imports the google-cloud-datastore library and creates a client object to interact with a Google Cloud Datastore database, allowing for CRUD operations on entities and queries."
RESPONSE:
# pip install google-cloud-datastore
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union
from google.cloud import datastore

# Create a Datastore client. Uses ADC or GOOGLE_CLOUD_PROJECT env var if project not specified.
client = datastore.Client()  # or datastore.Client(project="your-gcp-project-id")

KeyLike = Union[datastore.Key, Tuple[str, Union[int, str]]]


def make_key(kind: str, id_or_name: Optional[Union[int, str]] = None, parent: Optional[datastore.Key] = None) -> datastore.Key:
    if parent is not None:
        if id_or_name is None:
            return client.key(kind, parent=parent)
        return client.key(parent.kind, parent.id or parent.name, kind, id_or_name)
    if id_or_name is None:
        return client.key(kind)  # incomplete key; ID allocated on put
    return client.key(kind, id_or_name)


def create_entity(kind: str, data: Dict[str, Any], id_or_name: Optional[Union[int, str]] = None, parent: Optional[datastore.Key] = None) -> datastore.Entity:
    key = make_key(kind, id_or_name=id_or_name, parent=parent)
    entity = datastore.Entity(key=key)
    entity.update(data)
    client.put(entity)  # allocates ID if key is incomplete
    return entity


def get_entity(key_or_kind: KeyLike) -> Optional[datastore.Entity]:
    if isinstance(key_or_kind, datastore.Key):
        return client.get(key_or_kind)
    kind, id_or_name = key_or_kind
    return client.get(make_key(kind, id_or_name))


def upsert_entity(kind: str, data: Dict[str, Any], id_or_name: Union[int, str], parent: Optional[datastore.Key] = None) -> datastore.Entity:
    key = make_key(kind, id_or_name=id_or_name, parent=parent)
    entity = datastore.Entity(key=key)
    entity.update(data)
    client.put(entity)
    return entity


def update_entity(key: datastore.Key, updates: Dict[str, Any]) -> Optional[datastore.Entity]:
    entity = client.get(key)
    if not entity:
        return None
    entity.update(updates)
    client.put(entity)
    return entity


def delete_entity(key_or_kind: KeyLike) -> None:
    if isinstance(key_or_kind, datastore.Key):
        client.delete(key_or_kind)
    else:
        kind, id_or_name = key_or_kind
        client.delete(make_key(kind, id_or_name))


def query_entities(
    kind: str,
    filters: Optional[Sequence[Tuple[str, str, Any]]] = None,  # e.g., [("done", "=", False)]
    order: Optional[Union[str, Sequence[str]]] = None,         # e.g., "created" or ["-priority", "created"]
    limit: Optional[int] = None,
    offset: Optional[int] = None,
    keys_only: bool = False,
    projection: Optional[Sequence[str]] = None,
    ancestor: Optional[datastore.Key] = None,
) -> List[datastore.Entity]:
    q = client.query(kind=kind, projection=projection)
    if keys_only:
        q.keys_only()
    if ancestor is not None:
        q.ancestor = ancestor
    if filters:
        for prop, op, val in filters:
            q.add_filter(prop, op, val)
    if order:
        q.order = order
    return list(q.fetch(limit=limit, offset=offset))


# Example usage
if __name__ == "__main__":
    KIND = "Task"

    # Create
    task = create_entity(KIND, {"title": "Buy milk", "done": False, "priority": 2})
    print("Created:", task.key, dict(task))

    # Read
    fetched = get_entity(task.key)
    print("Fetched:", fetched.key if fetched else None, dict(fetched) if fetched else None)

    # Query
    results = query_entities(
        KIND,
        filters=[("done", "=", False)],
        order=["-priority", "title"],
        limit=10,
    )
    print("Query results:", [(e.key.id or e.key.name, dict(e)) for e in results])

    # Update
    updated = update_entity(task.key, {"done": True})
    print("Updated:", updated.key if updated else None, dict(updated) if updated else None)

    # Upsert (create or replace specific ID/name)
    upserted = upsert_entity(KIND, {"title": "Walk dog", "done": False}, id_or_name="task-walk-dog")
    print("Upserted:", upserted.key, dict(upserted))

    # Delete
    delete_entity(task.key)
    delete_entity((KIND, "task-walk-dog"))
    print("Deleted both entities")
------------------------------------------------------------

PROMPT 140:
"Generate Python code that imports the google-cloud-datastore library and creates a client object to interact with a Google Cloud Datastore database, allowing for CRUD operations on entities and queries."
RESPONSE:
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union
from google.cloud import datastore


class DatastoreService:
    """
    Simple helper for CRUD and query operations using google-cloud-datastore.
    Authentication and project selection follow the standard Google Cloud SDK flow,
    or you can pass project and namespace explicitly.
    """

    def __init__(self, project: Optional[str] = None, namespace: Optional[str] = None):
        # If project is None, the library will infer it from environment or ADC.
        self.client = datastore.Client(project=project, namespace=namespace)

    def _build_key(
        self,
        kind: str,
        key_id: Optional[int] = None,
        key_name: Optional[str] = None,
    ) -> datastore.Key:
        if key_id is not None and key_name is not None:
            raise ValueError("Specify only one of key_id or key_name.")
        if key_id is not None:
            return self.client.key(kind, int(key_id))
        if key_name is not None:
            return self.client.key(kind, str(key_name))
        # Incomplete key (Datastore will allocate an ID on put)
        return self.client.key(kind)

    # CREATE
    def create(
        self,
        kind: str,
        properties: Dict[str, Any],
        key_id: Optional[int] = None,
        key_name: Optional[str] = None,
    ) -> datastore.Entity:
        key = self._build_key(kind, key_id=key_id, key_name=key_name)
        entity = datastore.Entity(key=key)
        entity.update(properties or {})
        self.client.put(entity)
        return entity

    # READ (get by key)
    def get(
        self,
        kind: str,
        key_id: Optional[int] = None,
        key_name: Optional[str] = None,
    ) -> Optional[datastore.Entity]:
        key = self._build_key(kind, key_id=key_id, key_name=key_name)
        return self.client.get(key)

    # UPDATE (merge properties into existing entity)
    def update(
        self,
        kind: str,
        updates: Dict[str, Any],
        key_id: Optional[int] = None,
        key_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> datastore.Entity:
        key = self._build_key(kind, key_id=key_id, key_name=key_name)
        entity = self.client.get(key)
        if entity is None:
            if not create_if_missing:
                raise KeyError("Entity not found for update.")
            entity = datastore.Entity(key=key)
        entity.update(updates or {})
        self.client.put(entity)
        return entity

    # UPSERT (put as-is; will create or replace)
    def upsert(self, entity: datastore.Entity) -> None:
        self.client.put(entity)

    # DELETE
    def delete(
        self,
        kind: str,
        key_id: Optional[int] = None,
        key_name: Optional[str] = None,
    ) -> None:
        key = self._build_key(kind, key_id=key_id, key_name=key_name)
        self.client.delete(key)

    # QUERY (property filters, ordering, pagination)
    def query(
        self,
        kind: str,
        filters: Optional[Iterable[Tuple[str, str, Any]]] = None,
        order: Optional[Union[str, Sequence[str]]] = None,
        limit: Optional[int] = None,
        start_cursor: Optional[bytes] = None,
        projection: Optional[Sequence[str]] = None,
        keys_only: bool = False,
    ) -> Tuple[List[datastore.Entity], Optional[bytes]]:
        """
        filters: iterable of (property, operator, value), e.g. [("done", "=", False)]
        order: string or sequence of strings; prefix with "-" for descending, e.g. ["-created", "priority"]
        projection: list of property names to project
        keys_only: if True, returns entities with only keys (no properties)
        Returns (entities, next_page_cursor)
        """
        q = self.client.query(kind=kind)
        if projection:
            q.projection = list(projection)
        if keys_only:
            q.keys_only()

        if filters:
            for prop, op, val in filters:
                q.add_filter(prop, op, val)
        if order:
            if isinstance(order, str):
                q.order = [order]
            else:
                q.order = list(order)

        iterator = q.fetch(limit=limit, start_cursor=start_cursor)
        entities = list(iterator)
        next_cursor = iterator.next_page_token
        return entities, next_cursor

    # TRANSACTION (optional helper)
    def in_transaction(self):
        return self.client.transaction()


# Example usage:
if __name__ == "__main__":
    # Create a client (uses default credentials and project if not provided)
    ds = DatastoreService(project="YOUR_GCP_PROJECT_ID", namespace=None)

    # Create
    created = ds.create(
        kind="Task",
        properties={"description": "Write docs", "done": False, "priority": 1},
        # key_name="task-1",  # or use key_id=123, or omit both to auto-allocate ID
    )
    print("Created:", created.key)

    # Read
    fetched = ds.get(kind="Task", key_name=created.key.name or None, key_id=created.key.id or None)
    print("Fetched:", fetched)

    # Update
    updated = ds.update(
        kind="Task",
        updates={"done": True},
        key_name=created.key.name or None,
        key_id=created.key.id or None,
    )
    print("Updated:", updated["done"])

    # Query
    results, next_cursor = ds.query(
        kind="Task",
        filters=[("done", "=", True)],
        order=["-priority"],
        limit=10,
    )
    print("Query results:", [e.key.id_or_name for e in results], "next_cursor:", next_cursor)

    # Delete
    ds.delete(kind="Task", key_name=created.key.name or None, key_id=created.key.id or None)
    print("Deleted.")
------------------------------------------------------------

PROMPT 141:
"Generate Python code that imports the necessary modules and creates a DBAPI 2.0 interface and SQLAlchemy dialect for Databricks interactive clusters using the `databricks` library and `sqlalchemy` library."
RESPONSE:
# pip install -U databricks-sql-connector sqlalchemy

import os
from databricks import sql as dbsql  # DBAPI 2.0
import sqlalchemy as sa
import databricks.sqlalchemy  # ensure SQLAlchemy dialect is registered


# Read connection info from environment variables (recommended)
DATABRICKS_SERVER_HOSTNAME = os.getenv("DATABRICKS_SERVER_HOSTNAME", "<your-workspace-hostname>")
DATABRICKS_HTTP_PATH = os.getenv("DATABRICKS_HTTP_PATH", "sql/protocolv1/o/<workspace-id>/<cluster-id>")  # interactive cluster http_path
DATABRICKS_ACCESS_TOKEN = os.getenv("DATABRICKS_ACCESS_TOKEN", "<personal-access-token>")
DATABRICKS_CATALOG = os.getenv("DATABRICKS_CATALOG")  # optional, e.g., "main"
DATABRICKS_SCHEMA = os.getenv("DATABRICKS_SCHEMA")    # optional, e.g., "default"


def get_dbapi_connection(
    server_hostname: str,
    http_path: str,
    access_token: str,
):
    """
    Create a DBAPI 2.0 connection to a Databricks interactive cluster.
    """
    return dbsql.connect(
        server_hostname=server_hostname,
        http_path=http_path,
        access_token=access_token,
    )


def get_sqlalchemy_engine(
    server_hostname: str,
    http_path: str,
    access_token: str,
    catalog: str | None = None,
    schema_: str | None = None,
) -> sa.Engine:
    """
    Create a SQLAlchemy engine using the Databricks dialect targeting an interactive cluster.
    """
    # Connection URL embeds the http_path after the host.
    # Username must be 'token' when using a PAT.
    base_url = f"databricks://token:{access_token}@{server_hostname}:443/{http_path}"

    params = []
    if catalog:
        params.append(f"catalog={catalog}")
    if schema_:
        params.append(f"schema={schema_}")
    if params:
        base_url = base_url + "?" + "&".join(params)

    return sa.create_engine(base_url, pool_pre_ping=True)


if __name__ == "__main__":
    # DBAPI 2.0 usage
    with get_dbapi_connection(
        DATABRICKS_SERVER_HOSTNAME,
        DATABRICKS_HTTP_PATH,
        DATABRICKS_ACCESS_TOKEN,
    ) as conn:
        with conn.cursor() as cur:
            cur.execute("SELECT current_user(), current_catalog(), current_schema()")
            print(cur.fetchall())

    # SQLAlchemy usage
    engine = get_sqlalchemy_engine(
        DATABRICKS_SERVER_HOSTNAME,
        DATABRICKS_HTTP_PATH,
        DATABRICKS_ACCESS_TOKEN,
        catalog=DATABRICKS_CATALOG,
        schema_=DATABRICKS_SCHEMA,
    )
    with engine.connect() as conn:
        result = conn.execute(sa.text("SELECT 1 AS one"))
        print(result.fetchall())
------------------------------------------------------------

PROMPT 142:
"Generate Python code that creates a data framework for biology, complete with data models for various types of biological data, querying and analysis tools, and integration with popular bioinformatics tools."
RESPONSE:
```python
"""
bioframework.py

A lightweight Python framework for biological data with:
- Data models for sequences, genes, proteins, variants, samples, alignments, and expression matrices
- Querying utilities
- Analysis tools for sequences, variants, and gene expression
- Integrations with popular bioinformatics tools (optional): Biopython, pysam, NCBI Entrez/BLAST, and more

Dependencies:
  Required: pandas, numpy
  Optional (features enabled if installed):
    - biopython (SeqIO, Seq, Entrez, BLAST)
    - pysam (BAM/VCF/CRAM reading)
    - scipy (stats for differential expression)
    - requests (HTTP APIs e.g., Ensembl/UniProt)

Install:
  pip install pandas numpy
  pip install biopython pysam scipy requests

Author: You
License: MIT
"""

from __future__ import annotations

from dataclasses import dataclass, field, asdict
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union
import re
import json
import subprocess
import tempfile
import shutil
import os
from collections import defaultdict

import numpy as np
import pandas as pd

# Optional dependencies
try:
    from Bio import SeqIO
    from Bio.Seq import Seq
    from Bio.Blast import NCBIWWW, NCBIXML
    from Bio import Entrez as _Entrez
    BIOPYTHON_AVAILABLE = True
except Exception:
    SeqIO = None
    Seq = None
    NCBIWWW = None
    NCBIXML = None
    _Entrez = None
    BIOPYTHON_AVAILABLE = False

try:
    import pysam
    PYSAM_AVAILABLE = True
except Exception:
    pysam = None
    PYSAM_AVAILABLE = False

try:
    from scipy import stats as _spstats
    SCIPY_AVAILABLE = True
except Exception:
    _spstats = None
    SCIPY_AVAILABLE = False

try:
    import requests as _requests
    REQUESTS_AVAILABLE = True
except Exception:
    _requests = None
    REQUESTS_AVAILABLE = False


# ----------------------------
# Data models
# ----------------------------

@dataclass
class SequenceRecord:
    id: str
    description: str
    sequence: str
    molecule_type: str = "DNA"  # "DNA", "RNA", "protein"
    metadata: Dict[str, Any] = field(default_factory=dict)

    def gc_content(self) -> float:
        seq = self.sequence.upper()
        if not seq:
            return 0.0
        gc = seq.count("G") + seq.count("C")
        return 100.0 * gc / len(seq)

    def reverse_complement(self) -> "SequenceRecord":
        if self.molecule_type.upper() not in ("DNA", "RNA"):
            raise ValueError("Reverse complement only valid for DNA/RNA.")
        if BIOPYTHON_AVAILABLE:
            rseq = str(Seq(self.sequence).reverse_complement())
        else:
            # Simple fallback for DNA
            comp = str.maketrans("ACGTacgtNn", "TGCAtgcaNn")
            rseq = self.sequence.translate(comp)[::-1]
        return SequenceRecord(
            id=f"{self.id}_revcomp",
            description=f"{self.description} (reverse complement)",
            sequence=rseq,
            molecule_type=self.molecule_type,
            metadata=self.metadata.copy(),
        )

    def translate(self, to_stop: bool = True) -> "SequenceRecord":
        if self.molecule_type.upper() not in ("DNA", "RNA"):
            raise ValueError("Translate expects DNA/RNA.")
        if BIOPYTHON_AVAILABLE:
            pep = str(Seq(self.sequence).translate(to_stop=to_stop))
        else:
            # Minimalistic translation (assumes DNA; U->T)
            codon_table = {
                # Only a subset; recommend Biopython for full table
                "TTT": "F","TTC": "F","TTA": "L","TTG": "L",
                "CTT": "L","CTC": "L","CTA": "L","CTG": "L",
                "ATT": "I","ATC": "I","ATA": "I","ATG": "M",
                "GTT": "V","GTC": "V","GTA": "V","GTG": "V",
                "TCT": "S","TCC": "S","TCA": "S","TCG": "S",
                "CCT": "P","CCC": "P","CCA": "P","CCG": "P",
                "ACT": "T","ACC": "T","ACA": "T","ACG": "T",
                "GCT": "A","GCC": "A","GCA": "A","GCG": "A",
                "TAT": "Y","TAC": "Y","TAA": "*","TAG": "*",
                "CAT": "H","CAC": "H","CAA": "Q","CAG": "Q",
                "AAT": "N","AAC": "N","AAA": "K","AAG": "K",
                "GAT": "D","GAC": "D","GAA": "E","GAG": "E",
                "TGT": "C","TGC": "C","TGA": "*","TGG": "W",
                "CGT": "R","CGC": "R","CGA": "R","CGG": "R",
                "AGT": "S","AGC": "S","AGA": "R","AGG": "R",
                "GGT": "G","GGC": "G","GGA": "G","GGG": "G",
            }
            dna = self.sequence.upper().replace("U", "T")
            pep_list = []
            for i in range(0, len(dna) - 2, 3):
                aa = codon_table.get(dna[i:i+3], "X")
                if to_stop and aa == "*":
                    break
                pep_list.append(aa)
            pep = "".join(pep_list)
        return SequenceRecord(
            id=f"{self.id}_translated",
            description=f"{self.description} (translated)",
            sequence=pep,
            molecule_type="protein",
            metadata=self.metadata.copy(),
        )


@dataclass
class Gene:
    id: str
    name: Optional[str]
    chromosome: str
    start: int
    end: int
    strand: str  # "+"/"-"
    biotype: Optional[str] = None
    transcripts: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def length(self) -> int:
        return max(0, self.end - self.start + 1)


@dataclass
class Protein:
    id: str
    gene_id: Optional[str]
    name: Optional[str]
    sequence: str
    length: int = field(init=False)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        self.length = len(self.sequence)


@dataclass
class Variant:
    id: str
    chrom: str
    pos: int
    ref: str
    alt: str
    sample_id: Optional[str] = None
    gene: Optional[str] = None
    consequence: Optional[str] = None
    info: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Sample:
    id: str
    species: Optional[str] = None
    tissue: Optional[str] = None
    condition: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Alignment:
    id: str
    sample_id: str
    reference: Optional[str]
    path: str  # path to BAM/CRAM/SAM
    index_path: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ExpressionMatrix:
    """
    Expression matrix with genes as index and samples as columns.
    """
    data: pd.DataFrame  # rows: gene_id, columns: sample_id

    @staticmethod
    def from_long_table(df: pd.DataFrame, gene_col: str, sample_col: str, value_col: str) -> "ExpressionMatrix":
        pivot = df.pivot(index=gene_col, columns=sample_col, values=value_col).fillna(0.0)
        return ExpressionMatrix(pivot)

    @staticmethod
    def from_csv(path: str, sep: str = ",", index_col: Optional[str] = 0) -> "ExpressionMatrix":
        df = pd.read_csv(path, sep=sep, index_col=index_col)
        return ExpressionMatrix(df)

    def genes(self) -> List[str]:
        return list(self.data.index.astype(str))

    def samples(self) -> List[str]:
        return list(self.data.columns.astype(str))

    def subset(self, gene_ids: Optional[List[str]] = None, sample_ids: Optional[List[str]] = None) -> "ExpressionMatrix":
        sub = self.data
        if gene_ids is not None:
            sub = sub.loc[list(set(gene_ids).intersection(sub.index)), :]
        if sample_ids is not None:
            sub = sub.loc[:, list(set(sample_ids).intersection(sub.columns))]
        return ExpressionMatrix(sub)

    def tpm(self, gene_lengths: Dict[str, Union[int, float]]) -> "ExpressionMatrix":
        """
        TPM normalization from raw counts given gene lengths (bp).
        """
        lengths = pd.Series(gene_lengths, dtype=float)
        lengths = lengths.reindex(self.data.index)
        if lengths.isna().any():
            raise ValueError("Missing gene lengths for some genes.")
        rpk = self.data.div(lengths / 1000.0, axis=0)
        per_sample_scaling = rpk.sum(axis=0) / 1e6
        tpm = rpk.div(per_sample_scaling, axis=1)
        return ExpressionMatrix(tpm)

    def log2(self, pseudocount: float = 1.0) -> "ExpressionMatrix":
        return ExpressionMatrix(np.log2(self.data + pseudocount))

    def zscore_by_gene(self) -> "ExpressionMatrix":
        z = self.data.apply(lambda x: (x - x.mean()) / (x.std(ddof=0) + 1e-9), axis=1)
        return ExpressionMatrix(z)

    def differential_expression(self, group_a: List[str], group_b: List[str]) -> pd.DataFrame:
        """
        Naive differential expression between two groups of samples.
        Returns DataFrame with log2FC and optional p-values (if scipy installed).
        """
        a = self.subset(sample_ids=group_a).data
        b = self.subset(sample_ids=group_b).data
        # Align indices
        a = a.reindex(self.data.index, fill_value=np.nan)
        b = b.reindex(self.data.index, fill_value=np.nan)

        mean_a = a.mean(axis=1)
        mean_b = b.mean(axis=1)
        log2fc = np.log2((mean_a + 1.0) / (mean_b + 1.0))

        res = pd.DataFrame({"log2FC": log2fc})
        if SCIPY_AVAILABLE:
            pvals = []
            for gene in self.data.index:
                xa = a.loc[gene].dropna().values
                xb = b.loc[gene].dropna().values
                if len(xa) >= 2 and len(xb) >= 2:
                    stat, pv = _spstats.ttest_ind(xa, xb, equal_var=False)
                else:
                    pv = np.nan
                pvals.append(pv)
            res["pval"] = pvals
            res["pval_adj_bh"] = _benjamini_hochberg(res["pval"].values)
        return res.sort_values("log2FC", ascending=False)


def _benjamini_hochberg(pvals: Iterable[Optional[float]]) -> List[Optional[float]]:
    arr = np.array([np.inf if (p is None or (isinstance(p, float) and np.isnan(p))) else p for p in pvals], dtype=float)
    n = np.sum(np.isfinite(arr))
    if n == 0:
        return [None if not np.isfinite(p) else p for p in arr]
    idx = np.argsort(arr)
    ranks = np.empty_like(idx)
    ranks[idx] = np.arange(1, len(arr) + 1)
    adj = arr * len(arr) / ranks
    # monotonic
    adj_sorted = np.minimum.accumulate(adj[idx[::-1]])[::-1]
    adj_final = adj_sorted[np.argsort(idx)]
    adj_final = np.minimum(adj_final, 1.0)
    out = []
    for p, a in zip(arr, adj_final):
        out.append(None if not np.isfinite(p) else float(a))
    return out


# ----------------------------
# Query engine and in-memory DB
# ----------------------------

class Query:
    def __init__(self, items: Iterable[Any]):
        self._items = list(items)

    def filter(self, fn: Callable[[Any], bool]) -> "Query":
        return Query([x for x in self._items if fn(x)])

    def where(self, **kwargs) -> "Query":
        # Simple equality or regex match: value can be str or compiled regex
        def predicate(x: Any) -> bool:
            for k, v in kwargs.items():
                val = getattr(x, k, None)
                if hasattr(v, "search"):  # regex
                    if not (isinstance(val, str) and v.search(val)):
                        return False
                else:
                    if val != v:
                        return False
            return True
        return self.filter(predicate)

    def to_list(self) -> List[Any]:
        return list(self._items)

    def first(self) -> Optional[Any]:
        return self._items[0] if self._items else None

    def count(self) -> int:
        return len(self._items)


class BioDB:
    def __init__(self):
        self.sequences: Dict[str, SequenceRecord] = {}
        self.genes: Dict[str, Gene] = {}
        self.proteins: Dict[str, Protein] = {}
        self.variants: Dict[str, Variant] = {}
        self.samples: Dict[str, Sample] = {}
        self.alignments: Dict[str, Alignment] = {}
        self.expression: Optional[ExpressionMatrix] = None

    # Adders
    def add_sequence(self, seq: SequenceRecord): self.sequences[seq.id] = seq
    def add_gene(self, gene: Gene): self.genes[gene.id] = gene
    def add_protein(self, prot: Protein): self.proteins[prot.id] = prot
    def add_variant(self, var: Variant): self.variants[var.id] = var
    def add_sample(self, samp: Sample): self.samples[samp.id] = samp
    def add_alignment(self, aln: Alignment): self.alignments[aln.id] = aln
    def set_expression(self, expr: ExpressionMatrix): self.expression = expr

    # Queries
    def query_sequences(self) -> Query: return Query(self.sequences.values())
    def query_genes(self) -> Query: return Query(self.genes.values())
    def query_proteins(self) -> Query: return Query(self.proteins.values())
    def query_variants(self) -> Query: return Query(self.variants.values())
    def query_samples(self) -> Query: return Query(self.samples.values())
    def query_alignments(self) -> Query: return Query(self.alignments.values())

    # Convenience
    def find_genes_by_region(self, chrom: str, start: int, end: int) -> List[Gene]:
        return [
            g for g in self.genes.values()
            if g.chromosome == chrom and not (g.end < start or g.start > end)
        ]

    def find_variants_by_region(self, chrom: str, start: int, end: int) -> List[Variant]:
        return [v for v in self.variants.values() if v.chrom == chrom and start <= v.pos <= end]

    def samples_by_condition(self, condition: str) -> List[str]:
        return [s.id for s in self.samples.values() if s.condition == condition]

    def to_json(self, path: str):
        bundle = {
            "sequences": [asdict(x) for x in self.sequences.values()],
            "genes": [asdict(x) for x in self.genes.values()],
            "proteins": [asdict(x) for x in self.proteins.values()],
            "variants": [asdict(x) for x in self.variants.values()],
            "samples": [asdict(x) for x in self.samples.values()],
            "alignments": [asdict(x) for x in self.alignments.values()],
            "expression": self.expression.data.to_dict(orient="split") if self.expression else None,
        }
        with open(path, "w") as f:
            json.dump(bundle, f)

    @staticmethod
    def from_json(path: str) -> "BioDB":
        with open(path, "r") as f:
            bundle = json.load(f)
        db = BioDB()
        for d in bundle.get("sequences", []): db.add_sequence(SequenceRecord(**d))
        for d in bundle.get("genes", []): db.add_gene(Gene(**d))
        for d in bundle.get("proteins", []): db.add_protein(Protein(**d))
        for d in bundle.get("variants", []): db.add_variant(Variant(**d))
        for d in bundle.get("samples", []): db.add_sample(Sample(**d))
        for d in bundle.get("alignments", []): db.add_alignment(Alignment(**d))
        expr = bundle.get("expression")
        if expr is not None:
            df = pd.DataFrame(**expr)  # orient="split"
            df.set_index(df.columns[0], inplace=False)
            db.set_expression(ExpressionMatrix(pd.DataFrame(**expr)))
        return db


# ----------------------------
# IO and integrations
# ----------------------------

class IO:
    @staticmethod
    def parse_fasta(path: str, molecule_type: str = "DNA") -> List[SequenceRecord]:
        seqs = []
        if BIOPYTHON_AVAILABLE:
            for rec in SeqIO.parse(path, "fasta"):
                seqs.append(SequenceRecord(id=rec.id, description=rec.description, sequence=str(rec.seq),
                                           molecule_type=molecule_type))
        else:
            with open(path) as f:
                sid, desc, seq = None, "", []
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    if line.startswith(">"):
                        if sid is not None:
                            seqs.append(SequenceRecord(sid, desc, "".join(seq), molecule_type))
                        parts = line[1:].split(None, 1)
                        sid = parts[0]
                        desc = parts[1] if len(parts) > 1 else parts[0]
                        seq = []
                    else:
                        seq.append(line)
                if sid is not None:
                    seqs.append(SequenceRecord(sid, desc, "".join(seq), molecule_type))
        return seqs

    @staticmethod
    def write_fasta(seqs: List[SequenceRecord], path: str):
        if BIOPYTHON_AVAILABLE:
            from Bio.SeqRecord import SeqRecord
            records = [SeqRecord(Seq(s.sequence), id=s.id, description=s.description) for s in seqs]
            SeqIO.write(records, path, "fasta")
        else:
            with open(path, "w") as f:
                for s in seqs:
                    f.write(f">{s.id} {s.description}\n")
                    # wrap to 60
                    for i in range(0, len(s.sequence), 60):
                        f.write(s.sequence[i:i+60] + "\n")

    @staticmethod
    def read_vcf(path: str) -> List[Variant]:
        variants = []
        if PYSAM_AVAILABLE:
            vf = pysam.VariantFile(path)
            for rec in vf.fetch():
                for alt in rec.alts:
                    vid = f"{rec.chrom}:{rec.pos}:{rec.ref}>{alt}"
                    variants.append(Variant(
                        id=vid, chrom=rec.chrom, pos=rec.pos, ref=rec.ref, alt=alt,
                        info=dict(rec.info)
                    ))
        else:
            with open(path) as f:
                for line in f:
                    if line.startswith("#"):
                        continue
                    chrom, pos, vid, ref, alt, qual, flt, info = line.strip().split("\t", 7)
                    for a in alt.split(","):
                        variants.append(Variant(
                            id=vid if vid != "." else f"{chrom}:{pos}:{ref}>{a}",
                            chrom=chrom, pos=int(pos), ref=ref, alt=a,
                            info=_parse_vcf_info(info)
                        ))
        return variants

    @staticmethod
    def bam_stats(path: str) -> Dict[str, Any]:
        if not PYSAM_AVAILABLE:
            raise RuntimeError("pysam not available. pip install pysam")
        bam = pysam.AlignmentFile(path, "rb")
        stats = {
            "n_ref": bam.nreferences,
            "refs": list(bam.references),
            "mapped": bam.mapped,
            "unmapped": bam.unmapped,
            "lengths": list(bam.lengths),
        }
        bam.close()
        return stats

    @staticmethod
    def entrez_fetch_sequence(accession: str, email: str, api_key: Optional[str] = None) -> Optional[SequenceRecord]:
        if not BIOPYTHON_AVAILABLE:
            raise RuntimeError("Biopython required for Entrez. pip install biopython")
        _Entrez.email = email
        if api_key:
            _Entrez.api_key = api_key
        handle = _Entrez.efetch(db="nucleotide", id=accession, rettype="fasta", retmode="text")
        with tempfile.NamedTemporaryFile("w+", delete=False, suffix=".fasta") as tmp:
            tmp.write(handle.read())
            tmp.flush()
            handle.close()
            seqs = IO.parse_fasta(tmp.name)
        os.unlink(tmp.name)
        return seqs[0] if seqs else None

    @staticmethod
    def blast_remote(query_seq: str, program: str = "blastn", database: str = "nt", hitlist_size: int = 10) -> List[Dict[str, Any]]:
        if not BIOPYTHON_AVAILABLE:
            raise RuntimeError("Biopython required for BLAST. pip install biopython")
        result_handle = NCBIWWW.qblast(program=program, database=database, sequence=query_seq, hitlist_size=hitlist_size)
        blast_record = NCBIXML.read(result_handle)
        hits = []
        for alignment in blast_record.alignments:
            for hsp in alignment.hsps:
                hits.append({
                    "title": alignment.title,
                    "accession": alignment.accession,
                    "length": alignment.length,
                    "score": hsp.score,
                    "evalue": hsp.expect,
                    "identity": hsp.identities,
                    "align_length": hsp.align_length,
                })
        return hits

    @staticmethod
    def blast_local(query_fasta: str, db_path: str, program: str = "blastn", outfmt: int = 6, extra_args: Optional[List[str]] = None) -> pd.DataFrame:
        """
        Run a local BLAST+ search. Requires NCBI BLAST+ binaries in PATH.
        outfmt 6 (tabular) columns used by default.
        """
        cmd = [program, "-query", query_fasta, "-db", db_path, "-outfmt", str(outfmt)]
        if extra_args:
            cmd.extend(extra_args)
        with tempfile.NamedTemporaryFile("w+", delete=False) as tmp:
            tmp_path = tmp.name
        try:
            subprocess.run(cmd + ["-out", tmp_path], check=True, capture_output=True)
            df = pd.read_csv(tmp_path, sep="\t", header=None)
            return df
        finally:
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)

    @staticmethod
    def fetch_uniprot(protein_id: str) -> Dict[str, Any]:
        if not REQUESTS_AVAILABLE:
            raise RuntimeError("requests not available. pip install requests")
        url = f"https://rest.uniprot.org/uniprotkb/{protein_id}.json"
        r = _requests.get(url, timeout=30)
        r.raise_for_status()
        return r.json()

    @staticmethod
    def fetch_ensembl_gene(gene_id: str, species: str = "human") -> Dict[str, Any]:
        if not REQUESTS_AVAILABLE:
            raise RuntimeError("requests not available. pip install requests")
        sp = {"human": "homo_sapiens", "mouse": "mus_musculus"}.get(species.lower(), species)
        url = f"https://rest.ensembl.org/lookup/id/{gene_id}?expand=1"
        headers = {"Content-Type": "application/json"}
        r = _requests.get(url, headers=headers, timeout=30)
        r.raise_for_status()
        return r.json()


def _parse_vcf_info(info: str) -> Dict[str, Any]:
    d = {}
    for field in info.split(";"):
        if "=" in field:
            k, v = field.split("=", 1)
            d[k] = v
        elif field:
            d[field] = True
    return d


# ----------------------------
# Analysis utilities
# ----------------------------

class SeqAnalysis:
    @staticmethod
    def kmer_counts(sequence: str, k: int) -> Dict[str, int]:
        seq = sequence.upper()
        counts = defaultdict(int)
        for i in range(len(seq) - k + 1):
            kmer = seq[i:i+k]
            counts[kmer] += 1
        return dict(counts)

    @staticmethod
    def find_orfs(sequence: str, min_len: int = 100) -> List[Tuple[int, int, str]]:
        seq = sequence.upper().replace("U", "T")
        starts = {"ATG"}
        stops = {"TAA", "TAG", "TGA"}
        orfs = []
        for frame in range(3):
            i = frame
            while i < len(seq) - 2:
                codon = seq[i:i+3]
                if codon in starts:
                    j = i + 3
                    while j < len(seq) - 2:
                        if seq[j:j+3] in stops:
                            length = j + 3 - i
                            if length >= min_len:
                                orfs.append((i, j+3, f"frame{frame}"))
                            i = j + 3
                            break
                        j += 3
                i += 3
        return orfs


class VariantAnalysis:
    @staticmethod
    def transition_transversion_ratio(variants: Iterable[Variant]) -> float:
        transitions = {("A", "G"), ("G", "A"), ("C", "T"), ("T", "C")}
        ti = tv = 0
        for v in variants:
            r = v.ref.upper()
            a = v.alt.upper()
            if len(r) == 1 and len(a) == 1 and r in "ACGT" and a in "ACGT" and r != a:
                if (r, a) in transitions:
                    ti += 1
                else:
                    tv += 1
        return (ti / tv) if tv > 0 else float("inf")

    @staticmethod
    def filter_variants(variants: Iterable[Variant], chrom: Optional[str] = None,
                        region: Optional[Tuple[int, int]] = None,
                        min_pos: Optional[int] = None, max_pos: Optional[int] = None,
                        gene_regex: Optional[str] = None) -> List[Variant]:
        res = []
        rx = re.compile(gene_regex) if gene_regex else None
        for v in variants:
            if chrom and v.chrom != chrom:
                continue
            if region and not (region[0] <= v.pos <= region[1]):
                continue
            if min_pos and v.pos < min_pos:
                continue
            if max_pos and v.pos > max_pos:
                continue
            if rx and not (v.gene and rx.search(v.gene)):
                continue
            res.append(v)
        return res


# ----------------------------
# Example usage
# ----------------------------

def example():
    db = BioDB()

    # Add samples
    db.add_sample(Sample(id="S1", species="human", tissue="liver", condition="treated"))
    db.add_sample(Sample(id="S2", species="human", tissue="liver", condition="treated"))
    db.add_sample(Sample(id="S3", species="human", tissue="liver", condition="control"))
    db.add_sample(Sample(id="S4", species="human", tissue="liver", condition="control"))

    # Add genes
    db.add_gene(Gene(id="ENSG00000141510", name="TP53", chromosome="17", start=7661779, end=7687550, strand="+"))
    db.add_gene(Gene(id="ENSG00000139618", name="BRCA2", chromosome="13", start=32315474, end=32400266, strand="-"))

    # Sequences
    seq = SequenceRecord(id="seq1", description="Example DNA", sequence="ATGCGTACGTAGCTAGCTAGCTAGCTAA", molecule_type="DNA")
    db.add_sequence(seq)

    # Sequence analysis
    print("GC%:", seq.gc_content())
    print("ORFs:", SeqAnalysis.find_orfs(seq.sequence, min_len=9))

    # Expression matrix (gene x sample)
    expr_df = pd.DataFrame({
        "S1": [100, 50],
        "S2": [80, 40],
        "S3": [20, 60],
        "S4": [30, 55],
    }, index=["ENSG00000141510", "ENSG00000139618"])
    db.set_expression(ExpressionMatrix(expr_df))

    # Simple DE between treated and control
    treated = db.samples_by_condition("treated")
    control = db.samples_by_condition("control")
    de = db.expression.differential_expression(treated, control)
    print("DE results (top):")
    print(de.head())

    # Variants example
    variants = [
        Variant(id="v1", chrom="17", pos=7665000, ref="C", alt="T", gene="TP53"),
        Variant(id="v2", chrom="13", pos=32320000, ref="A", alt="G", gene="BRCA2"),
        Variant(id="v3", chrom="13", pos=32330000, ref="G", alt="C", gene="BRCA2"),
    ]
    for v in variants:
        db.add_variant(v)
    brca2_vars = db.query_variants().filter(lambda v: v.gene == "BRCA2").to_list()
    print("BRCA2 variants:", [v.id for v in brca2_vars])
    print("Ti/Tv:", VariantAnalysis.transition_transversion_ratio(variants))

    # Optional: read VCF if available
    # vlist = IO.read_vcf("example.vcf")

    # Optional: BLAST (remote)
    if BIOPYTHON_AVAILABLE:
        print("Translating sequence:")
        print(seq.translate().sequence)

    # Optional: BAM stats
    # if PYSAM_AVAILABLE:
    #     print(IO.bam_stats("example.bam"))

    # Serialize DB
    with tempfile.TemporaryDirectory() as td:
        path = os.path.join(td, "db.json")
        db.to_json(path)
        loaded = BioDB.from_json(path)
        print("Loaded genes:", [g.name for g in loaded.query_genes().to_list()])


if __name__ == "__main__":
    example()
```
------------------------------------------------------------

PROMPT 143:
"Generate Python code that imports the necessary libraries for data preparation, cleans and preprocesses data, converts data formats, aggregates and merges datasets, and performs data validation and visualization."
RESPONSE:
# Python code that imports libraries, cleans/preprocesses data, converts data formats,
# aggregates/merges datasets, and performs data validation and visualization.

import re
import io
import warnings
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Optional: Pandera for DataFrame schema validation (install via: pip install pandera)
try:
    import pandera as pa
    from pandera import Column, Check
    PANDERA_AVAILABLE = True
except Exception:
    PANDERA_AVAILABLE = False

warnings.filterwarnings("ignore")
pd.set_option("display.width", 160)
pd.set_option("display.max_columns", 50)
sns.set(style="whitegrid", context="notebook")


def create_example_dataframes(seed: int = 42):
    rng = np.random.default_rng(seed)

    # Example customers
    customers = pd.DataFrame({
        "customer_id": np.arange(1, 16),
        "name": [
            " alice  ", "Bob", "   CAROL", "dave", "Eve  ",
            "Mallory", "Peggy", "Trent", "Oscar", "Victor  ",
            "Walter", "Judy", "Heidi", "Sybil", "Trudy"
        ],
        "email": [
            "ALICE@example.com", "bob@example.com", "carol(at)example.com", "dave@example.com", "eve@example.com",
            "mallory@example.com", "peggy@example.com", "trent@example.com", "oscar@example", "victor@example.com",
            "walter@example.com", "judy@example.com", "heidi@example.com", "sybil@example.com", "trudy@example.com"
        ],
        "signup_date": pd.date_range("2023-01-01", periods=15, freq="15D").strftime("%Y/%m/%d").tolist(),
        "country": rng.choice(["US", "CA", "UK", None], size=15, p=[0.5, 0.2, 0.2, 0.1]).tolist(),
        "vip": rng.choice([True, False], size=15, p=[0.2, 0.8]).tolist(),
    })

    # Example orders
    n_orders = 120
    orders = pd.DataFrame({
        "order_id": np.arange(1001, 1001 + n_orders),
        "customer_id": rng.choice(np.arange(1, 18), size=n_orders, replace=True),  # includes some non-existing customers (16,17)
        "order_date": pd.to_datetime("2023-01-01") + pd.to_timedelta(rng.integers(0, 365, size=n_orders), unit="D"),
        "item_count": rng.integers(1, 6, size=n_orders),
        "price_per_item": rng.normal(40, 15, size=n_orders).clip(5, None),
        "discount_rate": rng.choice([0, 0.05, 0.10, 0.15], size=n_orders, p=[0.5, 0.2, 0.2, 0.1]),
        "payment_method": rng.choice(["card", "paypal", "bank_transfer", None], size=n_orders, p=[0.6, 0.25, 0.1, 0.05])
    })
    # Inject some quality issues
    orders.loc[rng.choice(orders.index, size=3, replace=False), "price_per_item"] = -1  # invalid
    orders.loc[rng.choice(orders.index, size=2, replace=False), "order_date"] = None

    # Shuffle and introduce duplicates
    customers = pd.concat([customers, customers.iloc[[2]].assign(email="duplicate_carol@example.com")], ignore_index=True)
    orders = pd.concat([orders, orders.iloc[[5]]], ignore_index=True)

    return customers, orders


# -----------------------------
# Cleaning and preprocessing
# -----------------------------

EMAIL_REGEX = re.compile(r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$")


def clean_customers(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # Standardize strings
    df["name"] = df["name"].astype(str).str.strip().str.title()
    df["email"] = df["email"].astype(str).str.strip().str.lower()

    # Validate emails; set invalid to NaN
    df.loc[~df["email"].str.match(EMAIL_REGEX, na=False), "email"] = np.nan

    # Parse dates
    df["signup_date"] = pd.to_datetime(df["signup_date"], errors="coerce")

    # Fill missing country
    df["country"] = df["country"].fillna("Unknown")

    # Deduplicate on customer_id keeping the first valid email
    df = df.sort_values(["customer_id", "email"], na_position="last").drop_duplicates(subset=["customer_id"], keep="first")

    # Basic type coercions
    df["vip"] = df["vip"].astype(bool)

    # Reset index
    return df.reset_index(drop=True)


def clean_orders(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # Parse dates
    df["order_date"] = pd.to_datetime(df["order_date"], errors="coerce")

    # Coerce numeric columns
    num_cols = ["item_count", "price_per_item", "discount_rate"]
    for c in num_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    # Impute/clip invalid values
    df["item_count"] = df["item_count"].fillna(1).clip(lower=1)
    df["price_per_item"] = df["price_per_item"].clip(lower=0)  # negatives -> 0
    df["discount_rate"] = df["discount_rate"].fillna(0).clip(0, 0.9)

    # Payment method
    df["payment_method"] = df["payment_method"].astype("string").fillna("unknown")

    # Compute totals
    df["total_amount"] = (df["item_count"] * df["price_per_item"] * (1 - df["discount_rate"])).round(2)

    # Remove impossible orders: missing date or zero/negative totals
    df = df[df["order_date"].notna()]
    df = df[df["total_amount"] > 0]

    # Outlier handling (IQR capping) for total_amount
    q1, q3 = df["total_amount"].quantile([0.25, 0.75])
    iqr = q3 - q1
    upper = q3 + 1.5 * iqr
    df["total_amount"] = df["total_amount"].clip(upper=upper)

    # Deduplicate orders by order_id, keep first
    df = df.drop_duplicates(subset=["order_id"], keep="first")

    return df.reset_index(drop=True)


def preprocess_orders(df_orders: pd.DataFrame) -> pd.DataFrame:
    df = df_orders.copy()
    # Derive time-based features
    df["order_month"] = df["order_date"].dt.to_period("M").dt.to_timestamp()
    df["weekday"] = df["order_date"].dt.day_name()

    # One-hot encode payment method
    payment_dummies = pd.get_dummies(df["payment_method"], prefix="pay", dummy_na=False)
    df = pd.concat([df.drop(columns=["payment_method"]), payment_dummies], axis=1)

    return df


# -----------------------------
# Data format conversions
# -----------------------------

def dataframe_to_formats(df: pd.DataFrame):
    formats = {}

    # CSV (in-memory)
    csv_buf = io.StringIO()
    df.to_csv(csv_buf, index=False)
    formats["csv"] = csv_buf.getvalue()

    # JSON Lines
    json_lines = df.to_json(orient="records", lines=True, date_format="iso")
    formats["jsonl"] = json_lines

    # Parquet (optional, requires pyarrow or fastparquet)
    try:
        pq_buf = io.BytesIO()
        df.to_parquet(pq_buf, index=False)
        formats["parquet_bytes"] = pq_buf.getvalue()
    except Exception as e:
        formats["parquet_bytes"] = None
        formats["parquet_error"] = str(e)

    return formats


def formats_to_dataframe(csv_text: str = None, jsonl_text: str = None, parquet_bytes: bytes = None) -> pd.DataFrame:
    # Prefer Parquet > JSONL > CSV if provided
    if parquet_bytes is not None:
        try:
            return pd.read_parquet(io.BytesIO(parquet_bytes))
        except Exception:
            pass
    if jsonl_text is not None:
        return pd.read_json(io.StringIO(jsonl_text), orient="records", lines=True)
    if csv_text is not None:
        return pd.read_csv(io.StringIO(csv_text), parse_dates=True)
    raise ValueError("No input provided to reconstruct DataFrame.")


# -----------------------------
# Aggregations and merges
# -----------------------------

def aggregate_orders(df_orders: pd.DataFrame) -> dict:
    df = df_orders.copy()

    # Monthly revenue
    monthly_rev = (df.set_index("order_date")
                     .resample("M")["total_amount"]
                     .sum()
                     .rename("monthly_revenue")
                     .reset_index())

    # Customer-level metrics
    customer_metrics = df.groupby("customer_id").agg(
        total_revenue=("total_amount", "sum"),
        avg_order_value=("total_amount", "mean"),
        order_count=("order_id", "nunique"),
        last_order=("order_date", "max"),
        first_order=("order_date", "min")
    ).reset_index()

    # Recency in days relative to the dataset's max date
    max_date = df["order_date"].max()
    customer_metrics["recency_days"] = (max_date - customer_metrics["last_order"]).dt.days

    return {
        "monthly_revenue": monthly_rev,
        "customer_metrics": customer_metrics
    }


def merge_customers_with_metrics(df_customers: pd.DataFrame, customer_metrics: pd.DataFrame) -> pd.DataFrame:
    merged = df_customers.merge(customer_metrics, on="customer_id", how="left")
    merged["tenure_days"] = (merged["last_order"].fillna(pd.Timestamp.utcnow().normalize()) - merged["signup_date"]).dt.days
    merged["total_revenue"] = merged["total_revenue"].fillna(0).round(2)
    merged["avg_order_value"] = merged["avg_order_value"].fillna(0).round(2)
    merged["order_count"] = merged["order_count"].fillna(0).astype(int)
    merged["recency_days"] = merged["recency_days"].fillna(np.inf)
    return merged


# -----------------------------
# Data validation
# -----------------------------

def basic_validations(df_customers: pd.DataFrame, df_orders: pd.DataFrame):
    errors = []

    # Customers validations
    if not df_customers["customer_id"].is_unique:
        errors.append("customer_id is not unique in customers.")
    if df_customers["customer_id"].isna().any():
        errors.append("customer_id contains nulls in customers.")
    if df_customers["signup_date"].isna().any():
        errors.append("signup_date contains nulls in customers.")
    if df_customers["email"].isna().any():
        errors.append("email contains nulls in customers (invalid or missing).")
    if not df_customers["email"].str.match(EMAIL_REGEX, na=False).all():
        errors.append("email contains invalid formats in customers.")

    # Orders validations
    if not df_orders["order_id"].is_unique:
        errors.append("order_id is not unique in orders.")
    if df_orders["order_date"].isna().any():
        errors.append("order_date contains nulls in orders.")
    if (df_orders["item_count"] <= 0).any():
        errors.append("item_count has non-positive values.")
    if (df_orders["price_per_item"] < 0).any():
        errors.append("price_per_item has negative values.")
    if ((df_orders["discount_rate"] < 0) | (df_orders["discount_rate"] >= 1)).any():
        errors.append("discount_rate out of [0,1) range.")
    if (df_orders["total_amount"] <= 0).any():
        errors.append("total_amount has non-positive values in orders.")

    # Referential integrity (orders referencing unknown customers)
    unknown_customers = set(df_orders["customer_id"].unique()) - set(df_customers["customer_id"].unique())
    if unknown_customers:
        errors.append(f"Orders reference unknown customer_ids: {sorted(unknown_customers)}")

    return errors


def pandera_validations(df_customers: pd.DataFrame, df_orders: pd.DataFrame):
    if not PANDERA_AVAILABLE:
        return ["Pandera not installed; skipping schema validation."]

    class CustomersSchema(pa.SchemaModel):
        customer_id: Column[int] = Column(int, unique=True, nullable=False)
        name: Column[str] = Column(str, nullable=False, checks=Check.str_length(min_value=1))
        email: Column[str] = Column(str, nullable=False, checks=Check.str_matches(EMAIL_REGEX.pattern))
        signup_date: Column[pd.Timestamp] = Column(pa.DateTime, nullable=False)
        country: Column[str] = Column(str, nullable=False)
        vip: Column[bool] = Column(bool, nullable=False)

    class OrdersSchema(pa.SchemaModel):
        order_id: Column[int] = Column(int, unique=True, nullable=False)
        customer_id: Column[int] = Column(int, nullable=False)
        order_date: Column[pd.Timestamp] = Column(pa.DateTime, nullable=False)
        item_count: Column[int] = Column(int, nullable=False, checks=Check.ge(1))
        price_per_item: Column[float] = Column(float, nullable=False, checks=Check.ge(0))
        discount_rate: Column[float] = Column(float, nullable=False, checks=[Check.ge(0), Check.lt(1)])
        total_amount: Column[float] = Column(float, nullable=False, checks=Check.gt(0))

    errors = []
    try:
        CustomersSchema.validate(df_customers, lazy=True)
    except pa.errors.SchemaErrors as e:
        errors.append(f"Customers schema errors: {e.failure_cases}")

    try:
        OrdersSchema.validate(df_orders, lazy=True)
    except pa.errors.SchemaErrors as e:
        errors.append(f"Orders schema errors: {e.failure_cases}")

    return errors


# -----------------------------
# Visualization
# -----------------------------

def visualize(df_orders: pd.DataFrame, monthly_rev: pd.DataFrame, customer_metrics: pd.DataFrame):
    plt.figure(figsize=(14, 10))

    # 1. Distribution of order totals
    plt.subplot(2, 2, 1)
    sns.histplot(df_orders["total_amount"], kde=True, bins=30, color="steelblue")
    plt.title("Distribution of Order Total Amounts")
    plt.xlabel("Total Amount")

    # 2. Monthly revenue
    plt.subplot(2, 2, 2)
    sns.lineplot(data=monthly_rev, x="order_date", y="monthly_revenue", marker="o")
    plt.title("Monthly Revenue")
    plt.xlabel("Month")
    plt.ylabel("Revenue")

    # 3. Top customers by revenue
    plt.subplot(2, 2, 3)
    top = (customer_metrics.sort_values("total_revenue", ascending=False)
                          .head(10)
                          .assign(customer=lambda d: d["customer_id"].astype(str)))
    sns.barplot(data=top, x="total_revenue", y="customer", orient="h", palette="viridis")
    plt.title("Top 10 Customers by Revenue")
    plt.xlabel("Total Revenue")
    plt.ylabel("Customer ID")

    # 4. Boxplot by weekday
    plt.subplot(2, 2, 4)
    sns.boxplot(data=df_orders, x="weekday", y="total_amount", order=["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"])
    plt.title("Order Totals by Weekday")
    plt.xlabel("Weekday")
    plt.ylabel("Total Amount")
    plt.xticks(rotation=30)

    plt.tight_layout()
    plt.show()


# -----------------------------
# Main demo
# -----------------------------

if __name__ == "__main__":
    # 1) Create example data
    customers_raw, orders_raw = create_example_dataframes()
    print("Raw customers sample:")
    print(customers_raw.head(), "\n")
    print("Raw orders sample:")
    print(orders_raw.head(), "\n")

    # 2) Clean
    customers = clean_customers(customers_raw)
    orders = clean_orders(orders_raw)

    print("Cleaned customers sample:")
    print(customers.head(), "\n")
    print("Cleaned orders sample:")
    print(orders.head(), "\n")

    # 3) Preprocess
    orders_prep = preprocess_orders(orders)
    print("Preprocessed orders columns:")
    print(orders_prep.columns.tolist(), "\n")

    # 4) Data format conversions (to CSV/JSON/Parquet and back)
    formats = dataframe_to_formats(orders_prep)
    reloaded_orders = formats_to_dataframe(
        csv_text=formats.get("csv"),
        jsonl_text=formats.get("jsonl"),
        parquet_bytes=formats.get("parquet_bytes"),
    )
    print("Reloaded orders equals original (ignoring column order)?", 
          set(reloaded_orders.columns) == set(orders_prep.columns), "\n")

    # 5) Aggregations
    aggs = aggregate_orders(orders_prep)
    monthly_rev = aggs["monthly_revenue"]
    customer_metrics = aggs["customer_metrics"]
    print("Monthly revenue sample:")
    print(monthly_rev.head(), "\n")
    print("Customer metrics sample:")
    print(customer_metrics.head(), "\n")

    # 6) Merge customers with metrics
    customer_360 = merge_customers_with_metrics(customers, customer_metrics)
    print("Merged customer 360 sample:")
    print(customer_360.head(), "\n")

    # 7) Data validation
    basic_errors = basic_validations(customers, orders_prep)
    if basic_errors:
        print("Basic validation errors detected:")
        for e in basic_errors:
            print(" -", e)
    else:
        print("Basic validations passed.")

    pandera_errors = pandera_validations(customers, orders_prep)
    if PANDERA_AVAILABLE:
        if pandera_errors:
            print("Pandera validation issues:")
            for e in pandera_errors:
                print(" -", e)
        else:
            print("Pandera validations passed.")
    else:
        print("Pandera not available; skipped schema validation.")

    # 8) Visualization
    visualize(orders_prep, monthly_rev, customer_metrics)

    print("Pipeline completed at", datetime.utcnow().isoformat(), "UTC")
------------------------------------------------------------

PROMPT 144:
"Generate Python code that uses the `jsonrpc` library to process JSON-RPC requests, including parsing request data from JSON, calling methods based on the request parameters, and returning responses in JSON format."
RESPONSE:
# pip install jsonrpc

import json
from http.server import BaseHTTPRequestHandler, HTTPServer
from jsonrpc import JSONRPCResponseManager, dispatcher


# Register JSON-RPC methods
@dispatcher.add_method
def add(a: float, b: float) -> float:
    return a + b

@dispatcher.add_method
def concat(prefix: str, items: list) -> str:
    return prefix + ''.join(map(str, items))

@dispatcher.add_method
def echo(data):
    return data


# Core function: process a JSON-RPC request string and return a JSON response string
def handle_jsonrpc(request_json: str) -> str:
    """
    Takes a JSON-RPC request as a JSON string, dispatches to registered methods,
    and returns the JSON-RPC response as a JSON string.
    """
    response = JSONRPCResponseManager.handle(request_json, dispatcher)
    # Notifications (no "id") produce no response body per JSON-RPC 2.0
    return response.json or ''


# Minimal HTTP server to accept JSON-RPC requests over POST
class JSONRPCHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        try:
            length = int(self.headers.get('Content-Length', '0'))
            raw = self.rfile.read(length).decode('utf-8')
            response_json = handle_jsonrpc(raw)

            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Cache-Control', 'no-store')
            self.end_headers()
            # For notifications, response_json may be empty (valid per spec)
            self.wfile.write(response_json.encode('utf-8'))
        except Exception as exc:
            # If something unexpected happens before jsonrpc can handle it, return a minimal error
            self.send_response(500)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({
                'jsonrpc': '2.0',
                'error': {'code': -32603, 'message': 'Internal error', 'data': str(exc)},
                'id': None
            }).encode('utf-8'))


if __name__ == '__main__':
    # Demo: local processing (no HTTP)
    example_request = json.dumps({
        'jsonrpc': '2.0',
        'method': 'add',
        'params': {'a': 3, 'b': 4},
        'id': 1
    })
    print('Local response:', handle_jsonrpc(example_request))

    # Start HTTP server for JSON-RPC
    host, port = '127.0.0.1', 8000
    print(f"Starting JSON-RPC server on http://{host}:{port} (POST requests only)")
    with HTTPServer((host, port), JSONRPCHandler) as httpd:
        httpd.serve_forever()
------------------------------------------------------------

PROMPT 145:
"Generate Python code that uses the `inspect` module to retrieve information about functions, classes, and variables defined in the Python standard library, and create documentation pages for them using the `nbdev` module."
RESPONSE:
# stdlib_nbdev_docs.py
# Generate nbdev documentation notebooks for Python standard library objects using inspect.

import os
import sys
import json
import inspect
import importlib
import subprocess
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional

try:
    import nbformat as nbf
except ImportError as e:
    raise RuntimeError("Please install nbformat: pip install nbformat") from e

try:
    # nbdev is required for show_doc and doc building
    from nbdev.showdoc import show_doc  # noqa: F401 (import check)
    import nbdev  # noqa: F401 (import check)
except ImportError as e:
    raise RuntimeError("Please install nbdev>=2: pip install nbdev") from e


# --------------------------
# Configuration
# --------------------------

# Set to True to try to build HTML docs with nbdev/Quarto after notebook generation
BUILD_DOCS = True

# Optional: Limit the number of stdlib modules processed (None = no limit)
MAX_MODULES = int(os.environ.get("STDLIB_MAX_MODULES", "10"))  # default small; set to 0 or empty for unlimited

# Optional: Explicit module allowlist; if not empty, only these will be processed
MODULE_ALLOWLIST: List[str] = [
    # Example curated set (uncomment or edit)
    "math",
    "itertools",
    "functools",
    "pathlib",
    "collections",
    "heapq",
    "json",
    "re",
    "datetime",
    "typing",
]

# If True, ignore MODULE_ALLOWLIST and try to enumerate full stdlib via sys.stdlib_module_names (Py3.10+)
USE_FULL_STDLIB = os.environ.get("FULL_STDLIB", "0") in {"1", "true", "yes"}

# Directories for nbdev project
NBS_PATH = "nbs"
DOCS_PATH = "_docs"

# Project metadata (used for nbdev settings.ini and Quarto)
PROJECT_NAME = "stdlib-docs"
REPO_NAME = "stdlib-docs"
LIB_NAME = "stdlib_docs"


# --------------------------
# Utilities
# --------------------------

def ensure_dirs():
    os.makedirs(NBS_PATH, exist_ok=True)
    os.makedirs(DOCS_PATH, exist_ok=True)


def write_if_missing(path: str, content: str):
    if not os.path.exists(path):
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)


def init_nbdev_project():
    # Minimal settings.ini
    settings = f"""[DEFAULT]
repo = {REPO_NAME}
lib_name = {LIB_NAME}
version = 0.0.1
min_python = 3.9
license = apache2
audience = Developers
author = stdlib-docs
author_email = example@example.com
nbs_path = {NBS_PATH}
doc_path = {DOCS_PATH}
branch = main
custom_sidebar = True
title = {PROJECT_NAME}
keywords = python,stdlib,docs,nbdev
"""
    write_if_missing("settings.ini", settings)

    # Minimal Quarto config
    quarto = f"""project:
  type: website
  output-dir: {DOCS_PATH}

website:
  title: "{PROJECT_NAME}"
  navbar:
    right:
      - text: "GitHub"
        href: https://github.com/yourname/{REPO_NAME}

format:
  html:
    theme: cosmo
"""
    write_if_missing("_quarto.yml", quarto)

    # Root README so Quarto home exists
    write_if_missing("README.md", f"# {PROJECT_NAME}\n\nGenerated on {datetime.utcnow().isoformat()}Z\n")


def is_public(name: str) -> bool:
    return not name.startswith("_")


def safe_getattr(obj: Any, name: str) -> Tuple[bool, Any]:
    try:
        return True, getattr(obj, name)
    except Exception:
        return False, None


def stdlib_modules() -> List[str]:
    # Decide which modules to document
    if USE_FULL_STDLIB and hasattr(sys, "stdlib_module_names"):
        names = sorted(sys.stdlib_module_names)
    elif MODULE_ALLOWLIST:
        names = MODULE_ALLOWLIST[:]
    elif hasattr(sys, "stdlib_module_names"):
        names = sorted(sys.stdlib_module_names)
    else:
        # Fallback curated list if Python <3.10 and no allowlist
        names = [
            "math", "itertools", "functools", "collections", "heapq", "json", "re",
            "datetime", "typing", "pathlib", "operator", "statistics"
        ]

    # Filter out test modules and internal
    names = [m for m in names if is_public(m) and not m.startswith("test")]

    # Apply MAX_MODULES limit if set (>0)
    try:
        max_mods = int(MAX_MODULES)
    except Exception:
        max_mods = 0
    if max_mods > 0:
        names = names[:max_mods]
    return names


def import_module(name: str):
    try:
        return importlib.import_module(name)
    except Exception:
        return None


def classify_members(mod) -> Dict[str, List[Tuple[str, Any]]]:
    items: Dict[str, List[Tuple[str, Any]]] = {"functions": [], "classes": [], "variables": []}
    if mod is None:
        return items
    for name in sorted(dir(mod)):
        if not is_public(name):
            continue
        ok, obj = safe_getattr(mod, name)
        if not ok:
            continue
        # Skip modules (submodules)
        if inspect.ismodule(obj):
            continue
        # Classify
        if inspect.isclass(obj):
            items["classes"].append((name, obj))
        elif inspect.isfunction(obj) or inspect.isbuiltin(obj) or inspect.ismethoddescriptor(obj):
            items["functions"].append((name, obj))
        else:
            items["variables"].append((name, obj))
    return items


def truncate_repr(val: Any, limit: int = 200) -> str:
    try:
        r = repr(val)
    except Exception:
        return "<unrepresentable>"
    if len(r) > limit:
        return r[:limit] + "…"
    return r


def make_show_doc_cell(mod_name: str, obj_name: str, kind: str) -> nbf.NotebookNode:
    # Uses nbdev.showdoc.show_doc to render nice docs
    code = f"""import importlib
from nbdev.showdoc import show_doc
_m = importlib.import_module("{mod_name}")
show_doc(getattr(_m, "{obj_name}"))  # {kind}
"""
    return nbf.v4.new_code_cell(code)


def make_variable_cell(mod_name: str, var_name: str) -> nbf.NotebookNode:
    code = f"""import importlib, inspect
_m = importlib.import_module("{mod_name}")
_obj = getattr(_m, "{var_name}")
_doc = inspect.getdoc(_obj)
print("Name:", "{var_name}")
print("Type:", type(_obj))
print("Value:", repr(_obj)[:500])
print("\\nDocstring:\\n" + (_doc or "<no docstring>"))
"""
    return nbf.v4.new_code_cell(code)


def build_notebook_for_module(mod_name: str, members: Dict[str, List[Tuple[str, Any]]]) -> nbf.NotebookNode:
    nb = nbf.v4.new_notebook()
    nb["metadata"]["kernelspec"] = {
        "name": "python3",
        "display_name": "Python 3",
        "language": "python"
    }
    nb["metadata"]["language_info"] = {"name": "python"}

    # Title/intro
    title = f"# {mod_name} (Python standard library)\n\nAuto-generated with inspect + nbdev.show_doc."
    nb.cells.append(nbf.v4.new_markdown_cell(title))

    # Import cell once
    nb.cells.append(nbf.v4.new_code_cell(f"import importlib, inspect\n_m = importlib.import_module('{mod_name}')\n_m.__doc__ and print(_m.__doc__.splitlines()[0])"))

    # Functions
    if members["functions"]:
        nb.cells.append(nbf.v4.new_markdown_cell("## Functions"))
        for name, _ in members["functions"]:
            nb.cells.append(make_show_doc_cell(mod_name, name, "function"))

    # Classes
    if members["classes"]:
        nb.cells.append(nbf.v4.new_markdown_cell("## Classes"))
        for name, _ in members["classes"]:
            nb.cells.append(make_show_doc_cell(mod_name, name, "class"))

    # Variables
    if members["variables"]:
        nb.cells.append(nbf.v4.new_markdown_cell("## Variables / Constants"))
        for name, _ in members["variables"]:
            nb.cells.append(make_variable_cell(mod_name, name))

    return nb


def write_notebook(mod_name: str, nb: nbf.NotebookNode) -> str:
    fname = mod_name.replace(".", "_") + ".ipynb"
    path = os.path.join(NBS_PATH, fname)
    with open(path, "w", encoding="utf-8") as f:
        nbf.write(nb, f)
    return path


def generate_sidebar():
    # Try nbdev.maker.make_sidebar (nbdev v2)
    try:
        from nbdev.maker import make_sidebar
        make_sidebar()
        return True
    except Exception:
        return False


def build_docs():
    # Use nbdev CLI to build Quarto site
    try:
        subprocess.run(["nbdev_docs"], check=True)
        return True
    except Exception:
        return False


def main():
    ensure_dirs()
    init_nbdev_project()

    modules = stdlib_modules()
    print(f"Selected {len(modules)} module(s): {modules}")

    generated = []
    for mod_name in modules:
        print(f"Processing: {mod_name}")
        mod = import_module(mod_name)
        if mod is None:
            print(f"  - Skipping (import failed)")
            continue
        members = classify_members(mod)
        # Quick counts
        counts = {k: len(v) for k, v in members.items()}
        print(f"  - Members: {json.dumps(counts)}")
        nb = build_notebook_for_module(mod_name, members)
        path = write_notebook(mod_name, nb)
        generated.append(path)

    print(f"\nGenerated {len(generated)} notebook(s) in '{NBS_PATH}'.")
    if not generated:
        print("No notebooks created. Check configuration or allowlist.")
        return

    ok_sidebar = generate_sidebar()
    if ok_sidebar:
        print("Sidebar generated.")
    else:
        print("Could not auto-generate sidebar (nbdev.maker.make_sidebar not available).")

    if BUILD_DOCS:
        ok_docs = build_docs()
        if ok_docs:
            print(f"Docs built into '{DOCS_PATH}'. You can preview with: nbdev_preview")
        else:
            print("Doc build failed or nbdev CLI not found. Try running 'nbdev_docs' manually, and ensure Quarto is installed.")
    else:
        print("Skipping doc build (BUILD_DOCS=False). You can run 'nbdev_docs' later.")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 146:
"Generate Python code that uses the `openapi` library to generate an OpenAPI document from a Python annotation, and then use the `pydantic` library to validate request and response data against the generated OpenAPI schema."
RESPONSE:
# pip install openapi-schema-pydantic==1.2.4 pydantic==1.10.*
from typing import get_type_hints, Callable, Type, Dict, Any
from pydantic import BaseModel, Field, EmailStr, conint, constr, ValidationError
from uuid import UUID, uuid4

# OpenAPI object model (3.0.3) as Pydantic models
from openapi_schema_pydantic import (
    OpenAPI,
    Info,
    PathItem,
    Operation,
    RequestBody,
    Response,
    MediaType,
    Reference,
    Components,
    Schema,
)


# ---- Domain models (Python annotations via Pydantic) ----
class CreateUserRequest(BaseModel):
    username: constr(min_length=3, max_length=30, regex=r"^[A-Za-z0-9_]+$") = Field(...)
    email: EmailStr
    age: conint(ge=0) | None = None


class UserResponse(BaseModel):
    id: UUID
    username: str
    email: EmailStr
    age: int | None = None


# An "endpoint" function using Python annotations for request and response
def create_user(payload: CreateUserRequest) -> UserResponse:
    """
    Create a user. In a real app, this would insert into a DB, etc.
    """


# ---- Generate OpenAPI document from the function's Python annotations ----
def openapi_from_annotated_handler(
    handler: Callable,
    *,
    path: str,
    method: str = "post",
    title: str = "Generated API",
    version: str = "1.0.0",
) -> OpenAPI:
    hints = get_type_hints(handler)
    # Assume the first parameter is the request model; 'return' is the response model.
    # You can adapt this to your signature style as needed.
    param_names = [p for p in hints.keys() if p != "return"]
    if not param_names:
        raise ValueError("Handler must have at least one annotated parameter for the request body")
    request_model: Type[BaseModel] = hints[param_names[0]]  # e.g., CreateUserRequest
    response_model: Type[BaseModel] = hints["return"]       # e.g., UserResponse

    # Generate JSON Schemas from Pydantic models, suitable for OpenAPI components
    # Using Pydantic v1's .schema()
    request_schema_dict = request_model.schema(ref_template="#/components/schemas/{model}")
    response_schema_dict = response_model.schema(ref_template="#/components/schemas/{model}")

    # Register them as reusable components
    components = Components(
        schemas={
            request_model.__name__: Schema.parse_obj(request_schema_dict),
            response_model.__name__: Schema.parse_obj(response_schema_dict),
        }
    )

    # Build the operation referencing those components
    op = Operation(
        operationId=f"{method}_{path.strip('/').replace('/', '_')}",
        summary=handler.__doc__.strip() if handler.__doc__ else None,
        requestBody=RequestBody(
            required=True,
            content={"application/json": MediaType(schema=Reference(ref=f"#/components/schemas/{request_model.__name__}"))},
        ),
        responses={
            "201": Response(
                description="Created",
                content={"application/json": MediaType(schema=Reference(ref=f"#/components/schemas/{response_model.__name__}"))},
            )
        },
    )

    paths = {
        path: PathItem(**{method: op})
    }

    doc = OpenAPI(
        openapi="3.0.3",
        info=Info(title=title, version=version),
        paths=paths,
        components=components,
    )
    return doc


# Build the OpenAPI document
openapi_doc = openapi_from_annotated_handler(
    create_user,
    path="/users",
    method="post",
    title="User Service",
    version="1.0.0",
)

# Print the generated OpenAPI document as JSON
print(openapi_doc.json(by_alias=True, exclude_none=True, indent=2))


# ---- Use Pydantic to validate request and response data against the schema-driven models ----
# In practice, you validate against the same models that produced the OpenAPI schema.
def validate_request(data: Dict[str, Any]) -> CreateUserRequest:
    # Raises ValidationError on failure
    return CreateUserRequest(**data)


def validate_response(data: Dict[str, Any]) -> UserResponse:
    # Raises ValidationError on failure
    return UserResponse(**data)


# Example: validate a request payload
incoming_request = {
    "username": "alice_123",
    "email": "alice@example.com",
    "age": 30,
}

try:
    validated_req = validate_request(incoming_request)
    print("Validated request:", validated_req.dict())
except ValidationError as e:
    print("Request validation failed:", e)

# Simulate building a response according to the annotated schema
response_payload = {
    "id": str(uuid4()),
    "username": validated_req.username,
    "email": validated_req.email,
    "age": validated_req.age,
}

try:
    validated_resp = validate_response(response_payload)
    print("Validated response:", validated_resp.dict())
except ValidationError as e:
    print("Response validation failed:", e)

# Example: show a failing validation
bad_request = {
    "username": "x",    # too short
    "email": "not-an-email",
    "age": -5,
}
try:
    validate_request(bad_request)
except ValidationError as e:
    print("Expected validation error for bad_request:\n", e)
------------------------------------------------------------

PROMPT 147:
"Generate Python code that uses the `import_module` function from the `sphinx.ext.dependency` module to load the `jquery` module, like this:\n```\nfrom sphinx.ext.dependency import import_module\nimport_module('jquery')\n```"
RESPONSE:
from sphinx.ext.dependency import import_module
import_module('jquery')
------------------------------------------------------------

PROMPT 148:
"Generate Python code that imports the necessary libraries and creates a wrapper function for the DiscRecordingUI framework on macOS, allowing users to easily interact with the framework's functionality in their Python scripts."
RESPONSE:
# Requires: macOS and PyObjC packages:
#   pip install pyobjc-core pyobjc-framework-Cocoa pyobjc-framework-DiscRecording pyobjc-framework-DiscRecordingUI

import sys

if sys.platform != "darwin":
    raise OSError("This code only runs on macOS.")

try:
    from Cocoa import NSApplication
except Exception as e:
    raise ImportError("Failed to import Cocoa. Install pyobjc-core and pyobjc-framework-Cocoa.") from e

try:
    # DiscRecording handles actual burning; DiscRecordingUI provides the UI panels
    from DiscRecording import DRBurn  # noqa: F401 (returned by the UI)
    from DiscRecordingUI import DRBurnSetupPanel
except Exception as e:
    raise ImportError(
        "Failed to import DiscRecording/DiscRecordingUI. "
        "Install pyobjc-framework-DiscRecording and pyobjc-framework-DiscRecordingUI."
    ) from e

try:
    # Depending on macOS SDK, either NSModalResponseOK or NSOKButton may be present
    from AppKit import NSModalResponseOK
    OK_RESPONSE = NSModalResponseOK
except Exception:
    try:
        from AppKit import NSOKButton
        OK_RESPONSE = NSOKButton
    except Exception:
        OK_RESPONSE = 1  # Fallback; OK is typically 1 in older APIs


def present_burn_setup_panel():
    """
    Shows the system's burn setup panel (DiscRecordingUI) and returns a configured DRBurn object
    if the user confirms the dialog; otherwise returns None.

    Returns:
        DRBurn | None
    """
    # Ensure an NSApplication exists so the modal panel can run
    NSApplication.sharedApplication()

    panel = DRBurnSetupPanel.setupPanel()
    result = panel.runSetupPanel()

    if int(result) == int(OK_RESPONSE):
        # The panel creates/configures a DRBurn object based on user choices (device, speed, etc.)
        burn = panel.burnObject()
        return burn
    return None


# Example usage
if __name__ == "__main__":
    burn = present_burn_setup_panel()
    if burn is None:
        print("User canceled burn setup.")
    else:
        # The DRBurn object now reflects the device and options chosen in the UI.
        # You can use DiscRecording to create a DRTrack (e.g., from a folder) and start the burn:
        #
        # from DiscRecording import DRFolder, DRTrack
        # root = DRFolder.folderWithPath_("/path/to/folder-to-burn")
        # track = DRTrack.trackForRootFolder_(root)
        # burn.writeLayout_(track)
        #
        # Note: burning runs asynchronously; you may need to keep a run loop alive and/or watch burn.status().
        print("Got configured DRBurn object from UI:", burn)
------------------------------------------------------------

PROMPT 149:
"Generate Python code that imports the functools module and creates a decorator function that takes another function as an argument, and returns a new function that caches the results of the original function using a dictionary."
RESPONSE:
import functools

def memoize(func):
    """Decorator that caches results of the function using a dictionary."""
    cache = {}

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # Build a hashable key from args and kwargs
        try:
            key = (args, frozenset(kwargs.items()))
        except TypeError:
            # If arguments aren't hashable, skip caching
            return func(*args, **kwargs)

        if key not in cache:
            cache[key] = func(*args, **kwargs)
        return cache[key]

    return wrapper
------------------------------------------------------------

PROMPT 150:
"Generate Python code that imports the necessary modules from the OSAKit framework and creates wrappers for its classes, functions, and structs, allowing developers to interact with the framework's functionality in a Pythonic way."
RESPONSE:
# osakit_wrapper.py
# Pythonic wrappers for Apple's OSAKit framework via PyObjC.
# Requirements:
#   - macOS
#   - PyObjC (pip install pyobjc)
#
# This module dynamically wraps OSAKit classes and functions to:
#   - Provide snake_case aliases for ObjC-style camelCase methods
#   - Convert NSError results into Python exceptions
#   - Offer simple proxies to keep interaction pythonic

from __future__ import annotations

import sys
import types
import functools
import inspect
from typing import Any, Callable, Dict, Optional, Tuple

try:
    if sys.platform != "darwin":
        raise RuntimeError("OSAKit is only available on macOS")

    import objc
    import OSAKit  # Provided by PyObjC
    from Foundation import NSError
except Exception as exc:  # pragma: no cover
    raise ImportError(
        "This module requires macOS and PyObjC with the OSAKit framework available.\n"
        "Install: pip install pyobjc"
    ) from exc


# ------------- Utilities ------------- #

def _camel_to_snake(name: str) -> str:
    """
    Convert Objective-C style camelCase and colon-encoded names to snake_case.
    Example: 'executeAndReturnError_' -> 'execute_and_return_error'
    """
    # Remove trailing underscores used by PyObjC for selector colons
    base = name.rstrip("_")
    # Convert camelCase to snake_case
    snake = []
    for i, ch in enumerate(base):
        if ch.isupper() and i > 0 and (not base[i - 1].isupper()):
            snake.append("_")
        snake.append(ch.lower())
    return "".join(snake)


def _is_objc_class(obj: Any) -> bool:
    return isinstance(obj, objc.objc_class)


def _is_objc_instance(obj: Any) -> bool:
    # objc.objc_object (instances) are duck-typed; check for objc attributes
    return hasattr(obj, "objc_instance") or obj.__class__.__name__.endswith("_objc")


def _looks_like_nserror(obj: Any) -> bool:
    return isinstance(obj, NSError)


def _extract_error(result: Any) -> Tuple[bool, Any, Optional[NSError]]:
    """
    Heuristically extract NSError from PyObjC method results.
    PyObjC often returns:
      - (value, error) when an NSError** is involved
      - value only, or raises Objective-C exception translated to Python
    """
    if isinstance(result, tuple) and result:
        last = result[-1]
        if _looks_like_nserror(last):
            value = result[0] if len(result) > 1 else None
            return True, value, last
    return False, result, None


# ------------- Exceptions ------------- #

class OSAKitError(Exception):
    def __init__(self, message: str, *, domain: Optional[str] = None, code: Optional[int] = None, user_info: Optional[dict] = None, nserror: Optional[NSError] = None):
        super().__init__(message)
        self.domain = domain
        self.code = code
        self.user_info = user_info or {}
        self.nserror = nserror

    @classmethod
    def from_nserror(cls, err: NSError) -> "OSAKitError":
        msg = err.localizedDescription() if hasattr(err, "localizedDescription") else str(err)
        domain = err.domain() if hasattr(err, "domain") else None
        code = int(err.code()) if hasattr(err, "code") else None
        user_info = dict(err.userInfo()) if hasattr(err, "userInfo") and err.userInfo() is not None else {}
        return cls(msg, domain=domain, code=code, user_info=user_info, nserror=err)


# ------------- Method and function wrappers ------------- #

class _ObjCMethodWrapper:
    def __init__(self, target: Any, attr_name: str, callable_obj: Callable[..., Any]):
        self._target = target
        self._attr_name = attr_name
        self._callable = callable_obj

    def __repr__(self) -> str:
        return f"<ObjCMethodWrapper {self._target}.{self._attr_name}>"

    def __call__(self, *args, **kwargs):
        try:
            result = self._callable(*args, **kwargs)
        except Exception as exc:
            # Directly propagate Python exceptions raised by PyObjC
            raise

        # Heuristic: handle (value, NSError) returns by raising Python exception
        has_err, value, err = _extract_error(result)
        if has_err and err is not None:
            raise OSAKitError.from_nserror(err)

        return value if has_err else result


def _wrap_module_function(fn: Callable[..., Any]) -> Callable[..., Any]:
    @functools.wraps(fn)
    def _wrapped(*args, **kwargs):
        res = fn(*args, **kwargs)
        has_err, value, err = _extract_error(res)
        if has_err and err is not None:
            raise OSAKitError.from_nserror(err)
        return value if has_err else res
    return _wrapped


# ------------- Proxy for classes/instances ------------- #

class ObjCProxy:
    """
    A dynamic proxy that:
      - Delegates unknown attributes to the underlying ObjC object/class
      - Exposes snake_case aliases for camelCase methods/selectors
      - Converts (value, NSError) returns into Python exceptions
    """
    __slots__ = ("_objc", "_snake_cache", "_dir_cache")

    def __init__(self, objc_obj: Any):
        self._objc = objc_obj
        self._snake_cache: Dict[str, Any] = {}
        self._dir_cache: Optional[set[str]] = None

    @property
    def objc(self) -> Any:
        return self._objc

    def __repr__(self) -> str:
        cls = self._objc if _is_objc_class(self._objc) else self._objc.__class__
        name = getattr(cls, "__name__", str(cls))
        return f"<ObjCProxy for {name}>"

    def __dir__(self):
        if self._dir_cache is None:
            names = set(dir(self._objc))
            # Add snake_case aliases
            for n in list(names):
                snake = _camel_to_snake(n)
                if snake != n:
                    names.add(snake)
            self._dir_cache = names
        return sorted(self._dir_cache)

    def _resolve_attr(self, name: str) -> Tuple[str, Any]:
        # Direct attribute
        if hasattr(self._objc, name):
            return name, getattr(self._objc, name)

        # Try snake_case -> camelCase mapping
        # Search all attributes whose snake conversion matches
        target_name = None
        for candidate in dir(self._objc):
            if _camel_to_snake(candidate) == name:
                target_name = candidate
                break

        if target_name is None:
            raise AttributeError(f"{self!r} has no attribute '{name}'")

        return target_name, getattr(self._objc, target_name)

    def __getattr__(self, name: str) -> Any:
        if name in self._snake_cache:
            return self._snake_cache[name]

        attr_name, value = self._resolve_attr(name)

        # Wrap callables to provide NSError->Exception behavior
        if callable(value):
            wrapped = _ObjCMethodWrapper(self._objc, attr_name, value)
            self._snake_cache[name] = wrapped
            return wrapped

        # Non-callable attributes pass through
        self._snake_cache[name] = value
        return value


# ------------- Discovery and registries ------------- #

def _discover_classes() -> Dict[str, Any]:
    classes: Dict[str, Any] = {}
    for name, obj in vars(OSAKit).items():
        if _is_objc_class(obj):
            classes[name] = obj
    return classes


def _discover_functions() -> Dict[str, Callable[..., Any]]:
    funcs: Dict[str, Callable[..., Any]] = {}
    for name, obj in vars(OSAKit).items():
        if isinstance(obj, types.BuiltinFunctionType) or isinstance(obj, types.FunctionType):
            funcs[name] = obj
    return funcs


def _discover_constants() -> Dict[str, Any]:
    consts: Dict[str, Any] = {}
    for name, obj in vars(OSAKit).items():
        if name.startswith("_"):
            continue
        if _is_objc_class(obj):
            continue
        if isinstance(obj, (types.BuiltinFunctionType, types.FunctionType)):
            continue
        consts[name] = obj
    return consts


CLASSES: Dict[str, Any] = _discover_classes()
FUNCTIONS: Dict[str, Callable[..., Any]] = _discover_functions()
CONSTANTS: Dict[str, Any] = _discover_constants()


# ------------- Public API ------------- #

def wrap(obj: Any) -> ObjCProxy:
    """
    Wrap an Objective-C class or instance from OSAKit with an ObjCProxy.
    """
    return ObjCProxy(obj)


def get_class(name: str) -> ObjCProxy:
    """
    Fetch an OSAKit class by name and wrap it.
    """
    cls = CLASSES.get(name)
    if cls is None:
        raise AttributeError(f"OSAKit has no class named '{name}'")
    return wrap(cls)


def list_classes() -> list[str]:
    return sorted(CLASSES.keys())


def list_functions() -> list[str]:
    return sorted(FUNCTIONS.keys())


def list_constants() -> list[str]:
    return sorted(CONSTANTS.keys())


# Create a module-like container exposing snake_case function aliases
class _FunctionsNamespace:
    def __init__(self, funcs: Dict[str, Callable[..., Any]]):
        self._orig = funcs
        self._wrapped: Dict[str, Callable[..., Any]] = {}
        self._names = set()

        for name, fn in funcs.items():
            snake = _camel_to_snake(name)
            wrapped = _wrap_module_function(fn)
            setattr(self, name, wrapped)    # original name
            setattr(self, snake, wrapped)   # snake_case alias
            self._wrapped[name] = wrapped
            if snake != name:
                self._wrapped[snake] = wrapped
            self._names.add(name)
            self._names.add(snake)

    def __dir__(self):
        return sorted(self._names)

    def __getattr__(self, name: str):
        if name in self._wrapped:
            return self._wrapped[name]
        raise AttributeError(f"No function named '{name}' in OSAKit")


Functions = _FunctionsNamespace(FUNCTIONS)


# Optional: lightweight struct exposure
# PyObjC already maps C structs to Python types when available; we re-expose them here.
def get_struct(name: str) -> Any:
    obj = getattr(OSAKit, name, None)
    if obj is None or not hasattr(obj, "__name__"):
        raise AttributeError(f"No struct named '{name}' in OSAKit")
    return obj


# ------------- Convenience: pre-wrapped well-known classes (if present) ------------- #

def OSAScript() -> ObjCProxy:
    """
    Convenience accessor for the OSAScript class proxy.
    Usage:
        script_cls = OSAScript()
        # Example construction pattern (initializer names depend on your use-case):
        # obj = script_cls.alloc().initWithSource_error_("tell app \"Finder\" to get name", None)
        # wrap(obj).execute_and_return_error()
    """
    return get_class("OSAScript")


def OSALanguage() -> ObjCProxy:
    return get_class("OSALanguage") if "OSALanguage" in CLASSES else get_class("OSAScriptLanguage")  # fallback if naming differs


def OSAScriptController() -> ObjCProxy:
    return get_class("OSAScriptController")


# ------------- Example (optional) ------------- #

if __name__ == "__main__":  # pragma: no cover
    print("OSAKit classes:", list_classes())
    print("Some functions:", list_functions()[:10])
    print("Constants count:", len(CONSTANTS))

    # Example: get class proxy
    try:
        script_cls = OSAScript()
        print("Got OSAScript class proxy:", script_cls)

        # Note: Actual initializers vary by macOS version and OSAKit API surface.
        # You can inspect available methods with dir():
        # print([n for n in dir(script_cls) if 'init' in n or 'source' in n])

    except AttributeError:
        print("OSAScript class not found in this environment.")
------------------------------------------------------------

PROMPT 151:
"Generate Python code that uses the SQLAlchemy library to create a schema display script that retrieves information about a database schema and displays it in a user-friendly format, similar to the output of the \"db schema\" command."
RESPONSE:
#!/usr/bin/env python3
"""
Schema display script using SQLAlchemy.

- Connects to a database via a SQLAlchemy URL
- Introspects schemas, tables, columns, constraints, indexes, and views
- Prints a readable, tree-like report similar to typical "db schema" CLI tools

Usage:
  python db_schema.py postgresql://user:pass@host:5432/dbname
  python db_schema.py sqlite:///example.db --all-schemas
  python db_schema.py mysql+pymysql://user:pass@host/db --schema my_schema
  python db_schema.py $DATABASE_URL --include "^users$" --include-views

Environment:
  DATABASE_URL can be used in place of the URL argument.

Requires:
  SQLAlchemy 1.4+ (works best with 2.x)

Note:
  This script introspects metadata only and does not query row data.
"""

import argparse
import os
import re
import sys
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

from sqlalchemy import create_engine, inspect
from sqlalchemy.engine import Engine


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Display database schema using SQLAlchemy.")
    parser.add_argument(
        "url",
        nargs="?",
        default=os.getenv("DATABASE_URL"),
        help="SQLAlchemy database URL (or set DATABASE_URL env var).",
    )
    parser.add_argument(
        "--schema",
        dest="schemas",
        action="append",
        help="Schema to include (repeatable). If omitted, defaults to sensible per-DB defaults.",
    )
    parser.add_argument(
        "--all-schemas",
        action="store_true",
        help="Include all non-system schemas (overrides --schema).",
    )
    parser.add_argument(
        "--include",
        action="append",
        default=[],
        help="Regex to include tables/views by name (repeatable).",
    )
    parser.add_argument(
        "--exclude",
        action="append",
        default=[],
        help="Regex to exclude tables/views by name (repeatable).",
    )
    parser.add_argument(
        "--include-views",
        action="store_true",
        help="Include views in output.",
    )
    parser.add_argument(
        "--show-view-sql",
        action="store_true",
        help="Include view SQL definitions (if available).",
    )
    parser.add_argument(
        "--include-system",
        action="store_true",
        help="Include system/internal schemas (e.g., information_schema, pg_catalog).",
    )
    parser.add_argument(
        "--width",
        type=int,
        default=120,
        help="Wrap output to this character width (soft limit).",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Reduce non-critical messages.",
    )
    return parser.parse_args(argv)


def is_system_schema(dialect: str, schema: str) -> bool:
    s = schema.lower()
    if dialect.startswith("postgresql"):
        return s in ("pg_catalog", "information_schema", "pg_toast", "pg_temp_1") or s.startswith("pg_toast_temp_")
    if dialect.startswith("mysql"):
        return s in ("information_schema", "performance_schema", "mysql", "sys")
    if dialect.startswith("mariadb"):
        return s in ("information_schema", "performance_schema", "mysql", "sys")
    if dialect.startswith("oracle"):
        return s in ("sys", "system")
    if dialect.startswith("mssql"):
        return s in ("information_schema",)
    if dialect.startswith("sqlite"):
        # SQLite uses a single logical schema; tables beginning with sqlite_ are internal.
        return False
    return False


def default_schemas(engine: Engine, all_schemas: bool, include_system: bool) -> List[str]:
    insp = inspect(engine)
    dialect = engine.url.get_backend_name()
    all_names = insp.get_schema_names()
    if dialect.startswith("postgresql"):
        if all_schemas:
            return [s for s in all_names if include_system or not is_system_schema(dialect, s)]
        # default: just public if present, else everything non-system
        if "public" in all_names:
            return ["public"]
        return [s for s in all_names if include_system or not is_system_schema(dialect, s)]
    # For other DBs, default to all non-system schemas (SQLite: usually one)
    return [s for s in all_names if include_system or not is_system_schema(dialect, s)] or all_names


def compile_regex_list(patterns: Sequence[str]) -> List[re.Pattern]:
    return [re.compile(p) for p in patterns]


def name_included(name: str, include_res: List[re.Pattern], exclude_res: List[re.Pattern]) -> bool:
    if include_res:
        if not any(r.search(name) for r in include_res):
            return False
    if exclude_res:
        if any(r.search(name) for r in exclude_res):
            return False
    return True


def indent(text: str, level: int = 1, spaces_per_level: int = 2) -> str:
    pad = " " * (level * spaces_per_level)
    return "\n".join((pad + line if line else line) for line in text.splitlines())


def join_list(items: Iterable[str]) -> str:
    return ", ".join(items)


def fmt_default(v) -> Optional[str]:
    if v is None:
        return None
    # v may be a string or SQL expression depending on dialect
    return str(v)


def safe_get(d: Dict, key: str, default=None):
    try:
        return d.get(key, default)
    except Exception:
        return default


def describe_columns(insp, table: str, schema: Optional[str]) -> List[str]:
    lines: List[str] = []
    try:
        cols = insp.get_columns(table, schema=schema)
    except Exception as e:
        lines.append(f"- [error fetching columns: {e}]")
        return lines

    for c in cols:
        name = c.get("name")
        typ = c.get("type")
        nullable = c.get("nullable")
        default = fmt_default(c.get("default"))
        autoinc = c.get("autoincrement")
        comment = c.get("comment")
        computed = c.get("computed")
        identity = c.get("identity")  # for some dialects (e.g., Postgres identity)

        parts = [f"{name}: {typ}"]
        parts.append("nullable" if nullable else "not null")
        if default:
            parts.append(f"default={default}")
        if autoinc and autoinc not in ("auto", "ignore_fk"):
            parts.append(f"autoincrement={autoinc}")
        if identity:
            # identity may be a dict; try to show generation type
            gen = safe_get(identity, "generation")
            parts.append(f"identity({gen})" if gen else "identity")
        if computed:
            sqltext = safe_get(computed, "sqltext")
            persisted = safe_get(computed, "persisted")
            if sqltext:
                parts.append(f"generated=({sqltext}){' persisted' if persisted else ''}")
        if comment:
            parts.append(f"comment={comment}")

        lines.append(f"- {'; '.join(parts)}")
    if not lines:
        lines.append("- [no columns]")
    return lines


def describe_pk(insp, table: str, schema: Optional[str]) -> Optional[str]:
    try:
        pk = insp.get_pk_constraint(table, schema=schema)
    except Exception:
        return None
    cols = pk.get("constrained_columns") or []
    name = pk.get("name")
    if not cols:
        return None
    return f"primary key: ({join_list(cols)})" + (f" name={name}" if name else "")


def describe_unique_constraints(insp, table: str, schema: Optional[str]) -> List[str]:
    try:
        uqs = insp.get_unique_constraints(table, schema=schema)
    except Exception:
        return []
    lines: List[str] = []
    for u in uqs or []:
        name = u.get("name")
        cols = u.get("column_names") or []
        if cols:
            lines.append(f"- {name or '[unnamed]'}: ({join_list(cols)})")
    return lines


def describe_check_constraints(insp, table: str, schema: Optional[str]) -> List[str]:
    try:
        cks = insp.get_check_constraints(table, schema=schema)
    except Exception:
        return []
    lines: List[str] = []
    for c in cks or []:
        name = c.get("name")
        sqltext = c.get("sqltext")
        if sqltext:
            lines.append(f"- {name or '[unnamed]'}: {sqltext}")
    return lines


def describe_fks(insp, table: str, schema: Optional[str]) -> List[str]:
    try:
        fks = insp.get_foreign_keys(table, schema=schema)
    except Exception:
        return []
    lines: List[str] = []
    for f in fks or []:
        name = f.get("name")
        cols = f.get("constrained_columns") or []
        r_schema = f.get("referred_schema")
        r_table = f.get("referred_table")
        r_cols = f.get("referred_columns") or []
        onupd = f.get("onupdate")
        ondel = f.get("ondelete")
        parts = [
            f"{name or '[unnamed]'}: ({join_list(cols)}) -> "
            f"{(r_schema + '.') if r_schema else ''}{r_table}({join_list(r_cols)})"
        ]
        if onupd:
            parts.append(f"on update {onupd}")
        if ondel:
            parts.append(f"on delete {ondel}")
        lines.append("- " + "; ".join(parts))
    return lines


def describe_indexes(insp, table: str, schema: Optional[str]) -> List[str]:
    try:
        idxs = insp.get_indexes(table, schema=schema)
    except Exception:
        return []
    lines: List[str] = []
    for ix in idxs or []:
        name = ix.get("name") or "[unnamed]"
        cols = ix.get("column_names") or []
        unique = ix.get("unique")
        exprs = ix.get("expressions") or []
        include = ix.get("dialect_options", {}).get("postgresql_include", None) or ix.get("include_columns")
        parts = [f"{name} on ({join_list(cols)})" if cols else name]
        if exprs:
            parts.append(f"expressions=[{join_list(map(str, exprs))}]")
        if unique:
            parts.append("unique")
        if include:
            parts.append(f"include=({join_list(include)})")
        lines.append("- " + "; ".join(parts))
    return lines


def describe_table_comment(insp, table: str, schema: Optional[str]) -> Optional[str]:
    try:
        data = insp.get_table_comment(table_name=table, schema=schema) or {}
    except Exception:
        return None
    text = data.get("text")
    return text or None


def describe_enums_if_any(insp, schema: Optional[str]) -> List[str]:
    # Only available on certain dialects (e.g., PostgreSQL)
    if not hasattr(insp, "get_enums"):
        return []
    try:
        enums = insp.get_enums(schema=schema) or []
    except Exception:
        return []
    lines: List[str] = []
    for e in enums:
        name = e.get("name")
        labels = e.get("labels") or []
        lines.append(f"- enum {name}: {{{join_list(labels)}}}")
    return lines


def describe_view(insp, view: str, schema: Optional[str], show_sql: bool) -> List[str]:
    lines: List[str] = []
    try:
        cols = insp.get_columns(view, schema=schema)
    except Exception as e:
        lines.append(f"- [error fetching columns: {e}]")
        return lines
    # Columns
    for c in cols or []:
        name = c.get("name")
        typ = c.get("type")
        nullable = c.get("nullable")
        lines.append(f"- {name}: {typ}; " + ("nullable" if nullable else "not null"))
    if not cols:
        lines.append("- [no columns]")

    if show_sql and hasattr(insp, "get_view_definition"):
        try:
            sql = insp.get_view_definition(view, schema=schema)
            if sql:
                lines.append("definition:")
                for line in str(sql).splitlines():
                    lines.append("  " + line)
        except Exception:
            pass
    return lines


def print_schema(engine: Engine, args: argparse.Namespace) -> None:
    insp = inspect(engine)
    dialect = engine.url.get_backend_name()
    include_res = compile_regex_list(args.include)
    exclude_res = compile_regex_list(args.exclude)

    if args.schemas:
        schemas = args.schemas
    else:
        schemas = default_schemas(engine, args.all_schemas, args.include_system)

    if not schemas:
        print("No schemas found.")
        return

    for schema in schemas:
        if not args.include_system and is_system_schema(dialect, schema):
            continue

        print(f'Schema: {schema}')
        # Enums/types (if any)
        enum_lines = describe_enums_if_any(insp, schema)
        if enum_lines:
            print(indent("Types:"))
            for line in enum_lines:
                print(indent(line, level=2))

        # Tables
        try:
            tables = insp.get_table_names(schema=schema)
        except Exception as e:
            print(indent(f"[error fetching tables: {e}]"))
            tables = []

        # Views
        views: List[str] = []
        if args.include_views:
            try:
                views = insp.get_view_names(schema=schema)
            except Exception as e:
                if not args.quiet:
                    print(indent(f"[error fetching views: {e}]"))

        # Filter by include/exclude
        tables = [t for t in tables if name_included(t, include_res, exclude_res)]
        views = [v for v in views if name_included(v, include_res, exclude_res)]

        # Print tables
        if tables:
            print(indent("Tables:"))
        for t in tables:
            fq = f"{schema}.{t}" if schema else t
            print(indent(f"- {fq}"))
            comment = describe_table_comment(insp, t, schema)
            if comment:
                print(indent(f"comment: {comment}", level=2))
            # Columns
            print(indent("columns:", level=2))
            for line in describe_columns(insp, t, schema):
                print(indent(line, level=3))
            # PK
            pk = describe_pk(insp, t, schema)
            if pk:
                print(indent(pk, level=2))
            # FKs
            fks = describe_fks(insp, t, schema)
            if fks:
                print(indent("foreign keys:", level=2))
                for line in fks:
                    print(indent(line, level=3))
            # Unique constraints
            uqs = describe_unique_constraints(insp, t, schema)
            if uqs:
                print(indent("unique constraints:", level=2))
                for line in uqs:
                    print(indent(line, level=3))
            # Checks
            cks = describe_check_constraints(insp, t, schema)
            if cks:
                print(indent("check constraints:", level=2))
                for line in cks:
                    print(indent(line, level=3))
            # Indexes
            idxs = describe_indexes(insp, t, schema)
            if idxs:
                print(indent("indexes:", level=2))
                for line in idxs:
                    print(indent(line, level=3))

        # Print views
        if views:
            print(indent("Views:"))
        for v in views:
            fq = f"{schema}.{v}" if schema else v
            print(indent(f"- {fq}"))
            for line in describe_view(insp, v, schema, args.show_view_sql):
                print(indent(line, level=3))

        print("")  # blank line between schemas


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = parse_args(argv)
    if not args.url:
        print("Error: no database URL given. Provide as positional arg or set DATABASE_URL.", file=sys.stderr)
        return 2

    try:
        engine = create_engine(args.url, future=True)
    except Exception as e:
        print(f"Error creating engine: {e}", file=sys.stderr)
        return 1

    try:
        with engine.connect() as _conn:
            pass
    except Exception as e:
        print(f"Error connecting to database: {e}", file=sys.stderr)
        return 1

    try:
        print_schema(engine, args)
    except KeyboardInterrupt:
        print("\nInterrupted.", file=sys.stderr)
        return 130
    except Exception as e:
        print(f"Error while introspecting schema: {e}", file=sys.stderr)
        return 1
    finally:
        engine.dispose()
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
------------------------------------------------------------

PROMPT 152:
"Generate Python code that imports the requests library and creates a generic REST client class with methods for making GET, POST, PUT, and DELETE requests to a specified URL, including support for HTTP authentication and JSON data parsing."
RESPONSE:
import requests
from requests.auth import HTTPBasicAuth, AuthBase
from urllib.parse import urljoin
from typing import Any, Dict, Optional, Tuple, Union


class RestClient:
    """
    Generic REST client using requests.Session with support for:
    - GET, POST, PUT, DELETE
    - HTTP authentication (basic, custom requests auth)
    - JSON request/response handling
    """

    def __init__(
        self,
        base_url: str,
        auth: Optional[Union[Tuple[str, str], AuthBase]] = None,
        headers: Optional[Dict[str, str]] = None,
        timeout: Optional[float] = 30.0,
    ):
        if not base_url.endswith("/"):
            base_url += "/"
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({"Accept": "application/json"})
        if headers:
            self.session.headers.update(headers)

        # Accept either (username, password) or a requests AuthBase instance
        if isinstance(auth, tuple):
            self.auth: Optional[AuthBase] = HTTPBasicAuth(*auth)
        else:
            self.auth = auth

        self.timeout = timeout

    def set_basic_auth(self, username: str, password: str) -> None:
        self.auth = HTTPBasicAuth(username, password)

    def set_bearer_token(self, token: str, prefix: str = "Bearer") -> None:
        self.session.headers["Authorization"] = f"{prefix} {token}"

    def set_header(self, key: str, value: str) -> None:
        self.session.headers[key] = value

    def _full_url(self, path: str) -> str:
        return urljoin(self.base_url, path.lstrip("/"))

    def _prepare_auth(
        self, auth: Optional[Union[Tuple[str, str], AuthBase]]
    ) -> Optional[AuthBase]:
        if auth is None:
            return self.auth
        if isinstance(auth, tuple):
            return HTTPBasicAuth(*auth)
        return auth

    def _request(
        self,
        method: str,
        path: str = "",
        params: Optional[Dict[str, Any]] = None,
        json: Optional[Any] = None,
        data: Optional[Union[Dict[str, Any], str, bytes]] = None,
        headers: Optional[Dict[str, str]] = None,
        auth: Optional[Union[Tuple[str, str], AuthBase]] = None,
        timeout: Optional[float] = None,
        parse_json: bool = True,
        return_response: bool = False,
    ):
        url = self._full_url(path)
        req_auth = self._prepare_auth(auth)
        resp = self.session.request(
            method=method,
            url=url,
            params=params,
            json=json,
            data=data,
            headers=headers,
            auth=req_auth,
            timeout=self._resolve_timeout(timeout),
        )
        resp.raise_for_status()

        if return_response:
            return resp

        if parse_json:
            try:
                return resp.json()
            except ValueError:
                # Fallback to text if response is not valid JSON
                return resp.text
        else:
            return resp.text

    def _resolve_timeout(self, timeout: Optional[float]) -> Optional[float]:
        return self.timeout if timeout is None else timeout

    def get(
        self,
        path: str = "",
        params: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
        auth: Optional[Union[Tuple[str, str], AuthBase]] = None,
        timeout: Optional[float] = None,
        parse_json: bool = True,
        return_response: bool = False,
    ):
        return self._request(
            "GET",
            path=path,
            params=params,
            headers=headers,
            auth=auth,
            timeout=timeout,
            parse_json=parse_json,
            return_response=return_response,
        )

    def post(
        self,
        path: str = "",
        params: Optional[Dict[str, Any]] = None,
        json: Optional[Any] = None,
        data: Optional[Union[Dict[str, Any], str, bytes]] = None,
        headers: Optional[Dict[str, str]] = None,
        auth: Optional[Union[Tuple[str, str], AuthBase]] = None,
        timeout: Optional[float] = None,
        parse_json: bool = True,
        return_response: bool = False,
    ):
        return self._request(
            "POST",
            path=path,
            params=params,
            json=json,
            data=data,
            headers=headers,
            auth=auth,
            timeout=timeout,
            parse_json=parse_json,
            return_response=return_response,
        )

    def put(
        self,
        path: str = "",
        params: Optional[Dict[str, Any]] = None,
        json: Optional[Any] = None,
        data: Optional[Union[Dict[str, Any], str, bytes]] = None,
        headers: Optional[Dict[str, str]] = None,
        auth: Optional[Union[Tuple[str, str], AuthBase]] = None,
        timeout: Optional[float] = None,
        parse_json: bool = True,
        return_response: bool = False,
    ):
        return self._request(
            "PUT",
            path=path,
            params=params,
            json=json,
            data=data,
            headers=headers,
            auth=auth,
            timeout=timeout,
            parse_json=parse_json,
            return_response=return_response,
        )

    def delete(
        self,
        path: str = "",
        params: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
        auth: Optional[Union[Tuple[str, str], AuthBase]] = None,
        timeout: Optional[float] = None,
        parse_json: bool = True,
        return_response: bool = False,
    ):
        return self._request(
            "DELETE",
            path=path,
            params=params,
            headers=headers,
            auth=auth,
            timeout=timeout,
            parse_json=parse_json,
            return_response=return_response,
        )

    def close(self) -> None:
        self.session.close()


# Example usage:
# client = RestClient("https://api.example.com", auth=("user", "pass"))
# response = client.get("items/123")
# print(response)
------------------------------------------------------------

PROMPT 153:
"Generate Python code that implements a data access protocol, allowing clients to connect to a server and perform CRUD operations on data stored in a database, using a custom serialization format for communication between client and server."
RESPONSE:
Below are three Python files that implement a simple data access protocol over TCP with a custom serialization format. The server stores data in an SQLite database and supports CRUD operations on a key-value table. The client exposes convenience methods for those operations. The protocol uses a custom length-prefixed, key-value list (KVL) binary format.

File: kv_protocol.py
--------------------------------
```python
# kv_protocol.py
# Custom KVL (Key-Value List) serialization with length-prefixed framing.

import struct
from typing import Iterable, List, Tuple, Dict, Union

# Payload is framed as: [len:4 bytes big-endian][payload bytes]
# Payload bytes encode fields as repeated segments:
#   [key_len:1][key_bytes][val_len:4 big-endian][val_bytes]
#
# - key_len is 0..255 (key must fit in 1 byte length)
# - key_bytes are UTF-8
# - val_bytes are arbitrary bytes
#
# Fields can repeat (e.g., 'key' repeated for LIST response).

MAX_PAYLOAD_SIZE = 10 * 1024 * 1024  # 10 MiB safety limit

def _to_bytes(value: Union[bytes, str, int, bool]) -> bytes:
    if isinstance(value, bytes):
        return value
    if isinstance(value, str):
        return value.encode("utf-8")
    if isinstance(value, bool):
        return b"1" if value else b"0"
    if isinstance(value, int):
        return str(value).encode("ascii")
    raise TypeError(f"Unsupported value type: {type(value)}")

def encode_fields(fields: Union[Dict[str, Union[bytes, str, int, bool, Iterable[Union[bytes, str, int, bool]]]], Iterable[Tuple[str, Union[bytes, str, int, bool]]]]) -> bytes:
    """
    Encode fields into payload bytes (without the 4-byte length prefix).
    Accepts either:
      - dict[str, value or iterable-of-values] (iterables emit repeated fields)
      - iterable of (key, value) pairs
    """
    chunks: List[bytes] = []

    def add_field(k: str, v) -> None:
        kb = k.encode("utf-8")
        if not (0 < len(kb) <= 255):
            raise ValueError("Key length must be 1..255 bytes in UTF-8")
        vb = _to_bytes(v)
        chunks.append(struct.pack("!B", len(kb)))
        chunks.append(kb)
        chunks.append(struct.pack("!I", len(vb)))
        chunks.append(vb)

    if isinstance(fields, dict):
        for k, v in fields.items():
            if isinstance(v, (list, tuple)):
                for item in v:
                    add_field(k, item)
            else:
                add_field(k, v)
    else:
        for k, v in fields:
            add_field(k, v)

    return b"".join(chunks)

def decode_fields(payload: bytes) -> List[Tuple[str, bytes]]:
    i = 0
    fields: List[Tuple[str, bytes]] = []
    n = len(payload)
    while i < n:
        if i + 1 > n:
            raise ValueError("Truncated key length")
        key_len = payload[i]
        i += 1
        if key_len == 0:
            raise ValueError("Zero-length keys not allowed")
        if i + key_len > n:
            raise ValueError("Truncated key bytes")
        key = payload[i:i+key_len].decode("utf-8")
        i += key_len
        if i + 4 > n:
            raise ValueError("Truncated value length")
        (val_len,) = struct.unpack("!I", payload[i:i+4])
        i += 4
        if i + val_len > n:
            raise ValueError("Truncated value bytes")
        val = payload[i:i+val_len]
        i += val_len
        fields.append((key, val))
    return fields

def pack_frame(fields: Union[Dict, Iterable[Tuple[str, Union[bytes, str, int, bool]]]]) -> bytes:
    payload = encode_fields(fields)
    if len(payload) > MAX_PAYLOAD_SIZE:
        raise ValueError("Payload too large")
    return struct.pack("!I", len(payload)) + payload

def unpack_frame_from_sock(sock) -> List[Tuple[str, bytes]]:
    """
    Read one frame from a blocking socket. Returns list of (key, value_bytes).
    Raises EOFError if the connection is closed cleanly.
    """
    buf = _recv_exact(sock, 4)
    if not buf:
        raise EOFError("Connection closed")
    (length,) = struct.unpack("!I", buf)
    if length < 0 or length > MAX_PAYLOAD_SIZE:
        raise ValueError("Invalid frame length")
    payload = _recv_exact(sock, length)
    if len(payload) != length:
        raise EOFError("Connection closed during frame receive")
    return decode_fields(payload)

def send_frame_to_sock(sock, fields: Union[Dict, Iterable[Tuple[str, Union[bytes, str, int, bool]]]]) -> None:
    data = pack_frame(fields)
    _send_all(sock, data)

def _recv_exact(sock, n: int) -> bytes:
    chunks = []
    got = 0
    while got < n:
        chunk = sock.recv(n - got)
        if not chunk:
            # Return what we have; caller determines if it's EOF
            return b"".join(chunks)
        chunks.append(chunk)
        got += len(chunk)
    return b"".join(chunks)

def _send_all(sock, data: bytes) -> None:
    view = memoryview(data)
    total = len(view)
    sent = 0
    while sent < total:
        n = sock.send(view[sent:])
        if n == 0:
            raise RuntimeError("Socket connection broken during send")
        sent += n

# Convenience decoders
def fields_to_multidict(fields: List[Tuple[str, bytes]]) -> Dict[str, List[bytes]]:
    md: Dict[str, List[bytes]] = {}
    for k, v in fields:
        md.setdefault(k, []).append(v)
    return md

def get_first_text(md: Dict[str, List[bytes]], key: str, default: str = None) -> str:
    vals = md.get(key)
    if not vals:
        return default
    return vals[0].decode("utf-8")

def get_first_bytes(md: Dict[str, List[bytes]], key: str, default: bytes = None) -> bytes:
    vals = md.get(key)
    if not vals:
        return default
    return vals[0]

def get_first_int(md: Dict[str, List[bytes]], key: str, default: int = None) -> int:
    vals = md.get(key)
    if not vals:
        return default
    try:
        return int(vals[0].decode("ascii"))
    except Exception:
        raise ValueError(f"Invalid integer for key '{key}'")
```


File: server.py
--------------------------------
```python
# server.py
# Threaded TCP server implementing CRUD over a key-value table in SQLite,
# using the custom KVL protocol from kv_protocol.py.

import socketserver
import sqlite3
import threading
import os
from typing import List, Tuple
from kv_protocol import (
    unpack_frame_from_sock,
    send_frame_to_sock,
    fields_to_multidict,
    get_first_text,
    get_first_bytes,
)

DB_SCHEMA = """
PRAGMA journal_mode=WAL;
PRAGMA foreign_keys=ON;
CREATE TABLE IF NOT EXISTS kv (
    k TEXT PRIMARY KEY,
    v BLOB NOT NULL
);
"""

def init_db(db_path: str) -> None:
    os.makedirs(os.path.dirname(db_path), exist_ok=True) if os.path.dirname(db_path) else None
    conn = sqlite3.connect(db_path)
    try:
        conn.executescript(DB_SCHEMA)
        conn.commit()
    finally:
        conn.close()

class KeyValueStore:
    def __init__(self, db_path: str):
        # Open a connection per thread/handler to avoid locking issues
        self.db_path = db_path

    def _connect(self) -> sqlite3.Connection:
        conn = sqlite3.connect(self.db_path, timeout=30, isolation_level=None)  # autocommit mode
        conn.execute("PRAGMA foreign_keys=ON;")
        return conn

    def create(self, key: str, value: bytes) -> None:
        with self._connect() as conn:
            try:
                conn.execute("INSERT INTO kv (k, v) VALUES (?, ?)", (key, value))
            except sqlite3.IntegrityError:
                raise KeyError("Key already exists")

    def read(self, key: str) -> bytes:
        with self._connect() as conn:
            row = conn.execute("SELECT v FROM kv WHERE k = ?", (key,)).fetchone()
            if not row:
                raise KeyError("Not found")
            return row[0]

    def update(self, key: str, value: bytes) -> None:
        with self._connect() as conn:
            cur = conn.execute("UPDATE kv SET v = ? WHERE k = ?", (value, key))
            if cur.rowcount == 0:
                raise KeyError("Not found")

    def delete(self, key: str) -> None:
        with self._connect() as conn:
            cur = conn.execute("DELETE FROM kv WHERE k = ?", (key,))
            if cur.rowcount == 0:
                raise KeyError("Not found")

    def list_keys(self, prefix: str = None, include_values: bool = False) -> List[Tuple[str, bytes]]:
        with self._connect() as conn:
            if prefix is None:
                if include_values:
                    rows = conn.execute("SELECT k, v FROM kv ORDER BY k").fetchall()
                else:
                    rows = conn.execute("SELECT k, NULL FROM kv ORDER BY k").fetchall()
            else:
                like = prefix + "%"
                if include_values:
                    rows = conn.execute("SELECT k, v FROM kv WHERE k LIKE ? ORDER BY k", (like,)).fetchall()
                else:
                    rows = conn.execute("SELECT k, NULL FROM kv WHERE k LIKE ? ORDER BY k", (like,)).fetchall()
        return [(r[0], r[1]) for r in rows]

class ThreadedKVLHandler(socketserver.BaseRequestHandler):
    store: KeyValueStore = None  # injected via server

    def handle(self):
        sock = self.request
        while True:
            try:
                fields = unpack_frame_from_sock(sock)
            except EOFError:
                return
            except Exception as e:
                # protocol error; send error and continue
                self._send_error(str(e))
                continue

            md = fields_to_multidict(fields)
            op = get_first_text(md, "op")
            if not op:
                self._send_error("Missing 'op'")
                continue

            op_u = op.upper()
            try:
                if op_u == "CREATE":
                    key = get_first_text(md, "key")
                    value = get_first_bytes(md, "value")
                    if key is None or value is None:
                        raise ValueError("Missing 'key' or 'value'")
                    self.store.create(key, value)
                    self._send_ok(message="Created", extra=[("key", key), ("size", str(len(value)))])
                elif op_u == "READ":
                    key = get_first_text(md, "key")
                    if key is None:
                        raise ValueError("Missing 'key'")
                    value = self.store.read(key)
                    self._send_ok(extra=[("key", key), ("value", value)])
                elif op_u == "UPDATE":
                    key = get_first_text(md, "key")
                    value = get_first_bytes(md, "value")
                    if key is None or value is None:
                        raise ValueError("Missing 'key' or 'value'")
                    self.store.update(key, value)
                    self._send_ok(message="Updated", extra=[("key", key), ("size", str(len(value)))])
                elif op_u == "DELETE":
                    key = get_first_text(md, "key")
                    if key is None:
                        raise ValueError("Missing 'key'")
                    self.store.delete(key)
                    self._send_ok(message="Deleted", extra=[("key", key)])
                elif op_u == "LIST":
                    prefix = get_first_text(md, "prefix", None)
                    include_values_text = get_first_text(md, "include_values", "0")
                    include_values = include_values_text in ("1", "true", "TRUE", "yes", "YES")
                    items = self.store.list_keys(prefix=prefix, include_values=include_values)
                    # Build response: status=OK, count, repeated 'key' and optionally 'val' fields
                    resp = [("status", "OK"), ("count", str(len(items)))]
                    for k, v in items:
                        resp.append(("key", k))
                        if include_values and v is not None:
                            resp.append(("val", v))
                    send_frame_to_sock(self.request, resp)
                else:
                    self._send_error(f"Unknown op '{op}'")
            except KeyError as ke:
                self._send_error(str(ke))
            except Exception as e:
                self._send_error(str(e))

    def _send_ok(self, message: str = None, extra: List[Tuple[str, str]] = None):
        fields: List[Tuple[str, bytes]] = [("status", b"OK")]
        if message:
            fields.append(("message", message))
        if extra:
            for k, v in extra:
                fields.append((k, v if isinstance(v, bytes) else str(v)))
        send_frame_to_sock(self.request, fields)

    def _send_error(self, message: str):
        send_frame_to_sock(self.request, [("status", "ERROR"), ("message", message or "Unknown error")])

class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer):
    daemon_threads = True
    allow_reuse_address = True

def serve(host: str = "127.0.0.1", port: int = 5050, db_path: str = "data/kv.sqlite3"):
    init_db(db_path)
    kv_store = KeyValueStore(db_path)
    ThreadedKVLHandler.store = kv_store  # inject into handler
    with ThreadedTCPServer((host, port), ThreadedKVLHandler) as server:
        print(f"KV server listening on {host}:{port}, db={db_path}")
        try:
            server.serve_forever()
        except KeyboardInterrupt:
            print("Shutting down...")

if __name__ == "__main__":
    serve()
```


File: client.py
--------------------------------
```python
# client.py
# Client library for the KVL protocol, with simple CLI for manual testing.

import socket
from typing import List, Tuple, Optional, Union
from kv_protocol import (
    unpack_frame_from_sock,
    send_frame_to_sock,
    fields_to_multidict,
    get_first_text,
    get_first_bytes,
)

class DataClient:
    def __init__(self, host: str = "127.0.0.1", port: int = 5050, timeout: float = 10.0):
        self.host = host
        self.port = port
        self.timeout = timeout
        self.sock: Optional[socket.socket] = None

    def connect(self):
        if self.sock:
            return
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.settimeout(self.timeout)
        s.connect((self.host, self.port))
        # After connecting, switch to blocking mode for stream operations
        s.settimeout(None)
        self.sock = s

    def close(self):
        if self.sock:
            try:
                self.sock.close()
            finally:
                self.sock = None

    def _request(self, fields) -> dict:
        if not self.sock:
            self.connect()
        send_frame_to_sock(self.sock, fields)
        resp_fields = unpack_frame_from_sock(self.sock)
        md = fields_to_multidict(resp_fields)
        status = get_first_text(md, "status")
        if status != "OK":
            message = get_first_text(md, "message", "Unknown error")
            raise RuntimeError(message)
        return md

    def create(self, key: str, value: Union[bytes, str]) -> None:
        v = value if isinstance(value, bytes) else str(value).encode("utf-8")
        md = self._request({"op": "CREATE", "key": key, "value": v})
        # No return value if OK

    def read(self, key: str) -> bytes:
        md = self._request({"op": "READ", "key": key})
        val = get_first_bytes(md, "value")
        return val

    def update(self, key: str, value: Union[bytes, str]) -> None:
        v = value if isinstance(value, bytes) else str(value).encode("utf-8")
        self._request({"op": "UPDATE", "key": key, "value": v})

    def delete(self, key: str) -> None:
        self._request({"op": "DELETE", "key": key})

    def list(self, prefix: Optional[str] = None, include_values: bool = False) -> List[Union[str, Tuple[str, bytes]]]:
        fields = {"op": "LIST"}
        if prefix is not None:
            fields["prefix"] = prefix
        fields["include_values"] = "1" if include_values else "0"
        md = self._request(fields)
        count_text = get_first_text(md, "count", "0")
        try:
            count = int(count_text)
        except Exception:
            count = 0
        keys = md.get("key", [])
        if not include_values:
            return [k.decode("utf-8") for k in keys]
        # When include_values=1, we also have repeated 'val' fields, same order
        vals = md.get("val", [])
        out: List[Tuple[str, bytes]] = []
        for i, kb in enumerate(keys):
            vb = vals[i] if i < len(vals) else b""
            out.append((kb.decode("utf-8"), vb))
        return out

# Simple CLI for manual testing:
# Examples:
#   python client.py create foo "bar"
#   python client.py read foo
#   python client.py update foo "baz"
#   python client.py delete foo
#   python client.py list
#   python client.py listprefix fo
if __name__ == "__main__":
    import sys
    def usage():
        print("Usage:")
        print("  client.py create <key> <value>")
        print("  client.py read <key>")
        print("  client.py update <key> <value>")
        print("  client.py delete <key>")
        print("  client.py list")
        print("  client.py listprefix <prefix>")
        print("Optional env vars: KV_HOST, KV_PORT")
    host = os.environ.get("KV_HOST", "127.0.0.1")
    port = int(os.environ.get("KV_PORT", "5050"))
    import os
    if len(sys.argv) < 2:
        usage()
        sys.exit(1)
    op = sys.argv[1].lower()
    client = DataClient(host=host, port=port)
    try:
        if op == "create" and len(sys.argv) == 4:
            client.create(sys.argv[2], sys.argv[3])
            print("Created")
        elif op == "read" and len(sys.argv) == 3:
            val = client.read(sys.argv[2])
            try:
                print(val.decode("utf-8"))
            except UnicodeDecodeError:
                print(val)
        elif op == "update" and len(sys.argv) == 4:
            client.update(sys.argv[2], sys.argv[3])
            print("Updated")
        elif op == "delete" and len(sys.argv) == 3:
            client.delete(sys.argv[2])
            print("Deleted")
        elif op == "list" and len(sys.argv) == 2:
            keys = client.list()
            for k in keys:
                print(k)
        elif op == "listprefix" and len(sys.argv) == 3:
            items = client.list(prefix=sys.argv[2], include_values=False)
            for k in items:
                print(k)
        else:
            usage()
            sys.exit(2)
    finally:
        client.close()
```

Notes and usage summary:
- The custom protocol is binary and length-prefixed, with repeated key-value fields. It is not JSON or any existing standard.
- Server database: SQLite table kv(k TEXT PRIMARY KEY, v BLOB NOT NULL).
- Operations:
  - CREATE: fields op, key, value
  - READ: fields op, key; response includes value
  - UPDATE: fields op, key, value
  - DELETE: fields op, key
  - LIST: fields op, optional prefix, optional include_values=1; response includes count, repeated key (and val if requested)
- Start server: python server.py
- Use client API from Python or via the simple CLI examples in client.py.
------------------------------------------------------------

PROMPT 154:
"Generate Python code that imports the inspect module, creates a decorator that takes a function and keyword-only arguments, and uses the inspect.getfullargspec function to dynamically create a new function with the given keyword-only arguments."
RESPONSE:
import inspect
import functools

def add_kwonly(**added_kwonly_defaults):
    """
    Decorator factory that returns a decorator adding keyword-only parameters
    to the decorated function. It uses inspect.getfullargspec to read the
    original function's signature and dynamically builds a new function
    with the additional keyword-only parameters.
    """
    def decorator(func):
        spec = inspect.getfullargspec(func)
        args = list(spec.args)
        varargs = spec.varargs
        varkw = spec.varkw
        orig_kwonlyargs = list(spec.kwonlyargs or [])
        orig_kwonlydefaults = dict(spec.kwonlydefaults or {})

        # Merge existing and new kw-only args/defaults
        kwonlyargs = orig_kwonlyargs[:]
        kwonlydefaults = orig_kwonlydefaults.copy()
        for k, v in added_kwonly_defaults.items():
            if k not in kwonlyargs:
                kwonlyargs.append(k)
            kwonlydefaults[k] = v

        # Build a function definition string with the combined signature
        params = []
        globs = {"_orig": func}
        # Positional (and positional-with-defaults)
        pos_defaults = spec.defaults or ()
        if pos_defaults:
            split = len(args) - len(pos_defaults)
            for i, name in enumerate(args):
                if i < split:
                    params.append(name)
                else:
                    dname = f"_d_{name}"
                    globs[dname] = pos_defaults[i - split]
                    params.append(f"{name}={dname}")
        else:
            params.extend(args)

        # *varargs or bare * for kw-only
        if varargs:
            params.append(f"*{varargs}")
        elif kwonlyargs:
            params.append("*")

        # Keyword-only params (existing + added)
        for name in kwonlyargs:
            if name in kwonlydefaults:
                dname = f"_kd_{name}"
                globs[dname] = kwonlydefaults[name]
                params.append(f"{name}={dname}")
            else:
                params.append(name)

        # **kwargs if present on original
        if varkw:
            params.append(f"**{varkw}")

        params_code = ", ".join(params)

        # Build the call to the original function:
        # - pass positional as-is
        # - pass through varargs if present
        # - pass through only the original's kw-only args
        # - pass **kwargs if original accepts it
        call_parts = []
        call_parts.extend(args)
        if varargs:
            call_parts.append(f"*{varargs}")
        for name in orig_kwonlyargs:
            call_parts.append(f"{name}={name}")
        if varkw:
            call_parts.append(f"**{varkw}")
        call_code = ", ".join(call_parts)

        new_name = func.__name__
        src = f"def {new_name}({params_code}):\n"
        src += f"    return _orig({call_code})\n"

        namespace = {}
        exec(src, globs, namespace)
        new_func = namespace[new_name]
        new_func = functools.update_wrapper(new_func, func)
        return new_func

    return decorator

# Example:
# @add_kwonly(debug=False, mode="prod")
# def foo(a, b=10, *, flag=True):
#     return a + b if flag else a - b
#
# foo(1, b=2, flag=False, debug=True, mode="dev")  # debug/mode accepted by wrapper but ignored by original function
------------------------------------------------------------

PROMPT 155:
"Generate Python code that imports the necessary libraries and creates an Ocient client object to interact with an Ocient database, allowing users to execute SQL queries, create and manage database objects, and perform other database operations."
RESPONSE:
# Requires:
#   pip install jaydebeapi
#   The Ocient JDBC driver JAR (e.g., ocient-jdbc-<version>.jar)
#
# This code creates a simple OcientClient that connects via JDBC from Python,
# executes SQL (DDL/DML), and offers basic transaction and object-management helpers.

from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union
import jaydebeapi
import urllib.parse


def _build_jdbc_url(host: str, port: int, database: str, params: Optional[Dict[str, Any]] = None) -> str:
    base = f"jdbc:ocient://{host}:{port}/{database}"
    if params:
        # Convert Python values to strings and URL-encode them
        q = urllib.parse.urlencode({k: str(v) for k, v in params.items()})
        return f"{base}?{q}"
    return base


class OcientClient:
    def __init__(
        self,
        jdbc_jar_path: Union[str, List[str]],
        host: str,
        port: int,
        database: str,
        user: str,
        password: str,
        jdbc_params: Optional[Dict[str, Any]] = None,
        autocommit: bool = False,
        driver_class: str = "com.ocient.jdbc.JDBCDriver",
    ) -> None:
        """
        Create an Ocient client using the Ocient JDBC driver via JayDeBeApi.

        Args:
            jdbc_jar_path: Path to the Ocient JDBC JAR or a list of JARs.
            host: Ocient server host.
            port: Ocient server port.
            database: Database name.
            user: Username.
            password: Password.
            jdbc_params: Optional JDBC URL parameters, e.g., {"ssl": "true"}.
            autocommit: Whether to enable autocommit on the connection.
            driver_class: JDBC driver class name (default Ocient).
        """
        self._jdbc_url = _build_jdbc_url(host, port, database, jdbc_params)
        self._props: Dict[str, Any] = {"user": user, "password": password}
        self._conn = jaydebeapi.connect(
            jclassname=driver_class,
            url=self._jdbc_url,
            driver_args=self._props,
            jars=jdbc_jar_path,
        )
        # Set autocommit if requested (via underlying Java connection)
        try:
            self._conn.jconn.setAutoCommit(bool(autocommit))  # type: ignore[attr-defined]
        except Exception:
            # Fallback if jconn not available for some reason
            if autocommit:
                # Some drivers honor autocommit via connection property already
                pass

    def cursor(self):
        return self._conn.cursor()

    def execute(self, sql: str, params: Optional[Sequence[Any]] = None) -> None:
        cur = self.cursor()
        try:
            if params is None:
                cur.execute(sql)
            else:
                cur.execute(sql, params)
        finally:
            cur.close()

    def query(
        self,
        sql: str,
        params: Optional[Sequence[Any]] = None,
        fetch: str = "all",
        arraysize: int = 1000,
    ) -> List[Tuple[Any, ...]]:
        """
        Execute a query and fetch results.
        fetch: "all" (default), "one", or "many"
        arraysize: batch size for fetchmany
        """
        cur = self.cursor()
        try:
            cur.arraysize = arraysize
            if params is None:
                cur.execute(sql)
            else:
                cur.execute(sql, params)

            if fetch == "one":
                row = cur.fetchone()
                return [] if row is None else [row]
            elif fetch == "many":
                return cur.fetchmany(arraysize)
            else:
                return cur.fetchall()
        finally:
            cur.close()

    def executemany(self, sql: str, seq_of_params: Iterable[Sequence[Any]]) -> None:
        cur = self.cursor()
        try:
            cur.executemany(sql, seq_of_params)
        finally:
            cur.close()

    # Transaction control
    def commit(self) -> None:
        self._conn.commit()

    def rollback(self) -> None:
        self._conn.rollback()

    def set_autocommit(self, enabled: bool) -> None:
        try:
            self._conn.jconn.setAutoCommit(bool(enabled))  # type: ignore[attr-defined]
        except Exception:
            pass

    # Convenience DDL helpers (optional)
    def create_schema(self, schema_name: str, if_not_exists: bool = True) -> None:
        ine = "IF NOT EXISTS " if if_not_exists else ""
        self.execute(f"CREATE SCHEMA {ine}{schema_name}")

    def drop_schema(self, schema_name: str, if_exists: bool = True, cascade: bool = False) -> None:
        ie = "IF EXISTS " if if_exists else ""
        cas = " CASCADE" if cascade else ""
        self.execute(f"DROP SCHEMA {ie}{schema_name}{cas}")

    def create_table(self, fq_table: str, column_defs: str, if_not_exists: bool = True, extra: str = "") -> None:
        """
        fq_table: fully qualified table name, e.g., schema.table
        column_defs: e.g., "id INT, name VARCHAR(100)"
        extra: any extra table options, distribution, etc.
        """
        ine = "IF NOT EXISTS " if if_not_exists else ""
        stmt = f"CREATE TABLE {ine}{fq_table} ({column_defs})"
        if extra:
            stmt += f" {extra}"
        self.execute(stmt)

    def drop_table(self, fq_table: str, if_exists: bool = True, purge: bool = False) -> None:
        ie = "IF EXISTS " if if_exists else ""
        stmt = f"DROP TABLE {ie}{fq_table}"
        if purge:
            stmt += " PURGE"
        self.execute(stmt)

    def close(self) -> None:
        try:
            self._conn.close()
        except Exception:
            pass

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc, tb):
        # Roll back on error; otherwise commit if not autocommit
        try:
            if exc is not None:
                self.rollback()
            else:
                # Commit only if autocommit is off
                try:
                    if not bool(self._conn.jconn.getAutoCommit()):  # type: ignore[attr-defined]
                        self.commit()
                except Exception:
                    # If we cannot detect autocommit, make a safe commit
                    self.commit()
        finally:
            self.close()


# Example usage:
if __name__ == "__main__":
    # Replace with your environment details and JDBC jar path
    JDBC_JAR = "/path/to/ocient-jdbc-<version>.jar"
    HOST = "your-ocient-host"
    PORT = 12345
    DATABASE = "your_database"
    USER = "your_user"
    PASSWORD = "your_password"

    # Optional JDBC parameters, e.g., SSL
    jdbc_params = {
        # "ssl": "true",
        # "loginTimeout": "30"
    }

    with OcientClient(
        jdbc_jar_path=JDBC_JAR,
        host=HOST,
        port=PORT,
        database=DATABASE,
        user=USER,
        password=PASSWORD,
        jdbc_params=jdbc_params,
        autocommit=False,
    ) as oc:

        # Create schema and table
        oc.create_schema("demo_schema")
        oc.create_table(
            "demo_schema.demo_table",
            "id INT, name VARCHAR(100)",
            if_not_exists=True
        )

        # Insert multiple rows
        rows = [(1, "Alice"), (2, "Bob"), (3, "Charlie")]
        oc.executemany("INSERT INTO demo_schema.demo_table (id, name) VALUES (?, ?)", rows)

        # Query data
        result = oc.query("SELECT id, name FROM demo_schema.demo_table ORDER BY id")
        for r in result:
            print(r)

        # Clean up (optional)
        # oc.drop_table("demo_schema.demo_table")
        # oc.drop_schema("demo_schema", cascade=True)
------------------------------------------------------------

PROMPT 156:
"Generate Python code that uses the `type_annotations` module to generate type hints for a given function based on sampled production types, using the `sample_production_types` function to generate a representative set of types from a large dataset."
RESPONSE:
# Requires a `type_annotations` module that provides `sample_production_types`.
# This script:
# - samples representative production values for a target function
# - infers PEP 484/PEP 585-compatible type hints from those samples
# - optionally applies them to the function at runtime and/or prints a stub

from __future__ import annotations

import inspect
from collections import Counter, defaultdict
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union

from type_annotations import sample_production_types  # provided by your environment


# ---------- Type inference utilities ----------

NoneType = type(None)

def _flatten(iterables: Iterable[Iterable[Any]]) -> List[Any]:
    out: List[Any] = []
    for it in iterables:
        out.extend(it)
    return out

def _is_empty_container(x: Any) -> bool:
    return isinstance(x, (list, dict, set, tuple)) and len(x) == 0

def _dedupe_ann_list(anns: List[Any]) -> List[Any]:
    seen = set()
    out = []
    for a in anns:
        key = repr(a)
        if key not in seen:
            seen.add(key)
            out.append(a)
    return out

def _union_or_single(anns: List[Any]) -> Any:
    anns = _dedupe_ann_list(anns)
    if not anns:
        return Any
    if len(anns) == 1:
        return anns[0]
    # typing.Union supports unpacking via pipe operator in 3.10+, but use Union[] for compatibility
    return Union[tuple(anns)]  # type: ignore[index]

def _optionalize(ann: Any) -> Any:
    # If ann is already Optional[...] or includes None, keep as-is
    # Else return Optional[ann]
    if ann is Any:
        return Optional[Any]
    return Optional[ann]

def infer_annotation_from_values(values: List[Any], *, min_fraction: float = 0.01, max_union: int = 8) -> Any:
    """
    Infer a typing annotation object from a list of observed runtime values.

    - min_fraction: drop very rare types below this share of observations
    - max_union: cap the number of union members; if exceeded, return Any
    """
    if not values:
        return Any

    total = len(values)

    # Count top-level Python types
    type_counts: Counter[type] = Counter(type(v) for v in values)

    # Filter out very rare types
    kept_types: Set[type] = {
        t for t, c in type_counts.items()
        if (c / total) >= min_fraction
    }
    if not kept_types:
        kept_types = set(type_counts.keys())

    # Special-case: presence of None mixes optional
    saw_none = NoneType in kept_types
    non_none_types = [t for t in kept_types if t is not NoneType]

    # If too many heterogeneous types observed, bail to Any
    if len(non_none_types) > max_union:
        return _optionalize(Any) if saw_none else Any

    # Partition values by their top-level type among the kept types
    by_type: Dict[type, List[Any]] = defaultdict(list)
    for v in values:
        t = type(v)
        if t in kept_types:
            by_type[t].append(v)

    member_anns: List[Any] = []

    for t, vals in by_type.items():
        if t is NoneType:
            # handled via saw_none as optional at the end
            continue

        if t is list:
            # Gather element types
            elems = _flatten(vals)
            elem_ann = Any if not elems else infer_annotation_from_values(elems, min_fraction=min_fraction, max_union=max_union)
            member_anns.append(List[elem_ann])  # type: ignore[index]

        elif t is set:
            elems = _flatten(vals)
            elem_ann = Any if not elems else infer_annotation_from_values(elems, min_fraction=min_fraction, max_union=max_union)
            member_anns.append(Set[elem_ann])  # type: ignore[index]

        elif t is dict:
            keys = []
            vals_ = []
            for d in vals:
                keys.extend(list(d.keys()))
                vals_.extend(list(d.values()))
            key_ann = Any if not keys else infer_annotation_from_values(keys, min_fraction=min_fraction, max_union=max_union)
            val_ann = Any if not vals_ else infer_annotation_from_values(vals_, min_fraction=min_fraction, max_union=max_union)
            member_anns.append(Dict[key_ann, val_ann])  # type: ignore[index]

        elif t is tuple:
            # Fixed-length vs variable-length tuples
            lengths = {len(v) for v in vals}
            if len(lengths) == 1:
                L = next(iter(lengths))
                if L == 0:
                    member_anns.append(Tuple[()])  # type: ignore[index]
                else:
                    per_index_vals: List[List[Any]] = [[] for _ in range(L)]
                    for tup in vals:
                        for i, item in enumerate(tup):
                            per_index_vals[i].append(item)
                    per_index_anns = [infer_annotation_from_values(iv, min_fraction=min_fraction, max_union=max_union)
                                      for iv in per_index_vals]
                    member_anns.append(Tuple[tuple(per_index_anns)])  # type: ignore[index]
            else:
                elems = _flatten(vals)
                elem_ann = Any if not elems else infer_annotation_from_values(elems, min_fraction=min_fraction, max_union=max_union)
                member_anns.append(Tuple[elem_ann, ...])  # type: ignore[index]

        else:
            # Primitive or custom class
            member_anns.append(t)

    union_ann = _union_or_single(member_anns)
    return _optionalize(union_ann) if saw_none else union_ann


# ---------- Sample normalization ----------

def normalize_samples_for_function(func, samples) -> Dict[str, List[Any]]:
    """
    Accepts various shapes returned by sample_production_types and normalizes to:
      {
        "<param_name>": [values...],
        "return": [values...]
      }

    Supported input shapes:
    - {"params": {name: [values...]}, "return": [values...]}
    - {name: [values...], "return": [values...]}  (flat)
    - Iterable of call records like:
        {"args": (..), "kwargs": {...}, "return": value}
    """
    sig = inspect.signature(func)
    param_names = list(sig.parameters.keys())

    # Case 1: dict with "params"
    if isinstance(samples, dict) and "params" in samples:
        out = {k: list(v) for k, v in samples["params"].items()}
        if "return" in samples:
            out["return"] = list(samples["return"])
        return out

    # Case 2: flat dict name -> [values]
    if isinstance(samples, dict) and any(k in samples for k in param_names):
        out = {k: list(samples.get(k, [])) for k in param_names}
        if "return" in samples:
            out["return"] = list(samples["return"])
        return out

    # Case 3: iterable of call-records
    if isinstance(samples, Iterable):
        out: Dict[str, List[Any]] = {k: [] for k in param_names}
        out["return"] = []
        for rec in samples:
            # rec may be tuple-like or dict-like; expect dict with args/kwargs/return
            if not isinstance(rec, dict):
                continue
            args = rec.get("args", ())
            kwargs = rec.get("kwargs", {})
            ret = rec.get("return", None)

            bound = sig.bind_partial(*args, **kwargs)
            bound.apply_defaults()
            for name in param_names:
                if name in bound.arguments:
                    out[name].append(bound.arguments[name])
            out["return"].append(ret)
        return out

    raise ValueError("Unsupported samples format from sample_production_types().")


# ---------- Main: generate annotations ----------

def generate_type_hints_from_samples(
    func,
    samples,
    *,
    min_fraction: float = 0.01,
    max_union: int = 8,
) -> Dict[str, Any]:
    """
    Return a __annotations__-compatible mapping for a function, inferred from sampled values.
    """
    norm = normalize_samples_for_function(func, samples)
    sig = inspect.signature(func)

    annotations: Dict[str, Any] = {}

    # Parameters
    for name, param in sig.parameters.items():
        vals = norm.get(name, [])
        # Special handling for *args and **kwargs if present
        if param.kind == inspect.Parameter.VAR_POSITIONAL:
            # Expect samples to have tuples for *args if present
            ann = infer_annotation_from_values(vals or [()], min_fraction=min_fraction, max_union=max_union)
            # Ensure a tuple type for *args
            if ann is Any:
                ann = Tuple[Any, ...]
            annotations[name] = ann
        elif param.kind == inspect.Parameter.VAR_KEYWORD:
            # Expect dicts for **kwargs if present
            ann = infer_annotation_from_values(vals or [{}], min_fraction=min_fraction, max_union=max_union)
            if ann is Any:
                ann = Dict[str, Any]
            annotations[name] = ann
        else:
            annotations[name] = infer_annotation_from_values(vals, min_fraction=min_fraction, max_union=max_union)

    # Return type
    ret_vals = norm.get("return", [])
    annotations["return"] = infer_annotation_from_values(ret_vals, min_fraction=min_fraction, max_union=max_union)

    return annotations


# ---------- Applying and exporting ----------

def apply_annotations(func, annotations: Dict[str, Any]) -> None:
    """
    Apply inferred annotations to the function at runtime (func.__annotations__).
    """
    func.__annotations__ = {}
    func.__annotations__.update(annotations)

def render_annotation(ann: Any) -> str:
    """
    Best-effort rendering to a PEP 484 string for stubs/logging.
    """
    return repr(ann).replace("typing.", "")

def print_suggested_stub(func, annotations: Dict[str, Any], module_name: Optional[str] = None) -> None:
    """
    Print a .pyi-style stub for the function with inferred types.
    """
    sig = inspect.signature(func)
    parts = []
    for name, param in sig.parameters.items():
        ann = annotations.get(name, Any)
        if param.kind == inspect.Parameter.VAR_POSITIONAL:
            parts.append(f"*{name}: {render_annotation(ann)}")
        elif param.kind == inspect.Parameter.VAR_KEYWORD:
            parts.append(f"**{name}: {render_annotation(ann)}")
        else:
            parts.append(f"{name}: {render_annotation(ann)}")
    ret = render_annotation(annotations.get("return", Any))
    qual_name = func.__name__
    if module_name:
        qual_name = f"{module_name}.{qual_name}"
    print(f"def {func.__name__}({', '.join(parts)}) -> {ret}: ...")


# ---------- Example usage ----------

if __name__ == "__main__":
    # Example target function (untyped)
    def lookup_user(user_id, flags=None, metadata=None):
        # Imagine this is your production function whose types you want to infer.
        # Implementation omitted.
        pass

    # Suppose you have a large dataset of production call logs or records.
    # sample_production_types should internally subsample that dataset to return representative samples.
    # The shape returned can be:
    # - {"params": {"user_id": [...], "flags": [...], "metadata": [...]}, "return": [...]}
    # - Or an iterable of {"args": (...), "kwargs": {...}, "return": ...} records.
    production_dataset = ...  # your large dataset or handle

    # Use the provided sampler to get representative samples for the target function
    samples = sample_production_types(
        production_dataset,
        lookup_user,
        sample_size=5000,      # adjust to suit
        stratify=True,         # if the sampler supports it
        random_state=42,       # optional, for reproducibility
    )

    # Infer type hints from the sampled values
    annotations = generate_type_hints_from_samples(
        lookup_user,
        samples,
        min_fraction=0.02,  # drop very rare outlier types (<2% share)
        max_union=6,        # cap union explosion
    )

    # Option 1: Apply annotations to the function at runtime
    apply_annotations(lookup_user, annotations)

    # Option 2: Print a .pyi-style stub suggestion
    print_suggested_stub(lookup_user, annotations)

    # Option 3: Inspect the resulting __annotations__
    # print(lookup_user.__annotations__)
------------------------------------------------------------

PROMPT 157:
"Generate Python code that imports the SQLAlchemy module, creates a declarative base, defines a custom repr method for each model class using the column metadata, and uses the `pretty_repr` function from the `sqlalchemy` library to format the output."
RESPONSE:
import sqlalchemy as sa
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from sqlalchemy.util import pretty_repr


class Base(DeclarativeBase):
    pass


class ReprMixin:
    def __repr__(self) -> str:
        # Build a mapping of column keys to their current values using column metadata
        table = getattr(self.__class__, "__table__", None)
        if table is not None:
            data = {col.key: getattr(self, col.key) for col in table.columns}
        else:
            # Fallback via mapper inspection if __table__ isn't available
            mapper = sa.inspect(self.__class__)
            data = {attr.key: getattr(self, attr.key) for attr in mapper.column_attrs}
        return f"{self.__class__.__name__}({pretty_repr(data)})"


# Example models
class User(ReprMixin, Base):
    __tablename__ = "user"
    id: Mapped[int] = mapped_column(primary_key=True)
    name: Mapped[str] = mapped_column(sa.String(50))
    email: Mapped[str | None] = mapped_column(sa.String(255), nullable=True)


class Address(ReprMixin, Base):
    __tablename__ = "address"
    id: Mapped[int] = mapped_column(primary_key=True)
    user_id: Mapped[int] = mapped_column(sa.ForeignKey("user.id"))
    street: Mapped[str] = mapped_column(sa.String(200))


if __name__ == "__main__":
    u = User(id=1, name="Ada", email="ada@example.com")
    a = Address(id=10, user_id=1, street="123 Main St")
    print(u)
    print(a)
------------------------------------------------------------

PROMPT 158:
"Generate Python code that defines an AWS CDK construct that creates a RESTful API using the AWS CDK API Gateway module, which retrieves data from a DynamoDB table using the AWS CDK DynamoDB module and returns the data in response to HTTP GET requests."
RESPONSE:
# dyn_rest_api_construct.py

from typing import Optional

from aws_cdk import (
    Stack,
    RemovalPolicy,
    aws_apigateway as apigw,
    aws_dynamodb as dynamodb,
    aws_iam as iam,
)
from constructs import Construct


class DynamoRestApi(Construct):
    """
    A CDK construct that:
      - Creates a DynamoDB table (partition key: id)
      - Exposes a RESTful API Gateway with two GET endpoints:
          GET /items           -> Scan the table
          GET /items/{id}      -> GetItem by id
      - Integrates API Gateway directly with DynamoDB (no Lambda)
    Note: The responses are in DynamoDB's AttributeValue JSON format.
    """

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        *,
        table_name: Optional[str] = None,
        removal_policy: RemovalPolicy = RemovalPolicy.DESTROY,
    ) -> None:
        super().__init__(scope, construct_id)

        # DynamoDB table
        self.table = dynamodb.Table(
            self,
            "Table",
            table_name=table_name,
            partition_key=dynamodb.Attribute(name="id", type=dynamodb.AttributeType.STRING),
            billing_mode=dynamodb.BillingMode.PAY_PER_REQUEST,
            removal_policy=removal_policy,
        )

        # API Gateway REST API
        self.api = apigw.RestApi(
            self,
            "RestApi",
            rest_api_name="DynamoReaderApi",
            description="REST API that reads items from a DynamoDB table",
            deploy_options=apigw.StageOptions(throttling_rate_limit=50, throttling_burst_limit=100),
        )

        # Role for API Gateway to call DynamoDB
        apigw_role = iam.Role(
            self,
            "ApiGatewayDynamoRole",
            assumed_by=iam.ServicePrincipal("apigateway.amazonaws.com"),
        )
        self.table.grant_read_data(apigw_role)

        # Integration: GET /items -> DynamoDB Scan
        scan_integration = apigw.AwsIntegration(
            service="dynamodb",
            action="Scan",
            integration_http_method="POST",
            options=apigw.IntegrationOptions(
                credentials_role=apigw_role,
                request_templates={
                    "application/json": f"""{{
  "TableName": "{self.table.table_name}"
}}"""
                },
                integration_responses=[
                    apigw.IntegrationResponse(
                        status_code="200",
                        response_templates={
                            # Pass-through the DynamoDB response body as-is (AttributeValue JSON)
                            "application/json": "$input.body"
                        },
                    )
                ],
            ),
        )

        items = self.api.root.add_resource("items")
        items.add_method(
            http_method="GET",
            integration=scan_integration,
            method_responses=[apigw.MethodResponse(status_code="200")],
        )

        # Integration: GET /items/{id} -> DynamoDB GetItem
        get_item_integration = apigw.AwsIntegration(
            service="dynamodb",
            action="GetItem",
            integration_http_method="POST",
            options=apigw.IntegrationOptions(
                credentials_role=apigw_role,
                request_templates={
                    # Uses the path parameter {id} to fetch a specific item
                    "application/json": f"""{{
  "TableName": "{self.table.table_name}",
  "Key": {{
    "id": {{ "S": "$input.params('id')" }}
  }}
}}"""
                },
                integration_responses=[
                    apigw.IntegrationResponse(
                        status_code="200",
                        response_templates={
                            # Pass-through the DynamoDB response body (contains "Item")
                            "application/json": "$input.body"
                        },
                    )
                ],
            ),
        )

        item_by_id = items.add_resource("{id}")
        item_by_id.add_method(
            http_method="GET",
            integration=get_item_integration,
            method_responses=[apigw.MethodResponse(status_code="200")],
        )


# Example usage in a CDK Stack
class ApiWithDynamoStack(Stack):
    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        api_construct = DynamoRestApi(self, "DynamoRestApi")

        # Optionally access:
        # api_construct.table
        # api_construct.api

"""
Notes:
- The responses are returned in DynamoDB's AttributeValue JSON (e.g., {"Item":{"id":{"S":"123"},"name":{"S":"Alice"}}}).
  If you want plain JSON, consider:
  - Using a Lambda function to format the response, or
  - Writing a more advanced API Gateway mapping template to transform AttributeValue JSON to plain JSON.
- For production, consider enabling request validation, IAM/authorizers, and more robust error handling.
"""
------------------------------------------------------------

PROMPT 159:
"Generate Python code that creates a PostgreSQL high-available orchestrator and CLI using the psycopg2 library to interact with PostgreSQL databases, the pyudev library to monitor hardware events, and the argparse library to create command line interfaces."
RESPONSE:
#!/usr/bin/env python3
import argparse
import json
import logging
import os
import signal
import socket
import sys
import threading
import time
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Optional, Tuple

import psycopg2
import psycopg2.extras
import psycopg2.errors

try:
    import pyudev  # Linux-only
except Exception:
    pyudev = None


# -----------------------------
# Data structures and defaults
# -----------------------------

DEFAULT_CONFIG_PATH = os.environ.get("PG_HA_CONFIG", "./pg_ha_config.json")
DEFAULT_CHECK_INTERVAL = 5
DEFAULT_CONNECT_TIMEOUT = 3
DEFAULT_STATEMENT_TIMEOUT_MS = 5000

logging.basicConfig(
    level=os.environ.get("PG_HA_LOGLEVEL", "INFO"),
    format="%(asctime)s %(levelname)s %(threadName)s %(message)s",
)

@dataclass
class NodeConfig:
    name: str
    host: str
    port: int = 5432
    dbname: str = "postgres"
    user: str = "postgres"
    password: Optional[str] = None
    data_device: Optional[str] = None  # e.g., /dev/sdb (for pyudev hardware monitoring)
    connect_timeout: int = DEFAULT_CONNECT_TIMEOUT

    def dsn(self) -> str:
        # We add connect_timeout and application_name to help identify connections
        parts = [
            f"host={self.host}",
            f"port={self.port}",
            f"dbname={self.dbname}",
            f"user={self.user}",
            f"connect_timeout={self.connect_timeout}",
            f"application_name=pg_ha_orchestrator",
        ]
        if self.password:
            parts.append(f"password={self.password}")
        return " ".join(parts)


class ConfigStore:
    def __init__(self, path: str = DEFAULT_CONFIG_PATH):
        self.path = path
        self.lock = threading.Lock()
        self.data: Dict[str, Any] = {
            "cluster_name": "cluster",
            "primary": None,
            "nodes": [],
        }
        self.load()

    def load(self):
        if os.path.exists(self.path):
            with open(self.path, "r") as f:
                self.data = json.load(f)
        else:
            self.save()

    def save(self):
        with self.lock:
            tmp = f"{self.path}.tmp"
            with open(tmp, "w") as f:
                json.dump(self.data, f, indent=2, sort_keys=True)
            os.replace(tmp, self.path)

    def list_nodes(self) -> List[NodeConfig]:
        return [NodeConfig(**n) for n in self.data.get("nodes", [])]

    def get_node(self, name_or_host: str) -> Optional[NodeConfig]:
        for n in self.list_nodes():
            if n.name == name_or_host or n.host == name_or_host:
                return n
        return None

    def set_primary(self, name: Optional[str]):
        self.data["primary"] = name
        self.save()

    def get_primary_name(self) -> Optional[str]:
        return self.data.get("primary")

    def add_node(self, node: NodeConfig):
        if self.get_node(node.name) or self.get_node(node.host):
            raise ValueError("Node with same name or host already exists")
        self.data["nodes"].append(asdict(node))
        self.save()

    def remove_node(self, name_or_host: str):
        nodes = self.list_nodes()
        new_nodes = [asdict(n) for n in nodes if n.name != name_or_host and n.host != name_or_host]
        if len(new_nodes) == len(nodes):
            raise ValueError("Node not found")
        self.data["nodes"] = new_nodes
        if self.get_primary_name() == name_or_host:
            self.set_primary(None)
        self.save()

    def set_cluster_name(self, name: str):
        self.data["cluster_name"] = name
        self.save()

    def cluster_name(self) -> str:
        return self.data.get("cluster_name", "cluster")


# -----------------------------
# PostgreSQL helpers
# -----------------------------

class PGClient:
    def __init__(self, node: NodeConfig):
        self.node = node
        self.conn = None

    def __enter__(self):
        self.connect()
        return self

    def __exit__(self, exc_type, exc, tb):
        self.close()

    def connect(self):
        self.conn = psycopg2.connect(self.node.dsn())
        self.conn.autocommit = True
        with self.conn.cursor() as cur:
            try:
                cur.execute(f"SET statement_timeout = {DEFAULT_STATEMENT_TIMEOUT_MS}")
            except psycopg2.Error:
                pass

    def close(self):
        if self.conn:
            try:
                self.conn.close()
            except Exception:
                pass
            self.conn = None

    def query_one(self, sql: str, params: Tuple = ()) -> Optional[Tuple]:
        with self.conn.cursor() as cur:
            cur.execute(sql, params)
            return cur.fetchone()

    def exec(self, sql: str, params: Tuple = ()):
        with self.conn.cursor() as cur:
            cur.execute(sql, params)

    def is_up(self) -> bool:
        try:
            with self:
                self.query_one("SELECT 1")
            return True
        except Exception:
            return False
        finally:
            self.close()

    def is_in_recovery(self) -> Optional[bool]:
        try:
            with self:
                row = self.query_one("SELECT pg_is_in_recovery()")
                return bool(row[0]) if row else None
        except Exception:
            return None
        finally:
            self.close()

    def current_wal_lsn(self) -> Optional[str]:
        try:
            with self:
                row = self.query_one("SELECT pg_current_wal_lsn()")
                return row[0] if row else None
        except Exception:
            return None
        finally:
            self.close()

    def last_replay_lsn(self) -> Optional[str]:
        try:
            with self:
                row = self.query_one("SELECT pg_last_wal_replay_lsn()")
                return row[0] if row else None
        except Exception:
            return None
        finally:
            self.close()

    def replay_delay(self) -> Optional[float]:
        # Seconds behind primary based on last replay timestamp
        try:
            with self:
                row = self.query_one(
                    "SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))"
                )
                return float(row[0]) if row and row[0] is not None else None
        except Exception:
            return None
        finally:
            self.close()

    def promote(self, wait_seconds: int = 60) -> bool:
        try:
            with self:
                # pg_promote(wait, wait_seconds) since PG12
                row = self.query_one("SELECT pg_promote(true, %s)", (wait_seconds,))
                if row and row[0] is True:
                    # Verify out of recovery
                    for _ in range(wait_seconds):
                        time.sleep(1)
                        r = self.query_one("SELECT pg_is_in_recovery()")
                        if r and r[0] is False:
                            return True
                    return False
                return False
        except Exception as e:
            logging.error("Promote failed on %s: %s", self.node.name, e)
            return False
        finally:
            self.close()

    def set_primary_conninfo(self, new_primary: NodeConfig, application_name: Optional[str] = None) -> bool:
        appname = application_name or self.node.name
        # Build a basic primary_conninfo string. In real setups you'd use a dedicated replication user.
        conninfo = f"host={new_primary.host} port={new_primary.port} user={self.node.user}"
        if self.node.password:
            conninfo += f" password={self.node.password}"
        conninfo += f" application_name={appname} sslmode=prefer"
        try:
            with self:
                self.exec("ALTER SYSTEM SET primary_conninfo = %s", (conninfo,))
                self.exec("SELECT pg_reload_conf()")
            return True
        except Exception as e:
            logging.error("Failed to set primary_conninfo on %s: %s", self.node.name, e)
            return False
        finally:
            self.close()

    def set_read_only(self, read_only: bool = True) -> bool:
        try:
            with self:
                self.exec("ALTER SYSTEM SET default_transaction_read_only = %s", ("on" if read_only else "off",))
                self.exec("SELECT pg_reload_conf()")
            return True
        except Exception as e:
            logging.error("Failed to set read-only=%s on %s: %s", read_only, self.node.name, e)
            return False
        finally:
            self.close()


# -----------------------------
# Orchestrator
# -----------------------------

class Orchestrator:
    def __init__(self, cfg: ConfigStore):
        self.cfg = cfg
        self._stop = threading.Event()
        self._lock = threading.RLock()

    def stop(self):
        self._stop.set()

    def list_nodes(self) -> List[NodeConfig]:
        return self.cfg.list_nodes()

    def get_primary(self) -> Optional[NodeConfig]:
        name = self.cfg.get_primary_name()
        if not name:
            return None
        return self.cfg.get_node(name)

    def detect_primary(self) -> Optional[NodeConfig]:
        # Try configured primary first
        primary = self.get_primary()
        if primary:
            is_up = PGClient(primary).is_up()
            is_primary = (PGClient(primary).is_in_recovery() is False)
            if is_up and is_primary:
                return primary
        # Fallback: discover among nodes
        for n in self.list_nodes():
            try:
                is_rec = PGClient(n).is_in_recovery()
                if is_rec is False:
                    self.cfg.set_primary(n.name)
                    return n
            except Exception:
                continue
        return None

    def node_status(self, node: NodeConfig, primary_lsn: Optional[str] = None) -> Dict[str, Any]:
        status: Dict[str, Any] = {
            "name": node.name,
            "host": node.host,
            "port": node.port,
            "up": False,
            "in_recovery": None,
            "role": "unknown",
            "replay_delay_sec": None,
            "lag_bytes": None,
            "lsn": None,
        }
        try:
            with PGClient(node) as cli:
                status["up"] = True
                r = cli.query_one("SELECT pg_is_in_recovery()")
                if r is not None:
                    status["in_recovery"] = bool(r[0])
                    status["role"] = "replica" if status["in_recovery"] else "primary"
                if status["role"] == "primary":
                    lsnrow = cli.query_one("SELECT pg_current_wal_lsn()")
                    status["lsn"] = lsnrow[0] if lsnrow else None
                else:
                    lsnrow = cli.query_one("SELECT pg_last_wal_replay_lsn()")
                    status["lsn"] = lsnrow[0] if lsnrow else None
                    drow = cli.query_one(
                        "SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))"
                    )
                    status["replay_delay_sec"] = float(drow[0]) if drow and drow[0] is not None else None

                # lag in bytes if we know primary LSN and this is a replica with replay_lsn
                if primary_lsn and status["role"] == "replica" and status["lsn"]:
                    prow = cli.query_one("SELECT pg_wal_lsn_diff(%s, %s)", (primary_lsn, status["lsn"]))
                    status["lag_bytes"] = int(prow[0]) if prow and prow[0] is not None else None

        except Exception as e:
            status["error"] = str(e)
        return status

    def cluster_status(self) -> List[Dict[str, Any]]:
        primary = self.detect_primary()
        primary_lsn = None
        if primary:
            try:
                primary_lsn = PGClient(primary).current_wal_lsn()
            except Exception:
                primary_lsn = None
        statuses = []
        for n in self.list_nodes():
            statuses.append(self.node_status(n, primary_lsn))
        return statuses

    def choose_failover_candidate(self, exclude: Optional[str] = None) -> Optional[NodeConfig]:
        primary = self.get_primary()
        candidates: List[Tuple[NodeConfig, float]] = []
        for n in self.list_nodes():
            if exclude and (n.name == exclude or n.host == exclude):
                continue
            if primary and n.name == primary.name:
                continue
            try:
                is_up = PGClient(n).is_up()
                in_recovery = PGClient(n).is_in_recovery()
                if is_up and in_recovery is True:
                    delay = PGClient(n).replay_delay()
                    candidates.append((n, delay if delay is not None else float("inf")))
            except Exception:
                continue
        candidates.sort(key=lambda x: x[1])
        return candidates[0][0] if candidates else None

    def perform_failover(self, failed_primary: Optional[NodeConfig] = None) -> Optional[NodeConfig]:
        with self._lock:
            logging.warning("Initiating failover. Failed primary: %s", failed_primary.name if failed_primary else None)
            candidate = self.choose_failover_candidate(exclude=failed_primary.name if failed_primary else None)
            if not candidate:
                logging.error("No suitable replica found for promotion.")
                return None

            logging.info("Promoting replica %s (%s)", candidate.name, candidate.host)
            if not PGClient(candidate).promote(wait_seconds=60):
                logging.error("Promotion failed for %s", candidate.name)
                return None

            self.cfg.set_primary(candidate.name)
            logging.info("New primary is %s", candidate.name)

            # Repoint other replicas to the new primary
            for n in self.list_nodes():
                if n.name == candidate.name:
                    continue
                try:
                    rec = PGClient(n).is_in_recovery()
                    if rec is True:
                        PGClient(n).set_primary_conninfo(candidate)
                except Exception as e:
                    logging.warning("Failed to repoint replica %s: %s", n.name, e)

            return candidate

    def promote(self, target: NodeConfig) -> bool:
        with self._lock:
            logging.info("Promoting %s", target.name)
            if PGClient(target).promote(wait_seconds=60):
                self.cfg.set_primary(target.name)
                # Repoint replicas
                for n in self.list_nodes():
                    if n.name == target.name:
                        continue
                    try:
                        rec = PGClient(n).is_in_recovery()
                        if rec is True:
                            PGClient(n).set_primary_conninfo(target)
                    except Exception as e:
                        logging.warning("Failed to repoint replica %s: %s", n.name, e)
                return True
            else:
                return False

    def demote(self, target: NodeConfig) -> bool:
        # There's no real demote-to-standby via SQL alone; this makes node read-only at SQL level.
        # Operational demotion normally requires service control and recovery configuration.
        logging.info("Setting %s to read-only (soft demote)", target.name)
        return PGClient(target).set_read_only(True)

    def align_replicas_to_primary(self) -> None:
        primary = self.detect_primary()
        if not primary:
            logging.error("No primary detected; cannot align replicas.")
            return
        for n in self.list_nodes():
            if n.name == primary.name:
                continue
            try:
                if PGClient(n).is_in_recovery() is True:
                    PGClient(n).set_primary_conninfo(primary)
            except Exception as e:
                logging.warning("Failed to align %s to primary %s: %s", n.name, primary.name, e)

    def monitor_loop(self, check_interval: int = DEFAULT_CHECK_INTERVAL, auto_failover: bool = True,
                     hardware_monitor: bool = True):
        logging.info("Starting monitor loop: interval=%ss auto_failover=%s hw_monitor=%s",
                     check_interval, auto_failover, hardware_monitor)

        devmon = None
        if hardware_monitor and pyudev:
            try:
                devmon = DeviceMonitor(self, self.cfg)
                devmon.start()
            except Exception as e:
                logging.warning("Failed to start hardware monitor: %s", e)

        def handle_sig(signum, frame):
            logging.info("Signal %s received, stopping monitor.", signum)
            self.stop()

        signal.signal(signal.SIGINT, handle_sig)
        signal.signal(signal.SIGTERM, handle_sig)

        while not self._stop.is_set():
            try:
                primary = self.detect_primary()
                if primary:
                    up = PGClient(primary).is_up()
                    in_recovery = PGClient(primary).is_in_recovery()
                    if not up or in_recovery:
                        logging.warning("Primary %s is down or not primary anymore (up=%s in_recovery=%s)",
                                        primary.name, up, in_recovery)
                        if auto_failover:
                            self.perform_failover(failed_primary=primary)
                else:
                    logging.warning("No primary configured/detected.")
                    if auto_failover:
                        self.perform_failover()
            except Exception as e:
                logging.error("Monitor loop error: %s", e)
            time.sleep(check_interval)

        if devmon:
            devmon.stop()
            devmon.join()
        logging.info("Monitor loop stopped.")


# -----------------------------
# Hardware device monitor
# -----------------------------

class DeviceMonitor(threading.Thread):
    def __init__(self, orchestrator: Orchestrator, cfg: ConfigStore):
        super().__init__(name="DeviceMonitor", daemon=True)
        if not pyudev:
            raise RuntimeError("pyudev not available on this system")
        self.orch = orchestrator
        self.cfg = cfg
        self._stop = threading.Event()
        self.context = pyudev.Context()
        self.monitor = pyudev.Monitor.from_netlink(self.context)
        self.monitor.filter_by("block")
        self.observer = None

    def run(self):
        logging.info("Hardware monitor started (listening for block device events).")
        self.observer = pyudev.MonitorObserver(self.monitor, callback=self._event_handler, name="UdevObserver")
        self.observer.start()
        while not self._stop.is_set():
            time.sleep(0.5)
        logging.info("Hardware monitor stopping...")
        try:
            self.observer.stop()
        except Exception:
            pass

    def stop(self):
        self._stop.set()

    def _event_handler(self, action, device):
        # device.device_node like /dev/sdb
        devnode = getattr(device, "device_node", None)
        if not devnode:
            return
        # Check if event concerns any configured node's data device
        impacted_nodes = [n for n in self.cfg.list_nodes() if n.data_device and os.path.realpath(n.data_device) == os.path.realpath(devnode)]
        if not impacted_nodes:
            return

        logging.warning("Hardware event action=%s on %s affects nodes: %s", action, devnode, [n.name for n in impacted_nodes])

        # If the impacted node is the current primary and the action is severe, trigger failover
        primary = self.orch.get_primary()
        if primary:
            for n in impacted_nodes:
                if n.name == primary.name and action in ("remove", "offline", "change"):
                    logging.error("Primary %s storage event detected (%s on %s). Initiating failover.", n.name, action, devnode)
                    try:
                        self.orch.perform_failover(failed_primary=primary)
                    except Exception as e:
                        logging.error("Failover attempt failed after hardware event: %s", e)


# -----------------------------
# CLI
# -----------------------------

def cmd_init(args):
    cfg = ConfigStore(args.config)
    cfg.set_cluster_name(args.cluster_name)
    if args.nodes:
        for nd in args.nodes:
            node = NodeConfig(
                name=nd["name"],
                host=nd["host"],
                port=int(nd.get("port", 5432)),
                dbname=nd.get("dbname", "postgres"),
                user=nd.get("user", "postgres"),
                password=nd.get("password"),
                data_device=nd.get("data_device"),
            )
            cfg.add_node(node)
    if args.primary:
        cfg.set_primary(args.primary)
    print(f"Initialized config at {cfg.path} for cluster {cfg.cluster_name()}.")


def cmd_add_node(args):
    cfg = ConfigStore(args.config)
    node = NodeConfig(
        name=args.name,
        host=args.host,
        port=args.port,
        dbname=args.dbname,
        user=args.user,
        password=args.password,
        data_device=args.data_device,
    )
    cfg.add_node(node)
    print(f"Added node {node.name} ({node.host}).")


def cmd_remove_node(args):
    cfg = ConfigStore(args.config)
    cfg.remove_node(args.node)
    print(f"Removed node {args.node}.")


def cmd_status(args):
    cfg = ConfigStore(args.config)
    orch = Orchestrator(cfg)
    statuses = orch.cluster_status()
    for s in statuses:
        print(f"{s['name']}@{s['host']}:{s['port']} up={s['up']} role={s['role']} in_recovery={s['in_recovery']} "
              f"lsn={s.get('lsn')} lag_bytes={s.get('lag_bytes')} delay_sec={s.get('replay_delay_sec')} "
              f"{'error='+s['error'] if 'error' in s else ''}")


def cmd_promote(args):
    cfg = ConfigStore(args.config)
    orch = Orchestrator(cfg)
    node = cfg.get_node(args.node)
    if not node:
        print(f"Node {args.node} not found.", file=sys.stderr)
        sys.exit(1)
    ok = orch.promote(node)
    print("Promote success." if ok else "Promote failed.")
    sys.exit(0 if ok else 2)


def cmd_demote(args):
    cfg = ConfigStore(args.config)
    orch = Orchestrator(cfg)
    node = cfg.get_node(args.node)
    if not node:
        print(f"Node {args.node} not found.", file=sys.stderr)
        sys.exit(1)
    ok = orch.demote(node)
    print("Demote (read-only) success." if ok else "Demote failed.")
    sys.exit(0 if ok else 2)


def cmd_align_replicas(args):
    cfg = ConfigStore(args.config)
    orch = Orchestrator(cfg)
    orch.align_replicas_to_primary()
    print("Replicas aligned to current primary.")


def cmd_monitor(args):
    cfg = ConfigStore(args.config)
    orch = Orchestrator(cfg)
    orch.monitor_loop(check_interval=args.interval, auto_failover=not args.no_auto_failover, hardware_monitor=not args.no_hw)
    print("Monitor exited.")


def cmd_watch_hw(args):
    if not pyudev:
        print("pyudev not installed or not available on this OS.", file=sys.stderr)
        sys.exit(1)
    ctx = pyudev.Context()
    monitor = pyudev.Monitor.from_netlink(ctx)
    monitor.filter_by("block")
    observer = pyudev.MonitorObserver(monitor, callback=lambda action, device: print(f"{action} {getattr(device, 'device_node', '')}"))
    print("Watching block device events. Ctrl-C to exit.")
    try:
        observer.start()
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        pass
    finally:
        observer.stop()


def parse_nodes_list(values: List[str]) -> List[Dict[str, Any]]:
    # Accept inputs like: name=pg1,host=10.0.0.1,port=5432,user=rep,password=secret,data_device=/dev/sdb
    nodes = []
    for v in values:
        parts = v.split(",")
        d = {}
        for p in parts:
            if "=" in p:
                k, val = p.split("=", 1)
                d[k.strip()] = val.strip()
        if "name" not in d or "host" not in d:
            raise ValueError("Each node must include at least name and host")
        nodes.append(d)
    return nodes


def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="PostgreSQL HA Orchestrator and CLI (psycopg2 + pyudev + argparse)")
    p.add_argument("--config", default=DEFAULT_CONFIG_PATH, help=f"Path to config (default: {DEFAULT_CONFIG_PATH})")

    sub = p.add_subparsers(dest="cmd", required=True)

    sp = sub.add_parser("init", help="Initialize cluster configuration")
    sp.add_argument("--cluster-name", required=True, help="Cluster name")
    sp.add_argument("--primary", help="Initial primary node name")
    sp.add_argument("--nodes", nargs="*", type=str, help="Nodes list: name=pg1,host=10.0.0.1,port=5432,user=postgres,password=...,data_device=/dev/sdb")
    sp.set_defaults(func=lambda args: cmd_init(_massage_init_args(args)))

    sp = sub.add_parser("add-node", help="Add a node to config")
    sp.add_argument("--name", required=True)
    sp.add_argument("--host", required=True)
    sp.add_argument("--port", type=int, default=5432)
    sp.add_argument("--dbname", default="postgres")
    sp.add_argument("--user", default="postgres")
    sp.add_argument("--password")
    sp.add_argument("--data-device", help="e.g., /dev/sdb (for hardware monitoring)")
    sp.set_defaults(func=cmd_add_node)

    sp = sub.add_parser("remove-node", help="Remove a node from config")
    sp.add_argument("node", help="Node name or host")
    sp.set_defaults(func=cmd_remove_node)

    sp = sub.add_parser("status", help="Show cluster status")
    sp.set_defaults(func=cmd_status)

    sp = sub.add_parser("promote", help="Promote a replica to primary")
    sp.add_argument("node", help="Node name or host")
    sp.set_defaults(func=cmd_promote)

    sp = sub.add_parser("demote", help="Soft-demote a primary to read-only")
    sp.add_argument("node", help="Node name or host")
    sp.set_defaults(func=cmd_demote)

    sp = sub.add_parser("align-replicas", help="Point all replicas to current primary")
    sp.set_defaults(func=cmd_align_replicas)

    sp = sub.add_parser("monitor", help="Run monitor loop")
    sp.add_argument("--interval", type=int, default=DEFAULT_CHECK_INTERVAL, help="Health check interval seconds")
    sp.add_argument("--no-auto-failover", action="store_true", help="Do not trigger automatic failover")
    sp.add_argument("--no-hw", action="store_true", help="Disable hardware monitoring (pyudev)")
    sp.set_defaults(func=cmd_monitor)

    sp = sub.add_parser("watch-hw", help="Watch and print block device events (debug)")
    sp.set_defaults(func=cmd_watch_hw)

    return p


def _massage_init_args(args):
    # Parse nodes string list into dictionaries
    if args.nodes:
        try:
            args.nodes = parse_nodes_list(args.nodes)
        except ValueError as e:
            print(f"Invalid --nodes format: {e}", file=sys.stderr)
            sys.exit(2)
    else:
        args.nodes = []
    return args


def main():
    parser = build_parser()
    args = parser.parse_args()
    try:
        args.func(args)
    except KeyboardInterrupt:
        pass


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 160:
"Generate Python code that imports the Pandas library and creates a set of ETL utility functions for data cleaning, transformation, and loading, including data wrangling, filtering, grouping, merging, joining, and reshaping, using concise and readable code."
RESPONSE:
from __future__ import annotations

from pathlib import Path
from functools import reduce
from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union, Literal

import pandas as pd


# ------------------------------
# Loading and saving
# ------------------------------

def load(path: str | Path, fmt: Optional[str] = None, **kwargs) -> pd.DataFrame:
    """Load a DataFrame from csv/parquet/excel/json by extension or fmt."""
    p = Path(path)
    ext = (fmt or p.suffix.lstrip(".")).lower()
    if ext in {"csv"}:
        return pd.read_csv(p, **kwargs)
    if ext in {"parquet", "pq"}:
        return pd.read_parquet(p, **kwargs)
    if ext in {"xlsx", "xls"}:
        return pd.read_excel(p, **kwargs)
    if ext in {"json"}:
        return pd.read_json(p, **kwargs)
    if ext in {"feather", "ft"}:
        return pd.read_feather(p, **kwargs)
    raise ValueError(f"Unsupported format: {ext}")


def save(
    df: pd.DataFrame,
    path: str | Path,
    fmt: Optional[str] = None,
    index: bool = False,
    **kwargs,
) -> None:
    """Save a DataFrame to csv/parquet/excel/json by extension or fmt."""
    p = Path(path)
    ext = (fmt or p.suffix.lstrip(".")).lower()
    if ext == "csv":
        df.to_csv(p, index=index, **kwargs)
    elif ext in {"parquet", "pq"}:
        df.to_parquet(p, index=index, **kwargs)
    elif ext in {"xlsx", "xls"}:
        df.to_excel(p, index=index, **kwargs)
    elif ext == "json":
        df.to_json(p, **kwargs)
    elif ext in {"feather", "ft"}:
        df.to_feather(p, **kwargs)
    else:
        raise ValueError(f"Unsupported format: {ext}")


# ------------------------------
# Data cleaning / wrangling
# ------------------------------

def clean_columns(
    df: pd.DataFrame,
    case: Literal["snake", "lower", "upper", "none"] = "snake",
) -> pd.DataFrame:
    """Normalize column names (trim, dedupe separators)."""
    out = df.copy()
    cols = (
        pd.Index(map(str, out.columns))
        .str.strip()
        .str.replace(r"[^\w]+", "_", regex=True)
        .str.replace(r"__+", "_", regex=True)
        .str.strip("_")
    )
    if case == "snake":
        cols = cols.str.lower()
    elif case == "lower":
        cols = cols.str.lower()
    elif case == "upper":
        cols = cols.str.upper()
    out.columns = cols
    return out


def rename_columns(
    df: pd.DataFrame,
    mapping: Dict[str, str] | Callable[[str], str],
) -> pd.DataFrame:
    """Rename columns via dict or callable."""
    return df.rename(columns=mapping)


def trim_whitespace(
    df: pd.DataFrame,
    cols: Optional[Sequence[str]] = None,
) -> pd.DataFrame:
    """Strip leading/trailing whitespace in object/string columns (or subset)."""
    out = df.copy()
    target = cols or out.select_dtypes(include=["object", "string"]).columns
    out[target] = out[target].apply(lambda s: s.astype("string").str.strip())
    return out


def to_datetime_cols(
    df: pd.DataFrame,
    cols: Sequence[str],
    dayfirst: bool = False,
    utc: bool = False,
    errors: Literal["raise", "coerce", "ignore"] = "coerce",
    format: Optional[str] = None,
) -> pd.DataFrame:
    """Convert given columns to datetime."""
    out = df.copy()
    for c in cols:
        out[c] = pd.to_datetime(out[c], errors=errors, dayfirst=dayfirst, utc=utc, format=format)
    return out


def convert_dtypes_infer(df: pd.DataFrame) -> pd.DataFrame:
    """Use pandas' convert_dtypes to infer best dtypes."""
    return df.convert_dtypes()


def fill_missing(
    df: pd.DataFrame,
    values: Any | Dict[str, Any] = None,
    method: Optional[Literal["ffill", "bfill"]] = None,
) -> pd.DataFrame:
    """Fill missing values via scalar/dict or method."""
    out = df.copy()
    if method:
        out = out.fillna(method=method)
    if values is not None:
        out = out.fillna(values)
    return out


def drop_empty(
    df: pd.DataFrame,
    axis: Literal["rows", "columns"] = "rows",
    how: Literal["any", "all"] = "all",
) -> pd.DataFrame:
    """Drop empty rows/columns based on NA."""
    ax = 0 if axis == "rows" else 1
    return df.dropna(axis=ax, how=how)


def deduplicate(
    df: pd.DataFrame,
    subset: Optional[Sequence[str]] = None,
    keep: Literal["first", "last", False] = "first",
) -> pd.DataFrame:
    """Drop duplicate rows."""
    return df.drop_duplicates(subset=subset, keep=keep)


def select_columns(df: pd.DataFrame, cols: Sequence[str]) -> pd.DataFrame:
    """Select a subset of columns (preserves order)."""
    return df.loc[:, list(cols)]


def reorder_columns(
    df: pd.DataFrame,
    first: Optional[Sequence[str]] = None,
    last: Optional[Sequence[str]] = None,
    sort_remaining: bool = False,
) -> pd.DataFrame:
    """Reorder columns by placing 'first' and/or 'last' explicitly."""
    first = list(first or [])
    last = list(last or [])
    middle = [c for c in df.columns if c not in set(first) | set(last)]
    if sort_remaining:
        middle = sorted(middle)
    cols = [*first, *middle, *last]
    return df.loc[:, cols]


def split_column(
    df: pd.DataFrame,
    col: str,
    into: Sequence[str],
    sep: str | None = None,
    n: int = -1,
    drop: bool = True,
) -> pd.DataFrame:
    """Split a text column into multiple columns."""
    out = df.copy()
    parts = out[col].astype("string").str.split(sep, n=n, expand=True)
    parts.columns = list(into)
    if drop:
        out = out.drop(columns=[col])
    for c in parts.columns:
        out[c] = parts[c]
    return out


def combine_columns(
    df: pd.DataFrame,
    cols: Sequence[str],
    new_col: str,
    sep: str = "_",
    drop: bool = False,
) -> pd.DataFrame:
    """Combine multiple columns into one string column."""
    out = df.copy()
    out[new_col] = out.loc[:, cols].astype("string").agg(sep.join, axis=1)
    if drop:
        out = out.drop(columns=list(cols))
    return out


# ------------------------------
# Filtering and transformation
# ------------------------------

def filter_rows(
    df: pd.DataFrame,
    expr: Optional[str] = None,
    func: Optional[Callable[[pd.DataFrame], pd.Series]] = None,
) -> pd.DataFrame:
    """Filter rows via DataFrame.query expression or boolean mask function."""
    if expr is not None:
        return df.query(expr)
    if func is not None:
        mask = func(df)
        return df.loc[mask]
    return df.copy()


def mutate(
    df: pd.DataFrame,
    **assignments: Callable[[pd.DataFrame], Any] | Any,
) -> pd.DataFrame:
    """Assign new or transformed columns (accepts callables or scalars)."""
    out = df.copy()
    resolved = {
        k: (v(out) if callable(v) else v)
        for k, v in assignments.items()
    }
    return out.assign(**resolved)


def sort_by(
    df: pd.DataFrame,
    by: str | Sequence[str],
    ascending: bool | Sequence[bool] = True,
) -> pd.DataFrame:
    """Sort rows by columns."""
    return df.sort_values(by=by, ascending=ascending)


# ------------------------------
# Grouping and aggregation
# ------------------------------

def groupby_agg(
    df: pd.DataFrame,
    by: str | Sequence[str],
    agg: Dict[str, Union[str, Sequence[str], Callable]],
    dropna: bool = False,
) -> pd.DataFrame:
    """Group by columns and aggregate; returns a flattened DataFrame."""
    g = df.groupby(by, dropna=dropna).agg(agg)
    if isinstance(g.columns, pd.MultiIndex):
        g.columns = ["_".join(filter(None, map(str, c))).strip("_") for c in g.columns.to_flat_index()]
    return g.reset_index()


def crosstab(
    df: pd.DataFrame,
    index: str | Sequence[str],
    columns: str | Sequence[str],
    values: Optional[str] = None,
    aggfunc: Union[str, Callable] = "size",
    normalize: Optional[str] = None,
) -> pd.DataFrame:
    """Convenience wrapper around pd.crosstab/pivot_table style counts."""
    if values is None and aggfunc == "size":
        return pd.crosstab(index=df[index], columns=df[columns], normalize=normalize)
    return pd.pivot_table(
        df, index=index, columns=columns, values=values, aggfunc=aggfunc, fill_value=0
    ).reset_index()


# ------------------------------
# Merging and joining
# ------------------------------

def merge_left(
    left: pd.DataFrame,
    right: pd.DataFrame,
    on: Optional[str | Sequence[str]] = None,
    left_on: Optional[str | Sequence[str]] = None,
    right_on: Optional[str | Sequence[str]] = None,
    suffixes: Tuple[str, str] = ("_x", "_y"),
) -> pd.DataFrame:
    """Left join convenience wrapper."""
    return left.merge(
        right, how="left", on=on, left_on=left_on, right_on=right_on, suffixes=suffixes
    )


def merge_inner(
    left: pd.DataFrame,
    right: pd.DataFrame,
    on: Optional[str | Sequence[str]] = None,
    left_on: Optional[str | Sequence[str]] = None,
    right_on: Optional[str | Sequence[str]] = None,
    suffixes: Tuple[str, str] = ("_x", "_y"),
) -> pd.DataFrame:
    """Inner join convenience wrapper."""
    return left.merge(
        right, how="inner", on=on, left_on=left_on, right_on=right_on, suffixes=suffixes
    )


def merge_many(
    dfs: Sequence[pd.DataFrame],
    on: Optional[str | Sequence[str]] = None,
    how: Literal["inner", "left", "right", "outer"] = "inner",
    suffixes: Tuple[str, str] = ("_x", "_y"),
) -> pd.DataFrame:
    """Reduce-merge a sequence of DataFrames."""
    if not dfs:
        raise ValueError("dfs must contain at least one DataFrame")
    return reduce(lambda l, r: l.merge(r, how=how, on=on, suffixes=suffixes), dfs)


# ------------------------------
# Reshaping
# ------------------------------

def melt_longer(
    df: pd.DataFrame,
    id_vars: Sequence[str],
    value_vars: Optional[Sequence[str]] = None,
    var_name: str = "variable",
    value_name: str = "value",
) -> pd.DataFrame:
    """Long format (melt)."""
    return pd.melt(
        df, id_vars=id_vars, value_vars=value_vars, var_name=var_name, value_name=value_name
    )


def pivot_wider(
    df: pd.DataFrame,
    index: str | Sequence[str],
    columns: str | Sequence[str],
    values: str | Sequence[str],
    aggfunc: Union[str, Callable] = "first",
    fill_value: Any = None,
) -> pd.DataFrame:
    """Wide format (pivot) with optional aggregation."""
    out = pd.pivot_table(
        df, index=index, columns=columns, values=values, aggfunc=aggfunc, fill_value=fill_value
    )
    if isinstance(out.columns, pd.MultiIndex):
        out.columns = ["_".join(map(str, c)).strip("_") for c in out.columns.to_flat_index()]
    return out.reset_index()


def stack_columns(
    df: pd.DataFrame,
    level: int | str = -1,
) -> pd.DataFrame:
    """Stack a column level (inverse of unstack)."""
    return df.stack(level=level).reset_index()


def unstack_index(
    df: pd.DataFrame,
    level: int | str = -1,
) -> pd.DataFrame:
    """Unstack an index level to columns."""
    out = df.set_index(df.columns.tolist()[:-1]).unstack(level=level)
    if isinstance(out.columns, pd.MultiIndex):
        out.columns = ["_".join(map(str, c)).strip("_") for c in out.columns.to_flat_index()]
    return out.reset_index()
------------------------------------------------------------

PROMPT 161:
"Generate Python code that uses the `requests` library to send a GET request to the Walrus API to retrieve information about a specific package, and then parses the JSON response using the `json` module to extract the package's name, version, and description."
RESPONSE:
import os
import json
import requests

def get_package_info(package_id, base_url, api_token=None, timeout=10):
    """
    Fetch package info from the Walrus API and parse JSON using the json module.
    """
    url = f"{base_url.rstrip('/')}/packages/{package_id}"
    headers = {
        "Accept": "application/json",
        # Include Authorization header only if a token is provided
        **({"Authorization": f"Bearer {api_token}"} if api_token else {}),
    }

    resp = requests.get(url, headers=headers, timeout=timeout)
    resp.raise_for_status()

    # Parse using the json module (not resp.json())
    try:
        data = json.loads(resp.text)
    except json.JSONDecodeError as e:
        raise ValueError(f"Response was not valid JSON: {e}") from e

    # Handle both top-level or nested 'data' shapes
    if isinstance(data, dict) and "data" in data and isinstance(data["data"], dict):
        payload = data["data"]
    else:
        payload = data if isinstance(data, dict) else {}

    name = payload.get("name")
    version = payload.get("version") or payload.get("latest_version")
    description = payload.get("description") or payload.get("summary")

    return {
        "name": name,
        "version": version,
        "description": description,
    }

if __name__ == "__main__":
    # Configure these values as appropriate for your environment
    WALRUS_API_BASE = os.getenv("WALRUS_API_BASE", "https://api.walrus.example.com/v1")
    WALRUS_API_TOKEN = os.getenv("WALRUS_API_TOKEN")  # optional
    PACKAGE_ID = "example-package"  # e.g., a package slug or ID

    try:
        info = get_package_info(PACKAGE_ID, WALRUS_API_BASE, WALRUS_API_TOKEN)
        print(f"Name: {info.get('name')}")
        print(f"Version: {info.get('version')}")
        print(f"Description: {info.get('description')}")
    except requests.HTTPError as http_err:
        print(f"HTTP error: {http_err}")
    except requests.RequestException as req_err:
        print(f"Request failed: {req_err}")
    except Exception as e:
        print(f"Error: {e}")
------------------------------------------------------------

PROMPT 162:
"Generate Python code that uses the `graphql` library to interact with GitHub APIs via GraphQL queries and mutations, utilizing the `requests` library to send HTTP requests and parse responses, and logs actions taken by the script using the `logging` library."
RESPONSE:
```python
#!/usr/bin/env python3
"""
GitHub GraphQL client using:
- graphql (graphql-core) to parse/validate GraphQL operations
- requests to send HTTP requests and parse responses
- logging to log actions taken by the script

Requirements:
  pip install requests graphql-core

Environment:
  GITHUB_TOKEN must be set to a Personal Access Token (classic) or fine-grained token
  with appropriate scopes (e.g., repo for mutations, public_repo for public repos, etc.).
"""

from __future__ import annotations

import json
import logging
import os
import time
from typing import Any, Dict, Iterable, Optional, Tuple

import requests
from graphql import Source, parse  # from package "graphql-core"


class GitHubGraphQLClient:
    def __init__(
        self,
        token: str,
        endpoint: str = "https://api.github.com/graphql",
        timeout: int = 30,
        user_agent: str = "gh-graphql-python/1.0",
        logger: Optional[logging.Logger] = None,
        session: Optional[requests.Session] = None,
    ) -> None:
        if not token:
            raise ValueError("GitHub token is required")
        self.endpoint = endpoint
        self.timeout = timeout

        self.session = session or requests.Session()
        self.session.headers.update(
            {
                "Authorization": f"Bearer {token}",
                # Accept header recommended by GitHub
                "Accept": "application/vnd.github+json",
                "Content-Type": "application/json",
                "User-Agent": user_agent,
            }
        )

        self.log = logger or logging.getLogger(self.__class__.__name__)

    def execute(
        self,
        query: str,
        variables: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
        max_retries: int = 3,
    ) -> Dict[str, Any]:
        """
        Execute a GraphQL operation against GitHub.

        - Uses graphql-core to parse the query for basic syntax validation before sending.
        - Uses requests to POST the operation.
        - Logs key actions and handles common error scenarios (HTTP/GraphQL errors, rate limits).

        Returns the "data" field of the GraphQL response.
        """
        # Validate query syntax using the "graphql" library
        try:
            parse(Source(query, name="GitHubGraphQLOperation"))
        except Exception as e:
            self.log.error("GraphQL query failed to parse: %s", e)
            raise

        payload: Dict[str, Any] = {"query": query}
        if variables is not None:
            payload["variables"] = variables
        if operation_name is not None:
            payload["operationName"] = operation_name

        attempt = 0
        while True:
            attempt += 1
            self.log.debug(
                "POST %s (attempt %d) op=%s vars=%s",
                self.endpoint,
                attempt,
                operation_name or "<anonymous>",
                json.dumps(variables or {}, ensure_ascii=False),
            )
            resp = None
            try:
                resp = self.session.post(
                    self.endpoint, json=payload, timeout=self.timeout
                )
            except requests.RequestException as e:
                self.log.warning("Network error during request: %s", e)
                if attempt <= max_retries:
                    sleep_s = 2**attempt
                    self.log.info("Retrying in %ss...", sleep_s)
                    time.sleep(sleep_s)
                    continue
                raise

            request_id = resp.headers.get("X-GitHub-Request-Id")
            rate_remaining = resp.headers.get("X-RateLimit-Remaining")
            rate_reset = resp.headers.get("X-RateLimit-Reset")

            self.log.info(
                "HTTP %s (id=%s, remaining=%s, reset=%s)",
                resp.status_code,
                request_id,
                rate_remaining,
                rate_reset,
            )

            # Handle HTTP-level issues
            if resp.status_code == 401:
                self.log.error("Unauthorized (401). Check your token.")
                resp.raise_for_status()

            # Rate limiting / abuse detection handling
            if resp.status_code in (429, 403):
                retry_after = resp.headers.get("Retry-After")
                if retry_after:
                    wait = int(retry_after)
                    self.log.warning("Rate limited. Retry after %ss", wait)
                    time.sleep(wait)
                    if attempt <= max_retries:
                        continue
                    resp.raise_for_status()
                elif rate_remaining == "0" and rate_reset:
                    try:
                        reset_epoch = int(rate_reset)
                        now = int(time.time())
                        wait = max(reset_epoch - now, 1)
                    except ValueError:
                        wait = 30
                    self.log.warning(
                        "Primary rate limit exhausted. Waiting %ss before retry.", wait
                    )
                    time.sleep(wait)
                    if attempt <= max_retries:
                        continue
                    resp.raise_for_status()
                elif attempt <= max_retries and resp.status_code == 403:
                    # Could be abuse detection/secondary limit; brief backoff
                    wait = 5 * attempt
                    self.log.warning(
                        "HTTP 403. Possible secondary rate limit. Backing off %ss...",
                        wait,
                    )
                    time.sleep(wait)
                    continue

            if resp.status_code >= 500 and attempt <= max_retries:
                wait = 2**attempt
                self.log.warning(
                    "Server error (%s). Retrying in %ss...", resp.status_code, wait
                )
                time.sleep(wait)
                continue

            # Raise for non-200 errors after handling retries
            if resp.status_code != 200:
                self.log.error(
                    "Non-200 response. Status=%s, Body=%s",
                    resp.status_code,
                    resp.text[:1000],
                )
                resp.raise_for_status()

            # Parse JSON
            try:
                body = resp.json()
            except ValueError:
                self.log.error("Failed to parse JSON response. Body: %s", resp.text)
                raise

            # GraphQL-level errors
            if "errors" in body:
                self.log.error("GraphQL errors: %s", json.dumps(body["errors"], ensure_ascii=False))
                # Raise a more informative exception
                messages = "; ".join(
                    f"{e.get('message','')}"
                    + (f" (path={'.'.join(map(str,e.get('path',[])))})" if e.get("path") else "")
                    for e in body["errors"]
                )
                raise RuntimeError(f"GraphQL errors: {messages}")

            data = body.get("data")
            if data is None:
                self.log.error("No 'data' field in response: %s", body)
                raise RuntimeError("GraphQL response missing 'data'")

            return data

    # Convenience operations

    def get_viewer_login(self) -> str:
        query = """
        query ViewerInfo {
          viewer {
            login
            name
          }
        }
        """
        data = self.execute(query)
        viewer = data["viewer"]
        self.log.info("Viewer: %s (%s)", viewer.get("login"), viewer.get("name"))
        return viewer["login"]

    def list_viewer_repos(self, first: int = 10) -> Iterable[Dict[str, Any]]:
        query = """
        query ListViewerRepos($first: Int!, $after: String) {
          viewer {
            repositories(
              first: $first
              after: $after
              orderBy: {field: UPDATED_AT, direction: DESC}
            ) {
              pageInfo { hasNextPage endCursor }
              nodes {
                nameWithOwner
                isPrivate
                stargazerCount
                updatedAt
              }
            }
          }
        }
        """
        variables: Dict[str, Any] = {"first": min(max(first, 1), 100), "after": None}
        repos: list[Dict[str, Any]] = []

        while True:
            data = self.execute(query, variables=variables, operation_name="ListViewerRepos")
            conn = data["viewer"]["repositories"]
            nodes = conn["nodes"] or []
            repos.extend(nodes)
            self.log.info("Fetched %d repos (total so far: %d)", len(nodes), len(repos))
            if conn["pageInfo"]["hasNextPage"] and len(repos) < first:
                variables["after"] = conn["pageInfo"]["endCursor"]
                remaining = first - len(repos)
                variables["first"] = min(remaining, 100)
            else:
                break
        return repos

    def get_repo_id(self, owner: str, name: str) -> str:
        query = """
        query RepoId($owner: String!, $name: String!) {
          repository(owner: $owner, name: $name) {
            id
            nameWithOwner
          }
        }
        """
        data = self.execute(query, variables={"owner": owner, "name": name}, operation_name="RepoId")
        repo = data.get("repository")
        if not repo:
            raise RuntimeError(f"Repository {owner}/{name} not found")
        self.log.info("Resolved repo ID for %s: %s", repo["nameWithOwner"], repo["id"])
        return repo["id"]

    def star_repository(self, owner: str, name: str) -> bool:
        repo_id = self.get_repo_id(owner, name)
        mutation = """
        mutation Star($repoId: ID!) {
          addStar(input: { starrableId: $repoId }) {
            starrable { viewerHasStarred }
          }
        }
        """
        data = self.execute(mutation, variables={"repoId": repo_id}, operation_name="Star")
        starred = bool(data["addStar"]["starrable"]["viewerHasStarred"])
        self.log.info("Starred %s/%s: %s", owner, name, starred)
        return starred

    def unstar_repository(self, owner: str, name: str) -> bool:
        repo_id = self.get_repo_id(owner, name)
        mutation = """
        mutation Unstar($repoId: ID!) {
          removeStar(input: { starrableId: $repoId }) {
            starrable { viewerHasStarred }
          }
        }
        """
        data = self.execute(mutation, variables={"repoId": repo_id}, operation_name="Unstar")
        starred = bool(data["removeStar"]["starrable"]["viewerHasStarred"])
        self.log.info("Unstarred %s/%s: now viewerHasStarred=%s", owner, name, starred)
        return not starred

    def create_issue(self, owner: str, name: str, title: str, body: Optional[str] = None) -> Tuple[int, str]:
        """
        Create an issue in the specified repository.
        Requires token scope: repo (or Issues write on fine-grained tokens).
        """
        repo_id = self.get_repo_id(owner, name)
        mutation = """
        mutation CreateIssue($repoId: ID!, $title: String!, $body: String) {
          createIssue(input: { repositoryId: $repoId, title: $title, body: $body }) {
            issue { number url title }
          }
        }
        """
        data = self.execute(
            mutation,
            variables={"repoId": repo_id, "title": title, "body": body},
            operation_name="CreateIssue",
        )
        issue = data["createIssue"]["issue"]
        self.log.info("Created issue #%s: %s", issue["number"], issue["url"])
        return int(issue["number"]), str(issue["url"])

    def get_rate_limit(self) -> Dict[str, Any]:
        query = """
        query {
          rateLimit {
            limit
            cost
            remaining
            resetAt
            used
          }
        }
        """
        data = self.execute(query)
        rl = data["rateLimit"]
        self.log.info(
            "Rate limit: remaining=%s used=%s cost=%s resetAt=%s",
            rl["remaining"], rl["used"], rl["cost"], rl["resetAt"]
        )
        return rl


def main() -> None:
    logging.basicConfig(
        level=os.environ.get("LOG_LEVEL", "INFO"),
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    )

    token = os.environ.get("GITHUB_TOKEN")
    if not token:
        raise SystemExit("Please set GITHUB_TOKEN environment variable.")

    endpoint = os.environ.get("GITHUB_GRAPHQL_ENDPOINT", "https://api.github.com/graphql")
    client = GitHubGraphQLClient(token=token, endpoint=endpoint)

    # 1) Show current user
    login = client.get_viewer_login()
    print(f"Authenticated as: {login}")

    # 2) List first 5 repos of the viewer
    print("\nYour most recently updated repositories (up to 5):")
    for repo in client.list_viewer_repos(first=5):
        print(f"- {repo['nameWithOwner']} | stars={repo['stargazerCount']} | private={repo['isPrivate']}")

    # 3) Optionally star/unstar a repository specified via env GITHUB_REPO="owner/name"
    repo_env = os.environ.get("GITHUB_REPO")
    if repo_env:
        try:
            owner, name = repo_env.split("/", 1)
        except ValueError:
            raise SystemExit("GITHUB_REPO must be in format 'owner/name'")
        print(f"\nStarring repo: {owner}/{name}")
        client.star_repository(owner, name)

        # Unstar to demonstrate mutation; comment this out if you want to keep it starred
        print(f"Unstarring repo: {owner}/{name}")
        client.unstar_repository(owner, name)

    # 4) Optionally create an issue if env vars are provided
    repo_issue_env = os.environ.get("GITHUB_REPO_ISSUE")
    issue_title = os.environ.get("GITHUB_ISSUE_TITLE")
    issue_body = os.environ.get("GITHUB_ISSUE_BODY", "Created via GraphQL API")
    if repo_issue_env and issue_title:
        try:
            owner, name = repo_issue_env.split("/", 1)
        except ValueError:
            raise SystemExit("GITHUB_REPO_ISSUE must be in format 'owner/name'")
        print(f"\nCreating issue in {owner}/{name}: {issue_title}")
        number, url = client.create_issue(owner, name, issue_title, issue_body)
        print(f"Issue created: #{number} at {url}")

    # 5) Show rate-limit info
    client.get_rate_limit()


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 163:
"Generate Python code that imports the necessary libraries, loads the LVIS dataset, and provides an interface for interacting with the dataset, including accessing its metadata, creating dataframes from the data, and visualizing the data using various methods."
RESPONSE:
# Requirements:
# pip install lvis pycocotools pandas matplotlib seaborn pillow requests tqdm

import os
import io
import json
import math
import shutil
import random
import zipfile
import pathlib
import warnings
from typing import List, Optional, Dict, Any, Tuple

import numpy as np
import pandas as pd
import requests
from PIL import Image, ImageOps
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sns

from lvis import LVIS
import pycocotools.mask as maskUtils


class LVISExplorer:
    """
    An interface to load LVIS annotations, access metadata, build DataFrames,
    and visualize images, categories, and annotations.
    """
    def __init__(
        self,
        ann_path: str,
        images_dir: Optional[str] = None,
        cache_dir: Optional[str] = ".lvis_cache",
        seed: int = 42,
    ):
        """
        ann_path: Path to LVIS annotations JSON (e.g., lvis_v1_val.json)
        images_dir: Path to COCO images directory (e.g., .../val2017 or .../train2017).
                    If None, images will be fetched via coco_url and cached in cache_dir.
        cache_dir: Local cache for any downloaded assets.
        """
        self.ann_path = ann_path
        self.images_dir = images_dir
        self.cache_dir = cache_dir
        self.rng = random.Random(seed)
        self._ensure_dir(self.cache_dir)

        self.lvis = LVIS(ann_path)

        # Mapping helpers
        self.cat_id_to_name = {c["id"]: c["name"] for c in self.lvis.dataset.get("categories", [])}
        self.cat_id_to_freq = {c["id"]: c.get("frequency") for c in self.lvis.dataset.get("categories", [])}
        self.cat_name_to_id = {v: k for k, v in self.cat_id_to_name.items()}

        # Lazy-built DataFrames
        self._categories_df = None
        self._images_df = None
        self._annotations_df = None

        # Colors for categories
        self._cat_colors = {}

    # ---------- Basic utilities ----------
    @staticmethod
    def _ensure_dir(p: str):
        pathlib.Path(p).mkdir(parents=True, exist_ok=True)

    def _get_color(self, cat_id: int) -> Tuple[float, float, float]:
        if cat_id not in self._cat_colors:
            self._cat_colors[cat_id] = tuple(self.rng.random() for _ in range(3))
        return self._cat_colors[cat_id]

    def _image_local_path(self, image_info: Dict[str, Any]) -> Optional[str]:
        """
        Determine local file path for an image.
        Priority:
          1) If images_dir provided:
             - If images_dir ends with train2017/val2017, join file_name.
             - Else, try images_dir/{train2017|val2017}/file_name.
          2) If coco_url available, download into cache_dir if not present.
        """
        file_name = image_info.get("file_name")
        if not file_name:
            return None

        if self.images_dir:
            candidate = pathlib.Path(self.images_dir)
            if candidate.name in ("train2017", "val2017"):
                p = candidate / file_name
                if p.exists():
                    return str(p)
            else:
                p1 = candidate / "train2017" / file_name
                p2 = candidate / "val2017" / file_name
                if p1.exists():
                    return str(p1)
                if p2.exists():
                    return str(p2)

        # Fallback: download from coco_url into cache_dir
        coco_url = image_info.get("coco_url")
        if coco_url:
            dest = pathlib.Path(self.cache_dir) / "images"
            self._ensure_dir(dest)
            local_path = dest / file_name
            if not local_path.exists():
                try:
                    resp = requests.get(coco_url, timeout=30)
                    resp.raise_for_status()
                    with open(local_path, "wb") as f:
                        f.write(resp.content)
                except Exception as e:
                    warnings.warn(f"Failed to download {coco_url}: {e}")
                    return None
            return str(local_path)
        return None

    def _ann_to_mask(self, ann: Dict[str, Any], height: int, width: int) -> Optional[np.ndarray]:
        seg = ann.get("segmentation", None)
        if seg is None:
            return None
        if isinstance(seg, list):
            # Polygon(s)
            rles = maskUtils.frPyObjects(seg, height, width)
            rle = maskUtils.merge(rles)
            m = maskUtils.decode(rle)
            return m.astype(bool)
        elif isinstance(seg, dict) and "counts" in seg:
            # RLE
            m = maskUtils.decode(seg)
            return m.astype(bool)
        else:
            return None

    # ---------- Metadata and DataFrames ----------
    def metadata(self) -> Dict[str, Any]:
        ds = self.lvis.dataset
        info = ds.get("info", {})
        licenses = ds.get("licenses", [])
        n_images = len(ds.get("images", []))
        n_anns = len(ds.get("annotations", []))
        n_cats = len(ds.get("categories", []))
        return {
            "info": info,
            "licenses_count": len(licenses),
            "num_images": n_images,
            "num_annotations": n_anns,
            "num_categories": n_cats,
            "annotation_file": os.path.abspath(self.ann_path),
        }

    @property
    def categories_df(self) -> pd.DataFrame:
        if self._categories_df is None:
            cats = self.lvis.dataset.get("categories", [])
            # Normalize into DataFrame, keep common LVIS fields if present
            df = pd.json_normalize(cats)
            # Standardize columns
            if "id" not in df.columns and "category_id" in df.columns:
                df = df.rename(columns={"category_id": "id"})
            # optional fields: name, synset, frequency, supercategory
            for col in ["id", "name", "synset", "frequency", "supercategory"]:
                if col not in df.columns:
                    df[col] = None
            df = df[["id", "name", "synset", "frequency", "supercategory"] + [c for c in df.columns if c not in {"id","name","synset","frequency","supercategory"}]]
            self._categories_df = df
        return self._categories_df

    @property
    def images_df(self) -> pd.DataFrame:
        if self._images_df is None:
            imgs = self.lvis.dataset.get("images", [])
            df = pd.json_normalize(imgs)
            # Standard columns often present: id, file_name, coco_url, height, width, license, date_captured
            for col in ["id", "file_name", "coco_url", "height", "width", "license", "date_captured"]:
                if col not in df.columns:
                    df[col] = None
            # provide local_path lazily resolvable placeholder (not computed to avoid IO overhead)
            self._images_df = df
        return self._images_df

    @property
    def annotations_df(self) -> pd.DataFrame:
        if self._annotations_df is None:
            anns = self.lvis.dataset.get("annotations", [])
            df = pd.json_normalize(anns)
            # Standardize columns
            for col in ["id", "image_id", "category_id", "area", "iscrowd", "bbox"]:
                if col not in df.columns:
                    df[col] = None
            # Add category name and frequency mapping
            df["category_name"] = df["category_id"].map(self.cat_id_to_name)
            df["category_frequency"] = df["category_id"].map(self.cat_id_to_freq)
            # Normalize bbox to columns [x,y,w,h]
            def bbox_to_cols(b):
                if isinstance(b, list) and len(b) == 4:
                    return pd.Series({"bbox_x": b[0], "bbox_y": b[1], "bbox_w": b[2], "bbox_h": b[3]})
                return pd.Series({"bbox_x": np.nan, "bbox_y": np.nan, "bbox_w": np.nan, "bbox_h": np.nan})
            bbox_cols = df["bbox"].apply(bbox_to_cols)
            df = pd.concat([df.drop(columns=["bbox"]), bbox_cols], axis=1)
            self._annotations_df = df
        return self._annotations_df

    def to_dataframes(self) -> Dict[str, pd.DataFrame]:
        return {
            "categories": self.categories_df.copy(),
            "images": self.images_df.copy(),
            "annotations": self.annotations_df.copy(),
        }

    # ---------- Query helpers ----------
    def get_category_id(self, name: str) -> Optional[int]:
        return self.cat_name_to_id.get(name)

    def get_category_name(self, cat_id: int) -> Optional[str]:
        return self.cat_id_to_name.get(cat_id)

    def get_image_info(self, image_id: int) -> Dict[str, Any]:
        imgs = self.lvis.loadImgs([image_id])
        return imgs[0] if imgs else {}

    def get_annotations_for_image(self, image_id: int, category_ids: Optional[List[int]] = None) -> List[Dict[str, Any]]:
        ann_ids = self.lvis.getAnnIds(imgIds=[image_id], catIds=category_ids)
        return self.lvis.loadAnns(ann_ids)

    # ---------- Visualization ----------
    def _load_image(self, image_id: int) -> Tuple[Optional[Image.Image], Dict[str, Any], Optional[str]]:
        info = self.get_image_info(image_id)
        path = self._image_local_path(info)
        img = None
        if path and os.path.exists(path):
            try:
                img = Image.open(path).convert("RGB")
            except Exception as e:
                warnings.warn(f"Failed to open {path}: {e}")
        else:
            # As a fallback, try coco_url in-memory
            url = info.get("coco_url")
            if url:
                try:
                    resp = requests.get(url, timeout=30)
                    resp.raise_for_status()
                    img = Image.open(io.BytesIO(resp.content)).convert("RGB")
                except Exception as e:
                    warnings.warn(f"Failed to fetch {url}: {e}")
        return img, info, path

    def show_image(
        self,
        image_id: int,
        category_ids: Optional[List[int]] = None,
        show_bbox: bool = True,
        show_mask: bool = True,
        alpha: float = 0.5,
        line_width: float = 2.0,
        title: Optional[str] = None,
        ax: Optional[plt.Axes] = None,
        figsize: Tuple[int, int] = (10, 10),
        max_anns: Optional[int] = None,
    ):
        """
        Display an image with optional bounding boxes and segmentation masks.
        """
        img, info, path = self._load_image(image_id)
        if img is None:
            print(f"Image {image_id} not available.")
            return

        anns = self.get_annotations_for_image(image_id, category_ids)
        if max_anns is not None and len(anns) > max_anns:
            anns = anns[:max_anns]

        if ax is None:
            fig, ax = plt.subplots(1, 1, figsize=figsize)
        ax.imshow(img)
        ax.axis("off")

        H, W = info.get("height", img.height), info.get("width", img.width)

        for ann in anns:
            cat_id = ann["category_id"]
            color = self._get_color(cat_id)

            # Mask
            if show_mask:
                m = self._ann_to_mask(ann, H, W)
                if m is not None:
                    # Create colored overlay
                    overlay = np.zeros((H, W, 4), dtype=np.float32)
                    overlay[..., 0:3] = color
                    overlay[..., 3] = alpha * m.astype(np.float32)
                    ax.imshow(overlay)

            # BBox
            if show_bbox:
                x, y, w, h = ann.get("bbox", [None, None, None, None])
                if None not in (x, y, w, h):
                    rect = patches.Rectangle(
                        (x, y), w, h, linewidth=line_width, edgecolor=color, facecolor="none"
                    )
                    ax.add_patch(rect)
                    # label
                    label = self.get_category_name(cat_id) or str(cat_id)
                    ax.text(
                        x,
                        max(0, y - 2),
                        label,
                        color="white",
                        bbox=dict(facecolor=color, alpha=0.8, edgecolor="none", pad=1),
                        fontsize=9,
                        verticalalignment="bottom",
                    )

        title_text = title if title is not None else f"Image {image_id} | {os.path.basename(path) if path else 'remote'}"
        ax.set_title(title_text)

    def show_grid(
        self,
        image_ids: List[int],
        ncols: int = 3,
        show_bbox: bool = True,
        show_mask: bool = False,
        figsize_per_cell: Tuple[float, float] = (5, 5),
        max_images: Optional[int] = None,
    ):
        if max_images is not None:
            image_ids = image_ids[:max_images]
        n_images = len(image_ids)
        if n_images == 0:
            print("No images to display.")
            return
        ncols = max(1, ncols)
        nrows = math.ceil(n_images / ncols)
        fig, axes = plt.subplots(nrows, ncols, figsize=(figsize_per_cell[0]*ncols, figsize_per_cell[1]*nrows))
        if nrows == 1 and ncols == 1:
            axes = np.array([[axes]])
        elif nrows == 1:
            axes = np.array([axes])
        elif ncols == 1:
            axes = np.array([[ax] for ax in axes])

        idx = 0
        for r in range(nrows):
            for c in range(ncols):
                ax = axes[r, c]
                ax.axis("off")
                if idx < n_images:
                    image_id = image_ids[idx]
                    self.show_image(image_id=image_id, show_bbox=show_bbox, show_mask=show_mask, ax=ax)
                idx += 1
        plt.tight_layout()

    def show_random_samples(
        self,
        n: int = 6,
        category_id: Optional[int] = None,
        ncols: int = 3,
        show_bbox: bool = True,
        show_mask: bool = False,
        seed: Optional[int] = None,
    ):
        rng = self.rng if seed is None else random.Random(seed)
        if category_id is None:
            img_ids = self.lvis.getImgIds()
        else:
            img_ids = self.lvis.getImgIds(catIds=[category_id])

        rng.shuffle(img_ids)
        img_ids = img_ids[:n]
        self.show_grid(img_ids, ncols=ncols, show_bbox=show_bbox, show_mask=show_mask)

    def plot_category_distribution(self, top_k: int = 50, figsize: Tuple[int, int] = (12, 8)):
        df = self.annotations_df
        counts = df.groupby(["category_id", "category_name"]).size().reset_index(name="count")
        counts = counts.sort_values("count", ascending=False)
        if top_k:
            counts = counts.head(top_k)
        plt.figure(figsize=figsize)
        sns.barplot(data=counts, x="count", y="category_name", palette="viridis")
        plt.title(f"Top {len(counts)} categories by instance count")
        plt.xlabel("Instances")
        plt.ylabel("Category")
        plt.tight_layout()

    def plot_instances_per_image(self, bins: int = 50, figsize: Tuple[int, int] = (10, 5)):
        ann_df = self.annotations_df
        counts = ann_df.groupby("image_id").size().rename("instances").reset_index()
        plt.figure(figsize=figsize)
        sns.histplot(counts["instances"], bins=bins, kde=False)
        plt.title("Distribution of instances per image")
        plt.xlabel("Instances per image")
        plt.ylabel("Number of images")
        plt.tight_layout()

    def plot_bbox_area_distribution(self, normalize_by_image: bool = True, bins: int = 50, figsize: Tuple[int, int] = (10, 5)):
        ann_df = self.annotations_df
        img_df = self.images_df[["id", "height", "width"]].rename(columns={"id": "image_id"})
        merged = ann_df.merge(img_df, on="image_id", how="left")
        if normalize_by_image:
            area = (merged["bbox_w"] * merged["bbox_h"]) / (merged["height"] * merged["width"])
            xlabel = "BBox area fraction of image"
        else:
            area = merged["bbox_w"] * merged["bbox_h"]
            xlabel = "BBox area (pixels)"
        plt.figure(figsize=figsize)
        sns.histplot(area.dropna(), bins=bins, kde=False)
        plt.title("Distribution of bounding box area")
        plt.xlabel(xlabel)
        plt.ylabel("Count")
        plt.tight_layout()

    # ---------- Download helpers ----------
    @staticmethod
    def download_lvis_annotations(dest_dir: str, version: str = "v1", split: str = "val") -> str:
        """
        Download LVIS annotation JSON into dest_dir.
        version: "v1" or "v0.5"
        split: "train", "val"
        Returns path to the JSON file.
        """
        assert version in {"v1", "v0.5"}
        assert split in {"train", "val"}

        base = "https://s3-us-west-2.amazonaws.com/dl.fbaipublicfiles.com/LVIS"
        filename = f"lvis_{version}_{split}.json"
        zipname = filename + ".zip"
        url = f"{base}/{zipname}"

        dest_dir = pathlib.Path(dest_dir)
        dest_dir.mkdir(parents=True, exist_ok=True)
        zip_path = dest_dir / zipname
        json_path = dest_dir / filename

        if json_path.exists():
            return str(json_path)

        print(f"Downloading {url} -> {zip_path}")
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(zip_path, "wb") as f:
                shutil.copyfileobj(r.raw, f)

        print(f"Extracting {zip_path}")
        with zipfile.ZipFile(zip_path, "r") as zf:
            zf.extractall(dest_dir)

        if not json_path.exists():
            raise FileNotFoundError(f"Expected {json_path} after extraction.")
        return str(json_path)


# -------------------- Example usage --------------------
if __name__ == "__main__":
    # 1) Set paths. Optionally download LVIS annotations if not present.
    # Change base_dir and images_dir to your local setup.
    base_dir = "./data/lvis"
    pathlib.Path(base_dir).mkdir(parents=True, exist_ok=True)

    # Download LVIS v1 val annotations (small enough for quick experimentation)
    ann_json = LVISExplorer.download_lvis_annotations(dest_dir=base_dir, version="v1", split="val")

    # Optional: point to COCO images if you have them locally; otherwise the explorer will fetch via coco_url.
    # For example:
    # images_dir = "/path/to/coco/val2017"
    images_dir = None  # set to a folder to avoid network fetches

    # 2) Initialize explorer
    explorer = LVISExplorer(ann_path=ann_json, images_dir=images_dir, cache_dir=os.path.join(base_dir, "cache"))

    # 3) Access metadata
    meta = explorer.metadata()
    print(json.dumps(meta, indent=2))

    # 4) Create DataFrames
    dfs = explorer.to_dataframes()
    print("Categories:", dfs["categories"].shape, "Images:", dfs["images"].shape, "Annotations:", dfs["annotations"].shape)

    # 5) Quick stats
    print("\nTop 10 categories by instance count:")
    top_counts = (
        dfs["annotations"]
        .groupby(["category_id", "category_name"])
        .size()
        .reset_index(name="count")
        .sort_values("count", ascending=False)
        .head(10)
    )
    print(top_counts)

    # 6) Visualizations
    # Category distribution (top 30)
    explorer.plot_category_distribution(top_k=30)
    plt.show()

    # Instances per image
    explorer.plot_instances_per_image(bins=40)
    plt.show()

    # BBox area distribution
    explorer.plot_bbox_area_distribution(normalize_by_image=True, bins=50)
    plt.show()

    # 7) Visualize random samples
    explorer.show_random_samples(n=6, ncols=3, show_bbox=True, show_mask=True)
    plt.show()

    # 8) Visualize examples for a specific category
    cat_name = "toilet paper"  # change to any LVIS category name
    cat_id = explorer.get_category_id(cat_name)
    if cat_id is not None:
        print(f"Showing samples for category: {cat_name} (id={cat_id})")
        explorer.show_random_samples(n=6, category_id=cat_id, ncols=3, show_bbox=True, show_mask=True)
        plt.show()
    else:
        print(f"Category not found: {cat_name}")
------------------------------------------------------------

PROMPT 164:
"Generate Python code that creates wrapper functions for PySpark, allowing users to interact with Spark APIs using Python syntax and data types, while also providing additional functionality such as handling missing values, data cleaning, and data preprocessing."
RESPONSE:
# pyspark_wrappers.py
# Convenience wrappers around PySpark to use Python-native syntax and add
# missing-value handling, data cleaning, and preprocessing utilities.

from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union

from pyspark.sql import DataFrame, SparkSession, Column, Window
from pyspark.sql import functions as F
from pyspark.sql import types as T

# Optional ML features (only used for encoding helpers)
try:
    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
    from pyspark.ml import Pipeline
    from pyspark.ml.functions import vector_to_array
except Exception:
    StringIndexer = None
    OneHotEncoder = None
    VectorAssembler = None
    Pipeline = None
    vector_to_array = None


# -----------------------------------------------------------------------------
# Session helpers
# -----------------------------------------------------------------------------

def get_spark(app_name: str = "PySparkWrappers", configs: Optional[Dict[str, str]] = None) -> SparkSession:
    """
    Get or create a SparkSession with optional configs.
    """
    builder = SparkSession.builder.appName(app_name)
    if configs:
        for k, v in configs.items():
            builder = builder.config(k, v)
    return builder.getOrCreate()


# -----------------------------------------------------------------------------
# Utility helpers
# -----------------------------------------------------------------------------

def _ensure_list(x: Optional[Union[str, Sequence[str]]]) -> List[str]:
    if x is None:
        return []
    if isinstance(x, str):
        return [x]
    return list(x)

def _is_iterable_but_not_str(x: Any) -> bool:
    return isinstance(x, Iterable) and not isinstance(x, (str, bytes, bytearray))

def _to_col(x: Any) -> Column:
    """
    Convert Python literal or column name to a Spark Column.
    """
    if isinstance(x, Column):
        return x
    if isinstance(x, str):
        return F.col(x)
    return F.lit(x)

def rename_columns(df: DataFrame, mapping: Dict[str, str]) -> DataFrame:
    """
    Rename DataFrame columns using a dict mapping old_name -> new_name.
    """
    for old, new in mapping.items():
        df = df.withColumnRenamed(old, new)
    return df

def with_columns(df: DataFrame, newcols: Dict[str, Any]) -> DataFrame:
    """
    Add or replace multiple columns.
    Values may be literals, column names, Column expressions, or callables that take (F, C) where:
      - F is pyspark.sql.functions
      - C is a helper lambda: C("colname") -> Column
    """
    C = lambda c: F.col(c)
    out = df
    for name, spec in newcols.items():
        if callable(spec):
            out = out.withColumn(name, spec(F, C))
        else:
            out = out.withColumn(name, _to_col(spec))
    return out

def where(df: DataFrame,
          conditions: Optional[Dict[str, Any]] = None,
          expr: Optional[str] = None) -> DataFrame:
    """
    Filter using a dict of conditions and/or a raw SQL expression.
      - conditions: {"col": value or [values] or None} translates to:
          - None -> col IS NULL
          - iterable -> col IN (...)
          - other -> col == value
      - expr: raw SQL expression passed to df.filter(expr)
    """
    cond: Optional[Column] = None
    if conditions:
        for k, v in conditions.items():
            c = F.col(k)
            if v is None:
                new = c.isNull()
            elif _is_iterable_but_not_str(v):
                vals = list(v)
                new = c.isin(vals)
            else:
                new = (c == F.lit(v))
            cond = new if cond is None else (cond & new)
    if expr:
        df = df.filter(expr)
    if cond is not None:
        df = df.filter(cond)
    return df


# -----------------------------------------------------------------------------
# IO helpers
# -----------------------------------------------------------------------------

def read_csv(spark: SparkSession,
             path: Union[str, List[str]],
             header: bool = True,
             infer_schema: bool = True,
             sep: str = ",",
             encoding: Optional[str] = None,
             null_values: Optional[List[str]] = None,
             options: Optional[Dict[str, str]] = None) -> DataFrame:
    """
    Read CSV into a Spark DataFrame.
    """
    reader = spark.read.option("header", str(header).lower()).option("inferSchema", str(infer_schema).lower()).option("sep", sep)
    if encoding:
        reader = reader.option("encoding", encoding)
    if null_values:
        reader = reader.option("nullValue", ",".join(null_values))
    if options:
        for k, v in options.items():
            reader = reader.option(k, v)
    return reader.csv(path)

def write_csv(df: DataFrame,
              path: str,
              header: bool = True,
              mode: str = "error",
              sep: str = ",",
              options: Optional[Dict[str, str]] = None) -> None:
    """
    Write DataFrame to CSV files.
    """
    writer = df.write.mode(mode).option("header", str(header).lower()).option("sep", sep)
    if options:
        for k, v in options.items():
            writer = writer.option(k, v)
    writer.csv(path)


# -----------------------------------------------------------------------------
# Missing values
# -----------------------------------------------------------------------------

def drop_missing(df: DataFrame,
                 subset: Optional[Sequence[str]] = None,
                 how: str = "any",
                 thresh: Optional[int] = None) -> DataFrame:
    """
    Drop rows with missing values.
      - how: 'any' or 'all'
      - subset: cols to consider
      - thresh: require at least 'thresh' non-null values (overrides how)
    """
    subset = _ensure_list(subset)
    if thresh is not None:
        return df.na.drop(thresh=thresh, subset=subset or None)
    return df.na.drop(how=how, subset=subset or None)

def fill_missing(df: DataFrame,
                 value: Optional[Union[Any, Dict[str, Any]]] = None,
                 subset: Optional[Sequence[str]] = None) -> DataFrame:
    """
    Fill missing values with a scalar or a per-column dict.
    """
    subset = _ensure_list(subset)
    if isinstance(value, dict):
        return df.na.fill(value)
    elif value is not None:
        if subset:
            return df.na.fill(value, subset)
        return df.na.fill(value)
    return df

def impute_missing(df: DataFrame,
                   strategies: Dict[str, Union[str, Any]],
                   approx_accuracy: int = 10000) -> DataFrame:
    """
    Impute missing values per column using strategies:
      - 'mean' (numeric)
      - 'median' (numeric) via percentile_approx
      - 'mode' (categorical; can be expensive)
      - any scalar literal (used as-is)

    Example:
      impute_missing(df, {"age": "median", "income": "mean", "city": "mode", "tier": 0})
    """
    # Separate by strategy type
    mean_cols = [c for c, s in strategies.items() if isinstance(s, str) and s.lower() == "mean"]
    median_cols = [c for c, s in strategies.items() if isinstance(s, str) and s.lower() == "median"]
    mode_cols = [c for c, s in strategies.items() if isinstance(s, str) and s.lower() == "mode"]
    literal_map = {c: s for c, s in strategies.items() if not isinstance(s, str)}

    impute_values: Dict[str, Any] = {}

    if mean_cols:
        agg_exprs = [F.avg(F.col(c)).alias(c) for c in mean_cols]
        row = df.select(agg_exprs).first()
        for c in mean_cols:
            impute_values[c] = row[c]

    if median_cols:
        # percentile_approx allows doing all in one pass
        agg_exprs = [F.percentile_approx(F.col(c), 0.5, approx_accuracy).alias(c) for c in median_cols]
        row = df.select(agg_exprs).first()
        for c in median_cols:
            impute_values[c] = row[c]

    if mode_cols:
        # Warning: per-column groupBy can be heavy on large data
        for c in mode_cols:
            mode_row = (
                df.groupBy(F.col(c))
                  .agg(F.count(F.lit(1)).alias("_cnt"))
                  .orderBy(F.desc("_cnt"))
                  .filter(F.col(c).isNotNull())
                  .select(c)
                  .limit(1)
                  .collect()
            )
            impute_values[c] = mode_row[0][0] if mode_row else None

    impute_values.update(literal_map)
    # Fill only columns that got a value
    impute_values = {k: v for k, v in impute_values.items() if v is not None}
    if not impute_values:
        return df
    return df.na.fill(impute_values)


# -----------------------------------------------------------------------------
# Type casting, parsing, and cleaning
# -----------------------------------------------------------------------------

def cast_columns(df: DataFrame, casts: Dict[str, Union[str, T.DataType]]) -> DataFrame:
    """
    Cast multiple columns by name to given Spark SQL types or type strings.
    """
    out = df
    for c, typ in casts.items():
        if isinstance(typ, str):
            out = out.withColumn(c, F.col(c).cast(typ))
        else:
            out = out.withColumn(c, F.col(c).cast(typ))
    return out

def parse_boolean(df: DataFrame, cols: Sequence[str], default: Optional[bool] = None) -> DataFrame:
    """
    Parse free-text boolean-like columns into booleans.
    Truthy: 'true','t','yes','y','1'
    Falsy:  'false','f','no','n','0'
    """
    truthy = {"true", "t", "yes", "y", "1"}
    falsy = {"false", "f", "no", "n", "0"}
    out = df
    for c in cols:
        lc = F.lower(F.trim(F.col(c).cast("string")))
        val = F.when(lc.isNull(), F.lit(default)) \
               .when(lc.isin([F.lit(x)._jc.toString() for x in truthy]), F.lit(True)) \
               .when(lc.isin([F.lit(x)._jc.toString() for x in falsy]), F.lit(False)) \
               .otherwise(F.lit(default))
        out = out.withColumn(c, val.cast("boolean"))
    return out

def clean_text(df: DataFrame,
               cols: Sequence[str],
               lowercase: bool = True,
               strip: bool = True,
               collapse_spaces: bool = True,
               remove_punct: bool = False,
               allow_chars_regex: Optional[str] = None) -> DataFrame:
    """
    Clean string columns.
      - lowercase: to lower
      - strip: trim leading/trailing whitespace
      - collapse_spaces: replace multiple whitespace with single space
      - remove_punct: remove punctuation
      - allow_chars_regex: keep only chars matching the regex class (e.g. '[a-zA-Z0-9 ]')
    """
    out = df
    for c in cols:
        col = F.col(c).cast("string")
        if strip:
            col = F.trim(col)
        if lowercase:
            col = F.lower(col)
        if collapse_spaces:
            col = F.regexp_replace(col, r"\s+", " ")
        if remove_punct:
            col = F.regexp_replace(col, r"[^\w\s]", "")
        if allow_chars_regex:
            col = F.regexp_replace(col, f"[^{allow_chars_regex}]", "")
        out = out.withColumn(c, col)
    return out

def parse_dates(df: DataFrame,
                cols: Sequence[str],
                patterns: Optional[Sequence[str]] = None,
                to: str = "timestamp") -> DataFrame:
    """
    Parse date/time strings using multiple patterns; coalesces the first successful parse.
      - to: 'date' or 'timestamp'
    """
    if patterns is None:
        patterns = [
            "yyyy-MM-dd",
            "yyyy/MM/dd",
            "MM/dd/yyyy",
            "dd/MM/yyyy",
            "yyyy-MM-dd HH:mm:ss",
            "yyyy/MM/dd HH:mm:ss",
            "MM/dd/yyyy HH:mm:ss",
        ]
    caster = F.to_timestamp if to == "timestamp" else F.to_date
    out = df
    for c in cols:
        exprs = [caster(F.col(c), p) for p in patterns]
        out = out.withColumn(c, F.coalesce(*exprs))
    return out

def standardize(df: DataFrame, cols: Sequence[str]) -> DataFrame:
    """
    Z-score standardize numeric columns: (x - mean) / stddev.
    """
    # Aggregate means and stddevs in one pass
    agg_exprs = []
    for c in cols:
        agg_exprs.append(F.avg(F.col(c)).alias(f"{c}__mean"))
        agg_exprs.append(F.stddev(F.col(c)).alias(f"{c}__std"))
    row = df.select(agg_exprs).first()
    out = df
    for c in cols:
        mu = row[f"{c}__mean"]
        sd = row[f"{c}__std"] or 0.0
        out = out.withColumn(c, F.when(F.lit(sd) == 0.0, F.lit(0.0)).otherwise((F.col(c) - F.lit(mu)) / F.lit(sd)))
    return out

def minmax_scale(df: DataFrame, cols: Sequence[str], feature_range: Tuple[float, float] = (0.0, 1.0)) -> DataFrame:
    """
    Min-max scale numeric columns to [min_val, max_val].
    """
    min_v, max_v = feature_range
    agg_exprs = []
    for c in cols:
        agg_exprs.append(F.min(F.col(c)).alias(f"{c}__min"))
        agg_exprs.append(F.max(F.col(c)).alias(f"{c}__max"))
    row = df.select(agg_exprs).first()
    out = df
    for c in cols:
        cmin = row[f"{c}__min"]
        cmax = row[f"{c}__max"]
        denom = (cmax - cmin) if cmax is not None and cmin is not None else None
        out = out.withColumn(
            c,
            F.when(F.lit(denom is None) | (F.lit(denom) == 0.0), F.lit(min_v))
             .otherwise(F.lit(min_v) + (F.col(c) - F.lit(cmin)) * (max_v - min_v) / F.lit(denom))
        )
    return out

def winsorize(df: DataFrame,
              cols: Sequence[str],
              lower_quantile: float = 0.01,
              upper_quantile: float = 0.99,
              approx_accuracy: int = 10000) -> DataFrame:
    """
    Winsorize numeric columns at given quantiles.
    """
    out = df
    for c in cols:
        q = df.approxQuantile(c, [lower_quantile, upper_quantile], 1.0 / approx_accuracy)
        if not q or len(q) < 2:
            continue
        lo, hi = q
        out = out.withColumn(c, F.when(F.col(c) < F.lit(lo), F.lit(lo))
                                .when(F.col(c) > F.lit(hi), F.lit(hi))
                                .otherwise(F.col(c)))
    return out


# -----------------------------------------------------------------------------
# Categorical encoding
# -----------------------------------------------------------------------------

def string_index(df: DataFrame,
                 cols: Sequence[str],
                 handle_invalid: str = "keep",
                 suffix: str = "_idx") -> Tuple[DataFrame, List[str]]:
    """
    StringIndex multiple columns. Returns (df, index_cols).
    """
    if StringIndexer is None or Pipeline is None:
        raise ImportError("pyspark.ml is required for string_index but was not found.")
    stages = []
    out_cols = []
    for c in cols:
        out_c = f"{c}{suffix}"
        stages.append(StringIndexer(inputCol=c, outputCol=out_c, handleInvalid=handle_invalid))
        out_cols.append(out_c)
    pipeline = Pipeline(stages=stages)
    model = pipeline.fit(df)
    out_df = model.transform(df)
    return out_df, out_cols

def one_hot_encode(df: DataFrame,
                   cols: Sequence[str],
                   drop_last: bool = True,
                   handle_invalid: str = "keep",
                   suffix: str = "_ohe",
                   to_array: bool = False) -> Tuple[DataFrame, List[str]]:
    """
    One-hot encode columns (strings or numeric indices). If input are strings, they will be StringIndexed first.
    Returns (df, ohe_columns). Optionally converts to array for easier inspection.
    """
    if OneHotEncoder is None or Pipeline is None:
        raise ImportError("pyspark.ml is required for one_hot_encode but was not found.")
    # Detect string-typed columns to index
    str_cols = [c for c, t in df.dtypes if c in set(cols) and t in ("string", "varchar", "char")]
    other_cols = [c for c in cols if c not in str_cols]

    stages = []
    encoded_cols = []

    # Index string columns
    for c in str_cols:
        idx_c = f"{c}__idx_for_ohe"
        stages.append(StringIndexer(inputCol=c, outputCol=idx_c, handleInvalid=handle_invalid))
        other_cols.append(idx_c)  # treat as numeric idx for OHE later

    # OHE
    ohe_out = []
    for c in other_cols:
        out_c = f"{c}{suffix}"
        stages.append(OneHotEncoder(inputCols=[c], outputCols=[out_c], dropLast=drop_last))
        ohe_out.append(out_c)

    pipeline = Pipeline(stages=stages)
    model = pipeline.fit(df)
    out_df = model.transform(df)

    if to_array and vector_to_array is not None:
        for c in ohe_out:
            out_df = out_df.withColumn(c, vector_to_array(F.col(c)))
    return out_df, ohe_out


# -----------------------------------------------------------------------------
# Joins and splits
# -----------------------------------------------------------------------------

def smart_join(left: DataFrame,
               right: DataFrame,
               on: Union[str, Sequence[str]],
               how: str = "left",
               broadcast_right: bool = False) -> DataFrame:
    """
    Join with optional broadcast hint for small right tables.
    """
    if broadcast_right:
        right = F.broadcast(right)
    return left.join(right, on=on, how=how)

def train_test_split(df: DataFrame,
                     test_size: float = 0.2,
                     seed: int = 42) -> Tuple[DataFrame, DataFrame]:
    """
    Split DataFrame into train and test sets using randomSplit.
    """
    test = float(test_size)
    train = 1.0 - test
    return tuple(df.randomSplit([train, test], seed=seed))  # type: ignore


# -----------------------------------------------------------------------------
# High-level preprocessing
# -----------------------------------------------------------------------------

def preprocess(df: DataFrame, config: Dict[str, Any]) -> DataFrame:
    """
    Apply a sequence of preprocessing steps based on a config dict.
    Supported keys (all optional):
      - rename: {old: new}
      - drop_missing: {subset: [...], how: 'any'|'all'|None, thresh: int|None}
      - fill_missing: {value: scalar|{col: scalar}, subset: [...]}
      - impute_missing: {col: 'mean'|'median'|'mode'|scalar, ...}
      - cast: {col: 'double'|DataType, ...}
      - parse_dates: {cols: [...], patterns: [...], to: 'date'|'timestamp'}
      - clean_text: {cols: [...], lowercase: bool, strip: bool, collapse_spaces: bool, remove_punct: bool, allow_chars_regex: str|None}
      - winsorize: {cols: [...], lower_quantile: float, upper_quantile: float}
      - scale: {'standardize': [...], 'minmax': {cols: [...], feature_range: [min, max]}}
    """
    out = df

    if "rename" in config:
        out = rename_columns(out, config["rename"])

    if "drop_missing" in config:
        dm = config["drop_missing"]
        out = drop_missing(out, subset=dm.get("subset"), how=dm.get("how", "any"), thresh=dm.get("thresh"))

    if "fill_missing" in config:
        fm = config["fill_missing"]
        out = fill_missing(out, value=fm.get("value"), subset=fm.get("subset"))

    if "impute_missing" in config:
        out = impute_missing(out, strategies=config["impute_missing"])

    if "cast" in config:
        out = cast_columns(out, casts=config["cast"])

    if "parse_dates" in config:
        pd = config["parse_dates"]
        out = parse_dates(out, cols=pd.get("cols", []), patterns=pd.get("patterns"), to=pd.get("to", "timestamp"))

    if "clean_text" in config:
        ct = config["clean_text"]
        out = clean_text(
            out,
            cols=ct.get("cols", []),
            lowercase=ct.get("lowercase", True),
            strip=ct.get("strip", True),
            collapse_spaces=ct.get("collapse_spaces", True),
            remove_punct=ct.get("remove_punct", False),
            allow_chars_regex=ct.get("allow_chars_regex"),
        )

    if "winsorize" in config:
        wz = config["winsorize"]
        out = winsorize(out, cols=wz.get("cols", []),
                        lower_quantile=wz.get("lower_quantile", 0.01),
                        upper_quantile=wz.get("upper_quantile", 0.99))

    if "scale" in config:
        sc = config["scale"]
        if "standardize" in sc and sc["standardize"]:
            out = standardize(out, cols=sc["standardize"])
        if "minmax" in sc and sc["minmax"]:
            mm = sc["minmax"]
            out = minmax_scale(out, cols=mm.get("cols", []), feature_range=tuple(mm.get("feature_range", [0.0, 1.0])))

    return out


# -----------------------------------------------------------------------------
# DataFrame wrapper class for convenient method-chaining
# -----------------------------------------------------------------------------

class SDF:
    """
    Thin wrapper around pyspark.sql.DataFrame with convenience methods.
    Usage:
      sdf = SDF(df).clean_text([...]).impute({...}).standardize([...]).df
    """
    def __init__(self, df: DataFrame):
        self.df = df

    def rename(self, mapping: Dict[str, str]) -> "SDF":
        self.df = rename_columns(self.df, mapping)
        return self

    def where(self, conditions: Optional[Dict[str, Any]] = None, expr: Optional[str] = None) -> "SDF":
        self.df = where(self.df, conditions, expr)
        return self

    def drop_missing(self, subset: Optional[Sequence[str]] = None, how: str = "any", thresh: Optional[int] = None) -> "SDF":
        self.df = drop_missing(self.df, subset, how, thresh)
        return self

    def fill_missing(self, value: Optional[Union[Any, Dict[str, Any]]] = None, subset: Optional[Sequence[str]] = None) -> "SDF":
        self.df = fill_missing(self.df, value, subset)
        return self

    def impute(self, strategies: Dict[str, Union[str, Any]]) -> "SDF":
        self.df = impute_missing(self.df, strategies)
        return self

    def cast(self, casts: Dict[str, Union[str, T.DataType]]) -> "SDF":
        self.df = cast_columns(self.df, casts)
        return self

    def parse_dates(self, cols: Sequence[str], patterns: Optional[Sequence[str]] = None, to: str = "timestamp") -> "SDF":
        self.df = parse_dates(self.df, cols, patterns, to)
        return self

    def clean_text(self, cols: Sequence[str], lowercase: bool = True, strip: bool = True,
                   collapse_spaces: bool = True, remove_punct: bool = False,
                   allow_chars_regex: Optional[str] = None) -> "SDF":
        self.df = clean_text(self.df, cols, lowercase, strip, collapse_spaces, remove_punct, allow_chars_regex)
        return self

    def winsorize(self, cols: Sequence[str], lower_quantile: float = 0.01, upper_quantile: float = 0.99) -> "SDF":
        self.df = winsorize(self.df, cols, lower_quantile, upper_quantile)
        return self

    def standardize(self, cols: Sequence[str]) -> "SDF":
        self.df = standardize(self.df, cols)
        return self

    def minmax_scale(self, cols: Sequence[str], feature_range: Tuple[float, float] = (0.0, 1.0)) -> "SDF":
        self.df = minmax_scale(self.df, cols, feature_range)
        return self

    def string_index(self, cols: Sequence[str], handle_invalid: str = "keep", suffix: str = "_idx") -> "SDF":
        new_df, _ = string_index(self.df, cols, handle_invalid, suffix)
        self.df = new_df
        return self

    def one_hot_encode(self, cols: Sequence[str], drop_last: bool = True, handle_invalid: str = "keep",
                       suffix: str = "_ohe", to_array: bool = False) -> "SDF":
        new_df, _ = one_hot_encode(self.df, cols, drop_last, handle_invalid, suffix, to_array)
        self.df = new_df
        return self

    def smart_join(self, right: DataFrame, on: Union[str, Sequence[str]], how: str = "left", broadcast_right: bool = False) -> "SDF":
        self.df = smart_join(self.df, right, on, how, broadcast_right)
        return self

    def with_columns(self, newcols: Dict[str, Any]) -> "SDF":
        self.df = with_columns(self.df, newcols)
        return self

    def select(self, cols: Sequence[Union[str, Column]]) -> "SDF":
        self.df = self.df.select(*[(_to_col(c) if not isinstance(c, (str, Column)) else c) for c in cols])
        return self

    def order_by(self, cols: Sequence[Union[str, Column]], ascending: bool = True) -> "SDF":
        sort_cols = []
        for c in cols:
            col = _to_col(c)
            sort_cols.append(col.asc() if ascending else col.desc())
        self.df = self.df.orderBy(*sort_cols)
        return self

    def limit(self, n: int) -> "SDF":
        self.df = self.df.limit(n)
        return self

    def to_pandas(self, limit: Optional[int] = None):
        d = self.df.limit(limit) if limit is not None else self.df
        return d.toPandas()


# -----------------------------------------------------------------------------
# Example usage (commented)
# -----------------------------------------------------------------------------
# if __name__ == "__main__":
#     spark = get_spark()
#     df = read_csv(spark, "s3://bucket/data.csv", infer_schema=True)
#
#     config = {
#         "rename": {"User ID": "user_id"},
#         "drop_missing": {"subset": ["user_id"], "how": "any"},
#         "impute_missing": {"age": "median", "income": "mean", "city": "mode"},
#         "cast": {"age": "double", "income": "double"},
#         "clean_text": {"cols": ["city"], "lowercase": True, "collapse_spaces": True},
#         "winsorize": {"cols": ["income"], "lower_quantile": 0.01, "upper_quantile": 0.99},
#         "scale": {"standardize": ["age"], "minmax": {"cols": ["income"], "feature_range": [0, 1]}},
#     }
#     df2 = preprocess(df, config)
#
#     # Or using the chainable wrapper:
#     sdf = (SDF(df)
#            .rename({"User ID": "user_id"})
#            .drop_missing(["user_id"])
#            .impute({"age": "median", "income": "mean"})
#            .cast({"age": "double", "income": "double"})
#            .clean_text(["city"])
#            .winsorize(["income"])
#            .standardize(["age"]))
#     final_df = sdf.df
#     final_df.show(5)
------------------------------------------------------------

PROMPT 165:
"Generate Python code that imports the necessary modules and classes from the `apache-airflow-providers-apache-pinot` package to create an Apache Pinot connection, database, and table, and then uses the `create_table` method to create a new table in the database."
RESPONSE:
# Requires: apache-airflow, apache-airflow-providers-apache-pinot

# Programmatically create an Airflow Connection for Pinot (controller endpoint),
# then use PinotAdminHook to create a database and a table.

from airflow.models import Connection
from airflow.settings import Session

# Pinot provider hooks
try:
    # Newer provider layout
    from airflow.providers.apache.pinot.hooks.pinot_admin import PinotAdminHook
except ImportError:
    # Fallback for older versions
    from airflow.providers.apache.pinot.hooks.pinot import PinotAdminHook


def ensure_airflow_connection(conn_id: str = "pinot_admin_default") -> str:
    session = Session()
    try:
        existing = session.query(Connection).filter(Connection.conn_id == conn_id).one_or_none()
        if existing is None:
            # Adjust host/port/protocol as appropriate for your Pinot Controller
            conn = Connection(
                conn_id=conn_id,
                conn_type="pinot",
                host="pinot-controller",  # e.g. "localhost" or controller service name
                port=9000,
                # Extra typically conveys controller/broker endpoint selection and protocol
                extra='{"endpoint": "controller", "protocol": "http"}',
            )
            session.add(conn)
            session.commit()
        return conn_id
    finally:
        session.close()


def main():
    pinot_conn_id = ensure_airflow_connection("pinot_admin_default")

    admin = PinotAdminHook(pinot_conn_id=pinot_conn_id)

    # Create a Pinot database (namespace). If your Pinot version doesn’t support databases,
    # skip this or ensure the default database is used.
    database_name = "analytics"

    # Some provider versions expose create_database; if yours does not, remove this call.
    # You may also need to set if_not_exists=True depending on your provider version.
    admin.create_database(database_name=database_name)

    # Minimal example table config; adjust to your schema/indexing/ingestion needs.
    # Note: The referenced schema (events_schema) should already exist in Pinot.
    table_config = {
        "tableName": "events",
        "databaseName": database_name,  # omit if your Pinot version doesn’t use databases
        "tableType": "OFFLINE",
        "segmentsConfig": {
            "timeColumnName": "eventTime",
            "timeType": "MILLISECONDS",
            "schemaName": "events_schema"
        },
        "tenants": {},
        "tableIndexConfig": {
            "loadMode": "MMAP"
        },
        "ingestionConfig": {},
        "metadata": {}
    }

    # Create the table using the provider’s create_table method.
    # If your provider version expects a different signature (e.g., add_table),
    # adjust accordingly.
    admin.create_table(table_config=table_config)

    print("Database and table creation request submitted.")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 166:
"Generate Python code that uses the Tencent Cloud Dbbrain SDK to perform various database operations such as creating a database, creating a table, inserting data into a table, selecting data from a table, updating data in a table, and deleting data from a table."
RESPONSE:
Short answer: The Tencent Cloud Dbbrain SDK is for diagnostics/monitoring and cannot execute SQL (DDL/DML). To create databases/tables and run CRUD, use a database driver (for example, PyMySQL for MySQL hosted on TencentDB/CynosDB). You can still initialize the Dbbrain client in the same script for diagnostics as needed.

Below is a complete Python example that:
- Initializes the Tencent Cloud Dbbrain client (for optional diagnostics/monitoring).
- Connects to a MySQL instance and performs create database, create table, insert, select, update, and delete operations.

Install dependencies:
- pip install pymysql tencentcloud-sdk-python

Python code:
```python
import os
import re
import sys
from contextlib import contextmanager

import pymysql
from tencentcloud.common import credential
from tencentcloud.dbbrain.v20210527 import dbbrain_client
# You can import models if you plan to call specific Dbbrain APIs:
# from tencentcloud.dbbrain.v20210527 import models


# -------------------------------
# Optional: Initialize Dbbrain SDK
# -------------------------------
def init_dbbrain_client():
    """
    Dbbrain is for diagnostics and monitoring; it cannot execute SQL.
    Initialize this client if you plan to call Dbbrain APIs (e.g., health score, diagnostics reports).
    """
    tc_secret_id = os.getenv("TENCENTCLOUD_SECRET_ID", "")
    tc_secret_key = os.getenv("TENCENTCLOUD_SECRET_KEY", "")
    region = os.getenv("TENCENTCLOUD_REGION", "ap-guangzhou")

    if not tc_secret_id or not tc_secret_key:
        print("Warning: TENCENTCLOUD_SECRET_ID/KEY not set. Dbbrain client will not be created.", file=sys.stderr)
        return None

    cred = credential.Credential(tc_secret_id, tc_secret_key)
    client = dbbrain_client.DbbrainClient(cred, region)
    return client


# Example usage (commented out):
# dbbrain = init_dbbrain_client()
# if dbbrain:
#     # Use dbbrain to call diagnostic APIs as needed. Example (pseudocode):
#     # req = models.DescribeDBDiagHistoryRequest()
#     # req.InstanceId = "cdb-xxxxx"
#     # req.Product = "mysql"
#     # resp = dbbrain.DescribeDBDiagHistory(req)
#     # print(resp.to_json_string())
#     pass


# -------------------------------------
# MySQL helpers for executing SQL safely
# -------------------------------------

def safe_identifier(name: str) -> str:
    """
    Allow only alphanumerics, underscore, and dash for identifiers like DB/table names.
    Returns a backtick-quoted identifier for MySQL.
    """
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", name):
        raise ValueError(f"Unsafe identifier: {name!r}")
    return f"`{name}`"


@contextmanager
def mysql_connection(host, port, user, password, database=None, autocommit=True, connect_timeout=10):
    conn = pymysql.connect(
        host=host,
        port=int(port),
        user=user,
        password=password,
        database=database,
        charset="utf8mb4",
        cursorclass=pymysql.cursors.DictCursor,
        connect_timeout=connect_timeout,
        autocommit=autocommit,
    )
    try:
        yield conn
    finally:
        conn.close()


def create_database(conn_params: dict, db_name: str, charset="utf8mb4", collate="utf8mb4_unicode_ci"):
    db_ident = safe_identifier(db_name)
    with mysql_connection(**{**conn_params, "database": None}) as conn:
        with conn.cursor() as cur:
            cur.execute(f"CREATE DATABASE IF NOT EXISTS {db_ident} CHARACTER SET {charset} COLLATE {collate}")
    print(f"Database {db_name} ensured.")


def create_table(conn_params: dict, db_name: str, table_name: str):
    db_ident = safe_identifier(db_name)
    tbl_ident = safe_identifier(table_name)
    with mysql_connection(**{**conn_params, "database": db_name}) as conn:
        with conn.cursor() as cur:
            sql = f"""
            CREATE TABLE IF NOT EXISTS {tbl_ident} (
                id BIGINT UNSIGNED NOT NULL AUTO_INCREMENT,
                name VARCHAR(255) NOT NULL,
                age INT NOT NULL,
                created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (id),
                KEY idx_age (age)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
            """
            cur.execute(sql)
    print(f"Table {db_name}.{table_name} ensured.")


def insert_data(conn_params: dict, db_name: str, table_name: str, rows):
    """
    rows: iterable of dicts like [{"name": "Alice", "age": 30}, ...]
    """
    tbl_ident = safe_identifier(table_name)
    with mysql_connection(**{**conn_params, "database": db_name}) as conn:
        with conn.cursor() as cur:
            sql = f"INSERT INTO {tbl_ident} (name, age) VALUES (%s, %s)"
            data = [(r["name"], r["age"]) for r in rows]
            cur.executemany(sql, data)
            affected = cur.rowcount
    print(f"Inserted {affected} rows into {db_name}.{table_name}.")


def select_data(conn_params: dict, db_name: str, table_name: str, min_age=None):
    tbl_ident = safe_identifier(table_name)
    with mysql_connection(**{**conn_params, "database": db_name}) as conn:
        with conn.cursor() as cur:
            if min_age is None:
                sql = f"SELECT id, name, age, created_at FROM {tbl_ident} ORDER BY id"
                cur.execute(sql)
            else:
                sql = f"SELECT id, name, age, created_at FROM {tbl_ident} WHERE age >= %s ORDER BY id"
                cur.execute(sql, (min_age,))
            rows = cur.fetchall()
    print(f"Selected {len(rows)} rows from {db_name}.{table_name}.")
    return rows


def update_data(conn_params: dict, db_name: str, table_name: str, min_age: int, increment: int):
    tbl_ident = safe_identifier(table_name)
    with mysql_connection(**{**conn_params, "database": db_name}) as conn:
        with conn.cursor() as cur:
            sql = f"UPDATE {tbl_ident} SET age = age + %s WHERE age >= %s"
            cur.execute(sql, (increment, min_age))
            affected = cur.rowcount
    print(f"Updated {affected} rows in {db_name}.{table_name}.")


def delete_data(conn_params: dict, db_name: str, table_name: str, max_age: int):
    tbl_ident = safe_identifier(table_name)
    with mysql_connection(**{**conn_params, "database": db_name}) as conn:
        with conn.cursor() as cur:
            sql = f"DELETE FROM {tbl_ident} WHERE age <= %s"
            cur.execute(sql, (max_age,))
            affected = cur.rowcount
    print(f"Deleted {affected} rows from {db_name}.{table_name}.")


def main():
    # Read DB connection from environment or set directly
    conn_params = {
        "host": os.getenv("DB_HOST", "127.0.0.1"),
        "port": int(os.getenv("DB_PORT", "3306")),
        "user": os.getenv("DB_USER", "root"),
        "password": os.getenv("DB_PASSWORD", ""),
    }

    db_name = os.getenv("DB_NAME", "demo_db")
    table_name = os.getenv("DB_TABLE", "people")

    # Optional: Initialize Dbbrain client (for diagnostics only)
    _dbbrain_client = init_dbbrain_client()

    # Execute DDL/DML via MySQL driver
    create_database(conn_params, db_name)
    create_table(conn_params, db_name, table_name)

    insert_data(conn_params, db_name, table_name, [
        {"name": "Alice", "age": 30},
        {"name": "Bob", "age": 25},
        {"name": "Charlie", "age": 40},
    ])

    rows = select_data(conn_params, db_name, table_name)
    for r in rows:
        print(r)

    update_data(conn_params, db_name, table_name, min_age=30, increment=1)

    rows = select_data(conn_params, db_name, table_name, min_age=30)
    for r in rows:
        print(r)

    delete_data(conn_params, db_name, table_name, max_age=26)

    rows = select_data(conn_params, db_name, table_name)
    for r in rows:
        print(r)


if __name__ == "__main__":
    main()
```

Notes:
- Dbbrain SDK cannot run SQL. It’s for diagnostics (health scores, slow logs, space analysis, etc.). You can call those APIs with the initialized client after you run your SQL operations.
- Replace MySQL with psycopg2 (PostgreSQL) or pyodbc/pymssql (SQL Server) if your TencentDB engine differs.
------------------------------------------------------------

PROMPT 167:
"Generate Python code that imports the necessary modules from the SafetyKit framework and provides wrappers for the various functions and data types in the framework, allowing developers to easily interact with SafetyKit functionality in their Python applications."
RESPONSE:
Before I generate concrete wrappers, I need to confirm which SafetyKit you’re targeting and your runtime environment:

- OpenAI SafetyKit (developer tooling for safety and policy checks; typically a Python or TypeScript library)?
- Apple SafetyKit (an Apple framework; only accessible on Apple platforms via Objective-C/Swift; Python would require PyObjC on macOS, and iOS access is not practical from Python)?

In the meantime, here’s a robust, environment-agnostic Python scaffold that:
- Attempts to import a Python SafetyKit package if present.
- Optionally bridges to an Apple framework via PyObjC if available.
- Exposes a small, consistent Python API (analyze_text, analyze_image, redact_text, enforce_policy).
- Provides configurable name-maps so you can bind to your actual SafetyKit functions without me guessing their exact names.

Save as safetykit_wrapper.py.

```python
from __future__ import annotations

import importlib
import inspect
import io
import sys
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union

# -----------------------
# Exceptions and Typings
# -----------------------

class SafetyKitError(Exception):
    pass

class SafetyKitNotAvailable(SafetyKitError):
    pass

class SafetyKitNotImplemented(SafetyKitError):
    pass

class Severity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"
    UNKNOWN = "unknown"

@dataclass
class Finding:
    id: str
    category: str
    severity: Severity
    message: str
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class AnalysisResult:
    passed: bool
    findings: List[Finding] = field(default_factory=list)
    summary: Optional[str] = None
    raw: Optional[Any] = None  # raw result from the underlying SDK

# -----------------------
# Utilities
# -----------------------

def _maybe_import(names: Sequence[str]) -> Optional[Any]:
    for name in names:
        try:
            return importlib.import_module(name)
        except Exception:
            continue
    return None

def _has_attr_any(mod: Any, candidates: Sequence[str]) -> Optional[str]:
    for c in candidates:
        if hasattr(mod, c):
            return c
    return None

def _to_bytes_image(img: Any) -> bytes:
    """
    Best-effort conversion of PIL.Image, numpy array, or bytes-like into bytes (PNG).
    """
    # Already bytes
    if isinstance(img, (bytes, bytearray, memoryview)):
        return bytes(img)

    # PIL Image
    try:
        from PIL.Image import Image as PILImage  # type: ignore
        if isinstance(img, PILImage):
            buf = io.BytesIO()
            img.save(buf, format="PNG")
            return buf.getvalue()
    except Exception:
        pass

    # numpy array
    try:
        import numpy as np  # type: ignore
        if isinstance(img, np.ndarray):
            from PIL import Image as PILImage  # type: ignore
            pil = PILImage.fromarray(img)
            buf = io.BytesIO()
            pil.save(buf, format="PNG")
            return buf.getvalue()
    except Exception:
        pass

    raise SafetyKitError("Unsupported image type; provide bytes, PIL.Image, or numpy.ndarray")

def _to_severity(value: Any) -> Severity:
    if isinstance(value, Severity):
        return value
    if isinstance(value, str):
        v = value.strip().lower()
        for s in Severity:
            if s.value == v:
                return s
    if isinstance(value, (int, float)):
        # crude numeric mapping (0..1 or 0..100)
        n = float(value)
        if n <= 0.25 or n <= 25:
            return Severity.LOW
        if n <= 0.5 or n <= 50:
            return Severity.MEDIUM
        if n <= 0.75 or n <= 75:
            return Severity.HIGH
        return Severity.CRITICAL
    return Severity.UNKNOWN

# -----------------------
# Backend Abstractions
# -----------------------

class Backend:
    """
    Abstract backend adapter. Subclasses should implement the call_* methods.
    """

    def analyze_text(self, text: str, context: Optional[Dict[str, Any]] = None) -> AnalysisResult:
        raise SafetyKitNotImplemented("analyze_text is not implemented for this backend")

    def analyze_image(self, image: Union[bytes, bytearray, memoryview, Any], context: Optional[Dict[str, Any]] = None) -> AnalysisResult:
        raise SafetyKitNotImplemented("analyze_image is not implemented for this backend")

    def redact_text(self, text: str, policies: Optional[Dict[str, Any]] = None) -> str:
        raise SafetyKitNotImplemented("redact_text is not implemented for this backend")

    def enforce_policy(self, content: Any, policy: Dict[str, Any]) -> Dict[str, Any]:
        raise SafetyKitNotImplemented("enforce_policy is not implemented for this backend")

# -----------------------
# Python Module Backend
# -----------------------

class PythonModuleBackend(Backend):
    """
    Generic adapter that calls into a Python SafetyKit-like module.
    Configure name_map to match your module's function names.
    """

    def __init__(self, module: Any, name_map: Optional[Dict[str, Sequence[str]]] = None):
        self.module = module
        self.name_map = name_map or {
            "analyze_text": ("analyze_text", "moderate_text", "classify_text", "evaluate_text"),
            "analyze_image": ("analyze_image", "moderate_image", "classify_image", "evaluate_image"),
            "redact_text": ("redact_text", "sanitize_text", "filter_text"),
            "enforce_policy": ("enforce_policy", "apply_policy", "check_policy"),
        }

    def _resolve(self, key: str) -> Callable[..., Any]:
        cands = self.name_map.get(key, ())
        name = _has_attr_any(self.module, cands)
        if not name:
            raise SafetyKitNotImplemented(f"No function found for {key}; tried {cands}")
        fn = getattr(self.module, name)
        if not callable(fn):
            raise SafetyKitError(f"Resolved attribute for {key} is not callable: {name}")
        return fn

    def analyze_text(self, text: str, context: Optional[Dict[str, Any]] = None) -> AnalysisResult:
        fn = self._resolve("analyze_text")
        raw = fn(text=text, context=context) if "context" in inspect.signature(fn).parameters else fn(text)
        return _normalize_analysis_result(raw)

    def analyze_image(self, image: Union[bytes, bytearray, memoryview, Any], context: Optional[Dict[str, Any]] = None) -> AnalysisResult:
        fn = self._resolve("analyze_image")
        img_bytes = _to_bytes_image(image)
        raw = fn(image=img_bytes, context=context) if "context" in inspect.signature(fn).parameters else fn(image=img_bytes)
        return _normalize_analysis_result(raw)

    def redact_text(self, text: str, policies: Optional[Dict[str, Any]] = None) -> str:
        fn = self._resolve("redact_text")
        result = fn(text=text, policies=policies) if "policies" in inspect.signature(fn).parameters else fn(text=text)
        if not isinstance(result, str):
            raise SafetyKitError("redact_text expected to return str")
        return result

    def enforce_policy(self, content: Any, policy: Dict[str, Any]) -> Dict[str, Any]:
        fn = self._resolve("enforce_policy")
        result = fn(content=content, policy=policy) if set(["content", "policy"]).issubset(inspect.signature(fn).parameters) else fn(content, policy)
        if not isinstance(result, dict):
            raise SafetyKitError("enforce_policy expected to return dict")
        return result

# -----------------------
# Apple SafetyKit via PyObjC Backend (skeleton)
# -----------------------

class AppleObjCBackend(Backend):
    """
    Skeleton adapter for Apple SafetyKit via PyObjC on macOS.
    You must provide factory callables that know how to call ObjC classes/methods.

    Example config:
      AppleObjCBackend(
        analyze_text_fn=lambda text, context: my_SKAnalyzer.shared().analyzeText_error_(text, None),
        analyze_image_fn=lambda image_bytes, context: my_SKAnalyzer.shared().analyzeImage_error_(image_bytes, None),
        redact_text_fn=lambda text, policies: my_SKRedactor.redact_text_policies_(text, policies),
        enforce_policy_fn=lambda content, policy: my_SKPolicyEngine.apply_policy_to_content_(policy, content),
      )
    """

    def __init__(
        self,
        analyze_text_fn: Optional[Callable[[str, Optional[Dict[str, Any]]], Any]] = None,
        analyze_image_fn: Optional[Callable[[bytes, Optional[Dict[str, Any]]], Any]] = None,
        redact_text_fn: Optional[Callable[[str, Optional[Dict[str, Any]]], Any]] = None,
        enforce_policy_fn: Optional[Callable[[Any, Dict[str, Any]], Any]] = None,
    ):
        self._analyze_text_fn = analyze_text_fn
        self._analyze_image_fn = analyze_image_fn
        self._redact_text_fn = redact_text_fn
        self._enforce_policy_fn = enforce_policy_fn

    def analyze_text(self, text: str, context: Optional[Dict[str, Any]] = None) -> AnalysisResult:
        if not self._analyze_text_fn:
            raise SafetyKitNotImplemented("analyze_text not configured")
        raw = self._analyze_text_fn(text, context)
        return _normalize_analysis_result(raw)

    def analyze_image(self, image: Union[bytes, bytearray, memoryview, Any], context: Optional[Dict[str, Any]] = None) -> AnalysisResult:
        if not self._analyze_image_fn:
            raise SafetyKitNotImplemented("analyze_image not configured")
        img_bytes = _to_bytes_image(image)
        raw = self._analyze_image_fn(img_bytes, context)
        return _normalize_analysis_result(raw)

    def redact_text(self, text: str, policies: Optional[Dict[str, Any]] = None) -> str:
        if not self._redact_text_fn:
            raise SafetyKitNotImplemented("redact_text not configured")
        result = self._redact_text_fn(text, policies)
        if not isinstance(result, str):
            raise SafetyKitError("redact_text expected to return str")
        return result

    def enforce_policy(self, content: Any, policy: Dict[str, Any]) -> Dict[str, Any]:
        if not self._enforce_policy_fn:
            raise SafetyKitNotImplemented("enforce_policy not configured")
        result = self._enforce_policy_fn(content, policy)
        if not isinstance(result, dict):
            raise SafetyKitError("enforce_policy expected to return dict")
        return result

def try_build_apple_backend() -> Optional[AppleObjCBackend]:
    """
    Attempts to load PyObjC and the SafetyKit framework, returns a backend skeleton if feasible.
    You must still wire the callables to actual ObjC methods for your version of SafetyKit.
    """
    try:
        import objc  # type: ignore
        from Foundation import NSObject  # type: ignore
    except Exception:
        return None

    # If the framework is present and public, you could try:
    #   objc.loadBundle('SafetyKit', globals(), bundle_path=objc.pathForFramework('/System/Library/Frameworks/SafetyKit.framework'))
    # But many Apple "SafetyKit" APIs may not be publicly available on macOS or may differ by OS version.
    # We return an unconfigured backend to be wired by the integrator.
    return AppleObjCBackend()

# -----------------------
# Result Normalization
# -----------------------

def _normalize_analysis_result(raw: Any) -> AnalysisResult:
    """
    Converts various shapes of results into AnalysisResult:
    Accepts:
      - dict-like with keys such as {"passed": bool, "findings": [...], "summary": str}
      - objects with attributes
      - simple booleans or score-based results (best-effort)
    """
    # Already normalized
    if isinstance(raw, AnalysisResult):
        return raw

    # Dict-like
    if isinstance(raw, dict):
        passed = bool(raw.get("passed", True if not raw.get("findings") else False))
        summary = raw.get("summary")
        findings_raw = raw.get("findings", [])
        findings: List[Finding] = []

        for f in findings_raw:
            if isinstance(f, Finding):
                findings.append(f)
                continue
            if isinstance(f, dict):
                fid = str(f.get("id", ""))
                category = str(f.get("category", "unspecified"))
                severity = _to_severity(f.get("severity", "unknown"))
                message = str(f.get("message", "")) if f.get("message") is not None else ""
                metadata = dict(f.get("metadata", {}))
                findings.append(Finding(id=fid, category=category, severity=severity, message=message, metadata=metadata))
                continue
            # Unknown finding type; coerce to a generic finding
            findings.append(Finding(id="", category="unspecified", severity=Severity.UNKNOWN, message=str(f)))

        return AnalysisResult(passed=passed, findings=findings, summary=summary, raw=raw)

    # Bool or numeric score
    if isinstance(raw, bool):
        return AnalysisResult(passed=raw, findings=[], summary=None, raw=raw)
    if isinstance(raw, (int, float)):
        sev = _to_severity(raw)
        passed = sev in (Severity.LOW, Severity.MEDIUM)
        return AnalysisResult(
            passed=passed,
            findings=[] if passed else [Finding(id="", category="risk", severity=sev, message=f"Score={raw}")],
            summary=f"score={raw}",
            raw=raw,
        )

    # Object with attributes
    passed = getattr(raw, "passed", None)
    findings_attr = getattr(raw, "findings", None)
    summary_attr = getattr(raw, "summary", None)
    if passed is not None or findings_attr is not None or summary_attr is not None:
        try:
            findings_list: List[Finding] = []
            if findings_attr:
                for f in findings_attr:
                    if isinstance(f, Finding):
                        findings_list.append(f)
                    else:
                        findings_list.append(
                            Finding(
                                id=str(getattr(f, "id", "")),
                                category=str(getattr(f, "category", "unspecified")),
                                severity=_to_severity(getattr(f, "severity", "unknown")),
                                message=str(getattr(f, "message", "")),
                                metadata=dict(getattr(f, "metadata", {}) or {}),
                            )
                        )
            return AnalysisResult(
                passed=bool(passed) if passed is not None else (not findings_list),
                findings=findings_list,
                summary=str(summary_attr) if summary_attr is not None else None,
                raw=raw,
            )
        except Exception:
            pass

    # Fallback
    return AnalysisResult(passed=True, findings=[], summary=None, raw=raw)

# -----------------------
# Public Facade
# -----------------------

class SafetyKit:
    """
    Facade that exposes a simple, stable Python API, delegating to a configured backend.
    """

    def __init__(self, backend: Optional[Backend] = None):
        self.backend = backend or auto_detect_backend()
        if self.backend is None:
            raise SafetyKitNotAvailable(
                "No SafetyKit backend found. Install/Provide a Python SafetyKit package or configure Apple PyObjC backend."
            )

    def analyze_text(self, text: str, context: Optional[Dict[str, Any]] = None) -> AnalysisResult:
        return self.backend.analyze_text(text, context)

    def analyze_image(self, image: Union[bytes, bytearray, memoryview, Any], context: Optional[Dict[str, Any]] = None) -> AnalysisResult:
        return self.backend.analyze_image(image, context)

    def redact_text(self, text: str, policies: Optional[Dict[str, Any]] = None) -> str:
        return self.backend.redact_text(text, policies)

    def enforce_policy(self, content: Any, policy: Dict[str, Any]) -> Dict[str, Any]:
        return self.backend.enforce_policy(content, policy)

# -----------------------
# Auto-detection
# -----------------------

def auto_detect_backend(name_map: Optional[Dict[str, Sequence[str]]] = None) -> Optional[Backend]:
    """
    Tries to locate a Python SafetyKit package, else falls back to Apple PyObjC skeleton.
    Update the list of candidate import names to match your environment.
    """
    candidates = (
        "safetykit",            # e.g., a Python package named 'safetykit'
        "openai_safetykit",     # hypothetical
        "safety_kit",           # hypothetical
        "SafetyKit",            # module alias
    )
    mod = _maybe_import(candidates)
    if mod is not None:
        return PythonModuleBackend(mod, name_map=name_map)

    apple_backend = try_build_apple_backend()
    if apple_backend is not None:
        return apple_backend

    return None

# -----------------------
# Example Usage
# -----------------------

if __name__ == "__main__":
    # Example: using auto-detected backend
    try:
        sk = SafetyKit()
    except SafetyKitNotAvailable as e:
        print(f"SafetyKit not available: {e}", file=sys.stderr)
        sys.exit(1)

    # Analyze text
    try:
        res = sk.analyze_text("Example text to analyze", context={"use_case": "moderation"})
        print("Passed:", res.passed)
        print("Findings:", [f"{f.category}:{f.severity.value}" for f in res.findings])
        if res.summary:
            print("Summary:", res.summary)
    except SafetyKitError as e:
        print(f"Error analyzing text: {e}", file=sys.stderr)

    # Redact text
    try:
        redacted = sk.redact_text("Contact me at john.doe@example.com or +1-555-123-4567", policies={"pii": True})
        print("Redacted:", redacted)
    except SafetyKitError as e:
        print(f"Error redacting text: {e}", file=sys.stderr)
```

How to adapt this to your SafetyKit:

- If you have a Python SafetyKit package:
  - Ensure it’s importable (e.g., pip install safetykit).
  - Update the name_map in PythonModuleBackend to match the actual function names in your package if they differ.
  - The wrapper expects those functions to return either:
    - a dict with keys like passed, findings, summary, or
    - a custom object with similar attributes, or
    - a boolean/score, which will be normalized.

- If you’re integrating Apple SafetyKit via PyObjC on macOS:
  - Use try_build_apple_backend() to get an AppleObjCBackend instance and then configure the callables with your actual Objective-C method invocations.
  - Replace the placeholder Objective-C calls with the correct class/method names for your SafetyKit version.

If you can confirm which SafetyKit you’re using (and provide any known API references or function names you need wrapped), I’ll tailor this scaffolding into concrete wrappers with exact imports and signatures.
------------------------------------------------------------

PROMPT 168:
"Generate Python code that uses the `tox` library to create a plugin that allows sharing of a common `tox.ini` file across repositories with high package counts by using the `os` library to read and write files, and the `json` library to parse and generate JSON data."
RESPONSE:
# tox_shared_config.py
# A tox 4 plugin that lets many repositories share a common tox.ini,
# optionally augmenting it per-repository from JSON data, and writing
# a merged tox.ini to disk before tox runs.
#
# It uses:
# - tox's plugin hooks to add options and run during configuration
# - os for file IO and path handling
# - json to parse repo-specific overrides and to emit a lock/report JSON
#
# Install as a plugin (example pyproject.toml entry):
# [project.entry-points."tox"]
# "shared-config" = "tox_shared_config"

import json
import os
from io import StringIO
from configparser import ConfigParser
from typing import Any, Dict, List, Tuple, Optional

try:
    # tox 4 plugin marker
    from tox.plugin import impl
except Exception as exc:  # pragma: no cover
    raise RuntimeError("This plugin requires tox 4.x") from exc


ENV_VAR_JSON = "TOX_SHARED_CONFIG_JSON"
ENV_VAR_ENABLE_MERGE = "TOX_SHARED_ENABLE_MERGE"
ENV_VAR_WRITE_MERGED = "TOX_SHARED_WRITE_MERGED"

DEFAULT_JSON_FILE = ".tox-shared.json"
DEFAULT_OUTPUT_ToxIni = "tox.ini"
LOCK_FILE_NAME = ".tox-shared.lock.json"


def _read_text(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def _write_text(path: str, data: str) -> None:
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(data)


def _normalize_value_for_ini(value: Any) -> str:
    # Convert a JSON value to an .ini-compatible string
    if isinstance(value, list):
        # tox allows multi-line for deps/commands; write each entry on its own line
        return "\n".join(str(v) for v in value)
    if isinstance(value, bool):
        return "true" if value else "false"
    return str(value)


def _apply_overrides(ini: ConfigParser, overrides: Dict[str, Dict[str, Any]]) -> None:
    for section, mapping in overrides.items():
        if not ini.has_section(section):
            ini.add_section(section)
        for key, value in mapping.items():
            ini.set(section, key, _normalize_value_for_ini(value))


def _ensure_tox_section(ini: ConfigParser) -> None:
    if not ini.has_section("tox"):
        ini.add_section("tox")


def _read_ini_from_text_or_path(shared_ini_text: Optional[str], shared_ini_path: Optional[str]) -> str:
    if shared_ini_text:
        return shared_ini_text
    if shared_ini_path:
        return _read_text(shared_ini_path)
    return ""


def _compute_envlist(
    base_envs: Optional[List[str]],
    packages: Optional[List[Dict[str, Any]]],
    default_envs: Optional[List[str]],
) -> List[str]:
    # Compute envlist to cover many packages with many python versions
    envs = base_envs or default_envs or []
    pkgs = packages or []
    if not pkgs:
        return envs
    if not envs:
        # Just per-package envs
        return [f"{pkg.get('name')}" for pkg in pkgs if "name" in pkg]

    # Cross product: env-pkg
    out: List[str] = []
    for e in envs:
        for pkg in pkgs:
            name = pkg.get("name")
            if not name:
                continue
            out.append(f"{e}-{name}")
    return out


def _add_package_sections(ini: ConfigParser, packages: List[Dict[str, Any]]) -> None:
    # For each package, create a dedicated testenv section factor that changes dir, etc.
    # The env name factor (e.g., py310-pkgA) will inherit from [testenv] but can override.
    for pkg in packages:
        name = pkg.get("name")
        path = pkg.get("path")
        extra = pkg.get("overrides", {})  # Optional extra overrides for this package
        if not name:
            continue
        section = f"testenv:{name}"
        if not ini.has_section(section):
            ini.add_section(section)
        if path:
            ini.set(section, "changedir", path)
        # Apply per-package overrides
        for key, value in extra.items():
            ini.set(section, key, _normalize_value_for_ini(value))


def _render_ini(ini: ConfigParser) -> str:
    # ConfigParser writes with lower-cased keys; acceptable for tox
    buf = StringIO()
    ini.write(buf)
    return buf.getvalue()


def _load_json(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _dump_json(path: str, obj: Dict[str, Any]) -> None:
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, sort_keys=True)


def _merge_shared_config(
    json_data: Dict[str, Any],
    cwd: str,
) -> Tuple[str, Dict[str, Any]]:
    """
    Merge the shared tox.ini (text or path) with repo-local overrides described in JSON,
    optionally expanding envlist for many packages, and return the merged ini text alongside
    a lock-report JSON.
    """
    shared_ini_text = json_data.get("shared_ini_text")
    shared_ini_path = json_data.get("shared_ini_path")
    overrides = json_data.get("overrides", {}) or {}
    output_path = json_data.get("output_path") or DEFAULT_OUTPUT_ToxIni
    packages = json_data.get("packages", []) or []
    base_envs = json_data.get("base_envs")  # list
    default_envs = json_data.get("default_envs")  # list
    lock: Dict[str, Any] = {"source": {}, "result": {}}

    # Resolve shared ini text
    if shared_ini_path and not os.path.isabs(shared_ini_path):
        shared_ini_path = os.path.normpath(os.path.join(cwd, shared_ini_path))

    src_ini_text = _read_ini_from_text_or_path(shared_ini_text, shared_ini_path)
    lock["source"]["shared_ini_path"] = shared_ini_path
    lock["source"]["has_shared_ini_text"] = bool(shared_ini_text)
    lock["source"]["json_overrides"] = bool(overrides)
    lock["source"]["packages_count"] = len(packages)

    ini = ConfigParser()
    if src_ini_text.strip():
        ini.read_string(src_ini_text)
    else:
        # Initialize empty base
        ini.add_section("tox")
        ini.add_section("testenv")

    # Ensure [tox] exists
    _ensure_tox_section(ini)

    # Apply overrides (global)
    _apply_overrides(ini, overrides)

    # Add package-specific sections
    if packages:
        _add_package_sections(ini, packages)

    # Compute envlist for many-package repos
    envlist = _compute_envlist(base_envs, packages, default_envs)
    if envlist:
        ini.set("tox", "envlist", "\n".join(envlist))

    merged_text = _render_ini(ini)

    lock["result"]["output_path"] = output_path
    lock["result"]["envlist"] = envlist
    lock["result"]["sections"] = [s for s in ini.sections()]
    return merged_text, lock


@impl
def tox_add_option(parser) -> None:
    """
    Adds options to tox:
    --shared-config-json: path to a JSON file defining shared config and overrides
    --merge-shared-toxini: merge shared config into memory and optionally write file
    --write-merged-toxini: write merged tox.ini to disk before running
    """
    parser.add_argument(
        "--shared-config-json",
        dest="shared_config_json",
        default=os.environ.get(ENV_VAR_JSON, DEFAULT_JSON_FILE),
        help=(
            f"Path to JSON file describing shared tox config "
            f"(env {ENV_VAR_JSON} can also set this)."
        ),
    )
    parser.add_argument(
        "--merge-shared-toxini",
        dest="merge_shared_toxini",
        action="store_true",
        default=os.environ.get(ENV_VAR_ENABLE_MERGE, "0") not in ("0", "", "false", "False"),
        help=(
            f"Merge shared tox.ini using the JSON file prior to run "
            f"(env {ENV_VAR_ENABLE_MERGE}=1 to enable by default)."
        ),
    )
    parser.add_argument(
        "--write-merged-toxini",
        dest="write_merged_toxini",
        action="store_true",
        default=os.environ.get(ENV_VAR_WRITE_MERGED, "0") not in ("0", "", "false", "False"),
        help=(
            f"Write the merged tox.ini to disk (default path from JSON or {DEFAULT_OUTPUT_ToxIni}) "
            f"(env {ENV_VAR_WRITE_MERGED}=1 to enable by default)."
        ),
    )


@impl
def tox_configure(config) -> None:
    """
    When configuration starts, if requested, merge shared config and optionally
    write the merged tox.ini to disk. Also write a lock/report JSON to help
    auditing large repos.
    """
    # Robustly fetch options across tox versions
    options = getattr(config, "options", None)
    if options is None:
        # Fallback: nothing to do
        return

    json_path: str = getattr(options, "shared_config_json", os.environ.get(ENV_VAR_JSON, DEFAULT_JSON_FILE))
    do_merge: bool = bool(getattr(options, "merge_shared_toxini", False))
    do_write: bool = bool(getattr(options, "write_merged_toxini", False))

    if not do_merge and not do_write:
        return

    # Resolve working directory. tox 4 config has attribute 'work_dir' and 'toxinidir' is cwd,
    # but keep it simple and use os.getcwd().
    cwd = os.getcwd()

    if not os.path.isabs(json_path):
        json_path = os.path.normpath(os.path.join(cwd, json_path))

    if not os.path.exists(json_path):
        # No JSON config found; nothing to merge
        return

    try:
        data = _load_json(json_path)
    except Exception as exc:
        raise SystemExit(f"Failed to parse shared config JSON at {json_path}: {exc}")

    merged_ini_text, lock = _merge_shared_config(data, cwd)

    # Optionally write merged tox.ini
    output_path = data.get("output_path") or DEFAULT_OUTPUT_ToxIni
    if not os.path.isabs(output_path):
        output_path = os.path.normpath(os.path.join(cwd, output_path))

    if do_write:
        _write_text(output_path, merged_ini_text)

    # Always emit a lock/report JSON next to the JSON config to help track what happened
    lock_path = os.path.join(os.path.dirname(json_path), LOCK_FILE_NAME)
    lock["meta"] = {
        "json_path": json_path,
        "output_path": output_path,
        "cwd": cwd,
        "wrote_tox_ini": bool(do_write),
    }
    try:
        _dump_json(lock_path, lock)
    except Exception:
        # Non-fatal; continue
        pass

    # Optionally, you may want tox to re-load configuration from the newly written tox.ini.
    # tox 4 does not expose a stable public API to force a re-parse here; typically users run:
    #   tox --merge-shared-toxini --write-merged-toxini
    # and tox will pick up the written file on subsequent invocations.
    # If you need "in-memory" mutation of the config, you'd interact with tox's internal
    # config sets, which is beyond the scope of this minimal, file-based plugin.

# End of plugin module

# Example .tox-shared.json schema this plugin understands:
# {
#   "shared_ini_path": "../central/tox.ini",     // Optional; absolute or relative
#   "shared_ini_text": "[tox]\nminversion=4.0\n",// Optional; used if path omitted
#   "output_path": "tox.ini",                    // Optional; defaults to tox.ini
#   "overrides": {                               // Optional; merged atop shared ini
#     "tox": {
#       "minversion": "4.14",
#       "requires": ["tox>=4.14,<5"]
#     },
#     "testenv": {
#       "deps": ["pytest", "coverage"],
#       "commands": ["pytest -q"]
#     }
#   },
#   "base_envs": ["py39", "py310", "py311"],     // Optional; base env matrix
#   "default_envs": ["py310"],                   // Used if base_envs missing
#   "packages": [                                // Optional; for many-package repos
#     {"name": "pkgA", "path": "packages/pkgA"},
#     {"name": "pkgB", "path": "packages/pkgB",
#      "overrides": {"deps": ["pytest-xdist"]}}
#   ]
# }
#
# Usage:
#   tox --merge-shared-toxini --write-merged-toxini --shared-config-json .tox-shared.json
#
# Environment variables:
#   TOX_SHARED_CONFIG_JSON=/path/to/.tox-shared.json
#   TOX_SHARED_ENABLE_MERGE=1
#   TOX_SHARED_WRITE_MERGED=1
------------------------------------------------------------

PROMPT 169:
"Generate Python code that imports the requests library and uses it to send HTTP requests to the BioThings API, parsing the responses into usable data structures such as lists, dictionaries, or Pandas DataFrames, depending on the specific endpoint and response format."
RESPONSE:
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import pandas as pd


class BioThingsAPIError(Exception):
    pass


def _build_session(total_retries: int = 3, backoff_factor: float = 0.3) -> requests.Session:
    session = requests.Session()
    retry = Retry(
        total=total_retries,
        read=total_retries,
        connect=total_retries,
        backoff_factor=backoff_factor,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "POST"]),
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    return session


def _request_json(
    session: requests.Session,
    method: str,
    url: str,
    *,
    params: dict | None = None,
    data: dict | None = None,
    json_body: dict | None = None,
    timeout: int = 20,
) -> dict | list:
    try:
        resp = session.request(method, url, params=params, data=data, json=json_body, timeout=timeout)
    except requests.RequestException as e:
        raise BioThingsAPIError(f"Request failed: {e}") from e

    if not resp.ok:
        # Try to include any JSON error message, otherwise text
        try:
            err = resp.json()
        except Exception:
            err = resp.text
        raise BioThingsAPIError(f"HTTP {resp.status_code} error for {url}: {err}")

    try:
        return resp.json()
    except ValueError as e:
        raise BioThingsAPIError(f"Failed to parse JSON response from {url}") from e


def _fields_param(fields: str | list[str] | None) -> str | None:
    if fields is None:
        return None
    if isinstance(fields, str):
        return fields
    return ",".join(fields)


def _to_dataframe(records: list[dict]) -> pd.DataFrame:
    # Use json_normalize to flatten nested dicts when possible
    return pd.json_normalize(records)


# ---------------------------
# MyGene.info
# ---------------------------

_MYGENE_BASE = "https://mygene.info/v3"


def mygene_query(
    query: str,
    *,
    fields: str | list[str] | None = None,
    species: str | None = "human",
    size: int = 10,
    session: requests.Session | None = None,
) -> tuple[list[dict], pd.DataFrame]:
    sess = session or _build_session()
    params = {
        "q": query,
        "size": size,
        "species": species,
        "fields": _fields_param(fields),
    }
    data = _request_json(sess, "GET", f"{_MYGENE_BASE}/query", params=params)
    hits = data.get("hits", [])
    return hits, _to_dataframe(hits)


def mygene_gene(
    gene_id: str | int,
    *,
    fields: str | list[str] | None = None,
    session: requests.Session | None = None,
) -> dict:
    sess = session or _build_session()
    params = {"fields": _fields_param(fields)} if fields else None
    return _request_json(sess, "GET", f"{_MYGENE_BASE}/gene/{gene_id}", params=params)


def mygene_query_all(
    query: str,
    *,
    fields: str | list[str] | None = None,
    species: str | None = "human",
    size: int = 1000,
    max_records: int | None = None,
    session: requests.Session | None = None,
) -> list[dict]:
    sess = session or _build_session()
    all_hits: list[dict] = []
    start = 0
    while True:
        params = {
            "q": query,
            "fields": _fields_param(fields),
            "species": species,
            "size": size,
            "from": start,
        }
        data = _request_json(sess, "GET", f"{_MYGENE_BASE}/query", params=params)
        hits = data.get("hits", [])
        if not hits:
            break
        all_hits.extend(hits)
        start += len(hits)
        if max_records is not None and len(all_hits) >= max_records:
            all_hits = all_hits[:max_records]
            break
        if start >= data.get("total", start):
            break
    return all_hits


# ---------------------------
# MyVariant.info
# ---------------------------

_MYVARIANT_BASE = "https://myvariant.info/v1"


def myvariant_batch(
    ids: list[str],
    *,
    fields: str | list[str] | None = None,
    assembly: str | None = None,
    session: requests.Session | None = None,
) -> tuple[list[dict], pd.DataFrame]:
    sess = session or _build_session()
    # Batch endpoint: POST /variant with 'ids' (comma-separated) and optional 'fields'
    data = {
        "ids": ",".join(ids),
    }
    if fields:
        data["fields"] = _fields_param(fields)
    if assembly:
        data["assembly"] = assembly
    resp = _request_json(sess, "POST", f"{_MYVARIANT_BASE}/variant", data=data)
    # Response is a list of dicts
    if not isinstance(resp, list):
        raise BioThingsAPIError("Unexpected response format from MyVariant batch endpoint")
    return resp, _to_dataframe(resp)


def myvariant_query(
    query: str,
    *,
    fields: str | list[str] | None = None,
    size: int = 10,
    session: requests.Session | None = None,
) -> tuple[list[dict], pd.DataFrame]:
    sess = session or _build_session()
    params = {"q": query, "size": size, "fields": _fields_param(fields)}
    data = _request_json(sess, "GET", f"{_MYVARIANT_BASE}/query", params=params)
    hits = data.get("hits", [])
    return hits, _to_dataframe(hits)


# ---------------------------
# MyChem.info
# ---------------------------

_MYCHEM_BASE = "https://mychem.info/v1"


def mychem_query(
    query: str,
    *,
    fields: str | list[str] | None = None,
    size: int = 10,
    session: requests.Session | None = None,
) -> tuple[list[dict], pd.DataFrame]:
    sess = session or _build_session()
    params = {"q": query, "size": size, "fields": _fields_param(fields)}
    data = _request_json(sess, "GET", f"{_MYCHEM_BASE}/query", params=params)
    hits = data.get("hits", [])
    return hits, _to_dataframe(hits)


# ---------------------------
# MyDisease.info
# ---------------------------

_MYDISEASE_BASE = "https://mydisease.info/v1"


def mydisease_get(
    disease_id: str,
    *,
    fields: str | list[str] | None = None,
    session: requests.Session | None = None,
) -> dict:
    sess = session or _build_session()
    params = {"fields": _fields_param(fields)} if fields else None
    return _request_json(sess, "GET", f"{_MYDISEASE_BASE}/disease/{disease_id}", params=params)


def mydisease_query(
    query: str,
    *,
    fields: str | list[str] | None = None,
    size: int = 10,
    session: requests.Session | None = None,
) -> tuple[list[dict], pd.DataFrame]:
    sess = session or _build_session()
    params = {"q": query, "size": size, "fields": _fields_param(fields)}
    data = _request_json(sess, "GET", f"{_MYDISEASE_BASE}/query", params=params)
    hits = data.get("hits", [])
    return hits, _to_dataframe(hits)


# ---------------------------
# Example usage
# ---------------------------

if __name__ == "__main__":
    session = _build_session()

    # MyGene: query genes and parse to list and DataFrame
    mg_hits, mg_df = mygene_query(
        "TP53",
        fields=["_id", "symbol", "name", "entrezgene", "taxid", "summary"],
        species="human",
        size=5,
        session=session,
    )
    print("MyGene hits (list of dicts):", mg_hits[:1])
    print("MyGene DataFrame columns:", list(mg_df.columns))
    print("MyGene DataFrame sample:")
    print(mg_df.head(3))

    # MyGene: single gene lookup returns a dictionary
    gene_doc = mygene_gene(1017, fields=["symbol", "name", "taxid", "entrezgene"], session=session)
    print("\nMyGene single gene dict keys:", list(gene_doc.keys()))

    # MyVariant: batch lookup returns list and DataFrame
    mv_ids = ["rs58991260", "chr7:g.140453136A>T"]
    mv_records, mv_df = myvariant_batch(
        mv_ids,
        fields=["_id", "dbsnp.rsid", "vcf.chrom", "vcf.pos", "vcf.ref", "vcf.alt"],
        session=session,
    )
    print("\nMyVariant batch list length:", len(mv_records))
    print("MyVariant DataFrame sample:")
    print(mv_df.head(3))

    # MyChem: compound query to hits list and DataFrame
    mc_hits, mc_df = mychem_query(
        "aspirin",
        fields=["_id", "drugbank", "chebi", "chembl.molecule_chembl_id", "unii"],
        size=5,
        session=session,
    )
    print("\nMyChem hits count:", len(mc_hits))
    print("MyChem DataFrame sample:")
    print(mc_df.head(3))

    # MyDisease: single disease doc as dictionary
    disease_doc = mydisease_get("DOID:9352", fields=["_id", "name", "definition", "xref"], session=session)
    print("\nMyDisease single disease dict keys:", list(disease_doc.keys()))

    # MyDisease: query to hits list and DataFrame
    md_hits, md_df = mydisease_query(
        "breast cancer",
        fields=["_id", "name", "definition", "doid", "xref"],
        size=5,
        session=session,
    )
    print("\nMyDisease hits count:", len(md_hits))
    print("MyDisease DataFrame sample:")
    print(md_df.head(3))

    # MyGene: paginate to collect many hits as a list of dicts
    all_tp53_related = mygene_query_all(
        "TP53",
        fields=["_id", "symbol", "name", "entrezgene", "taxid"],
        species="human",
        size=1000,
        max_records=2000,
        session=session,
    )
    print(f"\nMyGene paginated hits collected: {len(all_tp53_related)}")
------------------------------------------------------------

PROMPT 170:
"Generate Python code that uses the inspect module to retrieve information about the current framework and environment, and then augments SQL statements with this metadata by using string formatting to insert the information into comments at the beginning of the SQL code."
RESPONSE:
# Python 3.x
# Augment SQL statements with environment and call-site metadata using inspect,
# inserting the data as leading SQL comments.

from __future__ import annotations

import datetime as _dt
import getpass as _getpass
import inspect as _inspect
import os as _os
import platform as _platform
import socket as _socket
import sys as _sys
import threading as _threading
from importlib import metadata as _md
from typing import Dict, Optional


def _safe_get_pkg_version(dist_name: str) -> Optional[str]:
    try:
        return _md.version(dist_name)
    except _md.PackageNotFoundError:
        return None
    except Exception:
        return None


def _detect_frameworks() -> Dict[str, str]:
    # Probe for common frameworks and DB clients without importing them
    candidates = {
        "Django": "django",
        "Flask": "flask",
        "FastAPI": "fastapi",
        "Starlette": "starlette",
        "SQLAlchemy": "sqlalchemy",
        "psycopg2": "psycopg2",
        "psycopg": "psycopg",
        "asyncpg": "asyncpg",
        "PyMySQL": "pymysql",
        "mysqlclient": "mysqlclient",
        "oracledb": "oracledb",
        "cx_Oracle": "cx-Oracle",
        "snowflake-connector-python": "snowflake-connector-python",
        "duckdb": "duckdb",
        "pandas": "pandas",
    }
    found = {}
    for label, dist_name in candidates.items():
        ver = _safe_get_pkg_version(dist_name)
        if ver:
            found[label] = ver
    return found


def _guess_venv() -> Optional[str]:
    # Try to infer virtual environment name/path
    env = _os.environ.get("VIRTUAL_ENV")
    if env:
        return env
    # venv/virtualenv heuristics via prefixes
    base = getattr(_sys, "base_prefix", None) or getattr(_sys, "real_prefix", None)
    if base and base != _sys.prefix:
        return _sys.prefix
    return None


def _class_name_from_frame(frame) -> Optional[str]:
    # Best-effort to infer class name for bound methods
    loc = frame.f_locals
    if "self" in loc:
        try:
            return loc["self"].__class__.__name__
        except Exception:
            return None
    if "cls" in loc:
        try:
            cls = loc["cls"]
            return cls.__name__ if isinstance(cls, type) else None
        except Exception:
            return None
    return None


def _callsite_info(skip: int = 0) -> Dict[str, Optional[str]]:
    """
    Return information about the caller using inspect.
    skip: additional stack frames to skip beyond this helper and its direct caller.
    """
    frame = _inspect.currentframe()
    try:
        # 0: _callsite_info; 1: build_metadata; 2: augment_sql; 3: caller of augment_sql
        outer = _inspect.getouterframes(frame, context=0)
        idx = min(3 + max(skip, 0), len(outer) - 1)
        record = outer[idx]
        f = record.frame
        mod = _inspect.getmodule(f)
        info = {
            "module": mod.__name__ if mod else f.f_globals.get("__name__", None),
            "function": record.function,
            "class": _class_name_from_frame(f),
            "file": record.filename,
            "line": str(record.lineno),
        }
        return info
    finally:
        # Avoid reference cycles
        del frame
        try:
            del outer  # type: ignore
        except Exception:
            pass


def build_metadata(stack_skip: int = 0) -> Dict[str, str]:
    """
    Build a metadata dictionary capturing framework, environment,
    and call-site information using inspect and standard library tools.
    """
    when = _dt.datetime.now(_dt.timezone.utc).astimezone()
    callsite = _callsite_info(skip=stack_skip)

    meta: Dict[str, str] = {}
    # Time and runtime
    meta["timestamp"] = when.isoformat(timespec="seconds")
    meta["python_version"] = _platform.python_version()
    meta["implementation"] = _platform.python_implementation()
    meta["executable"] = _sys.executable
    meta["platform"] = _platform.platform()
    meta["pid"] = str(_os.getpid())
    meta["ppid"] = str(_os.getppid()) if hasattr(_os, "getppid") else ""
    meta["user"] = _getpass.getuser()
    meta["hostname"] = _socket.gethostname()
    meta["cwd"] = _os.getcwd()
    meta["thread"] = _threading.current_thread().name

    # Virtual env (best-effort)
    venv = _guess_venv()
    if venv:
        meta["virtual_env"] = venv

    # Call-site via inspect
    for k, v in callsite.items():
        if v:
            meta[f"callsite_{k}"] = v

    # Detected frameworks/libraries
    frameworks = _detect_frameworks()
    if frameworks:
        # Flatten as "name=version, name=version, ..."
        meta["frameworks"] = ", ".join(f"{k}={v}" for k, v in sorted(frameworks.items()))

    return meta


def _to_sql_comment_block(meta: Dict[str, str], prefix: str = "-- ") -> str:
    """
    Convert metadata to a SQL comment block (lines starting with -- ).
    Newlines in values are escaped to keep each as a single comment line.
    """
    lines = []
    lines.append(f"{prefix}metadata_begin")
    for k in sorted(meta.keys()):
        val = meta[k]
        if val is None:
            continue
        # Sanitize newlines and trim excessive whitespace
        sval = str(val).replace("\r", "\\r").replace("\n", "\\n").strip()
        lines.append(f"{prefix}{k}: {sval}")
    lines.append(f"{prefix}metadata_end")
    return "\n".join(lines)


def augment_sql(sql: str, stack_skip: int = 0, prefix: str = "-- ") -> str:
    """
    Prepend a SQL comment block containing environment and call-site metadata
    to the provided SQL string.

    stack_skip: increment if augment_sql is wrapped and you want the original caller.
    prefix: SQL comment line prefix (default '-- ').
    """
    if not isinstance(sql, str):
        raise TypeError("sql must be a string")
    meta = build_metadata(stack_skip=stack_skip + 1)  # +1 to account for this function
    header = _to_sql_comment_block(meta, prefix=prefix)
    if sql.startswith(prefix.strip()):
        # Always insert our header at the very top, separated by a blank line
        return f"{header}\n\n{sql}"
    return f"{header}\n{sql}"


# Example usage
if __name__ == "__main__":
    raw_sql = "SELECT NOW();"
    print(augment_sql(raw_sql))

    # Wrapped usage simulating another layer
    def run_query(sql: str) -> None:
        annotated = augment_sql(sql, stack_skip=1)  # adjust to point at the original caller
        print("\n--- Annotated SQL ---\n")
        print(annotated)

    def app_layer():
        query = "SELECT * FROM users WHERE active = TRUE;"
        run_query(query)

    app_layer()
------------------------------------------------------------

PROMPT 171:
"Generate Python code that creates a boilerplate for asyncio applications with a skeleton structure, including a main function, an async function, and a callback function, and imports the necessary libraries, using the asyncio module and the functools module for decoration."
RESPONSE:
import asyncio
import functools
from typing import Any, Callable


def log_calls(func: Callable):
    """Decorator that logs calls and return values for sync and async functions."""
    if asyncio.iscoroutinefunction(func):
        @functools.wraps(func)
        async def async_wrapper(*args: Any, **kwargs: Any):
            print(f"[CALL] {func.__name__} args={args} kwargs={kwargs}")
            result = await func(*args, **kwargs)
            print(f"[RET ] {func.__name__} -> {result!r}")
            return result
        return async_wrapper
    else:
        @functools.wraps(func)
        def sync_wrapper(*args: Any, **kwargs: Any):
            print(f"[CALL] {func.__name__} args={args} kwargs={kwargs}")
            result = func(*args, **kwargs)
            print(f"[RET ] {func.__name__} -> {result!r}")
            return result
        return sync_wrapper


@log_calls
async def do_work(n: int) -> str:
    """Example async worker function. Replace with your async logic."""
    await asyncio.sleep(1)
    return f"Work {n} done"


@log_calls
def on_task_done(task: asyncio.Task) -> None:
    """Example callback function for task completion."""
    try:
        result = task.result()
    except Exception as exc:
        print(f"[CB  ] Task {task.get_name()} raised: {exc!r}")
    else:
        print(f"[CB  ] Task {task.get_name()} result: {result!r}")


async def main() -> None:
    """Application entry point. Create tasks, register callbacks, orchestrate workflow."""
    print("[MAIN] Starting")
    task = asyncio.create_task(do_work(42), name="worker-42")
    task.add_done_callback(on_task_done)

    # Do other async work here while the task runs, if needed.
    # await other_tasks ...

    # Ensure the created task completes before exiting.
    await task
    print("[MAIN] Finished")


if __name__ == "__main__":
    asyncio.run(main())
------------------------------------------------------------

PROMPT 172:
"Generate Python code that implements the Astronomical Data Analysis Software Framework (ASDF) standard, providing a flexible and extensible format for storing and sharing astronomical data, including FITS files, World Coordinate System (WCS) information, and other metadata."
RESPONSE:
Below is a self-contained Python module that uses the official ASDF, Astropy, and ASDF-Astropy libraries to create, read, and convert ASDF files that include astronomical data, FITS content, WCS information, and arbitrary metadata. It does not reimplement the ASDF standard (which is large and already implemented by open-source libraries), but leverages those libraries to produce standards-compliant ASDF files and demonstrates an extensible pattern for working with FITS and WCS.

Save this as astronomy_asdf.py and see the usage examples at the bottom.

Dependencies:
- pip install asdf asdf-astropy astropy gwcs numpy

Code:

```python
#!/usr/bin/env python3
"""
astronomy_asdf.py

High-level utilities for producing and consuming ASDF files that bundle:
- N-dimensional image data
- FITS content (full HDUList or just headers)
- WCS information (astropy.wcs.WCS or gwcs.WCS if available)
- Arbitrary metadata

This uses the official ASDF libraries and their Astropy/GWCS converters to
ensure compliance with the ASDF standard.

Install:
  pip install asdf asdf-astropy astropy gwcs numpy

CLI:
  python astronomy_asdf.py to-asdf input.fits output.asdf --include-wcs --include-fits
  python astronomy_asdf.py from-asdf input.asdf output.fits
  python astronomy_asdf.py show input.asdf
"""

from __future__ import annotations

import argparse
import datetime as _dt
import json
import os
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

try:
    import asdf
    from asdf import AsdfFile
except Exception as exc:  # pragma: no cover
    raise RuntimeError("This module requires the 'asdf' package. Install with 'pip install asdf'") from exc

try:
    import astropy.io.fits as fits
except Exception as exc:  # pragma: no cover
    raise RuntimeError("This module requires 'astropy'. Install with 'pip install astropy'") from exc

# Optional: WCS support via astropy.wcs, serialized if asdf-astropy is installed.
try:
    from astropy.wcs import WCS as AstropyWCS
except Exception:
    AstropyWCS = None  # type: ignore

# Optional: GWCS support. If present, ASDF can serialize gwcs objects with gwcs' own ASDF tags.
try:
    import gwcs as gwcs_mod  # noqa: F401
    GWCS_AVAILABLE = True
except Exception:
    GWCS_AVAILABLE = False


def header_to_cards(header: fits.Header) -> List[Dict[str, Any]]:
    """
    Convert a FITS Header to a list of cards that is safely serializable.
    This preserves order and duplicates, unlike a naive dict.

    Returns list of dicts [{'key':..., 'value':..., 'comment':...}, ...]
    """
    cards = []
    for k, v, c in header.cards:
        # Ensure JSON serializable values (astropy cards often are already)
        try:
            _ = json.dumps(v)
            value = v
        except TypeError:
            value = str(v)
        cards.append({"key": k, "value": value, "comment": c if c is not None else ""})
    return cards


def cards_to_header(cards: List[Dict[str, Any]]) -> fits.Header:
    """
    Reconstruct a FITS Header from header_to_cards output.
    """
    h = fits.Header()
    for card in cards:
        k = card.get("key")
        v = card.get("value")
        c = card.get("comment", "")
        # FITS expects '' rather than None for blank comments
        if c is None:
            c = ""
        # Some reserved keywords need special handling; let astropy handle generally:
        try:
            h.append((k, v, c), end=True)
        except Exception:
            # Fallback: coerce to string if astropy complains about type
            h.append((k, str(v), c), end=True)
    return h


def extract_primary_data_and_header(hdul: fits.HDUList) -> Tuple[Optional[np.ndarray], Optional[fits.Header]]:
    """
    Extract primary image data and header if present.
    """
    primary = hdul[0] if len(hdul) > 0 else None
    if primary is None:
        return None, None
    data = primary.data if hasattr(primary, "data") else None
    header = primary.header if hasattr(primary, "header") else None
    return data, header


def derive_wcs_from_header(header: Optional[fits.Header]) -> Optional[Any]:
    """
    Try to derive an astropy.wcs.WCS from a FITS header.
    Returns None if not possible or astropy.wcs is unavailable.
    """
    if header is None or AstropyWCS is None:
        return None
    try:
        w = AstropyWCS(header)
        return w
    except Exception:
        return None


def pick_basic_metadata(header: Optional[fits.Header]) -> Dict[str, Any]:
    """
    Extract a few commonly used metadata fields from a FITS header.
    """
    if header is None:
        return {}
    keys = [
        "OBJECT",
        "OBSERVER",
        "DATE-OBS",
        "TELESCOP",
        "INSTRUME",
        "FILTER",
        "EXPTIME",
        "BUNIT",
    ]
    meta = {}
    for k in keys:
        if k in header:
            val = header[k]
            try:
                json.dumps(val)
                meta[k] = val
            except TypeError:
                meta[k] = str(val)
    return meta


def build_asdf_tree(
    hdul: fits.HDUList,
    include_wcs: bool = True,
    include_fits: bool = True,
    extra_metadata: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Construct the ASDF tree (a nested dict) to be placed into an AsdfFile.

    - Stores primary data array (if present) under 'data'
    - Stores FITS header cards under 'fits_header_cards'
    - Stores optional WCS under 'wcs' (if available and serializable)
    - Optionally stores the entire HDUList under 'fits_hdulist' (requires asdf-astropy)
    - Stores lightweight metadata under 'meta'
    """
    data, header = extract_primary_data_and_header(hdul)
    fits_cards = header_to_cards(header) if header is not None else []

    tree: Dict[str, Any] = {
        "asdf_standard_version": str(asdf.versioning.default_version),
        "created": _dt.datetime.now(tz=_dt.timezone.utc).isoformat(),
        "meta": {},
        "data": data if data is not None else None,
        "fits_header_cards": fits_cards,
    }

    # Merge light-weight metadata
    base_meta = pick_basic_metadata(header)
    if extra_metadata:
        base_meta.update(extra_metadata)
    tree["meta"] = base_meta

    # WCS (astropy.wcs or gwcs) if requested and available
    if include_wcs:
        wcs_obj = derive_wcs_from_header(header)
        if wcs_obj is not None:
            # If asdf-astropy is installed, this will be covered by its converters.
            tree["wcs"] = wcs_obj

    # Optionally include full FITS content (requires asdf-astropy to round-trip HDUList)
    if include_fits:
        try:
            tree["fits_hdulist"] = hdul
        except Exception:
            # In rare cases if serialization of the full HDUList is not supported
            # by the installed converters, skip it quietly.
            pass

    return tree


def write_asdf(
    tree: Dict[str, Any],
    output_path: str,
    array_compression: str = "zlib",
    array_storage: str = "internal",
    real_inline_threshold: Optional[int] = None,
) -> None:
    """
    Write an ASDF file to disk.

    array_compression: 'zlib' (default), 'bzp2', 'lz4' (lz4 requires plugin)
    array_storage: 'internal' or 'external'
    real_inline_threshold: if provided, arrays smaller than this (bytes) may be inlined
                           (None means library default)
    """
    af = AsdfFile(tree)

    # Configure array storage policy; "external" will store arrays in external blocks.
    kwargs = {
        "all_array_storage": array_storage,
        "all_array_compression": array_compression,
    }
    if real_inline_threshold is not None:
        kwargs["inline_threshold"] = real_inline_threshold

    af.write_to(output_path, **kwargs)


def convert_fits_to_asdf(
    fits_path: str,
    asdf_path: str,
    include_wcs: bool = True,
    include_fits: bool = True,
    array_compression: str = "zlib",
    array_storage: str = "internal",
    extra_metadata: Optional[Dict[str, Any]] = None,
) -> None:
    """
    Convert a FITS file to ASDF, embedding primary image data, header, WCS, and optional entire HDUList.
    """
    with fits.open(fits_path, memmap=False) as hdul:
        tree = build_asdf_tree(
            hdul,
            include_wcs=include_wcs,
            include_fits=include_fits,
            extra_metadata=extra_metadata,
        )
    write_asdf(
        tree,
        asdf_path,
        array_compression=array_compression,
        array_storage=array_storage,
    )


def open_asdf_tree(asdf_path: str) -> Dict[str, Any]:
    """
    Open an ASDF file and return the raw tree dictionary.
    """
    with asdf.open(asdf_path, copy_arrays=False) as af:
        # Note: af.tree is a dict-like that may include tagged objects
        # (e.g., WCS) that ASDF-Astropy/GWCS converts into Python objects.
        return dict(af.tree)


def asdf_to_fits(
    asdf_path: str,
    output_fits_path: str,
    prefer_embedded_hdulist: bool = True,
    overwrite: bool = True,
) -> None:
    """
    Convert an ASDF file back to a FITS file.

    If prefer_embedded_hdulist is True, and the ASDF file contains a full 'fits_hdulist',
    we write that back. Otherwise, we try to rebuild FITS from 'data' and 'fits_header_cards'.
    """
    with asdf.open(asdf_path, copy_arrays=False) as af:
        tree = af.tree

        if prefer_embedded_hdulist and "fits_hdulist" in tree and tree["fits_hdulist"] is not None:
            hdul = tree["fits_hdulist"]
            # Ensure it's a real HDUList
            if isinstance(hdul, fits.HDUList):
                hdul.writeto(output_fits_path, overwrite=overwrite)
                return

        # Rebuild from data and header cards
        data = tree.get("data", None)
        cards = tree.get("fits_header_cards", [])
        header = cards_to_header(cards) if cards else fits.Header()

        # If no header is present but WCS exists, attempt to insert WCS into FITS header
        # (Only if astropy.wcs is available and WCS in tree looks like astropy.wcs.WCS)
        wcs_obj = tree.get("wcs", None)
        if AstropyWCS is not None and isinstance(wcs_obj, AstropyWCS):
            try:
                wcs_header = wcs_obj.to_header(relax=True)
                # Merge WCS header while preserving existing header cards
                for k, v in wcs_header.items():
                    header[k] = v
            except Exception:
                pass

        if data is None:
            # Create a minimal empty primary HDU
            phdu = fits.PrimaryHDU(header=header)
            fits.HDUList([phdu]).writeto(output_fits_path, overwrite=overwrite)
            return

        phdu = fits.PrimaryHDU(data=data, header=header)
        hdul = fits.HDUList([phdu])
        hdul.writeto(output_fits_path, overwrite=overwrite)


def update_asdf_metadata(
    asdf_path: str,
    out_path: Optional[str],
    metadata_updates: Dict[str, Any],
) -> str:
    """
    Update the 'meta' section of an ASDF file and write the result.

    Returns the path to the updated file (out_path if provided, else overwrites input).
    """
    target_path = out_path or asdf_path
    with asdf.open(asdf_path, mode="rw" if out_path is None else "r") as af:
        tree = af.tree
        meta = dict(tree.get("meta", {}))
        meta.update(metadata_updates)
        tree["meta"] = meta
        if out_path is None:
            af.update()
        else:
            AsdfFile(tree).write_to(out_path)
    return target_path


def summarize_asdf(asdf_path: str) -> str:
    """
    Return a human-readable summary of the ASDF file contents.
    """
    with asdf.open(asdf_path, copy_arrays=False) as af:
        tree = af.tree
        lines = []
        lines.append(f"File: {asdf_path}")
        lines.append(f"ASDF Standard version: {tree.get('asdf_standard_version', 'unknown')}")
        lines.append(f"Created: {tree.get('created', 'unknown')}")
        meta = tree.get("meta", {})
        lines.append("Meta keys: " + ", ".join(sorted(meta.keys())) if isinstance(meta, dict) else "Meta: <non-dict>")

        data = tree.get("data", None)
        if isinstance(data, np.ndarray):
            lines.append(f"Data: ndarray shape={data.shape} dtype={data.dtype}")
        else:
            lines.append("Data: None or non-ndarray")

        cards = tree.get("fits_header_cards", [])
        lines.append(f"FITS header cards: {len(cards)}")

        wcs_obj = tree.get("wcs", None)
        if wcs_obj is None:
            lines.append("WCS: None")
        else:
            lines.append(f"WCS: {type(wcs_obj).__name__}")

        has_hdul = "fits_hdulist" in tree and tree["fits_hdulist"] is not None
        lines.append(f"Embedded FITS HDUList: {has_hdul}")

        return "\n".join(lines)


def _parse_kv_list(pairs: List[str]) -> Dict[str, Any]:
    """
    Parse CLI key=value pairs into a dict.
    """
    out: Dict[str, Any] = {}
    for p in pairs:
        if "=" not in p:
            raise ValueError(f"Invalid --meta '{p}'; expected key=value")
        k, v = p.split("=", 1)
        # Try to parse JSON values; fallback to string
        try:
            out[k] = json.loads(v)
        except json.JSONDecodeError:
            out[k] = v
    return out


def main():
    parser = argparse.ArgumentParser(description="ASDF toolkit for astronomical data (FITS, WCS, metadata).")
    sub = parser.add_subparsers(dest="cmd", required=True)

    p_to = sub.add_parser("to-asdf", help="Convert FITS -> ASDF")
    p_to.add_argument("input_fits", help="Input FITS file")
    p_to.add_argument("output_asdf", help="Output ASDF file")
    p_to.add_argument("--include-wcs", action="store_true", default=False, help="Embed WCS if available")
    p_to.add_argument("--include-fits", action="store_true", default=False, help="Embed full FITS HDUList (requires asdf-astropy)")
    p_to.add_argument("--array-storage", choices=["internal", "external"], default="internal", help="Where to store arrays")
    p_to.add_argument("--array-compression", default="zlib", help="Array compression (e.g., zlib)")
    p_to.add_argument("--meta", nargs="*", default=[], help="Extra metadata key=value pairs; values can be JSON")

    p_from = sub.add_parser("from-asdf", help="Convert ASDF -> FITS")
    p_from.add_argument("input_asdf", help="Input ASDF file")
    p_from.add_argument("output_fits", help="Output FITS file")
    p_from.add_argument("--prefer-embedded-hdulist", action="store_true", default=False, help="Use embedded FITS if present")

    p_show = sub.add_parser("show", help="Summarize an ASDF file")
    p_show.add_argument("input_asdf", help="ASDF file to inspect")

    p_upd = sub.add_parser("update-meta", help="Update metadata inside an ASDF file")
    p_upd.add_argument("input_asdf", help="Input ASDF file")
    p_upd.add_argument("--out", default=None, help="Output ASDF file (default: in-place update)")
    p_upd.add_argument("--meta", nargs="+", default=[], help="Metadata updates key=value; values can be JSON")

    args = parser.parse_args()

    if args.cmd == "to-asdf":
        extra_meta = _parse_kv_list(args.meta) if args.meta else {}
        convert_fits_to_asdf(
            args.input_fits,
            args.output_asdf,
            include_wcs=args.include_wcs,
            include_fits=args.include_fits,
            array_compression=args.array_compression,
            array_storage=args.array_storage,
            extra_metadata=extra_meta,
        )
        print(f"Wrote ASDF: {args.output_asdf}")

    elif args.cmd == "from-asdf":
        asdf_to_fits(
            args.input_asdf,
            args.output_fits,
            prefer_embedded_hdulist=args.prefer_embedded_hdulist,
            overwrite=True,
        )
        print(f"Wrote FITS: {args.output_fits}")

    elif args.cmd == "show":
        print(summarize_asdf(args.input_asdf))

    elif args.cmd == "update-meta":
        updates = _parse_kv_list(args.meta) if args.meta else {}
        out = update_asdf_metadata(args.input_asdf, args.out, updates)
        print(f"Updated ASDF: {out}")

    else:  # pragma: no cover
        parser.print_help()


if __name__ == "__main__":
    main()
```

Notes:
- This code relies on the official asdf and asdf-astropy converters to serialize/deserialize Astropy and (optionally) GWCS objects in a standards-compliant way. It does not reinvent the ASDF standard; it uses the battle-tested libraries that implement it.
- WCS serialization: If asdf-astropy is installed, an astropy.wcs.WCS object attached to the tree under "wcs" will be serialized with ASDF tags. If gwcs is installed and you attach a gwcs.WCS object instead, its ASDF tags will be used. The example derives WCS from FITS headers via astropy.wcs where possible.
- FITS support: The full HDUList can be embedded via asdf-astropy (toggle with --include-fits). Even without embedding the full FITS, the primary image data and header cards are stored so you can reconstruct a FITS file.
- Arrays can be stored internally (default) or externally using ASDF’s external array storage. Compression is configurable (zlib by default).
- The module includes a CLI to convert FITS<->ASDF, show summaries, and update metadata in-place.
------------------------------------------------------------

PROMPT 173:
"Generate Python code that imports the `dataclasses` and `json` modules, defines a dataclass with a single field, serializes it to a JSON payload using the `json.dumps()` function, and then deserializes it back into an instance of the dataclass using the `json.loads()` function."
RESPONSE:
import dataclasses
import json

@dataclasses.dataclass
class Item:
    value: int

# Create an instance
item = Item(value=123)

# Serialize to JSON
payload = json.dumps(dataclasses.asdict(item))
print("JSON payload:", payload)

# Deserialize back into the dataclass
data = json.loads(payload)
item2 = Item(**data)
print("Deserialized:", item2)
------------------------------------------------------------

PROMPT 174:
"Generate Python code that imports the required modules and sets up a client object for the Textract service, like this:\n```\nfrom botocore.client import Client\nfrom typing import Callable\nfrom boto3.textract import Textract\n\ntextract_client = Client(service=Textract())\n```"
RESPONSE:
```python
import boto3

# Create a Textract client
textract_client = boto3.client("textract")
```
------------------------------------------------------------

PROMPT 175:
"Generate Python code that creates a script called `backup_restore.py` that takes in a database name, backup folder, and media folder as arguments, connects to the database, backups the database files, saves them to the backup folder, and restores the database from the backup files if necessary."
RESPONSE:
#!/usr/bin/env python3
"""
backup_restore.py

A simple SQLite-focused backup/restore utility.

It:
- Connects to an SQLite database
- Creates consistent database backups using the SQLite online backup API
- Backs up an associated "media" folder
- Saves backups into a structured backup directory
- Restores the database (and optionally media) from the latest or specified backup
- Can automatically restore if an integrity check detects corruption

Usage examples:
  Backup:
    python backup_restore.py /path/to/db.sqlite3 /path/to/backups /path/to/media --backup

  Restore from latest backup:
    python backup_restore.py /path/to/db.sqlite3 /path/to/backups /path/to/media --restore

  Auto-restore only if corrupt:
    python backup_restore.py /path/to/db.sqlite3 /path/to/backups /path/to/media --auto-restore-if-corrupt

  Restore from specific backup directory or backup file:
    python backup_restore.py /path/to/db.sqlite3 /path/to/backups /path/to/media --restore --from /path/to/backups/db/2024-10-01_120000

Notes:
- This script assumes an SQLite database file path for the "database" argument.
- Backups are stored under: <backup_folder>/<db_base_name>/<timestamp>/
- Media is archived as media.tar.gz within each backup directory.
"""

import argparse
import datetime as dt
import json
import os
import shutil
import sqlite3
import sys
import tarfile
from pathlib import Path
from typing import Optional, Tuple, List


def log(msg: str, quiet: bool = False):
    if not quiet:
        print(msg)


def timestamp() -> str:
    # Safe for file names and chronological sorting
    return dt.datetime.now().strftime("%Y-%m-%d_%H%M%S")


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def sqlite_integrity_check(db_path: Path, timeout: float = 5.0) -> Tuple[bool, str]:
    try:
        with sqlite3.connect(str(db_path), timeout=timeout) as conn:
            cur = conn.execute("PRAGMA integrity_check;")
            result = cur.fetchone()
            if not result:
                return False, "No result from PRAGMA integrity_check"
            status = result[0]
            if status.strip().lower() == "ok":
                return True, "ok"
            return False, status
    except Exception as e:
        return False, f"Exception during integrity_check: {e}"


def sqlite_online_backup(src_db: Path, dest_db: Path, timeout: float = 5.0) -> None:
    with sqlite3.connect(str(src_db), timeout=timeout) as src_conn:
        ensure_dir(dest_db.parent)
        # Create dest DB; backup API creates/overwrites content of the destination connection
        with sqlite3.connect(str(dest_db)) as dest_conn:
            src_conn.backup(dest_conn)


def copy_if_exists(src: Path, dest_dir: Path) -> Optional[Path]:
    if src.exists():
        ensure_dir(dest_dir)
        dest = dest_dir / src.name
        shutil.copy2(src, dest)
        return dest
    return None


def archive_media(media_dir: Path, dest_tar_gz: Path) -> Optional[Path]:
    if not media_dir.exists():
        return None
    ensure_dir(dest_tar_gz.parent)
    with tarfile.open(dest_tar_gz, "w:gz") as tar:
        # Add the media directory contents under "media/" root in archive
        tar.add(str(media_dir), arcname="media")
    return dest_tar_gz


def extract_media(archive_path: Path, target_dir: Path) -> None:
    ensure_dir(target_dir)
    with tarfile.open(archive_path, "r:gz") as tar:
        # Extract under target_dir; archive has "media/..." inside
        members = tar.getmembers()
        # Avoid path traversal
        for m in members:
            if not Path(m.name).as_posix().startswith("media"):
                raise ValueError(f"Archive contains unexpected path: {m.name}")
        tar.extractall(path=target_dir.parent)
        # Move extracted "media" content into target_dir
        extracted_root = target_dir.parent / "media"
        if extracted_root.exists():
            # Merge move: copy contents into target_dir
            for item in extracted_root.iterdir():
                dest = target_dir / item.name
                if item.is_dir():
                    if dest.exists():
                        # Merge directories
                        for child in item.rglob("*"):
                            rel = child.relative_to(item)
                            child_dest = dest / rel
                            if child.is_dir():
                                child_dest.mkdir(parents=True, exist_ok=True)
                            else:
                                ensure_dir(child_dest.parent)
                                shutil.copy2(child, child_dest)
                    else:
                        shutil.move(str(item), str(dest))
                else:
                    shutil.move(str(item), str(dest))
            # Remove the temporary extracted root
            shutil.rmtree(extracted_root, ignore_errors=True)


def write_manifest(manifest_path: Path, data: dict) -> None:
    ensure_dir(manifest_path.parent)
    with manifest_path.open("w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, sort_keys=True)


def read_manifest(manifest_path: Path) -> Optional[dict]:
    if not manifest_path.exists():
        return None
    try:
        with manifest_path.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


def backup_sqlite_and_media(
    db_path: Path,
    backup_root: Path,
    media_dir: Path,
    include_media: bool,
    label: Optional[str],
    quiet: bool,
    retention: int,
) -> Path:
    if not db_path.exists():
        raise FileNotFoundError(f"Database file not found: {db_path}")

    db_base = db_path.stem  # base name without suffix
    db_dir = backup_root / db_base
    ensure_dir(db_dir)

    ts = timestamp()
    dirname = ts if not label else f"{ts}_{sanitize_label(label)}"
    backup_dir = db_dir / dirname
    ensure_dir(backup_dir)

    log(f"Creating backup at: {backup_dir}", quiet)

    # 1) Database backup (consistent snapshot)
    dest_sqlite = backup_dir / "database.sqlite"
    sqlite_online_backup(db_path, dest_sqlite)

    # 2) Copy SQLite -wal and -shm if present (optional; not strictly needed with online backup)
    wal_src = Path(str(db_path) + "-wal")
    shm_src = Path(str(db_path) + "-shm")
    copied_wal = copy_if_exists(wal_src, backup_dir)
    copied_shm = copy_if_exists(shm_src, backup_dir)

    # 3) Media archive
    media_archive = None
    if include_media:
        media_archive = backup_dir / "media.tar.gz"
        media_archive = archive_media(media_dir, media_archive)

    # 4) Manifest
    ok, integrity_msg = sqlite_integrity_check(dest_sqlite)
    manifest = {
        "created_at": dt.datetime.now().isoformat(timespec="seconds"),
        "source_db_path": str(db_path.resolve()),
        "db_base_name": db_base,
        "files": {
            "database_sqlite": "database.sqlite",
            "wal": copied_wal.name if copied_wal else None,
            "shm": copied_shm.name if copied_shm else None,
            "media_tar_gz": media_archive.name if media_archive else None,
        },
        "integrity_check": {
            "ok": ok,
            "message": integrity_msg,
        },
        "tool": "backup_restore.py",
        "version": "1.0",
    }
    write_manifest(backup_dir / "manifest.json", manifest)
    log(f"Backup completed. Integrity: {'OK' if ok else 'FAILED'} - {integrity_msg}", quiet)

    # 5) Retention policy
    apply_retention(db_dir, retention, quiet)

    return backup_dir


def sanitize_label(label: str) -> str:
    # Allow only safe filename characters
    allowed = "-_.@"
    return "".join(c for c in label if c.isalnum() or c in allowed)[:40]


def apply_retention(db_base_dir: Path, retention: int, quiet: bool) -> None:
    if retention <= 0:
        return
    if not db_base_dir.exists():
        return
    # Get subdirectories sorted by name (timestamp-based), oldest first
    dirs = sorted([d for d in db_base_dir.iterdir() if d.is_dir()], key=lambda p: p.name)
    if len(dirs) <= retention:
        return
    to_delete = dirs[: len(dirs) - retention]
    for d in to_delete:
        try:
            shutil.rmtree(d)
            log(f"Retention: removed old backup {d}", quiet)
        except Exception as e:
            log(f"Retention: failed to remove {d}: {e}", quiet)


def find_latest_backup_dir(backup_root: Path, db_base: str) -> Optional[Path]:
    db_dir = backup_root / db_base
    if not db_dir.exists():
        return None
    dirs = sorted([d for d in db_dir.iterdir() if d.is_dir()], key=lambda p: p.name)
    return dirs[-1] if dirs else None


def resolve_backup_source_dir(
    from_arg: Optional[Path],
    backup_root: Path,
    db_base: str,
) -> Optional[Path]:
    if from_arg:
        if from_arg.is_dir():
            return from_arg
        if from_arg.is_file():
            return from_arg.parent
    return find_latest_backup_dir(backup_root, db_base)


def restore_sqlite_and_media(
    db_path: Path,
    backup_root: Path,
    media_dir: Path,
    include_media: bool,
    from_arg: Optional[Path],
    quiet: bool,
) -> Path:
    db_base = db_path.stem
    src_dir = resolve_backup_source_dir(from_arg, backup_root, db_base)
    if not src_dir or not src_dir.exists():
        raise FileNotFoundError("No suitable backup directory found to restore from.")

    manifest = read_manifest(src_dir / "manifest.json")
    sqlite_src = src_dir / "database.sqlite"
    if not sqlite_src.exists():
        # Fallback: pick the first .sqlite-like file in the directory
        candidates = [p for p in src_dir.iterdir() if p.is_file() and p.suffix.lower().startswith(".sqlite")]
        if not candidates:
            raise FileNotFoundError(f"No database.sqlite found in backup directory: {src_dir}")
        sqlite_src = sorted(candidates)[0]

    log(f"Restoring database from: {sqlite_src}", quiet)

    # Safety: backup current DB
    if db_path.exists():
        safety_backup = db_path.with_suffix(db_path.suffix + f".pre-restore.{timestamp()}.bak")
        shutil.copy2(db_path, safety_backup)
        log(f"Safety copy of current DB created at: {safety_backup}", quiet)

    # Atomic replace destination DB
    tmp_dest = db_path.with_suffix(db_path.suffix + ".restoring")
    ensure_dir(db_path.parent)
    shutil.copy2(sqlite_src, tmp_dest)
    os.replace(tmp_dest, db_path)
    log(f"Database restored to: {db_path}", quiet)

    # Restore media if requested and available
    if include_media:
        media_tar = src_dir / "media.tar.gz"
        if media_tar.exists():
            log(f"Restoring media from: {media_tar}", quiet)
            extract_media(media_tar, media_dir)
        else:
            log("No media archive found in backup; skipping media restore.", quiet)

    # Post-restore integrity check
    ok, msg = sqlite_integrity_check(db_path)
    log(f"Post-restore integrity: {'OK' if ok else 'FAILED'} - {msg}", quiet)
    if not ok:
        raise RuntimeError(f"Integrity check failed after restore: {msg}")

    return src_dir


def auto_restore_if_corrupt(
    db_path: Path,
    backup_root: Path,
    media_dir: Path,
    include_media: bool,
    quiet: bool,
) -> Optional[Path]:
    ok, msg = sqlite_integrity_check(db_path)
    if ok:
        log(f"Integrity OK: {msg}. No restore needed.", quiet)
        return None
    log(f"Integrity check FAILED: {msg}. Attempting restore from latest backup...", quiet)
    return restore_sqlite_and_media(
        db_path=db_path,
        backup_root=backup_root,
        media_dir=media_dir,
        include_media=include_media,
        from_arg=None,
        quiet=quiet,
    )


def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description="SQLite database and media backup/restore utility")
    p.add_argument("database", help="Path to SQLite database file (e.g., /path/to/db.sqlite3)")
    p.add_argument("backup_folder", help="Path to folder where backups will be stored")
    p.add_argument("media_folder", help="Path to media directory to back up/restore")
    group = p.add_mutually_exclusive_group()
    group.add_argument("--backup", action="store_true", help="Perform a backup")
    group.add_argument("--restore", action="store_true", help="Restore from a backup")
    group.add_argument("--auto-restore-if-corrupt", dest="auto_restore", action="store_true",
                       help="Run integrity check and restore automatically if corrupt")

    p.add_argument("--from", dest="from_backup", default=None,
                   help="Path to a specific backup directory or database.sqlite file to restore from")
    p.add_argument("--no-media", dest="include_media", action="store_false", default=True,
                   help="Skip media backup/restore")
    p.add_argument("--label", default=None, help="Optional label appended to backup directory name")
    p.add_argument("--retention", type=int, default=10, help="Number of backups to keep per database (default: 10)")
    p.add_argument("--quiet", action="store_true", help="Reduce output verbosity")

    return p.parse_args(argv)


def main(argv: Optional[List[str]] = None) -> int:
    args = parse_args(argv)

    db_path = Path(args.database).expanduser().resolve()
    backup_root = Path(args.backup_folder).expanduser().resolve()
    media_dir = Path(args.media_folder).expanduser().resolve()
    from_arg = Path(args.from_backup).expanduser().resolve() if args.from_backup else None

    try:
        if args.restore:
            restore_sqlite_and_media(
                db_path=db_path,
                backup_root=backup_root,
                media_dir=media_dir,
                include_media=args.include_media,
                from_arg=from_arg,
                quiet=args.quiet,
            )
        elif args.auto_restore:
            restored_from = auto_restore_if_corrupt(
                db_path=db_path,
                backup_root=backup_root,
                media_dir=media_dir,
                include_media=args.include_media,
                quiet=args.quiet,
            )
            if not restored_from:
                # If no restore was needed, perform a backup for good measure
                backup_sqlite_and_media(
                    db_path=db_path,
                    backup_root=backup_root,
                    media_dir=media_dir,
                    include_media=args.include_media,
                    label=args.label,
                    quiet=args.quiet,
                    retention=args.retention,
                )
        else:
            # Default action: backup
            backup_sqlite_and_media(
                db_path=db_path,
                backup_root=backup_root,
                media_dir=media_dir,
                include_media=args.include_media,
                label=args.label,
                quiet=args.quiet,
                retention=args.retention,
            )

        return 0
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    sys.exit(main())
------------------------------------------------------------

PROMPT 176:
"Generate Python code that uses the requests library to perform Single-Sign On HTTP Negotiate authentication on Windows:\n```\nimport requests\n\nauth = requests.auth.HTTPNegotiateAuth(('username', 'password'))\nresponse = requests.get('https://example.com', auth=auth)\nprint(response.text)\n```"
RESPONSE:
The requests library doesn’t include HTTP Negotiate auth by itself. On Windows, use the requests-negotiate-sspi plugin, which performs SPNEGO/SSPI SSO with your current logon session.

Install:
pip install requests requests-negotiate-sspi

Single Sign-On (no explicit credentials):
import requests
from requests_negotiate_sspi import HttpNegotiateAuth

url = "https://example.com"

with requests.Session() as s:
    r = s.get(url, auth=HttpNegotiateAuth())
    r.raise_for_status()
    print(r.text)

Optional: specify explicit DOMAIN\username and password (if you can’t use SSO):
import requests
from requests_negotiate_sspi import HttpNegotiateAuth

url = "https://example.com"
auth = HttpNegotiateAuth(username="DOMAIN\\username", password="password")

with requests.Session() as s:
    r = s.get(url, auth=auth)
    r.raise_for_status()
    print(r.text)
------------------------------------------------------------

PROMPT 177:
"Generate Python code that imports the necessary libraries for data manipulation and visualization, such as Pandas and Matplotlib, and provides functions for cleaning, transforming, and visualizing data, as well as loading and saving data in various formats, including CSV, Excel, and SQL databases."
RESPONSE:
# data_utils.py

from __future__ import annotations

import logging
from typing import Any, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple, Union

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sqlalchemy import create_engine
from sqlalchemy.engine import Engine


# Configure basic logging and seaborn aesthetics
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
sns.set_theme(style="whitegrid")


# -----------------------------
# I/O: Loaders and Savers
# -----------------------------

def load_csv(
    path: str,
    dtype: Optional[Mapping[str, Any]] = None,
    parse_dates: Optional[Union[List[str], List[int]]] = None,
    low_memory: bool = True,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    Load a CSV file into a DataFrame.
    """
    logging.info(f"Loading CSV from {path}")
    return pd.read_csv(path, dtype=dtype, parse_dates=parse_dates, low_memory=low_memory, **kwargs)


def save_csv(
    df: pd.DataFrame,
    path: str,
    index: bool = False,
    **kwargs: Any,
) -> None:
    """
    Save a DataFrame to a CSV file.
    """
    logging.info(f"Saving CSV to {path}")
    df.to_csv(path, index=index, **kwargs)


def load_excel(
    path: str,
    sheet_name: Union[int, str] = 0,
    dtype: Optional[Mapping[str, Any]] = None,
    parse_dates: Optional[Union[List[str], List[int]]] = None,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    Load an Excel sheet into a DataFrame.
    """
    logging.info(f"Loading Excel from {path}, sheet={sheet_name}")
    return pd.read_excel(path, sheet_name=sheet_name, dtype=dtype, parse_dates=parse_dates, **kwargs)


def save_excel(
    df: pd.DataFrame,
    path: str,
    sheet_name: str = "Sheet1",
    index: bool = False,
    engine: Optional[str] = None,
    **kwargs: Any,
) -> None:
    """
    Save a DataFrame to an Excel file.
    """
    logging.info(f"Saving Excel to {path}, sheet={sheet_name}")
    with pd.ExcelWriter(path, engine=engine) as writer:
        df.to_excel(writer, index=index, sheet_name=sheet_name, **kwargs)


def make_engine(conn_str: str, echo: bool = False) -> Engine:
    """
    Create a SQLAlchemy Engine from a connection string.
    Examples of conn_str:
      - sqlite:///mydb.sqlite
      - postgresql+psycopg2://user:pass@host:5432/dbname
      - mysql+pymysql://user:pass@host:3306/dbname
    """
    logging.info("Creating SQLAlchemy engine")
    return create_engine(conn_str, echo=echo, future=True)


def load_sql_query(
    query: str,
    conn_str: Optional[str] = None,
    engine: Optional[Engine] = None,
    params: Optional[Mapping[str, Any]] = None,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    Load data from an arbitrary SQL query.
    Provide either conn_str or engine.
    """
    if (conn_str is None) == (engine is None):
        raise ValueError("Provide exactly one of conn_str or engine.")
    eng = engine or make_engine(conn_str)  # type: ignore[arg-type]
    logging.info("Executing SQL query")
    with eng.connect() as conn:
        return pd.read_sql_query(sql=query, con=conn, params=params, **kwargs)


def load_sql_table(
    table_name: str,
    schema: Optional[str] = None,
    conn_str: Optional[str] = None,
    engine: Optional[Engine] = None,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    Load an entire table using pandas.read_sql_table.
    Provide either conn_str or engine.
    """
    if (conn_str is None) == (engine is None):
        raise ValueError("Provide exactly one of conn_str or engine.")
    eng = engine or make_engine(conn_str)  # type: ignore[arg-type]
    logging.info(f"Reading SQL table {schema + '.' if schema else ''}{table_name}")
    with eng.connect() as conn:
        return pd.read_sql_table(table_name=table_name, con=conn, schema=schema, **kwargs)


def save_sql_table(
    df: pd.DataFrame,
    table_name: str,
    conn_str: Optional[str] = None,
    engine: Optional[Engine] = None,
    schema: Optional[str] = None,
    if_exists: str = "replace",
    index: bool = False,
    dtype: Optional[Mapping[str, Any]] = None,
    chunksize: Optional[int] = 1000,
    method: Optional[Union[str, Any]] = None,
) -> None:
    """
    Write a DataFrame to a SQL table via DataFrame.to_sql.
    Provide either conn_str or engine.
    """
    if (conn_str is None) == (engine is None):
        raise ValueError("Provide exactly one of conn_str or engine.")
    eng = engine or make_engine(conn_str)  # type: ignore[arg-type]
    full_name = f"{schema+'.' if schema else ''}{table_name}"
    logging.info(f"Writing DataFrame to SQL table {full_name} (if_exists={if_exists})")
    with eng.begin() as conn:  # transactional write
        df.to_sql(
            name=table_name,
            con=conn,
            schema=schema,
            if_exists=if_exists,
            index=index,
            dtype=dtype,
            chunksize=chunksize,
            method=method,
        )


# -----------------------------
# Cleaning Utilities
# -----------------------------

def standardize_column_names(
    df: pd.DataFrame,
    case: str = "lower",
    strip: bool = True,
    replace_spaces: str = "_",
    remove_non_alnum: bool = False,
    inplace: bool = False,
) -> pd.DataFrame:
    """
    Standardize column names (case, whitespace, optional non-alphanumeric removal).
    """
    def transform(col: str) -> str:
        s = str(col)
        if strip:
            s = s.strip()
        if case == "lower":
            s = s.lower()
        elif case == "upper":
            s = s.upper()
        if replace_spaces is not None:
            s = s.replace(" ", replace_spaces)
        if remove_non_alnum:
            s = "".join(ch for ch in s if ch.isalnum() or ch == "_")
        return s

    new_cols = [transform(c) for c in df.columns]
    if inplace:
        df.columns = new_cols
        return df
    out = df.copy()
    out.columns = new_cols
    return out


def remove_duplicates(
    df: pd.DataFrame,
    subset: Optional[Union[str, List[str]]] = None,
    keep: str = "first",
    inplace: bool = False,
) -> pd.DataFrame:
    """
    Remove duplicate rows.
    """
    if inplace:
        df.drop_duplicates(subset=subset, keep=keep, inplace=True)
        return df
    return df.drop_duplicates(subset=subset, keep=keep)


def trim_whitespace(
    df: pd.DataFrame,
    columns: Optional[Sequence[str]] = None,
    inplace: bool = False,
) -> pd.DataFrame:
    """
    Strip leading/trailing whitespace from object/string columns.
    """
    cols = list(columns) if columns is not None else [c for c in df.columns if pd.api.types.is_string_dtype(df[c])]
    def _trim(s: pd.Series) -> pd.Series:
        return s.astype("string").str.strip()
    if inplace:
        for c in cols:
            df[c] = _trim(df[c])
        return df
    out = df.copy()
    for c in cols:
        out[c] = _trim(out[c])
    return out


def impute_missing(
    df: pd.DataFrame,
    strategy: str = "mean",
    columns: Optional[Sequence[str]] = None,
    value: Optional[Any] = None,
    inplace: bool = False,
) -> pd.DataFrame:
    """
    Impute missing values in specified columns.
    strategy: 'mean', 'median', 'mode', or 'constant'
    """
    if columns is None:
        columns = df.columns

    out = df if inplace else df.copy()

    for col in columns:
        if strategy == "constant":
            if value is None:
                raise ValueError("Provide 'value' when strategy='constant'.")
            fill_val = value
        elif strategy == "mean":
            fill_val = out[col].mean(numeric_only=True)
        elif strategy == "median":
            fill_val = out[col].median(numeric_only=True)
        elif strategy == "mode":
            mode_series = out[col].mode(dropna=True)
            fill_val = mode_series.iloc[0] if not mode_series.empty else np.nan
        else:
            raise ValueError("strategy must be one of {'mean','median','mode','constant'}")

        out[col] = out[col].fillna(fill_val)

    return out


def drop_missing(
    df: pd.DataFrame,
    how: str = "any",
    subset: Optional[Sequence[str]] = None,
    thresh: Optional[int] = None,
    inplace: bool = False,
) -> pd.DataFrame:
    """
    Drop rows with missing data.
    """
    if inplace:
        df.dropna(how=how, subset=subset, thresh=thresh, inplace=True)
        return df
    return df.dropna(how=how, subset=subset, thresh=thresh)


def infer_and_convert_dtypes(
    df: pd.DataFrame,
    datetime_cols: Optional[Sequence[str]] = None,
    numeric_cols: Optional[Sequence[str]] = None,
    category_cols: Optional[Sequence[str]] = None,
    inplace: bool = False,
) -> pd.DataFrame:
    """
    Convert dtypes: optional explicit datetime/numeric/category columns, then pandas convert_dtypes.
    """
    out = df if inplace else df.copy()

    if datetime_cols:
        for c in datetime_cols:
            out[c] = pd.to_datetime(out[c], errors="coerce")

    if numeric_cols:
        for c in numeric_cols:
            out[c] = pd.to_numeric(out[c], errors="coerce")

    if category_cols:
        for c in category_cols:
            out[c] = out[c].astype("category")

    out = out.convert_dtypes()
    return out


# -----------------------------
# Transformations
# -----------------------------

def select_columns(df: pd.DataFrame, columns: Sequence[str]) -> pd.DataFrame:
    """
    Return a DataFrame with selected columns.
    """
    return df.loc[:, list(columns)]


def filter_rows(df: pd.DataFrame, query: str, engine: str = "python") -> pd.DataFrame:
    """
    Filter rows using pandas.DataFrame.query syntax.
    """
    return df.query(expr=query, engine=engine)


def add_column(
    df: pd.DataFrame,
    new_col: str,
    func: Any,
    axis: int = 1,
    args: Optional[Tuple[Any, ...]] = None,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    Add a new column computed via a function applied with DataFrame.apply.
    func receives rows (axis=1) or columns (axis=0).
    """
    out = df.copy()
    out[new_col] = out.apply(func, axis=axis, args=args or (), **kwargs)
    return out


def scale_numeric(
    df: pd.DataFrame,
    columns: Optional[Sequence[str]] = None,
    method: str = "standard",
    feature_range: Tuple[float, float] = (0.0, 1.0),
    suffix: Optional[str] = None,
) -> pd.DataFrame:
    """
    Scale numeric columns.
    method:
      - 'standard': z-score (x - mean) / std
      - 'minmax': (x - min) / (max - min) scaled to feature_range
      - 'robust': (x - median) / IQR
    """
    out = df.copy()
    if columns is None:
        columns = [c for c in out.columns if pd.api.types.is_numeric_dtype(out[c])]
    if suffix is None:
        suffix = {"standard": "_z", "minmax": "_mm", "robust": "_rb"}.get(method, "_scaled")

    for c in columns:
        s = out[c].astype(float)
        if method == "standard":
            mu = s.mean()
            sd = s.std(ddof=0)
            scaled = (s - mu) / (sd if sd != 0 else 1.0)
        elif method == "minmax":
            lo, hi = feature_range
            s_min, s_max = s.min(), s.max()
            denom = (s_max - s_min) if (s_max - s_min) != 0 else 1.0
            scaled = (s - s_min) / denom
            scaled = scaled * (hi - lo) + lo
        elif method == "robust":
            q1, q3 = s.quantile(0.25), s.quantile(0.75)
            iqr = q3 - q1 if (q3 - q1) != 0 else 1.0
            scaled = (s - s.median()) / iqr
        else:
            raise ValueError("method must be one of {'standard','minmax','robust'}")
        out[f"{c}{suffix}"] = scaled

    return out


def encode_categoricals(
    df: pd.DataFrame,
    columns: Optional[Sequence[str]] = None,
    method: str = "onehot",
    categories: Optional[Mapping[str, Sequence[Any]]] = None,
    drop_first: bool = False,
    dtype: Optional[Any] = None,
) -> pd.DataFrame:
    """
    Encode categorical columns.
    method:
      - 'onehot': pd.get_dummies; respects provided categories if present.
      - 'ordinal': map using 'categories' dict where values are ordered categories.
    """
    out = df.copy()
    if columns is None:
        columns = [c for c in out.columns if pd.api.types.is_object_dtype(out[c]) or pd.api.types.is_categorical_dtype(out[c])]

    if method == "onehot":
        # Ensure category dtype with specified categories when provided
        for c in columns:
            if categories and c in categories:
                out[c] = pd.Categorical(out[c], categories=categories[c])
        out = pd.get_dummies(out, columns=list(columns), drop_first=drop_first, dtype=dtype)
    elif method == "ordinal":
        if not categories:
            raise ValueError("For 'ordinal' encoding, provide 'categories' mapping of column -> ordered categories.")
        for c in columns:
            if c not in categories:
                raise ValueError(f"Missing categories for column '{c}'")
            cat_list = list(categories[c])
            mapping = {k: i for i, k in enumerate(cat_list)}
            out[c] = out[c].map(mapping).astype("Int64")
    else:
        raise ValueError("method must be 'onehot' or 'ordinal'")

    return out


# -----------------------------
# Visualization Utilities
# -----------------------------

def plot_hist(
    df: pd.DataFrame,
    columns: Optional[Sequence[str]] = None,
    bins: int = 30,
    figsize: Tuple[int, int] = (10, 6),
    kde: bool = False,
    show: bool = True,
):
    """
    Plot histograms for specified or all numeric columns.
    """
    if columns is None:
        columns = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    n = len(columns)
    if n == 0:
        raise ValueError("No numeric columns to plot.")

    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=figsize, constrained_layout=True)
    axes = np.atleast_1d(axes)
    for ax, col in zip(axes, columns):
        sns.histplot(df[col].dropna(), bins=bins, kde=kde, ax=ax)
        ax.set_title(f"Histogram: {col}")
    if show:
        plt.show()
    return fig, axes


def plot_box(
    df: pd.DataFrame,
    columns: Optional[Sequence[str]] = None,
    figsize: Tuple[int, int] = (10, 6),
    show: bool = True,
):
    """
    Plot boxplots for specified or all numeric columns.
    """
    if columns is None:
        columns = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    data = df[columns]
    fig, ax = plt.subplots(figsize=figsize)
    sns.boxplot(data=data, orient="h", ax=ax)
    ax.set_title("Boxplots")
    if show:
        plt.show()
    return fig, ax


def plot_scatter(
    df: pd.DataFrame,
    x: str,
    y: str,
    hue: Optional[str] = None,
    style: Optional[str] = None,
    figsize: Tuple[int, int] = (8, 6),
    show: bool = True,
):
    """
    Scatter plot (optionally colored by 'hue' and styled by 'style').
    """
    fig, ax = plt.subplots(figsize=figsize)
    sns.scatterplot(data=df, x=x, y=y, hue=hue, style=style, ax=ax)
    ax.set_title(f"Scatter: {y} vs {x}")
    if show:
        plt.show()
    return fig, ax


def plot_correlation_heatmap(
    df: pd.DataFrame,
    columns: Optional[Sequence[str]] = None,
    annot: bool = False,
    cmap: str = "coolwarm",
    figsize: Tuple[int, int] = (10, 8),
    show: bool = True,
):
    """
    Plot a correlation heatmap for numeric columns.
    """
    if columns is None:
        columns = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    corr = df[columns].corr(numeric_only=True)
    fig, ax = plt.subplots(figsize=figsize)
    sns.heatmap(corr, annot=annot, cmap=cmap, vmin=-1, vmax=1, square=True, ax=ax)
    ax.set_title("Correlation Heatmap")
    if show:
        plt.show()
    return fig, ax


def plot_time_series(
    df: pd.DataFrame,
    x: Union[str, pd.Series],
    y: Union[str, Sequence[str]],
    agg: Optional[str] = None,
    freq: Optional[str] = None,
    figsize: Tuple[int, int] = (12, 6),
    show: bool = True,
):
    """
    Plot time series; optionally resample by 'freq' and aggregate with 'agg'.
    x can be a column name or a Series. y can be a column name or list of columns.
    """
    data = df.copy()

    # Prepare x axis
    if isinstance(x, str):
        data[x] = pd.to_datetime(data[x], errors="coerce")
        data.set_index(x, inplace=True)
    else:
        idx = pd.to_datetime(pd.Series(x), errors="coerce")
        data = data.set_index(idx)

    # Determine y columns
    y_cols = [y] if isinstance(y, str) else list(y)

    if freq:
        if agg is None:
            agg = "mean"
        data = data[y_cols].resample(freq).agg(agg)
    else:
        data = data[y_cols]

    fig, ax = plt.subplots(figsize=figsize)
    data.plot(ax=ax)
    ax.set_title("Time Series")
    ax.set_xlabel("Time")
    if show:
        plt.show()
    return fig, ax


# -----------------------------
# Example pipeline helper
# -----------------------------

def example_pipeline(
    df: pd.DataFrame,
) -> pd.DataFrame:
    """
    Example end-to-end transformation:
      - standardize col names
      - trim whitespace on string cols
      - infer/convert dtypes
      - impute missing numerics with median
      - scale numerics (z-score)
      - one-hot encode categoricals
    """
    df1 = standardize_column_names(df)
    df2 = trim_whitespace(df1)
    df3 = infer_and_convert_dtypes(df2)
    num_cols = [c for c in df3.columns if pd.api.types.is_numeric_dtype(df3[c])]
    cat_cols = [c for c in df3.columns if pd.api.types.is_string_dtype(df3[c]) or pd.api.types.is_categorical_dtype(df3[c])]
    df4 = impute_missing(df3, strategy="median", columns=num_cols)
    df5 = scale_numeric(df4, columns=num_cols, method="standard")
    df6 = encode_categoricals(df5, columns=cat_cols, method="onehot", drop_first=True)
    return df6


# If this file is run directly, demonstrate minimal usage (no actual I/O paths).
if __name__ == "__main__":
    # Demo DataFrame
    demo = pd.DataFrame(
        {
            "ID ": [1, 2, 2, 3, 4],
            "Date": ["2024-01-01", "2024/01/02", None, "2024-01-04", "2024-01-05"],
            "Category": ["A", "B", "A", "C", None],
            "Value": [10, 20, np.nan, 40, 50],
            "Text": [" foo ", "bar", " baz", None, "qux "],
        }
    )

    demo = standardize_column_names(demo, inplace=True)
    demo = remove_duplicates(demo, subset="id")
    demo = trim_whitespace(demo)
    demo = infer_and_convert_dtypes(demo, datetime_cols=["date"], numeric_cols=["value"])
    demo = impute_missing(demo, strategy="median", columns=["value"])
    demo_scaled = scale_numeric(demo, columns=["value"], method="standard")
    demo_encoded = encode_categoricals(demo_scaled, columns=["category"], method="onehot", drop_first=True)

    # Quick visualizations (will display if run as script)
    plot_hist(demo, columns=["value"], show=False)
    plot_box(demo, show=False)
    plot_scatter(demo, x="id", y="value", hue="category", show=False)
    plot_correlation_heatmap(demo_encoded, show=False)
    # plt.show()  # uncomment to display all accumulated figures in some environments
------------------------------------------------------------

PROMPT 178:
"Generate Python code that imports a random name generator library, such as `randomname` or `faker`, and uses it to create a function that generates a random name with a specified format, such as first name, last name, and email address, similar to the functionality provided by Heroku's random name generator."
RESPONSE:
# pip install faker
from __future__ import annotations

import re
import unicodedata
from typing import Dict, Tuple

try:
    from faker import Faker
except ImportError as e:
    raise SystemExit(
        "Missing dependency: faker. Install it with `pip install faker`."
    ) from e


def _slugify_ascii(s: str) -> str:
    """Convert a string to lowercase ASCII letters/numbers only."""
    s = unicodedata.normalize("NFKD", s).encode("ascii", "ignore").decode("ascii")
    s = re.sub(r"[^a-zA-Z0-9]+", "", s)
    return s.lower()


def generate_random_identity(
    fmt: str = "{first} {last} <{email}>",
    domain: str = "example.com",
    locale: str = "en_US",
    seed: int | None = None,
) -> Tuple[str, Dict[str, str]]:
    """
    Generate a random identity using Faker and format it with the given template.

    Available placeholders for `fmt`:
      - {first}     -> first name
      - {last}      -> last name
      - {username}  -> username-style slug (e.g., john.doe)
      - {email}     -> email address (username@domain)
      - {domain}    -> the domain used for the email
      - {display}   -> "First Last"

    Example formats:
      - "{first} {last} <{email}>"
      - "{username}@{domain}"
      - "{last}, {first}"
    """
    fake = Faker(locale)
    if seed is not None:
        fake.seed_instance(seed)

    first = fake.first_name()
    last = fake.last_name()
    username = f"{_slugify_ascii(first)}.{_slugify_ascii(last)}"
    email = f"{username}@{domain}"

    data = {
        "first": first,
        "last": last,
        "username": username,
        "email": email,
        "domain": domain,
        "display": f"{first} {last}",
    }

    try:
        return fmt.format(**data), data
    except KeyError as ke:
        raise ValueError(
            f"Unknown placeholder in fmt: {ke}. "
            "Valid placeholders are {first}, {last}, {username}, {email}, {domain}, {display}."
        ) from ke


if __name__ == "__main__":
    # Example usage
    formatted, parts = generate_random_identity(
        fmt="{first} {last} <{email}>",
        domain="example.org",
        locale="en_US",
        seed=None,  # set an int for reproducible results
    )
    print(formatted)
    print(parts)
------------------------------------------------------------

PROMPT 179:
"Generate Python code that imports the necessary modules, defines a retry decorator function that takes a maximum number of attempts and a callback function as arguments, and returns a decorated function that retries the original function up to the maximum number of attempts before calling the callback function."
RESPONSE:
from functools import wraps
from typing import Callable, Any

def retry(max_attempts: int, on_failure: Callable[[Exception], None]) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """
    A decorator factory that retries a function up to `max_attempts` times.
    If all attempts fail, it calls `on_failure` with the last exception, then re-raises it.

    Args:
        max_attempts: Maximum number of attempts (must be >= 1).
        on_failure: A callback function that accepts the last Exception.

    Returns:
        A decorator that can be applied to functions to add retry behavior.
    """
    if max_attempts < 1:
        raise ValueError("max_attempts must be >= 1")

    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        @wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            attempts = 0
            while attempts < max_attempts:
                try:
                    return func(*args, **kwargs)
                except Exception as exc:
                    attempts += 1
                    if attempts >= max_attempts:
                        on_failure(exc)
                        raise
        return wrapper
    return decorator

# Example usage:
if __name__ == "__main__":
    def notify_failure(e: Exception) -> None:
        print(f"Function failed after retries: {e}")

    @retry(max_attempts=3, on_failure=notify_failure)
    def flaky():
        print("Trying...")
        raise RuntimeError("Still failing")

    try:
        flaky()
    except Exception as e:
        print(f"Caught: {e}")
------------------------------------------------------------

PROMPT 180:
"Generate Python code that imports the required modules and annotates the GuardDuty service client class with type hints using mypy-boto3-builder, like so:\n```python\nfrom typing import Optional\nfrom botocore.client import Client\nfrom boto3.guardduty import GuardDuty\n\nguard_duty_client = Client(service='guardduty')\n```"
RESPONSE:
from typing import Optional
import boto3
from mypy_boto3_guardduty import GuardDutyClient

guard_duty_client: GuardDutyClient = boto3.client("guardduty")
------------------------------------------------------------

PROMPT 181:
"Generate Python code that imports a package or module from a Jupyter Notebook file (.ipynb) using the `import_module` function from the `IPython.nb` module, which loads the notebook file and returns a dictionary of its cells, then uses the `exec` function to execute the cell containing the package or module import statement."
RESPONSE:
Note: There isn’t an import_module function in IPython.nb. Here’s a minimal, working way to emulate what you described: read a .ipynb with nbformat, return a dictionary of its code cells, then exec the cell that contains the desired import statement.

```python
import re
import nbformat

def import_module(notebook_path):
    """
    Load a .ipynb file and return a dict of its code cells:
    {cell_index: cell_source}
    """
    nb = nbformat.read(notebook_path, as_version=4)
    return {i: c.source for i, c in enumerate(nb.cells) if c.cell_type == "code"}

def exec_import_cell(cells_dict, module_name, namespace=None):
    """
    Find and exec the first cell that imports `module_name`.
    Returns the namespace into which the import was executed.
    """
    pattern = re.compile(
        rf'^\s*(?:import\s+{re.escape(module_name)}\b|from\s+{re.escape(module_name)}\b\s+import\b)',
        re.MULTILINE
    )
    for idx, src in cells_dict.items():
        if pattern.search(src):
            ns = {} if namespace is None else namespace
            exec(src, ns)
            return ns
    raise ImportError(f"No import statement for '{module_name}' found in the notebook cells.")

# Example usage:
if __name__ == "__main__":
    cells = import_module("path/to/notebook.ipynb")
    ns = exec_import_cell(cells, "numpy")  # executes the cell that imports numpy
    # Access the imported module (depends on how it was imported in the cell)
    # e.g., if the cell had "import numpy as np":
    np = ns.get("np") or ns.get("numpy")
    if np:
        print("NumPy version:", np.__version__)
```

Caution: Executing notebook cells runs arbitrary code. Only use with trusted notebooks.
------------------------------------------------------------

PROMPT 182:
"Generate Python code that imports the functools module and creates a decorator function called deprecated, which takes a message string as an argument and returns a new function that wraps the original function while printing a warning message when invoked, similar to the @deprecated decorator in the docstring of the functools module."
RESPONSE:
import functools
import warnings

def deprecated(message: str):
    """Decorator to mark functions as deprecated with a custom message."""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            warnings.warn(
                f"{func.__name__} is deprecated: {message}",
                category=DeprecationWarning,
                stacklevel=2,
            )
            return func(*args, **kwargs)
        return wrapper
    return decorator
------------------------------------------------------------

PROMPT 183:
"Generate Python code that implements a user-friendly database interface for humans, allowing them to perform CRUD operations on databases with ease, including creating tables, inserting data, updating records, and querying data, using intuitive functions and natural language syntax, while hiding complex SQL queries and underlying database details."
RESPONSE:
import sqlite3
import re
import json
import ast
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union


class HumanDB:
    """
    A user-friendly database interface that hides SQL details and supports both
    intuitive Python functions and simple natural-language commands.

    Features:
    - Create tables
    - Insert/Update/Delete rows
    - Query data
    - Simple natural-language commands (e.g., "show users where age > 30 limit 5")

    Backed by SQLite; uses parameterized queries for safety.
    """

    TYPE_ALIASES = {
        "int": "INTEGER",
        "integer": "INTEGER",
        "number": "INTEGER",
        "real": "REAL",
        "float": "REAL",
        "double": "REAL",
        "decimal": "REAL",
        "text": "TEXT",
        "string": "TEXT",
        "str": "TEXT",
        "bool": "INTEGER",
        "boolean": "INTEGER",
        "blob": "BLOB",
        "timestamp": "TEXT",
        "datetime": "TEXT",
        "date": "TEXT",
        "time": "TEXT",
    }

    def __init__(self, path: str = ":memory:", autocommit: bool = True):
        self.conn = sqlite3.connect(path, isolation_level=None if autocommit else "")
        self.conn.row_factory = sqlite3.Row
        self.autocommit = autocommit
        self._exec("PRAGMA foreign_keys = ON")

    def close(self):
        self.conn.close()

    # ---------------------------
    # Core SQL utilities (hidden)
    # ---------------------------
    def _exec(self, sql: str, params: Sequence[Any] = ()) -> sqlite3.Cursor:
        cur = self.conn.cursor()
        cur.execute(sql, params)
        return cur

    def _execmany(self, sql: str, seq_of_params: Sequence[Sequence[Any]]) -> sqlite3.Cursor:
        cur = self.conn.cursor()
        cur.executemany(sql, seq_of_params)
        return cur

    def _commit(self):
        if not self.autocommit:
            self.conn.commit()

    def _rollback(self):
        if not self.autocommit:
            self.conn.rollback()

    # ---------------------------
    # Helpers
    # ---------------------------
    @staticmethod
    def _infer_sql_type(value: Any) -> str:
        if value is None:
            return "TEXT"
        if isinstance(value, bool):
            return "INTEGER"
        if isinstance(value, int) and not isinstance(value, bool):
            return "INTEGER"
        if isinstance(value, float):
            return "REAL"
        if isinstance(value, (bytes, bytearray, memoryview)):
            return "BLOB"
        # default to text (includes str, datetime as ISO strings, etc.)
        return "TEXT"

    @classmethod
    def _normalize_type(cls, t: Union[str, type]) -> str:
        if isinstance(t, type):
            if t in (int,):
                return "INTEGER"
            if t in (float,):
                return "REAL"
            if t in (bytes, bytearray, memoryview):
                return "BLOB"
            if t in (bool,):
                return "INTEGER"
            return "TEXT"
        # string type
        t_l = t.strip().lower()
        return cls.TYPE_ALIASES.get(t_l, t.upper())

    def _table_exists(self, table: str) -> bool:
        cur = self._exec(
            "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
            (table,),
        )
        return cur.fetchone() is not None

    def _get_columns(self, table: str) -> Dict[str, str]:
        cur = self._exec(f"PRAGMA table_info({self._quote_ident(table)})")
        cols = {}
        for row in cur.fetchall():
            cols[row["name"]] = row["type"]
        return cols

    def _ensure_columns(self, table: str, sample: Dict[str, Any]) -> None:
        existing = self._get_columns(table)
        to_add = []
        for k, v in sample.items():
            if k not in existing:
                to_add.append((k, self._infer_sql_type(v)))
        for name, sql_type in to_add:
            self._exec(
                f"ALTER TABLE {self._quote_ident(table)} ADD COLUMN {self._quote_ident(name)} {sql_type}"
            )

    @staticmethod
    def _quote_ident(identifier: str) -> str:
        if not re.match(r"^[A-Za-z_][A-Za-z0-9_]*$", identifier):
            raise ValueError(f"Invalid identifier: {identifier}")
        return f'"{identifier}"'

    @staticmethod
    def _parse_kv_pairs(s: str) -> Dict[str, Any]:
        """
        Parse key=value pairs separated by commas or spaces.
        - Supports quoted strings and basic types via ast.literal_eval when possible.
        Examples:
          "name='Alice', age=30" -> {"name": "Alice", "age": 30}
          "name=Alice age=30 active=true" -> {"name":"Alice","age":30,"active":True}
        """
        # Try JSON first
        s_strip = s.strip()
        try:
            if s_strip.startswith("{") and s_strip.endswith("}"):
                obj = json.loads(s_strip)
                if isinstance(obj, dict):
                    return obj
        except Exception:
            pass

        # Tokenize on commas first; if none, split on spaces but keep quoted tokens
        parts = []
        if "," in s:
            parts = [p.strip() for p in s.split(",") if p.strip()]
        else:
            # Split by spaces but keep quoted blocks intact
            parts = re.findall(r"""(?:[^\s"']+|"(?:\\.|[^"])*"|'(?:\\.|[^'])*')+""", s)

        out: Dict[str, Any] = {}
        for p in parts:
            if "=" not in p:
                # allow bare words like "active" => True
                key = p.strip()
                if key:
                    out[key] = True
                continue
            k, v = p.split("=", 1)
            key = k.strip()
            val = v.strip()
            if (val.startswith("'") and val.endswith("'")) or (val.startswith('"') and val.endswith('"')):
                val = val[1:-1]
            else:
                # try to parse python literal (int, float, list, bool, None)
                try:
                    val = ast.literal_eval(val)
                except Exception:
                    # keep raw string
                    pass
            out[key] = val
        return out

    @staticmethod
    def _sanitize_order_by(s: str, table_cols: Sequence[str]) -> str:
        """
        Only allow "col [ASC|DESC]" comma separated; reject anything else.
        """
        items = [x.strip() for x in s.split(",") if x.strip()]
        cleaned = []
        colset = set(table_cols)
        for item in items:
            m = re.match(r"^([A-Za-z_][A-Za-z0-9_]*)(?:\s+(ASC|DESC))?$", item, re.I)
            if not m:
                continue
            col = m.group(1)
            if col not in colset:
                # allow but cautiously if unknown column (maybe alias); we drop it
                continue
            direction = (m.group(2) or "ASC").upper()
            cleaned.append(f'"{col}" {direction}')
        return ", ".join(cleaned)

    @staticmethod
    def _parse_value_token(val: str) -> Any:
        v = val.strip()
        if (v.startswith("'") and v.endswith("'")) or (v.startswith('"') and v.endswith('"')):
            return v[1:-1]
        try:
            return ast.literal_eval(v)
        except Exception:
            # keywords
            vl = v.lower()
            if vl == "null" or vl == "none":
                return None
            if vl == "true":
                return True
            if vl == "false":
                return False
            return v

    def _cond_to_sql(self, cond: str, table: str) -> Tuple[str, List[Any]]:
        """
        Convert a simple natural-language-ish condition string into SQL WHERE.
        Supports:
          - col = 5, col != 5, col > 5, col >= 5, col < 5, col <= 5
          - col contains 'abc', col startswith 'A', col endswith 'Z'
          - col in [1,2,3] or in ('a','b')
          - col is null, col is not null
          - AND / OR (case-insensitive)
        """
        tokens = self._split_and_or(cond)
        sql_parts: List[str] = []
        params: List[Any] = []

        for ttype, text in tokens:
            if ttype in ("AND", "OR"):
                sql_parts.append(ttype)
                continue

            expr = text.strip()
            # is null / is not null
            m = re.match(r"^([A-Za-z_][A-Za-z0-9_]*)\s+is\s+(not\s+)?null$", expr, re.I)
            if m:
                col = self._quote_ident(m.group(1))
                if m.group(2):  # not null
                    sql_parts.append(f"{col} IS NOT NULL")
                else:
                    sql_parts.append(f"{col} IS NULL")
                continue

            # contains/starts/ends
            m = re.match(r"^([A-Za-z_][A-Za-z0-9_]*)\s+(contains|startswith|starts with|endswith|ends with)\s+(.+)$", expr, re.I)
            if m:
                col = self._quote_ident(m.group(1))
                op = m.group(2).replace(" ", "").lower()
                val = self._parse_value_token(m.group(3))
                if val is None:
                    sql_parts.append("1=0")
                    continue
                if op == "contains":
                    like = f"%{val}%"
                elif op in ("startswith", "startswith"):
                    like = f"{val}%"
                else:
                    like = f"%{val}"
                sql_parts.append(f"{col} LIKE ?")
                params.append(like)
                continue

            # IN list
            m = re.match(r"^([A-Za-z_][A-Za-z0-9_]*)\s+in\s+(.+)$", expr, re.I)
            if m:
                col = self._quote_ident(m.group(1))
                seq_raw = m.group(2).strip()
                try:
                    seq = ast.literal_eval(seq_raw)
                    if not isinstance(seq, (list, tuple, set)):
                        raise ValueError
                    seq = list(seq)
                except Exception:
                    # Comma-separated fall-back
                    seq = [self._parse_value_token(x) for x in re.split(r"\s*,\s*", seq_raw)]
                if not seq:
                    sql_parts.append("1=0")
                else:
                    placeholders = ", ".join(["?"] * len(seq))
                    sql_parts.append(f"{col} IN ({placeholders})")
                    params.extend(seq)
                continue

            # Comparison operators
            m = re.match(r"^([A-Za-z_][A-Za-z0-9_]*)\s*(=|==|!=|<>|>=|<=|>|<|is|equals|not equals)\s*(.+)$", expr, re.I)
            if m:
                col = self._quote_ident(m.group(1))
                op = m.group(2).lower()
                rhs = m.group(3)
                if op in ("=", "==", "is", "equals"):
                    sql_op = "="
                elif op in ("!=", "<>", "not equals"):
                    sql_op = "!="
                else:
                    sql_op = op
                val = self._parse_value_token(rhs)
                sql_parts.append(f"{col} {sql_op} ?")
                params.append(val)
                continue

            # bare truthy column: "active"
            m = re.match(r"^([A-Za-z_][A-Za-z0-9_]*)$", expr)
            if m:
                col = self._quote_ident(m.group(1))
                sql_parts.append(f"{col} = 1")
                continue

            # Fallback, reject unsafe expressions
            raise ValueError(f"Unrecognized condition: {expr}")

        if not sql_parts:
            return "1=1", []
        return " ".join(sql_parts), params

    @staticmethod
    def _split_and_or(s: str) -> List[Tuple[str, str]]:
        """
        Split a condition string into [(type, text)] where type in {"TEXT", "AND", "OR"}.
        Preserves quoted strings.
        """
        tokens = []
        pattern = re.compile(r"\b(AND|OR)\b", re.I)
        idx = 0
        for m in pattern.finditer(s):
            prev = s[idx:m.start()]
            if prev.strip():
                tokens.append(("TEXT", prev.strip()))
            tokens.append((m.group(1).upper(), m.group(1).upper()))
            idx = m.end()
        tail = s[idx:]
        if tail.strip():
            tokens.append(("TEXT", tail.strip()))
        return tokens

    # ---------------------------
    # Public API: Pythonic calls
    # ---------------------------
    def create_table(
        self,
        table: str,
        columns: Union[Dict[str, Union[str, type]], List[Tuple[str, Union[str, type]]], List[Dict[str, Any]]],
        if_not_exists: bool = True,
        primary_key: Optional[Union[str, List[str]]] = None,
    ):
        """
        Create a table.
        columns:
          - dict: {"name": "TEXT", "age": int}
          - list of tuples: [("name","TEXT"), ("age", int)]
          - list of example dict rows: [{"name":"A", "age":1}, {"name":"B"}] to infer schema
        """
        if isinstance(columns, list):
            if columns and isinstance(columns[0], dict):
                # infer schema from examples
                keys = set()
                types: Dict[str, str] = {}
                for row in columns:  # type: ignore
                    keys.update(row.keys())
                    for k, v in row.items():
                        types[k] = self._infer_sql_type(v)
                colspec = [(k, types.get(k, "TEXT")) for k in sorted(keys)]
            else:
                # list of tuples
                colspec = [(k, self._normalize_type(t)) for k, t in columns]  # type: ignore
        elif isinstance(columns, dict):
            colspec = [(k, self._normalize_type(t)) for k, t in columns.items()]
        else:
            raise ValueError("Unsupported columns specification")

        cols_sql = ", ".join(f'{self._quote_ident(k)} {t}' for k, t in colspec)
        pk_sql = ""
        if primary_key:
            if isinstance(primary_key, str):
                pk_cols = [primary_key]
            else:
                pk_cols = primary_key
            pk_quoted = ", ".join(self._quote_ident(c) for c in pk_cols)
            pk_sql = f", PRIMARY KEY ({pk_quoted})"

        ine = "IF NOT EXISTS " if if_not_exists else ""
        sql = f"CREATE TABLE {ine}{self._quote_ident(table)} ({cols_sql}{pk_sql})"
        self._exec(sql)
        self._commit()

    def insert(self, table: str, rows: Union[Dict[str, Any], List[Dict[str, Any]]], auto_alter: bool = True):
        """
        Insert one or many rows. Automatically adds new columns if auto_alter=True.
        """
        if isinstance(rows, dict):
            rows = [rows]
        if not rows:
            return
        if not self._table_exists(table):
            # create table inferred from first row
            sample = rows[0]
            inferred = {k: self._infer_sql_type(v) for k, v in sample.items()}
            self.create_table(table, inferred, if_not_exists=True)

        if auto_alter:
            union_keys = {}
            for r in rows:
                union_keys.update(r)
            self._ensure_columns(table, union_keys)

        cols = sorted({k for r in rows for k in r.keys()})
        placeholders = ", ".join(["?"] * len(cols))
        col_sql = ", ".join(self._quote_ident(c) for c in cols)
        sql = f"INSERT INTO {self._quote_ident(table)} ({col_sql}) VALUES ({placeholders})"
        params = []
        for r in rows:
            params.append([r.get(c, None) for c in cols])
        self._execmany(sql, params)
        self._commit()

    def update(
        self,
        table: str,
        set_values: Dict[str, Any],
        where: Optional[Union[str, Dict[str, Any]]] = None,
    ) -> int:
        """
        Update rows where a condition matches.
        where can be:
          - str: simple condition string ("age > 20 and active")
          - dict: {"id": 5, "name": "Bob"} treated as AND equality checks.
        Returns number of affected rows.
        """
        if not set_values:
            return 0
        if not self._table_exists(table):
            raise ValueError(f"Table '{table}' does not exist")

        set_cols = sorted(set_values.keys())
        set_sql = ", ".join(f'{self._quote_ident(c)} = ?' for c in set_cols)
        params: List[Any] = [set_values[c] for c in set_cols]

        where_sql = "1=1"
        if where:
            if isinstance(where, dict):
                wparts = []
                for k, v in where.items():
                    wparts.append(f'{self._quote_ident(k)} = ?')
                    params.append(v)
                where_sql = " AND ".join(wparts)
            else:
                ws, p = self._cond_to_sql(where, table)
                where_sql = ws
                params.extend(p)

        cur = self._exec(
            f"UPDATE {self._quote_ident(table)} SET {set_sql} WHERE {where_sql}",
            params,
        )
        self._commit()
        return cur.rowcount

    def delete(self, table: str, where: Optional[Union[str, Dict[str, Any]]] = None) -> int:
        """
        Delete rows matching condition.
        """
        if not self._table_exists(table):
            raise ValueError(f"Table '{table}' does not exist")

        params: List[Any] = []
        where_sql = "1=1"
        if where:
            if isinstance(where, dict):
                wparts = []
                for k, v in where.items():
                    wparts.append(f'{self._quote_ident(k)} = ?')
                    params.append(v)
                where_sql = " AND ".join(wparts)
            else:
                ws, p = self._cond_to_sql(where, table)
                where_sql = ws
                params.extend(p)

        cur = self._exec(
            f"DELETE FROM {self._quote_ident(table)} WHERE {where_sql}",
            params,
        )
        self._commit()
        return cur.rowcount

    def select(
        self,
        table: str,
        columns: Union[str, Sequence[str]] = "*",
        where: Optional[Union[str, Dict[str, Any]]] = None,
        order_by: Optional[str] = None,
        limit: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        Query table and return list of dict rows.
        """
        if not self._table_exists(table):
            raise ValueError(f"Table '{table}' does not exist")

        if columns == "*":
            col_sql = "*"
        else:
            col_sql = ", ".join(self._quote_ident(c) for c in columns)

        params: List[Any] = []
        where_sql = "1=1"
        if where:
            if isinstance(where, dict):
                wparts = []
                for k, v in where.items():
                    wparts.append(f'{self._quote_ident(k)} = ?')
                    params.append(v)
                where_sql = " AND ".join(wparts)
            else:
                ws, p = self._cond_to_sql(where, table)
                where_sql = ws
                params.extend(p)

        order_sql = ""
        if order_by:
            table_cols = list(self._get_columns(table).keys())
            sanitized = self._sanitize_order_by(order_by, table_cols)
            if sanitized:
                order_sql = f" ORDER BY {sanitized}"

        limit_sql = ""
        if limit is not None:
            if not isinstance(limit, int) or limit < 0:
                raise ValueError("limit must be a non-negative integer")
            limit_sql = f" LIMIT {limit}"

        sql = f"SELECT {col_sql} FROM {self._quote_ident(table)} WHERE {where_sql}{order_sql}{limit_sql}"
        cur = self._exec(sql, params)
        return [dict(row) for row in cur.fetchall()]

    def tables(self) -> List[str]:
        cur = self._exec("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' ORDER BY name")
        return [r["name"] for r in cur.fetchall()]

    def describe(self, table: str) -> List[Dict[str, Any]]:
        cur = self._exec(f"PRAGMA table_info({self._quote_ident(table)})")
        return [dict(r) for r in cur.fetchall()]

    # ---------------------------
    # Natural-language interface
    # ---------------------------
    def ask(self, command: str) -> Union[str, List[Dict[str, Any]], int, None]:
        """
        Execute a simple natural-language-like command.
        Examples:
          - "create table users with columns id int, name text, age int"
          - "insert into users name='Alice', age=30"
          - "add to users name=Bob age=25"
          - "show users where age > 20 order by age desc limit 5"
          - "update users set age=31 where name='Alice'"
          - "delete from users where age < 18"
          - "describe users"
          - "show tables"
        Returns:
          - For queries: list of dict rows
          - For update/delete: number of affected rows
          - For create/insert/describe/tables: a message or list depending on context
        """
        s = command.strip()
        s_l = s.lower()

        # show tables
        if re.match(r"^(show|list)\s+tables$", s_l):
            return self.tables()

        # describe
        m = re.match(r"^(describe|schema of|columns of|what columns (?:are in|in))\s+([A-Za-z_][A-Za-z0-9_]*)$", s_l)
        if m:
            table = m.group(2)
            return self.describe(table)

        # create table
        m = re.match(
            r"^create(?:\s+a)?\s+table(?:\s+named)?\s+([A-Za-z_][A-Za-z0-9_]*)(?:\s+(?:with|having)\s+(?:columns|fields)\s+(.+))?$",
            s, re.I,
        )
        if m:
            table = m.group(1)
            cols_part = m.group(2)
            cols: Dict[str, str] = {}
            if cols_part:
                items = [x.strip() for x in cols_part.split(",") if x.strip()]
                for item in items:
                    bits = item.split()
                    if len(bits) == 1:
                        col = bits[0]
                        cols[col] = "TEXT"
                    else:
                        col = bits[0]
                        typ = " ".join(bits[1:])
                        cols[col] = self._normalize_type(typ)
            else:
                cols = {"id": "INTEGER", "name": "TEXT"}
            self.create_table(table, cols, if_not_exists=True)
            return f"Table {table} created"

        # insert/add
        m = re.match(r"^(?:insert(?:\s+into)?|add(?:\s+to)?)\s+([A-Za-z_][A-Za-z0-9_]*)\s+(.+)$", s, re.I)
        if m:
            table = m.group(1)
            data_part = m.group(2)
            # allow JSON array/object or key=val pairs; if multiple rows, prefer JSON array
            data_str = data_part.strip()
            rows: List[Dict[str, Any]] = []
            if (data_str.startswith("[") and data_str.endswith("]")) or (data_str.startswith("{") and data_str.endswith("}")):
                try:
                    obj = json.loads(data_str)
                    if isinstance(obj, list):
                        rows = [r for r in obj if isinstance(r, dict)]
                    elif isinstance(obj, dict):
                        rows = [obj]
                except Exception:
                    pass
            if not rows:
                rows = [self._parse_kv_pairs(data_str)]
            self.insert(table, rows, auto_alter=True)
            return f"Inserted {len(rows)} row(s) into {table}"

        # update
        m = re.match(r"^update\s+([A-Za-z_][A-Za-z0-9_]*)\s+set\s+(.+?)(?:\s+where\s+(.+))?$", s, re.I)
        if m:
            table = m.group(1)
            set_part = m.group(2)
            where_part = m.group(3)
            set_vals = self._parse_kv_pairs(set_part)
            n = self.update(table, set_vals, where_part)
            return n

        # delete/remove
        m = re.match(r"^(?:delete\s+from|remove\s+from|delete|remove)\s+([A-Za-z_][A-Za-z0-9_]*)(?:\s+where\s+(.+))?$", s, re.I)
        if m:
            table = m.group(1)
            where_part = m.group(2)
            n = self.delete(table, where_part)
            return n

        # select/show/list/get
        m = re.match(
            r"^(?:show|list|get|select)\s+(?:all\s+)?([A-Za-z_][A-Za-z0-9_]*)(?:\s+where\s+(.+?))?(?:\s+order\s+by\s+(.+?))?(?:\s+limit\s+(\d+))?$",
            s, re.I,
        )
        if m:
            table = m.group(1)
            where_part = m.group(2)
            order_part = m.group(3)
            limit_part = m.group(4)
            limit = int(limit_part) if limit_part else None
            return self.select(table, "*", where_part, order_part, limit)

        raise ValueError("Sorry, I couldn't understand that command.")

    # ---------------------------
    # Convenience: pretty print
    # ---------------------------
    @staticmethod
    def pretty(rows: List[Dict[str, Any]]) -> str:
        if not rows:
            return "(no rows)"
        cols = list(rows[0].keys())
        widths = [max(len(str(c)), max(len(str(r.get(c, ""))) for r in rows)) for c in cols]
        line = " | ".join(str(c).ljust(w) for c, w in zip(cols, widths))
        sep = "-+-".join("-" * w for w in widths)
        body = "\n".join(" | ".join(str(r.get(c, "")).ljust(w) for c, w in zip(cols, widths)) for r in rows)
        return f"{line}\n{sep}\n{body}"


# Example usage (uncomment to try):
# if __name__ == "__main__":
#     db = HumanDB()
#     db.create_table("users", {"id": "INTEGER", "name": "TEXT", "age": "INTEGER"}, primary_key="id")
#     db.insert("users", {"id": 1, "name": "Alice", "age": 30})
#     db.insert("users", [{"id": 2, "name": "Bob", "age": 25}, {"id": 3, "name": "Carol"}])
#     print("Tables:", db.tables())
#     print("Describe users:", db.describe("users"))
#     print(HumanDB.pretty(db.select("users")))
#     print(HumanDB.pretty(db.select("users", where="age >= 26 or name contains 'ar'")))
#     print("Updated:", db.update("users", {"age": 31}, where={"name": "Alice"}))
#     print("Deleted:", db.delete("users", "age < 26"))
#     print(HumanDB.pretty(db.select("users")))
#     # Natural language:
#     print(db.ask("insert into users name='Dave', age=28"))
#     print(HumanDB.pretty(db.ask("show users where age > 27 order by age desc limit 3")))
------------------------------------------------------------

PROMPT 184:
"Generate Python code that imports the required modules and annotates the boto3 MediaTailor 1.34.5 service using mypy-boto3-builder 7.23.0, like so:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import TypeAnnotator\n\nannotator = TypeAnnotator(MediaTailorClient)\ngenerated_code = annotator.generate()\n```"
RESPONSE:
# Ensure you have:
#   boto3==1.34.5
#   mypy-boto3-builder==7.23.0
# installed in your environment before running this script.

from __future__ import annotations

import sys
import subprocess
from pathlib import Path

# Optional: sanity-check installed versions
try:
    from importlib.metadata import version, PackageNotFoundError  # Python 3.8+
except Exception:  # fallback for very old Pythons
    from importlib_metadata import version, PackageNotFoundError  # type: ignore

def ensure_version(pkg: str, expected: str) -> None:
    try:
        v = version(pkg)
    except PackageNotFoundError:
        raise RuntimeError(f"{pkg} is not installed. Please install {pkg}=={expected}.")
    if v != expected:
        raise RuntimeError(f"{pkg}=={expected} is required, found {pkg}=={v}.")

ensure_version("boto3", "1.34.5")
ensure_version("mypy-boto3-builder", "7.23.0")

# Generate stubs for MediaTailor using mypy-boto3-builder
output_dir = Path.cwd() / "type_stubs"
output_dir.mkdir(parents=True, exist_ok=True)

# Invoke the builder CLI to generate the mypy_boto3_mediatailor package
subprocess.check_call(
    [
        sys.executable,
        "-m",
        "mypy_boto3_builder",
        "--service",
        "mediatailor",
        "--boto3-version",
        "1.34.5",
        "--output",
        str(output_dir),
    ]
)

# Make the generated package importable
if str(output_dir) not in sys.path:
    sys.path.insert(0, str(output_dir))

# Now use the generated types to annotate your MediaTailor client
import boto3
from mypy_boto3_mediatailor import MediaTailorClient

mediatailor_client: MediaTailorClient = boto3.client("mediatailor")

# Example: the variable is now strongly typed for static type checkers like mypy
print(f"Created typed MediaTailor client: {type(mediatailor_client)}")
------------------------------------------------------------

PROMPT 185:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 Firehose 1.34.1 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import TypeVar\nfrom botocore.client import BaseClient\nfrom botocore.types import ServiceMetadata\nfrom mypy_boto3_builder import Botohouse\n\nBotohouse(ServiceMetadata(name=\"Firehose\", version=\"1.34.1\"), BaseClient)\n```"
RESPONSE:
The mypy-boto3-builder project doesn’t provide a runtime API like Botohouse. To annotate Firehose types, install the generated stubs (mypy-boto3-firehose or boto3-stubs[firehose]) and type your boto3 client directly.

Example:
```python
import boto3
from mypy_boto3_firehose import FirehoseClient
from mypy_boto3_firehose.paginator import ListDeliveryStreamsPaginator
from mypy_boto3_firehose.type_defs import PutRecordBatchResponseTypeDef, RecordTypeDef

# Typed Firehose client
firehose: FirehoseClient = boto3.client("firehose")

# Typed paginator
paginator: ListDeliveryStreamsPaginator = firehose.get_paginator("list_delivery_streams")
for page in paginator.paginate(Limit=50):
    print(page["DeliveryStreamNames"])

# Typed call example
def put_records(stream_name: str, payloads: list[bytes]) -> PutRecordBatchResponseTypeDef:
    records: list[RecordTypeDef] = [{"Data": p} for p in payloads]
    return firehose.put_record_batch(
        DeliveryStreamName=stream_name,
        Records=records,
    )
```

Note: Ensure you have the matching stubs installed, for example:
- pip install "boto3-stubs[firehose]==1.34.1"
  or
- pip install "mypy-boto3-firehose==1.34.1"
------------------------------------------------------------

PROMPT 186:
"Generate Python code that imports the required modules and annotates the types for the boto3.ComprehendMedical 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder.generators.comprehend_medical import ComprehendMedicalGenerator\n\n generator = ComprehendMedicalGenerator()\n client: BaseClient = generator.generate()\n```"
RESPONSE:
```python
# Requires:
#   boto3==1.34.*
#   mypy-boto3-comprehendmedical==1.34.0
#   (generated by mypy-boto3-builder==7.21.0)

from typing import List
import boto3

from mypy_boto3_comprehendmedical import ComprehendMedicalClient
from mypy_boto3_comprehendmedical.type_defs import (
    DetectEntitiesV2ResponseTypeDef,
    EntityTypeDef,
)

# Typed Comprehend Medical client
client: ComprehendMedicalClient = boto3.client("comprehendmedical")

# Call with full typing on the response
response: DetectEntitiesV2ResponseTypeDef = client.detect_entities_v2(
    Text="The patient was prescribed 5 mg of Coumadin daily."
)

entities: List[EntityTypeDef] = response["Entities"]
entity_texts: List[str] = [e["Text"] for e in entities]

print(entity_texts)
```
------------------------------------------------------------

PROMPT 187:
"Generate Python code that imports the required modules and annotates the AppMesh 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder.generators.service import ServiceGenerator\n\nappmesh = ServiceGenerator(\n   service_name='appmesh',\n   module_name='boto3_appmesh',\n   client_class=BaseClient,\n   version='1.34.0',\n)\n```"
RESPONSE:
```python
from typing import List
from botocore.client import BaseClient
from mypy_boto3_builder.generators.service import ServiceGenerator

appmesh = ServiceGenerator(
    service_name="appmesh",
    module_name="boto3_appmesh",
    client_class=BaseClient,
    version="1.34.0",
)
```
------------------------------------------------------------

PROMPT 188:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 Inspector service client using mypy-boto3-builder, like so:\n```python\nfrom typing import TYPE_CHECKING\nfrom mypy_boto3_builder import type_annotations as ta\nfrom boto3 import client as boto3_client\n\nif TYPE_CHECKING:\n   from mypy_boto3_builder.type_annotations import Boto3Client  # noqa: F401\n\ninspector = boto3_client('inspector')\n```"
RESPONSE:
```python
from typing import TYPE_CHECKING
from mypy_boto3_builder import type_annotations as ta
from boto3 import client as boto3_client

if TYPE_CHECKING:
    from mypy_boto3_builder.type_annotations import Boto3Client  # noqa: F401

inspector: ta.Boto3Client["inspector"] = boto3_client("inspector")
```
------------------------------------------------------------

PROMPT 189:
"Generate Python code that creates and validates BagIt packages using the `bagit` library, such as:\n```\nimport bagit\n\n# create a new bag\nbag = bagit.Bag()\n\n# add files to the bag\nbag.add_file('file1.txt')\nbag.add_file('file2.txt')\n\n# set bag metadata\nbag.set_metadata('title', 'My Bag')\nbag.set_metadata('creator', 'Me')\n\n# validate the bag\nif bag.validate():\n   print(\"The bag is valid\")\nelse:\n   print(\"The bag is invalid\")\n```"
RESPONSE:
The bagit Python API doesn’t expose a Bag() constructor with add_file/set_metadata. Instead, you create a bag from a directory using make_bag, and validate it by loading it with Bag(path) and calling validate(). Here’s a working example that creates a bag, adds files, sets metadata, and validates:

```python
# pip install bagit

import os
import shutil
from pathlib import Path
import bagit

def create_bag(bag_dir, files, metadata=None, checksums=("sha256",)):
    """
    Create a new BagIt package from a directory of files.

    - bag_dir: directory to become the bag
    - files: iterable of file paths to include
    - metadata: dict of fields for bag-info.txt (e.g., {"Title": "My Bag", "Creator": "Me"})
    - checksums: tuple/list of checksum algorithms (e.g., ("sha256", "md5"))
    """
    bag_dir = Path(bag_dir)
    bag_dir.mkdir(parents=True, exist_ok=True)

    # Copy files into the top-level bag directory; make_bag will place them under data/
    for src in files:
        src = Path(src)
        dst = bag_dir / src.name
        shutil.copy2(src, dst)

    # Create the bag and write manifests/metadata
    bag = bagit.make_bag(
        str(bag_dir),
        bag_info=metadata or {},
        checksums=list(checksums),
    )
    return bag

def add_files_to_existing_bag(bag_dir, files):
    """
    Add files to an existing BagIt package by copying into bag_dir/data and
    re-running make_bag to update manifests and bag-info.
    """
    bag_dir = Path(bag_dir)
    data_dir = bag_dir / "data"
    data_dir.mkdir(parents=True, exist_ok=True)

    for src in files:
        src = Path(src)
        dst = data_dir / src.name
        shutil.copy2(src, dst)

    # Re-run make_bag to update manifests (no metadata change here)
    bag = bagit.make_bag(str(bag_dir))
    return bag

def validate_bag(bag_dir, fast=False):
    """
    Validate a BagIt package. fast=True skips checksum verification of payload files.
    Returns True if valid, False otherwise.
    """
    bag = bagit.Bag(str(bag_dir))
    try:
        bag.validate(fast=fast)  # raises BagValidationError on failure
        return True
    except bagit.BagValidationError as e:
        print(f"Bag validation failed: {e}")
        return False

if __name__ == "__main__":
    # Example usage

    # Prepare some example files
    Path("file1.txt").write_text("hello\n")
    Path("file2.txt").write_text("world\n")

    # Create a new bag with metadata
    metadata = {
        "Title": "My Bag",
        "Creator": "Me",
        # You can add any additional bag-info fields you need:
        # "Source-Organization": "Example Org",
        # "External-Description": "Sample BagIt package",
    }
    bag_dir = "my-bag"

    create_bag(bag_dir, ["file1.txt", "file2.txt"], metadata, checksums=("sha256", "md5"))

    # Validate the new bag (full checksum verification)
    print("Bag valid after creation?", validate_bag(bag_dir, fast=False))

    # Add another file to the existing bag
    Path("file3.txt").write_text("more data\n")
    add_files_to_existing_bag(bag_dir, ["file3.txt"])

    # Validate again after updating
    print("Bag valid after adding file?", validate_bag(bag_dir, fast=False))

    # Read back metadata if desired
    bag = bagit.Bag(bag_dir)
    print("Bag metadata (bag-info.txt):")
    for k, v in bag.info.items():
        print(f"  {k}: {v}")
```

Notes:
- To “add files,” copy them into bag_dir/data and re-run bagit.make_bag on the existing bag to regenerate manifests.
- bag.validate() raises BagValidationError on failure; use try/except (as shown) for robust checks.
- Metadata keys in bag-info.txt are exactly what you provide in the dict (e.g., Title, Creator).
------------------------------------------------------------

PROMPT 190:
"Generate Python code that sends a response object to a custom resource using an Amazon S3 presigned URL:\n```\nimport boto3\n\ns3 = boto3.client('s3')\n\n# Create a presigned URL for the custom resource\npresigned_url = s3.generate_presigned_url(\n   'put_object',\n   Params={\n       'Bucket': '<bucket-name>',\n       'Key': '<key>',\n   },\n   Expires=3600 # 1 hour\n)\n\n# Send the response object to the custom resource using the presigned URL\nresponse = requests.put(presigned_url, data='<response-data>')\n```"
RESPONSE:
Here’s a complete Python example that generates an S3 presigned URL for put_object and then uploads a JSON response object to it:

```python
import json
import sys
from typing import Any, Dict, Optional

import boto3
from botocore.exceptions import BotoCoreError, ClientError
import requests


def create_presigned_put_url(
    bucket: str,
    key: str,
    expires_in: int = 3600,
    extra_params: Optional[Dict[str, Any]] = None,
) -> str:
    """
    Create a presigned URL that allows PUT uploads to the specified S3 object.
    """
    s3 = boto3.client("s3")
    params = {"Bucket": bucket, "Key": key}
    if extra_params:
        params.update(extra_params)

    try:
        url = s3.generate_presigned_url(
            ClientMethod="put_object",
            Params=params,
            ExpiresIn=expires_in,
        )
    except (BotoCoreError, ClientError) as e:
        raise RuntimeError(f"Failed to create presigned URL: {e}") from e

    return url


def put_via_presigned_url(
    presigned_url: str,
    body_bytes: bytes,
    content_type: str = "application/json",
    timeout_seconds: int = 30,
) -> None:
    """
    Upload bytes to S3 using a presigned URL (PUT).
    """
    headers = {
        "Content-Type": content_type,
        "Content-Length": str(len(body_bytes)),
    }

    resp = requests.put(presigned_url, data=body_bytes, headers=headers, timeout=timeout_seconds)
    try:
        resp.raise_for_status()
    except requests.HTTPError as e:
        raise RuntimeError(
            f"Upload failed with status {resp.status_code}: {resp.text}"
        ) from e


if __name__ == "__main__":
    # Replace with your values
    BUCKET_NAME = "<bucket-name>"
    OBJECT_KEY = "<key>"  # e.g., "custom-resource/responses/abc123.json"

    # Example response object to send
    response_obj = {
        "Status": "SUCCESS",
        "Reason": "Operation completed",
        "Data": {
            "Message": "Hello from custom resource",
            "Timestamp": "2025-10-29T12:00:00Z",
        },
    }

    try:
        url = create_presigned_put_url(
            bucket=BUCKET_NAME,
            key=OBJECT_KEY,
            expires_in=3600,  # 1 hour
            # If you want to require certain headers, include them in Params here.
            # For example, to require a specific ContentType:
            # extra_params={"ContentType": "application/json"}
        )

        body = json.dumps(response_obj, separators=(",", ":")).encode("utf-8")
        put_via_presigned_url(url, body, content_type="application/json")
        print("Response uploaded successfully.")
    except Exception as exc:
        print(f"Error: {exc}", file=sys.stderr)
        sys.exit(1)
```

Notes:
- Ensure your AWS credentials are configured for boto3 (environment variables, shared config, or IAM role).
- If you include headers like ContentType in the Params when generating the URL, you must send the same header with the upload.
------------------------------------------------------------

PROMPT 191:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.Lightsail service using mypy-boto3-builder, like this:\n```python\nfrom typing import Tuple\nfrom botocore.client import LightsailClient\nfrom mypy_boto3_builder import TypeAnnotator\n\n lightsail_client = LightsailClient(\n   region='us-west-2',\n   endpoint='https://lightsail.us-west-2.amazonaws.com/',\n)\n\ntype_annotations = TypeAnnotator(lightsail_client).get_type_annotations()\n\n# Print the type annotations\nprint(type_annotations)\n```"
RESPONSE:
# pip install boto3 mypy-boto3-lightsail
from typing import Dict, Any, get_type_hints
from inspect import getmembers
import boto3
from mypy_boto3_lightsail import LightsailClient

# Create a typed Lightsail client using mypy-boto3-lightsail (generated by mypy-boto3-builder)
lightsail_client: LightsailClient = boto3.client(
    "lightsail",
    region_name="us-west-2",
    endpoint_url="https://lightsail.us-west-2.amazonaws.com/",
)

def collect_type_annotations(protocol_cls: type) -> Dict[str, Dict[str, str]]:
    annotations: Dict[str, Dict[str, str]] = {}
    for name, member in getmembers(protocol_cls):
        if name.startswith("_") or not callable(member):
            continue
        try:
            hints = get_type_hints(member)
        except Exception:
            hints = {}
        annotations[name] = {k: str(v) for k, v in hints.items()}
    return annotations

type_annotations = collect_type_annotations(LightsailClient)

# Print the type annotations
print(type_annotations)
------------------------------------------------------------

PROMPT 192:
"Generate Python code that imports the necessary libraries and annotates the types of the boto3.ServerlessApplicationRepository 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import Optional\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder.generators import ServiceGenerator\n\ngenerator = ServiceGenerator(\n   service_name='serverlessapplicationrepository',\n   version='1.34.0',\n   builder_version='7.21.0',\n)\n\ngenerated_code = generator.generate()\n\n# Print the generated code\nprint(generated_code)\n```"
RESPONSE:
# Requires:
#   pip install mypy-boto3-builder==7.21.0 boto3==1.34.0 botocore==1.34.0
# This uses mypy-boto3-builder to generate type annotations for the
# Serverless Application Repository service (boto3 1.34.0) and prints
# the generated code.

from typing import List
from pathlib import Path
import tempfile
import shutil

from mypy_boto3_builder.builder import Builder
from mypy_boto3_builder.service_name import ServiceNameCatalog


def generate_serverless_app_repo_types(
    version: str = "1.34.0",
    builder_version: str = "7.21.0",
) -> str:
    # Build into a temporary directory
    output_dir = Path(tempfile.mkdtemp(prefix="mypy-boto3-builder-"))
    try:
        builder = Builder(
            service_names=[ServiceNameCatalog.serverlessapplicationrepository],
            version=version,
            builder_version=builder_version,
            output_path=output_dir,
        )
        # Generate the stubs/docs/package for the requested service
        builder.build()

        # Find the generated service package folder
        # The importable package is named "mypy_boto3_serverlessapplicationrepository"
        candidates = list(output_dir.rglob("mypy_boto3_serverlessapplicationrepository"))
        if not candidates:
            raise RuntimeError("Generated package not found")
        package_dir = candidates[0]

        # Read the generated Python and stub files and concatenate them into a single string
        parts: List[str] = []
        for path in sorted(package_dir.rglob("*")):
            if path.is_file() and path.suffix in {".py", ".pyi"}:
                rel = path.relative_to(package_dir.parent)
                parts.append(f"# File: {rel}\n{path.read_text(encoding='utf-8')}")
        return "\n\n".join(parts)
    finally:
        # Clean up the temporary directory
        shutil.rmtree(output_dir, ignore_errors=True)


if __name__ == "__main__":
    generated_code = generate_serverless_app_repo_types()
    print(generated_code)
------------------------------------------------------------

PROMPT 193:
"Generate Python code that imports the necessary modules and annotates the boto3.Organizations 1.34.0 service with type hints using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import BotoflowPlugin\n\n# Annotate the boto3.Organizations service\norganizations_service = BaseClient(\n   service=boto3.client('organizations', version='1.34.0'),\n   plugins=[BotoflowPlugin()],\n)\n\n# Example usage: Get an organization\nresponse = organizations_service.get_organization(\n   OrganizationId='ORGANIZATION_ID',\n)\nprint(response.organization)\n```"
RESPONSE:
```python
from __future__ import annotations

import boto3
from mypy_boto3_organizations import OrganizationsClient
from mypy_boto3_organizations.type_defs import (
    DescribeOrganizationResponseTypeDef,
    OrganizationTypeDef,
)

# Typed boto3 Organizations client (boto3==1.34.x, stubs generated by mypy-boto3-builder 7.21.0)
organizations_client: OrganizationsClient = boto3.client("organizations")

# Example usage: Describe the current organization
response: DescribeOrganizationResponseTypeDef = organizations_client.describe_organization()
organization: OrganizationTypeDef = response["Organization"]

print(organization)
# For example, to print the Organization ID:
print(organization["Id"])
```
------------------------------------------------------------

PROMPT 194:
"Generate Python code that imports the required modules and annotates the types of the boto3.Route53Resolver service using mypy-boto3-builder, like this:\n```\nfrom typing import TypeVar, Generic\nfrom botocore.client import BaseClient\nfrom boto3.route53_resolver import Route53Resolver\n\n_RT = TypeVar('_RT', bound='Route53Resolver')\n\nclass MyClient(BaseClient):\n   def __init__(self) -> None:\n       super().__init__()\n       self.route53_resolver: Route53Resolver = Route53Resolver(client=self)\n\n   def resolve(self, *, query: str, timeout: float = ...) -> _RT:\n       return self.route53_resolver.resolve(query=query, timeout=timeout)\n```"
RESPONSE:
```python
from __future__ import annotations

from typing import Optional, Dict, Any
import boto3
from boto3.session import Session

# Types generated by mypy-boto3-builder (install: pip install "boto3-stubs[route53resolver]")
from mypy_boto3_route53resolver import Route53ResolverClient
from mypy_boto3_route53resolver.type_defs import (
    ListResolverRulesResponseTypeDef,
)


class MyClient:
    def __init__(self, session: Optional[Session] = None) -> None:
        self._session = session or boto3.Session()
        # Typed Route 53 Resolver client
        self.route53_resolver: Route53ResolverClient = self._session.client("route53resolver")

    def list_rules(
        self,
        *,
        max_results: Optional[int] = None,
        next_token: Optional[str] = None,
    ) -> ListResolverRulesResponseTypeDef:
        params: Dict[str, Any] = {}
        if max_results is not None:
            params["MaxResults"] = max_results
        if next_token is not None:
            params["NextToken"] = next_token

        return self.route53_resolver.list_resolver_rules(**params)
```
------------------------------------------------------------

PROMPT 195:
"Generate Python code that imports the Flask-Cors package and creates a Flask app with CORS enabled, including a route that returns \"Hello, World!\" when accessed from a different domain.\n\nHere is a possible implementation:\n```\nfrom flask import Flask, request, abort\nfrom flask_cors import CORS\n\napp = Flask(__name__)\napp.config['CORS_ENABLED'] = True\napp.config['CORS_ALLOWED_ORIGINS'] = ['http://example.com']\n\n@app.route('/', methods=['GET'])\ndef hello():\n   return 'Hello, World!'\n\nif __name__ == '__main__':\n   app.run(debug=True)\n```\nThis code creates a Flask app with CORS enabled, allowing requests from `http://example.com` to access the `/` route, which returns \"Hello, World!\"."
RESPONSE:
from flask import Flask
from flask_cors import CORS

app = Flask(__name__)

# Enable CORS for the "/" route, allowing requests from http://example.com
CORS(app, resources={r"/": {"origins": ["http://example.com"]}})

@app.route("/", methods=["GET"])
def hello():
    return "Hello, World!"

if __name__ == "__main__":
    app.run(debug=True)
------------------------------------------------------------

PROMPT 196:
"Generate Python code that imports the required modules and annotates the Pricing service of boto3 version 1.34.0 using mypy-boto3-builder version 7.21.0, like so:\n```\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom boto3.services.pricing import Pricing\n\n# Annotate Pricing service\npricing = Pricing(\n   client=BaseClient(),\n   service_name=\"Pricing\",\n   api_version=\"2016-12-01\"\n)\n\n# Example usage: Get prices for EC2 instances in us-west-2 region\nresponse = pricing.get_products(\n   ServiceCode=\"Amazon Elastic Compute Cloud\",\n   Filters=[{\"Type\": \"TERMINATION_PROTECTION\", \"Value\": \"ENABLED\"}],\n   Region=\"us-west-2\"\n)\n\n# Print the response\nprint(response)\n```"
RESPONSE:
```python
from typing import List

import boto3
from mypy_boto3_pricing import PricingClient
from mypy_boto3_pricing.type_defs import FilterTypeDef, GetProductsResponseTypeDef

# Create a typed Pricing client (pricing endpoints are typically in us-east-1)
pricing: PricingClient = boto3.client("pricing", region_name="us-east-1")

# Example: Get prices for EC2 t3.micro Linux in us-west-2
filters: List[FilterTypeDef] = [
    {"Type": "TERM_MATCH", "Field": "instanceType", "Value": "t3.micro"},
    {"Type": "TERM_MATCH", "Field": "operatingSystem", "Value": "Linux"},
    {"Type": "TERM_MATCH", "Field": "tenancy", "Value": "Shared"},
]

response: GetProductsResponseTypeDef = pricing.get_products(
    ServiceCode="AmazonEC2",
    Filters=filters,
    RegionCode="us-west-2",
    FormatVersion="aws_v1",
    MaxResults=20,
)

print(response)
```
------------------------------------------------------------

PROMPT 197:
"Generate Python code that imports the required modules and annotates the types of the boto3.WAF 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import List, Dict\nfrom botocore.client import BaseClient\nfrom boto3.waf import WAF, Rule, Actions, Trigger, XssMatchSet, RegexpPatternSet, StringMatchSet, IPSet\n\nwaf = WAF(\n   client=BaseClient,\n   config={\n       'region': 'us-east-1',\n       'describe_limits': {'max_items': 100},\n   },\n)\n\nrules = waf.list_rules(Name='ExampleRule')\n\nfor rule in rules['Rules']:\n   print(rule['RuleId'])\n\nactions = waf.list_actions(RuleId=rule['RuleId'])\n\nfor action in actions['Actions']:\n   print(action['ActionType'])\n\ntriggers = waf.list_triggers(RuleId=rule['"
RESPONSE:
```python
# Requires:
#   boto3==1.34.0
#   boto3-stubs[waf]==1.34.0  # generated by mypy-boto3-builder==7.21.0
# pip install "boto3==1.34.0" "boto3-stubs[waf]==1.34.0"

from typing import List

import boto3
from boto3.session import Session

from mypy_boto3_waf.client import WAFClient
from mypy_boto3_waf.type_defs import (
    ListRulesOutputTypeDef,
    RuleSummaryTypeDef,
    ListWebACLsOutputTypeDef,
    WebACLSummaryTypeDef,
    GetWebACLOutputTypeDef,
    ActivatedRuleTypeDef,
    WafActionTypeDef,
    ListXssMatchSetsOutputTypeDef,
    XssMatchSetSummaryTypeDef,
    ListRegexPatternSetsOutputTypeDef,
    RegexPatternSetSummaryTypeDef,
    ListIPSetsOutputTypeDef,
    IPSetSummaryTypeDef,
)

# WAF Classic is a global service that uses the us-east-1 endpoint
session: Session = boto3.session.Session(region_name="us-east-1")
waf: WAFClient = session.client("waf")

# List rules
rules_resp: ListRulesOutputTypeDef = waf.list_rules(Limit=100)
rules: List[RuleSummaryTypeDef] = rules_resp["Rules"]

for rule in rules:
    print(f"RuleId: {rule['RuleId']}, Name: {rule.get('Name', '')}")

# List WebACLs and show actions of activated rules
webacls_resp: ListWebACLsOutputTypeDef = waf.list_web_acls(Limit=100)
webacls: List[WebACLSummaryTypeDef] = webacls_resp["WebACLs"]

for webacl_summary in webacls:
    webacl_id: str = webacl_summary["WebACLId"]
    webacl_name: str = webacl_summary.get("Name", "")

    webacl_resp: GetWebACLOutputTypeDef = waf.get_web_acl(WebACLId=webacl_id)
    activated_rules: List[ActivatedRuleTypeDef] = webacl_resp["WebACL"]["Rules"]

    print(f"WebACL: {webacl_name} ({webacl_id})")
    for activated_rule in activated_rules:
        action: WafActionTypeDef = activated_rule["Action"]
        print(
            f"  Priority: {activated_rule['Priority']}, "
            f"RuleId: {activated_rule['RuleId']}, "
            f"Action: {action['Type']}"
        )

# List XSS match sets
xss_resp: ListXssMatchSetsOutputTypeDef = waf.list_xss_match_sets(Limit=100)
xss_sets: List[XssMatchSetSummaryTypeDef] = xss_resp["XssMatchSets"]

for xss in xss_sets:
    print(f"XssMatchSet: {xss.get('Name', '')} ({xss['XssMatchSetId']})")

# List Regex pattern sets
regex_resp: ListRegexPatternSetsOutputTypeDef = waf.list_regex_pattern_sets(Limit=100)
regex_sets: List[RegexPatternSetSummaryTypeDef] = regex_resp["RegexPatternSets"]

for rps in regex_sets:
    print(f"RegexPatternSet: {rps.get('Name', '')} ({rps['RegexPatternSetId']})")

# List IP sets
ipsets_resp: ListIPSetsOutputTypeDef = waf.list_ip_sets(Limit=100)
ip_sets: List[IPSetSummaryTypeDef] = ipsets_resp["IPSets"]

for ipset in ip_sets:
    print(f"IPSet: {ipset.get('Name', '')} ({ipset['IPSetId']})")
```
------------------------------------------------------------

PROMPT 198:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.EMR 1.34.9 service using mypy-boto3-builder 7.23.0, like this:\n```\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import BotocorePlugin\n\n# Annotate the types of the boto3.EMR 1.34.9 service\nclass EMR(BaseClient):\n   def list_clusters(self) -> List[dict]:\n       return self.get_paginator('list_clusters').paginate()\n\n   def create_cluster(self, release_label: str) -> dict:\n       return self.create_resource('cluster', release_label=release_label)\n\n   def delete_cluster(self, cluster_id: str) -> None:\n       self.delete_resource('cluster', cluster_id)\n\n# Register the type annotations for the boto3.EMR 1.34.9 service\nBotocorePlugin(EMR).register"
RESPONSE:
# Requires:
#   pip install boto3==1.34.9 boto3-stubs[emr]==1.34.9
# The stubs are generated by mypy-boto3-builder 7.23.0.

from __future__ import annotations

from typing import List, Optional

import boto3
from mypy_boto3_emr import EMRClient
from mypy_boto3_emr.paginator import ListClustersPaginator
from mypy_boto3_emr.type_defs import (
    ClusterSummaryTypeDef,
    JobFlowInstancesConfigTypeDef,
    RunJobFlowOutputTypeDef,
)


class EMR:
    def __init__(self, client: Optional[EMRClient] = None) -> None:
        self._client: EMRClient = client or boto3.client("emr")

    def list_clusters(self) -> List[ClusterSummaryTypeDef]:
        paginator: ListClustersPaginator = self._client.get_paginator("list_clusters")
        clusters: List[ClusterSummaryTypeDef] = []
        for page in paginator.paginate():
            clusters.extend(page.get("Clusters", []))
        return clusters

    def create_cluster(
        self,
        name: str,
        release_label: str,
        instances: JobFlowInstancesConfigTypeDef,
        log_uri: Optional[str] = None,
    ) -> RunJobFlowOutputTypeDef:
        params = dict(Name=name, ReleaseLabel=release_label, Instances=instances)
        if log_uri is not None:
            params["LogUri"] = log_uri
        return self._client.run_job_flow(**params)

    def delete_cluster(self, cluster_id: str) -> None:
        self._client.terminate_job_flows(JobFlowIds=[cluster_id])
------------------------------------------------------------

PROMPT 199:
"Generate Python code that imports the required modules and annotates the TimestreamWrite service of boto3 version 1.34.0 using mypy-boto3-builder version 7.21.0, like so:\n```python\nfrom typing import Callable\nfrom botocore.client import BaseClient\nfrom boto3.annotations import ServiceAnnotation, AnnotationArguments\nfrom mypy_boto3_builder import build_service\n\nService = ServiceAnnotation(\n   service=\"timestream-write\",\n   version=\"1.34.0\",\n   client=BaseClient,\n   annotations={\n       \"param_validation\": {\n           \"type\": \"callable\",\n           \"args\": [\"self\", \"params\"]\n       },\n       \"return_response\": {\n           \"type\": \"callable\",\n           \"args\": [\"self\", \"response\"]\n       }\n   }\n)\n\ngenerated_service = build_service(Service)\n```"
RESPONSE:
from typing import Callable, Any, Mapping, MutableMapping
import boto3
from botocore.client import BaseClient
from mypy_boto3_timestream_write import TimestreamWriteClient

# Callback type annotations
ParamValidation = Callable[[TimestreamWriteClient, MutableMapping[str, Any]], None]
ReturnResponse = Callable[[TimestreamWriteClient, Mapping[str, Any]], None]

def build_timestream_write_service(
    param_validation: ParamValidation,
    return_response: ReturnResponse,
) -> TimestreamWriteClient:
    """
    Creates a typed Timestream Write client (boto3==1.34.0) and wires typed callbacks
    that mimic param_validation and return_response hooks.
    """
    client: TimestreamWriteClient = boto3.client("timestream-write")

    # Bridge botocore event kwargs to typed callbacks
    def _before_parameter_build(**kwargs: Any) -> None:
        params = kwargs.get("params", {})
        param_validation(client, params)

    def _after_call(**kwargs: Any) -> None:
        parsed = kwargs.get("parsed", {})
        return_response(client, parsed)

    client.meta.events.register("before-parameter-build.timestream-write", _before_parameter_build)
    client.meta.events.register("after-call.timestream-write", _after_call)

    return client

# Example usage with typed callbacks
def _param_validation(self: TimestreamWriteClient, params: MutableMapping[str, Any]) -> None:
    # Add validation for request params here
    pass

def _return_response(self: TimestreamWriteClient, response: Mapping[str, Any]) -> None:
    # Inspect the parsed service response here
    pass

generated_service: TimestreamWriteClient = build_timestream_write_service(
    param_validation=_param_validation,
    return_response=_return_response,
)
------------------------------------------------------------

PROMPT 200:
"Generate Python code that imports the necessary modules and annotates the types of the Elastic Load Balancing service client in boto3 version 1.34.0 using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import Optional\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import type_annotations as ta\n\nclass ElasticLoadBalancing(BaseClient):\n   @ta.optional((\"load_balancer\", str))\n   def create_load_balancer(self, Name: str, AvailabilityZones: list[str], Subnets: list[str]) -> dict:\n       pass\n   \n   @ta.optional((\"load_balancer\", str))\n   def DescribeLoadBalancers(self, LoadBalancerArns: list[str]) -> list[dict]:\n       pass\n   \n   @ta.optional((\"load_balancer\", str))\n   def UpdateLoadBalancer(self, LoadBalancerArn: str, Name: str, AvailabilityZones: list[str], Sub"
RESPONSE:
```python
from __future__ import annotations

from typing import Any, Dict, Mapping, Optional, Sequence
from typing_extensions import Literal
from botocore.client import BaseClient


class ElasticLoadBalancing(BaseClient):
    def create_load_balancer(
        self,
        *,
        Name: str,
        Subnets: Optional[Sequence[str]] = None,
        SubnetMappings: Optional[Sequence[Mapping[str, Any]]] = None,
        SecurityGroups: Optional[Sequence[str]] = None,
        Scheme: Optional[Literal["internet-facing", "internal"]] = None,
        Tags: Optional[Sequence[Mapping[str, str]]] = None,
        Type: Optional[Literal["application", "network", "gateway"]] = None,
        IpAddressType: Optional[
            Literal["ipv4", "dualstack", "dualstack-without-public-ipv4"]
        ] = None,
        CustomerOwnedIpv4Pool: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Corresponds to elbv2.create_load_balancer in boto3 1.34.0.
        """
        raise NotImplementedError

    def describe_load_balancers(
        self,
        *,
        LoadBalancerArns: Optional[Sequence[str]] = None,
        Names: Optional[Sequence[str]] = None,
        Marker: Optional[str] = None,
        PageSize: Optional[int] = None,
    ) -> Dict[str, Any]:
        """
        Corresponds to elbv2.describe_load_balancers in boto3 1.34.0.
        """
        raise NotImplementedError

    def set_subnets(
        self,
        *,
        LoadBalancerArn: str,
        Subnets: Optional[Sequence[str]] = None,
        SubnetMappings: Optional[Sequence[Mapping[str, Any]]] = None,
    ) -> Dict[str, Any]:
        """
        Corresponds to elbv2.set_subnets in boto3 1.34.0.
        """
        raise NotImplementedError

    def set_security_groups(
        self,
        *,
        LoadBalancerArn: str,
        SecurityGroups: Sequence[str],
    ) -> Dict[str, Any]:
        """
        Corresponds to elbv2.set_security_groups in boto3 1.34.0.
        """
        raise NotImplementedError

    def set_ip_address_type(
        self,
        *,
        LoadBalancerArn: str,
        IpAddressType: Literal[
            "ipv4", "dualstack", "dualstack-without-public-ipv4"
        ],
    ) -> Dict[str, Any]:
        """
        Corresponds to elbv2.set_ip_address_type in boto3 1.34.0.
        """
        raise NotImplementedError

    def modify_load_balancer_attributes(
        self,
        *,
        LoadBalancerArn: str,
        Attributes: Sequence[Mapping[str, Any]],
    ) -> Dict[str, Any]:
        """
        Corresponds to elbv2.modify_load_balancer_attributes in boto3 1.34.0.
        """
        raise NotImplementedError
```
------------------------------------------------------------

PROMPT 201:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.CloudWatchLogs service using mypy-boto3-builder, like this:\n```\nfrom typing import TypeVar, Type Alias\nfrom mypy_boto3_builder import TypeAnnotator\nimport boto3\n\nannotator = TypeAnnotator(boto3)\nannotator.register_service(\"cloudwatchlogs\")\n\n# Define type aliases for CloudWatch Logs resources\nclass CloudWatchLogsService:\n   def __init__(self, client: boto3.client(\"cloudwatchlogs\")) -> None:\n       self.client = client\n   \n   def put_log_events(self, log_group_name: str, log_stream_name: str, events: List[dict]) -> None:\n       self.client.put_log_events(log_group_name, log_stream_name, events)\n   \n   def get_log_events(self, log_group_name: str, log_stream_name: str, next_token: str) -> List[dict]:\n       return self"
RESPONSE:
from typing import Optional, Sequence
import boto3

# Types are provided by the mypy-boto3-cloudwatch-logs package (generated by mypy-boto3-builder)
from mypy_boto3_cloudwatch_logs import CloudWatchLogsClient
from mypy_boto3_cloudwatch_logs.type_defs import (
    InputLogEventTypeDef,
    PutLogEventsResponseTypeDef,
    GetLogEventsResponseTypeDef,
)


class CloudWatchLogsService:
    def __init__(self, client: CloudWatchLogsClient) -> None:
        self.client = client

    def put_log_events(
        self,
        log_group_name: str,
        log_stream_name: str,
        events: Sequence[InputLogEventTypeDef],
        sequence_token: Optional[str] = None,
    ) -> PutLogEventsResponseTypeDef:
        return self.client.put_log_events(
            logGroupName=log_group_name,
            logStreamName=log_stream_name,
            logEvents=list(events),
            sequenceToken=sequence_token,
        )

    def get_log_events(
        self,
        log_group_name: str,
        log_stream_name: str,
        *,
        next_token: Optional[str] = None,
        start_time: Optional[int] = None,
        end_time: Optional[int] = None,
        limit: Optional[int] = None,
        start_from_head: Optional[bool] = None,
    ) -> GetLogEventsResponseTypeDef:
        return self.client.get_log_events(
            logGroupName=log_group_name,
            logStreamName=log_stream_name,
            nextToken=next_token,
            startTime=start_time,
            endTime=end_time,
            limit=limit,
            startFromHead=start_from_head,
        )


def get_service() -> CloudWatchLogsService:
    session = boto3.Session()
    # CloudWatch Logs service name is "logs"
    client: CloudWatchLogsClient = session.client("logs")
    return CloudWatchLogsService(client)
------------------------------------------------------------

PROMPT 202:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 AutoScalingPlans service version 1.34.0 using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import List, Dict\nfrom boto3 import ServiceResource\n\n# Define the types of the boto3 AutoScalingPlans service version 1.34.0\nclass AutoScalingPlans(ServiceResource):\n   def describe_ scaling_plans(self) -> List[dict]:\n       \"\"\"\n       Returns a list of dictionaries representing the current scaling plans.\n       \"\"\"\n       return self.client.describe_scaling_plans()\n\n   def create_scaling_plan(self, name: str, max_size: int, min_size: int, vpc_zone_identifier: str) -> dict:\n       \"\"\"\n       Creates a new scaling plan.\n       \"\"\"\n       return self.client.create_scaling_plan(name, max_size, min_size, vpc_zone_identifier)\n\n   def update_"
RESPONSE:
```python
from __future__ import annotations

from datetime import datetime
from typing import Any, Optional, Sequence

import boto3
from mypy_boto3_autoscaling_plans import AutoScalingPlansClient
from mypy_boto3_autoscaling_plans.type_defs import (
    ApplicationSourceTypeDef,
    CreateScalingPlanResponseTypeDef,
    DescribeScalingPlanResourcesResponseTypeDef,
    DescribeScalingPlansResponseTypeDef,
    GetScalingPlanResourceForecastDataResponseTypeDef,
    ScalingInstructionTypeDef,
)


def get_autoscaling_plans_client() -> AutoScalingPlansClient:
    """
    Return a typed AutoScalingPlans boto3 client (boto3>=1.34.0 with mypy-boto3-builder>=7.21.0).
    """
    return boto3.client("autoscaling-plans")


class AutoScalingPlansService:
    """
    A small typed wrapper around the boto3 AutoScalingPlans client.
    """

    def __init__(self, client: Optional[AutoScalingPlansClient] = None) -> None:
        self.client: AutoScalingPlansClient = client or get_autoscaling_plans_client()

    def describe_scaling_plans(
        self,
        *,
        scaling_plan_names: Optional[Sequence[str]] = None,
        application_sources: Optional[Sequence[ApplicationSourceTypeDef]] = None,
        max_results: Optional[int] = None,
        next_token: Optional[str] = None,
    ) -> DescribeScalingPlansResponseTypeDef:
        """
        List scaling plans with optional filters/pagination.
        """
        params: dict[str, Any] = {}
        if scaling_plan_names is not None:
            params["ScalingPlanNames"] = list(scaling_plan_names)
        if application_sources is not None:
            params["ApplicationSources"] = list(application_sources)
        if max_results is not None:
            params["MaxResults"] = max_results
        if next_token is not None:
            params["NextToken"] = next_token

        return self.client.describe_scaling_plans(**params)

    def create_scaling_plan(
        self,
        *,
        scaling_plan_name: str,
        application_source: ApplicationSourceTypeDef,
        scaling_instructions: Sequence[ScalingInstructionTypeDef],
    ) -> CreateScalingPlanResponseTypeDef:
        """
        Create a new scaling plan.
        """
        return self.client.create_scaling_plan(
            ScalingPlanName=scaling_plan_name,
            ApplicationSource=application_source,
            ScalingInstructions=list(scaling_instructions),
        )

    def update_scaling_plan(
        self,
        *,
        scaling_plan_name: str,
        application_source: ApplicationSourceTypeDef,
        scaling_instructions: Sequence[ScalingInstructionTypeDef],
    ) -> dict[str, Any]:
        """
        Update an existing scaling plan.
        Returns an empty dict with ResponseMetadata when successful.
        """
        return self.client.update_scaling_plan(
            ScalingPlanName=scaling_plan_name,
            ApplicationSource=application_source,
            ScalingInstructions=list(scaling_instructions),
        )

    def delete_scaling_plan(
        self,
        *,
        scaling_plan_name: str,
        scaling_plan_version: int,
    ) -> dict[str, Any]:
        """
        Delete a scaling plan by name and version.
        Returns an empty dict with ResponseMetadata when successful.
        """
        return self.client.delete_scaling_plan(
            ScalingPlanName=scaling_plan_name,
            ScalingPlanVersion=scaling_plan_version,
        )

    def describe_scaling_plan_resources(
        self,
        *,
        scaling_plan_name: str,
        scaling_plan_version: int,
        max_results: Optional[int] = None,
        next_token: Optional[str] = None,
    ) -> DescribeScalingPlanResourcesResponseTypeDef:
        """
        List resources in a scaling plan with pagination.
        """
        params: dict[str, Any] = {
            "ScalingPlanName": scaling_plan_name,
            "ScalingPlanVersion": scaling_plan_version,
        }
        if max_results is not None:
            params["MaxResults"] = max_results
        if next_token is not None:
            params["NextToken"] = next_token

        return self.client.describe_scaling_plan_resources(**params)

    def get_scaling_plan_resource_forecast_data(
        self,
        *,
        scaling_plan_name: str,
        scaling_plan_version: int,
        service_namespace: str,
        resource_id: str,
        scalable_dimension: str,
        forecast_data_type: str,
        start_time: datetime,
        end_time: datetime,
    ) -> GetScalingPlanResourceForecastDataResponseTypeDef:
        """
        Get forecast data for a specific resource in a scaling plan.
        """
        return self.client.get_scaling_plan_resource_forecast_data(
            ScalingPlanName=scaling_plan_name,
            ScalingPlanVersion=scaling_plan_version,
            ServiceNamespace=service_namespace,
            ResourceId=resource_id,
            ScalableDimension=scalable_dimension,
            ForecastDataType=forecast_data_type,
            StartTime=start_time,
            EndTime=end_time,
        )
```
------------------------------------------------------------

PROMPT 203:
"Generate Python code that implements a module allowing shell-like infix syntax using pipes:\n```python\nimport tokenize\n\ndef infix_syntax(left, op, right):\n   return f\"{left} {op} {right}\"\n\ndef pipe_tokenizer(tokens):\n   return itertools.chain(*[infix_syntax(t[0], t[1], t[2]) for t in zip(tokens[::2], tokens[1::2], tokens[2::3])])\n\ntokenize.tokengen(pipe_tokenizer)\n```\nThis code defines an `infix_syntax` function that takes three arguments and returns a string representing the infixed expression. The `pipe_tokenizer` function is then defined to iterate over the input tokens, applying the `infix_syntax` function to each pair of adjacent tokens, and returning an iterator over the resulting expressions. Finally, the `tokenize.tokengen` function is called with the `pipe_tokenizer` function as its argument, which will apply the infix syntax to any input passed through it."
RESPONSE:
# infixpipe.py
# A tiny module that enables shell-like infix syntax using the | operator.

from typing import Any, Callable, Dict, Tuple


class PipeOp:
    __slots__ = ("_f", "_args", "_kwargs")

    def __init__(self, f: Callable[..., Any], args: Tuple[Any, ...] = (), kwargs: Dict[str, Any] | None = None):
        self._f = f
        self._args = args
        self._kwargs = {} if kwargs is None else kwargs

    def __call__(self, *args, **kwargs):
        # Supports partial application: op(a, b) returns a new pipe op expecting the piped value first.
        if not args and not kwargs:
            return self
        return PipeOp(self._f, self._args + args, {**self._kwargs, **kwargs})

    def _apply(self, value: Any) -> Any:
        return self._f(value, *self._args, **self._kwargs)

    def __ror__(self, value: Any) -> Any:
        # Enables: value | op
        return self._apply(value)

    def __or__(self, other):
        # Composition: (value | op1) | op2  -> op2(op1(value))
        if isinstance(other, PipeOp):
            return PipeOp(lambda x: other._apply(self._apply(x)))
        if callable(other):
            return PipeOp(lambda x: other(self._apply(x)))
        return NotImplemented

    def __repr__(self):
        name = getattr(self._f, "__name__", repr(self._f))
        return f"<pipe {name} args={self._args} kwargs={self._kwargs}>"


def pipe(func: Callable[..., Any]) -> PipeOp:
    """
    Decorator that makes a function pipeable. The function must accept the value
    being piped as its first argument.

    Example:
      @pipe
      def strip(s: str) -> str:
          return s.strip()

      @pipe
      def replace(s: str, old: str, new: str) -> str:
          return s.replace(old, new)

      result = "  hi  " | strip | replace("h", "H")
    """
    return PipeOp(func)


def P(func: Callable[..., Any]) -> PipeOp:
    """
    Wrap any single-argument callable so it can be used in a pipe.

    Example:
      result = "  hi  " | P(str.upper)
      nums = range(5) | P(list)
    """
    return PipeOp(lambda x, *a, **k: func(x, *a, **k))


def call(name: str, *cargs, **ckwargs) -> PipeOp:
    """
    Call a method on the piped value.

    Example:
      result = "  hi  " | call("strip") | call("upper")
    """
    return PipeOp(lambda obj: getattr(obj, name)(*cargs, **ckwargs))


def attr(name: str) -> PipeOp:
    """
    Access an attribute of the piped value.

    Example:
      result = some_obj | attr("name")
    """
    return PipeOp(lambda obj: getattr(obj, name))


def item(key: Any) -> PipeOp:
    """
    Index into the piped value with key.

    Example:
      result = {"a": 1} | item("a")
      second = [10, 20, 30] | item(1)
    """
    return PipeOp(lambda obj: obj[key])


# Example usage and quick tests
if __name__ == "__main__":
    @pipe
    def strip(s: str) -> str:
        return s.strip()

    @pipe
    def replace(s: str, old: str, new: str) -> str:
        return s.replace(old, new)

    @pipe
    def split(s: str, sep: str | None = None) -> list[str]:
        return s.split(sep)

    @pipe
    def take(xs, n: int):
        return list(xs)[:n]

    text = "  hello world  " | strip | call("title") | replace("World", "Pipe")
    assert text == "Hello Pipe"

    up = "hi" | P(str.upper)
    assert up == "HI"

    words = "a b c d e" | split() | take(3) | P(tuple)
    assert words == ("a", "b", "c")

    second = [10, 20, 30] | item(1)
    assert second == 20

    # Composition
    upper_then_len = P(str.upper) | P(len)
    assert ("hi" | upper_then_len) == 2

    print("All examples passed.")
------------------------------------------------------------

PROMPT 204:
"Generate Python code that creates a dictionary-like object that ignores NoneType values, similar to the `defaultdict` function from the `collections` module, but with the additional feature of ignoring NoneType values.\n\nHere is an example of how you could use this dict-like object:\n```\nmy_dict = MyDict({'a': 1, 'b': None, 'c': 3})\nprint(my_dict['a']) # output: 1\nprint(my_dict['b']) # output: None\nprint(my_dict['c']) # output: 3\n```\nThis code would create a dictionary-like object called `MyDict` that has the same features as the `defaultdict` function, but also ignores NoneType values. The `MyDict` object would have the following methods:\n\n* `__init__`: Initializes the `MyDict` object with a dictionary of default values.\n* `get()`: Returns the value associated with the given key, or the default value if the key is not present in the dictionary and the value is not NoneType.\n* `keys()`: Returns a list of all keys in the dictionary.\n"
RESPONSE:
```python
from collections import UserDict
from typing import Any, Callable, Mapping, Optional


class MyDict(UserDict):
    """
    A dict-like object similar to collections.defaultdict but with the additional
    behavior of ignoring None defaults:
      - If a missing key is accessed via indexing and the default_factory returns
        None, the key is not inserted and None is returned.
      - get() returns the provided default only if it is not None; otherwise it
        returns None (and does not insert).
      - Explicitly stored None values are preserved and returned as-is.
    """

    def __init__(
        self,
        initial: Optional[Mapping[Any, Any]] = None,
        default_factory: Optional[Callable[[], Any]] = None,
    ):
        super().__init__()
        self.default_factory = default_factory
        if initial:
            # Preserve user-provided None values.
            self.data.update(initial)

    def __missing__(self, key: Any) -> Any:
        """
        defaultdict-like behavior: use default_factory for missing keys.
        If the factory returns None, do not insert the key (ignore).
        """
        if self.default_factory is None:
            raise KeyError(key)
        value = self.default_factory()
        if value is None:
            # Ignore None defaults: do not insert the missing key
            return None
        self.data[key] = value
        return value

    def __getitem__(self, key: Any) -> Any:
        try:
            return self.data[key]
        except KeyError:
            return self.__missing__(key)

    def get(self, key: Any, default: Optional[Any] = None) -> Optional[Any]:
        """
        Return the value for key if key is in the dictionary.
        If not, return 'default' only if it is not None; otherwise return None.
        This does not insert the key.
        """
        if key in self.data:
            return self.data[key]
        return default if default is not None else None

    def keys(self) -> list:
        """
        Return a list of keys (not a view) as requested.
        """
        return list(self.data.keys())


# Example usage:
if __name__ == "__main__":
    # Matches the example in the prompt
    my_dict = MyDict({'a': 1, 'b': None, 'c': 3})
    print(my_dict['a'])  # output: 1
    print(my_dict['b'])  # output: None
    print(my_dict['c'])  # output: 3

    # defaultdict-like behavior with ignore-None default:
    # Missing key access will attempt to create a default; if it's None, it won't insert.
    d = MyDict(default_factory=lambda: None)
    print(d.get('x'))     # None
    print('x' in d)       # False
    print(d['y'])         # None (ignored, not inserted)
    print('y' in d)       # False

    e = MyDict(default_factory=list)
    print(e['k'])         # [] (inserted)
    print('k' in e)       # True
```
------------------------------------------------------------

PROMPT 205:
"Generate Python code that exports blocking and asynchronous library versions from a single asynchronous implementation using the `asyncio` module and the `inspect` module, like this:\n```\nimport asyncio\nimport inspect\n\nasync def my_coroutine():\n   # My coroutine code here\n\n# Get the current task name\ntask_name = inspect.currentframe().f_code.co_name\n\n# Check if the task is running synchronously or asynchronously\nif asyncio.iscoroutine(my_coroutine) or asyncio.isfuture(my_coroutine):\n   print(\"Async version:\", task_name)\nelse:\n   print(\"Blocking version:\", task_name)\n```\nThis code defines an asynchronous function called `my_coroutine`, which can be run either synchronously or asynchronously depending on the context in which it is called. The `inspect` module is used to get the name of the currently executing task, and the `asyncio` module is used to check whether the task is being executed synchronously or asynchronously. Depending on the result of this check, the appropriate message"
RESPONSE:
Here’s a robust pattern that exposes both blocking and asynchronous APIs from a single async implementation. It uses asyncio for execution and inspect to report the function name and call-site.

```python
import asyncio
import inspect
import threading
from functools import wraps


def _print_mode(mode: str, fn):
    # Example use of inspect: function name and call-site
    caller = inspect.stack()[1].function
    print(f"{mode}: {fn.__name__} (called from: {caller})")


class _BackgroundLoop:
    """A singleton background event loop to run coroutines from a running loop."""
    _instance = None
    _lock = threading.Lock()

    def __init__(self):
        self._ready = threading.Event()
        self._thread = threading.Thread(target=self._run, name="bg-asyncio-loop", daemon=True)
        self._thread.start()
        self._ready.wait()

    def _run(self):
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
        self._ready.set()
        self.loop.run_forever()

    @classmethod
    def instance(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = _BackgroundLoop()
            return cls._instance

    def run(self, coro):
        # Schedule the coroutine on the background loop and wait for the result.
        fut = asyncio.run_coroutine_threadsafe(coro, self.loop)
        return fut.result()

    def shutdown(self):
        if getattr(self, "loop", None) and self.loop.is_running():
            self.loop.call_soon_threadsafe(self.loop.stop)
            self._thread.join(timeout=2)


def _run_sync(coro):
    # Run a coroutine synchronously.
    try:
        # Are we already inside a running event loop?
        asyncio.get_running_loop()
    except RuntimeError:
        # No loop: safe to use asyncio.run
        return asyncio.run(coro)
    else:
        # We are in an event loop: offload to a background loop/thread.
        # Note: this blocks the current thread. Avoid calling from async code.
        return _BackgroundLoop.instance().run(coro)


def make_sync(async_fn):
    """Wrap an async function to provide a blocking version."""
    @wraps(async_fn)
    def sync_wrapper(*args, **kwargs):
        _print_mode("Blocking version", async_fn)
        return _run_sync(async_fn(*args, **kwargs))
    return sync_wrapper


def export_versions(async_impl, *, base_name=None, into_globals=None):
    """
    Given an async implementation function, return (sync_fn, async_fn),
    and optionally export them into a module's globals:
      - <base_name>        -> sync (blocking) version
      - <base_name>_async  -> async version
    """
    base = base_name or async_impl.__name__.removesuffix("_async")
    sync_fn = make_sync(async_impl)

    @wraps(async_impl)
    async def async_fn(*args, **kwargs):
        _print_mode("Async version", async_impl)
        return await async_impl(*args, **kwargs)

    if into_globals is not None:
        into_globals[base] = sync_fn
        into_globals[f"{base}_async"] = async_fn

    return sync_fn, async_fn


# ===========================
# Example library code below
# ===========================

# Single async implementation
async def _my_coroutine(delay: float = 0.1):
    await asyncio.sleep(delay)
    return "done"


# Export both versions into this module's namespace
my_coroutine, my_coroutine_async = export_versions(
    _my_coroutine, base_name="my_coroutine", into_globals=globals()
)


if __name__ == "__main__":
    # Blocking usage
    print(my_coroutine(0.05))

    # Async usage
    async def main():
        print(await my_coroutine_async(0.05))
        # Avoid calling blocking version from async context (it will block the loop).
        # If absolutely necessary, run it in a thread executor.

    asyncio.run(main())
```

How it works:
- Write one async implementation.
- make_sync wraps it into a blocking function:
  - If no loop is running, it uses asyncio.run.
  - If a loop is already running, it uses a background event loop thread.
- export_versions returns both the blocking and async versions and can inject them into module globals.
- inspect is used to print what version ran and the caller function for clarity.
------------------------------------------------------------

PROMPT 206:
"Generate Python code that imports the `request` module and returns the current request object, such as `request.get_current_request()`"
RESPONSE:
Which framework are you using? Here are two common ways:

Flask
from flask import request

def get_current_request():
    # request is a LocalProxy; this returns the real request object
    return request._get_current_object()

If you literally want request.get_current_request(), define a tiny request.py module
# request.py
from contextvars import ContextVar

_current_request = ContextVar("current_request", default=None)

def set_current_request(req):
    _current_request.set(req)

def get_current_request():
    return _current_request.get()

def clear_current_request():
    _current_request.set(None)

# usage.py
import request

def get_current_request():
    return request.get_current_request()

# Example of wiring (Flask)
# from flask import Flask, request as flask_request
# import request as request_ctx
# app = Flask(__name__)
# @app.before_request
# def _push():
#     request_ctx.set_current_request(flask_request._get_current_object())
# @app.teardown_request
# def _pop(exc):
#     request_ctx.clear_current_request()
------------------------------------------------------------

PROMPT 207:
"Generate Python code that creates a CLI tool using the datadog-check module to create, validate, and run custom checks for Datadog monitoring."
RESPONSE:
#!/usr/bin/env python3
"""
ddcheck: A lightweight CLI to create, validate, and run Datadog custom checks.

Prerequisites:
  pip install datadog-checks-base PyYAML

Usage examples:
  # Create a new check scaffold
  python ddcheck.py create my_check --out ./integrations

  # Validate a check file and its config
  python ddcheck.py validate ./integrations/my_check/my_check.py --config ./integrations/my_check/conf.yaml

  # Run a check once and print emitted metrics/service checks/events
  python ddcheck.py run ./integrations/my_check/my_check.py --config ./integrations/my_check/conf.yaml --times 1
"""
import argparse
import importlib.util
import inspect
import os
import sys
import textwrap
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Type, Union

try:
    import yaml
except Exception as e:
    print("Missing dependency: PyYAML. Install with: pip install PyYAML", file=sys.stderr)
    raise

try:
    from datadog_checks.base import AgentCheck
    from datadog_checks.base.stubs import aggregator as agg
    from datadog_checks.base.stubs import datadog_agent as dda
except Exception as e:
    print(
        "Missing dependency: datadog-checks-base. Install with: pip install datadog-checks-base",
        file=sys.stderr,
    )
    raise


# -----------------------------
# Utilities for dynamic loading
# -----------------------------
def load_module_from_path(path: Union[str, Path], module_name: Optional[str] = None):
    path = str(path)
    if not os.path.exists(path):
        raise FileNotFoundError(f"File not found: {path}")
    if module_name is None:
        module_name = Path(path).stem
    spec = importlib.util.spec_from_file_location(module_name, path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Could not load module from {path}")
    module = importlib.util.module_from_spec(spec)
    sys.modules[module_name] = module
    spec.loader.exec_module(module)
    return module


def find_check_class(module, class_name: Optional[str] = None) -> Type[AgentCheck]:
    candidates = []
    for _, obj in inspect.getmembers(module, inspect.isclass):
        if issubclass(obj, AgentCheck) and obj is not AgentCheck:
            candidates.append(obj)
    if not candidates:
        raise ValueError("No AgentCheck subclass found in the provided module.")
    if class_name:
        for c in candidates:
            if c.__name__ == class_name:
                return c
        raise ValueError(f"AgentCheck subclass named '{class_name}' not found. Available: {[c.__name__ for c in candidates]}")
    if len(candidates) > 1:
        raise ValueError(f"Multiple AgentCheck subclasses found: {[c.__name__ for c in candidates]}. "
                         f"Use --class-name to disambiguate.")
    return candidates[0]


# -----------------------------
# Config loading/normalization
# -----------------------------
@dataclass
class LoadedConfig:
    name: str
    init_config: Dict[str, Any]
    instances: List[Dict[str, Any]]


def load_config(path: Optional[Union[str, Path]], explicit_name: Optional[str] = None) -> LoadedConfig:
    """
    Supports:
      - dict with keys: init_config, instances
      - list of instances
    """
    name = explicit_name or "custom_check"
    init_config: Dict[str, Any] = {}
    instances: List[Dict[str, Any]] = [{}]

    if path:
        with open(path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}

        if isinstance(data, dict) and "instances" in data:
            init_config = data.get("init_config") or {}
            instances = data.get("instances") or []
            if not isinstance(instances, list):
                raise ValueError("config.instances must be a list")
        elif isinstance(data, list):
            instances = data
        else:
            raise ValueError("Config must be a dict with 'instances' or a list of instance dicts")
    return LoadedConfig(name=name, init_config=init_config, instances=instances)


# -----------------------------
# Aggregator output formatting
# -----------------------------
def metric_stub_to_dict(m) -> Dict[str, Any]:
    # Try to be resilient to stub differences across versions
    return {
        "name": getattr(m, "name", getattr(m, "metric", None)),
        "value": getattr(m, "value", None),
        "type": getattr(m, "metric_type", None),
        "tags": list(getattr(m, "tags", []) or []),
        "hostname": getattr(m, "hostname", None),
        "device_name": getattr(m, "device_name", None),
        "timestamp": getattr(m, "timestamp", None),
        "interval": getattr(m, "interval", None),
    }


def service_check_stub_to_dict(sc) -> Dict[str, Any]:
    return {
        "check": getattr(sc, "check", None),
        "status": getattr(sc, "status", None),
        "tags": list(getattr(sc, "tags", []) or []),
        "hostname": getattr(sc, "hostname", None),
        "message": getattr(sc, "message", None),
        "timestamp": getattr(sc, "timestamp", None),
    }


def event_stub_to_dict(ev) -> Dict[str, Any]:
    return {
        "title": getattr(ev, "title", None),
        "text": getattr(ev, "text", None),
        "tags": list(getattr(ev, "tags", []) or []),
        "host": getattr(ev, "host", None),
        "timestamp": getattr(ev, "timestamp", None),
        "alert_type": getattr(ev, "alert_type", None),
        "aggregation_key": getattr(ev, "aggregation_key", None),
        "source_type_name": getattr(ev, "source_type_name", None),
    }


def print_aggregated_output(as_json: bool = False):
    import json

    # The aggregator stub keeps containers in accessible structures
    # Metrics
    metrics_out: List[Dict[str, Any]] = []
    try:
        # agg.metrics is usually a dict[name] -> list[MetricStub]
        for name, stubs in getattr(agg, "metrics", {}).items():
            for m in stubs:
                metrics_out.append(metric_stub_to_dict(m))
    except Exception:
        # fallback: some versions have agg.metrics as list
        for m in getattr(agg, "metrics", []):
            metrics_out.append(metric_stub_to_dict(m))

    # Service checks
    service_checks_out: List[Dict[str, Any]] = []
    try:
        for sc in getattr(agg, "service_checks", []):
            service_checks_out.append(service_check_stub_to_dict(sc))
    except Exception:
        pass

    # Events
    events_out: List[Dict[str, Any]] = []
    try:
        for ev in getattr(agg, "events", []):
            events_out.append(event_stub_to_dict(ev))
    except Exception:
        pass

    payload = {
        "metrics": metrics_out,
        "service_checks": service_checks_out,
        "events": events_out,
    }

    if as_json:
        print(json.dumps(payload, indent=2, sort_keys=True))
        return

    # Pretty print
    if metrics_out:
        print("Metrics:")
        for m in metrics_out:
            print(f"  - {m['name']} value={m['value']} type={m['type']} tags={m['tags']}")
    else:
        print("Metrics: (none)")

    if service_checks_out:
        print("Service checks:")
        for sc in service_checks_out:
            print(f"  - {sc['check']} status={sc['status']} tags={sc['tags']} message={sc['message']}")
    else:
        print("Service checks: (none)")

    if events_out:
        print("Events:")
        for ev in events_out:
            print(f"  - {ev['title']} tags={ev['tags']} text={ev['text']}")
    else:
        print("Events: (none)")


# -----------------------------
# Commands: create / validate / run
# -----------------------------
def cmd_create(args: argparse.Namespace):
    check_name = args.name.strip()
    if not check_name:
        raise ValueError("Check name must not be empty")
    dest_root = Path(args.out or ".").resolve()
    pkg_dir = dest_root / check_name
    pkg_dir.mkdir(parents=True, exist_ok=True)

    class_name = "".join(p.capitalize() for p in check_name.replace("-", "_").split("_")) + "Check"
    module_file = pkg_dir / f"{check_name}.py"
    init_file = pkg_dir / "__init__.py"
    conf_example = pkg_dir / "conf.yaml.example"

    if not init_file.exists():
        init_file.write_text('__version__ = "0.0.1"\n', encoding="utf-8")

    if not module_file.exists():
        module_file.write_text(
            textwrap.dedent(
                f"""\
                from datadog_checks.base import AgentCheck

                class {class_name}(AgentCheck):
                    def check(self, instance):
                        # Example: emit a simple gauge with optional tags from instance config
                        value = float(instance.get("value", 1.0))
                        tags = list(instance.get("tags", []))
                        self.gauge("{check_name}.example", value, tags=tags)
                        # Example service check
                        self.service_check("{check_name}.health", self.OK, tags=tags, message="ok")
                """
            ),
            encoding="utf-8",
        )

    if not conf_example.exists():
        conf_example.write_text(
            textwrap.dedent(
                f"""\
                init_config: {{}}

                instances:
                  - value: 42
                    tags:
                      - env:dev
                      - check:{check_name}
                """
            ),
            encoding="utf-8",
        )

    print(f"Created scaffold for check '{check_name}' in {pkg_dir}")


def instantiate_check(check_cls: Type[AgentCheck], config: LoadedConfig, check_id: Optional[str] = None) -> AgentCheck:
    # Set up minimal agent stubs info
    dda.set_check_metadata = getattr(dda, "set_check_metadata", lambda *a, **k: None)
    dda.set_external_tags = getattr(dda, "set_external_tags", lambda *a, **k: None)

    name_for_agent = check_id or f"custom:{check_cls.__name__.lower()}"
    try:
        check = check_cls(name_for_agent, config.init_config, config.instances)
    except TypeError:
        # older signature might be (name, init_config, instances=None)
        check = check_cls(name_for_agent, config.init_config, instances=config.instances)
    return check


def cmd_validate(args: argparse.Namespace):
    module = load_module_from_path(args.path, module_name=args.module_name)
    check_cls = find_check_class(module, args.class_name)

    # Basic presence check
    print(f"Found check class: {check_cls.__name__}")

    # Config checks
    cfg = None
    if args.config:
        cfg = load_config(args.config, explicit_name=args.name)
        print(f"Loaded config with {len(cfg.instances)} instance(s)")

        # Try to instantiate the check
        try:
            check = instantiate_check(check_cls, cfg, check_id=args.name)
            print("Instantiation: OK")
        except Exception as e:
            print(f"Instantiation failed: {e}")
            sys.exit(2)

        # Optional dry run to catch obvious config errors
        if not args.no_run:
            agg.reset()
            errors = 0
            for i, inst in enumerate(cfg.instances):
                try:
                    check.check(inst)
                except Exception as e:
                    errors += 1
                    print(f"Instance #{i} check() raised: {e}")
            if errors:
                print(f"Validation completed with {errors} error(s).")
                sys.exit(2)
            print("Validation run: OK (no exceptions raised)")

    print("Validation successful.")


def cmd_run(args: argparse.Namespace):
    module = load_module_from_path(args.path, module_name=args.module_name)
    check_cls = find_check_class(module, args.class_name)

    cfg = load_config(args.config, explicit_name=args.name)
    check = instantiate_check(check_cls, cfg, check_id=args.name)

    interval = float(args.interval or 0)
    times = int(args.times or 1)

    for n in range(times):
        if n > 0 and interval > 0:
            time.sleep(interval)

        agg.reset()
        run_errors = 0
        for i, inst in enumerate(cfg.instances):
            try:
                check.check(inst)
            except Exception as e:
                run_errors += 1
                print(f"Run {n+1}, instance #{i} check() raised: {e}")

        if run_errors:
            print(f"Run {n+1} completed with {run_errors} error(s).")
        else:
            print(f"Run {n+1} completed successfully.")

        print_aggregated_output(as_json=args.json)


# -----------------------------
# CLI definition
# -----------------------------
def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        prog="ddcheck",
        description="CLI to create, validate, and run Datadog custom checks using datadog-checks-base.",
    )
    sub = p.add_subparsers(dest="command", required=True)

    # create
    p_create = sub.add_parser("create", help="Scaffold a new custom check")
    p_create.add_argument("name", help="Check package/name (e.g., my_check)")
    p_create.add_argument("--out", help="Output directory (default: current directory)")
    p_create.set_defaults(func=cmd_create)

    # validate
    p_validate = sub.add_parser("validate", help="Validate a custom check and optionally its config")
    p_validate.add_argument("path", help="Path to the check Python file (e.g., ./my_check/my_check.py)")
    p_validate.add_argument("--module-name", help="Optional module name for import isolation")
    p_validate.add_argument("--class-name", help="If multiple AgentCheck subclasses exist, specify one")
    p_validate.add_argument("--config", help="Path to YAML config (either list[instance] or dict with instances)")
    p_validate.add_argument("--name", help="Runtime check name/id (default: custom_check)")
    p_validate.add_argument("--no-run", action="store_true", help="Do not call check(); only instantiate")
    p_validate.set_defaults(func=cmd_validate)

    # run
    p_run = sub.add_parser("run", help="Run a custom check locally and print emitted data")
    p_run.add_argument("path", help="Path to the check Python file (e.g., ./my_check/my_check.py)")
    p_run.add_argument("--module-name", help="Optional module name for import isolation")
    p_run.add_argument("--class-name", help="If multiple AgentCheck subclasses exist, specify one")
    p_run.add_argument("--config", required=True, help="Path to YAML config")
    p_run.add_argument("--name", help="Runtime check name/id (default: custom_check)")
    p_run.add_argument("--times", type=int, default=1, help="Number of times to run (default: 1)")
    p_run.add_argument("--interval", type=float, default=0, help="Seconds to sleep between runs")
    p_run.add_argument("--json", action="store_true", help="Emit JSON instead of human-readable output")
    p_run.set_defaults(func=cmd_run)

    return p


def main(argv=None):
    parser = build_parser()
    args = parser.parse_args(argv)
    args.func(args)


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 208:
"Generate Python code that imports the Teradata Python module, connects to a Teradata database, executes a SQL script, and retrieves the results."
RESPONSE:
# Requires: pip install teradatasql
# This example:
# - Imports the Teradata Python module
# - Connects to Teradata
# - Reads a SQL script from disk
# - Executes it
# - Retrieves all result sets and prints a preview

import os
import teradatasql


def read_sql_file(path):
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def fetch_all_result_sets(cursor):
    """Collect all result sets from the cursor as lists of dicts."""
    result_sets = []
    while True:
        if cursor.description is not None:
            cols = [d[0] for d in cursor.description]
            rows = cursor.fetchall()
            result_sets.append([dict(zip(cols, row)) for row in rows])

        # Advance to the next result set if present
        try:
            more = cursor.nextset()
        except Exception:
            more = None

        if not more:
            break

    return result_sets


def main():
    # Connection settings (override via environment variables as needed)
    host = os.getenv("TD_HOST", "tdhost.example.com")
    user = os.getenv("TD_USER", "dbc")
    password = os.getenv("TD_PASS", "dbc")
    database = os.getenv("TD_DATABASE")  # optional default database/schema
    script_path = os.getenv("TD_SQL_FILE", "script.sql")

    sql = read_sql_file(script_path)

    conn_kwargs = {
        "host": host,
        "user": user,
        "password": password,
        # Optional but recommended:
        "encryptdata": True,   # encrypt traffic
        # "logmech": "TD2",    # or LDAP/Kerberos as appropriate
        # "charset": "UTF8",
    }
    if database:
        conn_kwargs["database"] = database

    # Connect, execute the script, and fetch results
    with teradatasql.connect(**conn_kwargs) as con:
        # autocommit True is often convenient for ad-hoc scripts
        con.autocommit = True
        with con.cursor() as cur:
            cur.execute(sql)
            result_sets = fetch_all_result_sets(cur)

    # Example: print a brief preview of the results
    for i, rs in enumerate(result_sets, start=1):
        print(f"Result set {i}: {len(rs)} rows")
        for row in rs[:5]:
            print(row)


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 209:
"Generate Python code that imports the pandas library, creates a dataframe from a CSV file, and uses SQL syntax to query the data in the dataframe."
RESPONSE:
import pandas as pd
import duckdb

# Load CSV into a DataFrame
df = pd.read_csv("your_file.csv")  # replace with your CSV path

# Create a DuckDB connection and register the DataFrame as a table
con = duckdb.connect()  # in-memory connection
con.register("df", df)

# Example SQL queries on the DataFrame
# 1) Preview first 5 rows
preview = con.execute("SELECT * FROM df LIMIT 5").df()

# 2) Count rows
row_count = con.execute("SELECT COUNT(*) AS n_rows FROM df").df()

# 3) Example: filter/aggregate if you know column names
# top_groups = con.execute("""
#     SELECT some_column, AVG(numeric_col) AS avg_val
#     FROM df
#     WHERE numeric_col IS NOT NULL
#     GROUP BY some_column
#     ORDER BY avg_val DESC
#     LIMIT 10
# """).df()

print(preview)
print(row_count)

con.close()
------------------------------------------------------------

PROMPT 210:
"Generate Python code that imports the necessary modules and creates a function that can serialize and deserialize Python dataclasses using Databind."
RESPONSE:
# Requires: pip install databind-json

from dataclasses import dataclass
from typing import Any, Type, TypeVar, Union

try:
    from databind.json import dump, load
except ImportError as e:
    raise ImportError("Please install 'databind-json' to use this utility (pip install databind-json).") from e

T = TypeVar("T")

def serde(value: Union[str, Any], cls: Type[T] | None = None, *, pretty: bool = False) -> Union[str, T]:
    """
    Serialize or deserialize dataclasses using Databind.

    - If cls is None, value is treated as a dataclass instance and serialized to a JSON string.
    - If cls is provided, value must be a JSON string that will be deserialized into an instance of cls.
    - Set pretty=True for indented JSON output when serializing.
    """
    if cls is None:
        # Serialize dataclass instance -> JSON string
        return dump(value, indent=2 if pretty else None)
    else:
        # Deserialize JSON string -> dataclass instance
        if not isinstance(value, (str, bytes, bytearray)):
            raise TypeError("When 'cls' is provided, 'value' must be a JSON string (or bytes).")
        return load(value, cls)


# Example usage
if __name__ == "__main__":
    @dataclass
    class Person:
        name: str
        age: int

    alice = Person("Alice", 30)

    json_text = serde(alice, pretty=True)
    print("Serialized:", json_text)

    alice2 = serde(json_text, Person)
    print("Deserialized:", alice2)
------------------------------------------------------------

PROMPT 211:
"Generate Python code that creates a Flask middleware function to add a unique request ID header to every incoming request, similar to the `request_id` package."
RESPONSE:
# app.py
import uuid
from typing import Callable, Iterable, Tuple
from flask import Flask, g, request, jsonify

def _to_environ_key(header_name: str) -> str:
    return "HTTP_" + header_name.upper().replace("-", "_")

def _default_request_id() -> str:
    return uuid.uuid4().hex

class RequestIDMiddleware:
    """
    WSGI middleware that ensures each request has a request ID.
    - Reads an incoming header if present (and trusted).
    - Otherwise generates a new ID.
    - Exposes it to the app via environ and adds it to the response headers.
    """
    def __init__(
        self,
        app,
        header_name: str = "X-Request-ID",
        trust_request_header: bool = True,
        generator: Callable[[], str] = _default_request_id,
        max_len: int = 200,
    ):
        self.app = app
        self.header_name = header_name
        self.environ_key = _to_environ_key(header_name)
        self.trust_request_header = trust_request_header
        self.generator = generator
        self.max_len = max_len

    def __call__(self, environ, start_response):
        rid = None

        if self.trust_request_header:
            candidate = environ.get(self.environ_key)
            if self._is_valid(candidate):
                rid = candidate

        if not rid:
            rid = self.generator()

        # Make the ID available to the Flask app
        environ[self.environ_key] = rid
        environ["REQUEST_ID"] = rid  # convenient alias

        def _start_response(status: str, headers: Iterable[Tuple[str, str]], exc_info=None):
            # Ensure only one header with our name
            filtered = [(k, v) for (k, v) in headers if k.lower() != self.header_name.lower()]
            filtered.append((self.header_name, rid))
            return start_response(status, filtered, exc_info)

        return self.app(environ, _start_response)

    def _is_valid(self, value: str) -> bool:
        if not isinstance(value, str):
            return False
        if len(value) == 0 or len(value) > self.max_len:
            return False
        # Keep it to printable characters
        return all(ch.isprintable() for ch in value)

app = Flask(__name__)

# Wrap Flask's WSGI app with the middleware
app.wsgi_app = RequestIDMiddleware(app.wsgi_app, header_name="X-Request-ID", trust_request_header=True)

@app.before_request
def attach_request_id_to_context():
    # Expose request ID on Flask's context for app code and logs
    g.request_id = request.environ.get("REQUEST_ID")

@app.after_request
def ensure_response_has_request_id(resp):
    # Redundant safety: make sure response always has the header
    rid = getattr(g, "request_id", None) or request.environ.get("REQUEST_ID")
    if rid:
        resp.headers["X-Request-ID"] = rid
    return resp

@app.route("/ping")
def ping():
    return jsonify(ok=True, request_id=get_request_id())

def get_request_id() -> str:
    return getattr(g, "request_id", "")

if __name__ == "__main__":
    app.run(debug=True, port=5000)
------------------------------------------------------------

PROMPT 212:
"Generate Python code that parses microformats data from HTML pages using the `BeautifulSoup` library and returns extracted information in a standardized format."
RESPONSE:
```python
"""
Microformats2 parser using BeautifulSoup.

- Parses h-* items, their p-/u-/dt-/e- properties, nested items, and rel links
- Returns data in a mf2-compatible JSON structure:
  {
    "items": [ { "type": ["h-card"], "properties": {...}, "children": [...] }, ... ],
    "rels": { "tag": [url, ...], ... },
    "rel-urls": { url: {"rels": [...], "text": "...", "title": "...", ...}, ... }
  }

Notes:
- This is a pragmatic implementation intended to cover common mf2 patterns.
- It supports:
  - Root detection (h-*)
  - Property extraction for p-, u-, dt-, e-
  - Value-class pattern (value, value-title)
  - Nested h-* items as property values
  - Children items
  - Implied name/url/photo (basic)
  - rel and rel-urls extraction
- It uses only BeautifulSoup (bs4) and standard library.

Usage:
    from microformats_bs4 import parse_microformats

    html = "<div class='h-card'><a class='p-name u-url' href='https://example.com'>Alice</a></div>"
    data = parse_microformats(html, base_url="https://example.com")
    print(data)
"""

from __future__ import annotations

import re
from typing import Dict, List, Any, Optional, Tuple, Iterable, Set
from urllib.parse import urljoin

from bs4 import BeautifulSoup, Tag, NavigableString


H_CLASS_RE = re.compile(r"^h-\S+")
PROP_CLASS_RE = re.compile(r"^(p|u|dt|e)-\S+")
VALUE_CLASS_RE = re.compile(r"^value(-title)?$")

# --------------------------
# Public API
# --------------------------

def parse_microformats(html: str, base_url: Optional[str] = None) -> Dict[str, Any]:
    """
    Parse microformats2 data from an HTML string using BeautifulSoup.

    Args:
        html: HTML content as a string.
        base_url: Optional base URL to resolve relative links. If None, <base href> is used if present.

    Returns:
        A dict with keys: "items", "rels", "rel-urls" (mf2 JSON representation)
    """
    soup = BeautifulSoup(html, "html.parser")

    # Determine base URL: explicit argument takes precedence, else <base href> if present
    if base_url is None:
        base_el = soup.find("base", href=True)
        if base_el and base_el.get("href"):
            base_url = base_el["href"]

    items = _parse_roots(soup, base_url)
    rels, rel_urls = _extract_rels(soup, base_url)
    return {"items": items, "rels": rels, "rel-urls": rel_urls}


# --------------------------
# Core parsing
# --------------------------

def _parse_roots(soup: BeautifulSoup, base_url: Optional[str]) -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    for el in soup.find_all(_has_h_class):
        if _nearest_h_ancestor(el) is None:
            items.append(_parse_item(el, base_url))
    return items


def _parse_item(root: Tag, base_url: Optional[str]) -> Dict[str, Any]:
    item_types = _h_classes(root)  # e.g. ["h-card"]
    props: Dict[str, List[Any]] = {}
    children: List[Dict[str, Any]] = []
    consumed_nested_items: Set[int] = set()  # id(tag) of nested items used as property values

    # Collect property elements within this root's scope (including root itself)
    candidates: List[Tag] = []
    if _has_property_class(root):
        candidates.append(root)
    candidates.extend(_descendants_within(root, match=_has_property_class, stop_at_h=True))

    # Extract properties (respect nested h-* items)
    for prop_el in candidates:
        if _nearest_h_ancestor(prop_el) is not root:
            continue  # out of scope for this root

        prop_classes = _property_classes(prop_el)
        if not prop_classes:
            continue

        # If this property element contains nested items whose nearest h-ancestor is prop_el,
        # treat those nested items as the property values (objects).
        nested_item_els = _descendants_within(prop_el, match=_has_h_class, stop_at_h=False)
        nested_item_els = [n for n in nested_item_els if _nearest_h_ancestor(n) is prop_el]

        if nested_item_els:
            nested_items = [_parse_item(n, base_url) for n in nested_item_els]
            for n in nested_item_els:
                consumed_nested_items.add(id(n))
            for kind, name in prop_classes:
                for obj in nested_items:
                    _props_add(props, name, obj)
            continue

        # No nested h-* items: extract scalar value(s)
        for kind, name in prop_classes:
            if kind == "p":
                value = _extract_p(prop_el)
                _props_add(props, name, value)
            elif kind == "u":
                value = _extract_u(prop_el, base_url)
                _props_add(props, name, value)
            elif kind == "dt":
                value = _extract_dt(prop_el)
                _props_add(props, name, value)
            elif kind == "e":
                value = _extract_e(prop_el)
                _props_add(props, name, value)

    # Children: nested h-* items that are direct descendants of root (nearest ancestor is root),
    # but were NOT consumed as property values above.
    nested_roots = _descendants_within(root, match=_has_h_class, stop_at_h=False)
    for child_el in nested_roots:
        if _nearest_h_ancestor(child_el) is root and id(child_el) not in consumed_nested_items:
            children.append(_parse_item(child_el, base_url))

    item: Dict[str, Any] = {
        "type": item_types or ["h-item"],
        "properties": props or {},
    }
    if children:
        item["children"] = children

    # Implied properties (basic support)
    _apply_implied_properties(root, item, base_url)

    return item


# --------------------------
# Property extraction
# --------------------------

def _extract_p(el: Tag) -> str:
    # Value-class pattern first
    vcp = _value_class_string(el, kind="p")
    if vcp is not None:
        return vcp

    # Single element extraction rules for p-*
    # Priority: <data|input value> -> <abbr title> -> <img|area alt> -> text
    if el.name in ("data", "input") and el.get("value"):
        return _normalize_space(el.get("value", ""))

    if el.name == "abbr" and el.get("title"):
        return _normalize_space(el["title"])

    if el.name in ("img", "area") and el.get("alt"):
        return _normalize_space(el["alt"])

    # Fallback: textual content
    return _normalize_space(el.get_text(separator=" ", strip=True))


def _extract_u(el: Tag, base_url: Optional[str]) -> str:
    # Value-class pattern first
    vcp = _value_class_string(el, kind="u")
    if vcp is not None:
        return _resolve_url(vcp, base_url)

    # Single element extraction rules for u-*
    # Priority: href/src/data/value -> abbr@title -> text
    # href-bearing
    for attr in ("href", "src", "data"):
        if el.has_attr(attr):
            return _resolve_url(el.get(attr, ""), base_url)

    if el.name in ("data", "input") and el.get("value"):
        return _resolve_url(el.get("value", ""), base_url)

    if el.name == "abbr" and el.get("title"):
        return _resolve_url(el["title"], base_url)

    # Fallback: text
    return _resolve_url(_normalize_space(el.get_text(separator=" ", strip=True)), base_url)


def _extract_dt(el: Tag) -> str:
    # Value-class pattern first
    vcp = _value_class_string(el, kind="dt")
    if vcp is not None:
        return vcp

    # Single element extraction rules for dt-*
    # Priority: time/datetime/@datetime -> data/input@value -> abbr@title -> text
    if el.name in ("time",) and el.get("datetime"):
        return _normalize_space(el.get("datetime", ""))

    if el.get("datetime"):  # any element with datetime attribute
        return _normalize_space(el.get("datetime", ""))

    if el.name in ("data", "input") and el.get("value"):
        return _normalize_space(el.get("value", ""))

    if el.name == "abbr" and el.get("title"):
        return _normalize_space(el["title"])

    # Fallback: text
    return _normalize_space(el.get_text(separator=" ", strip=True))


def _extract_e(el: Tag) -> Dict[str, str]:
    # e-* values are HTML inner content plus plaintext
    html = "".join(str(c) for c in el.contents)
    text = el.get_text(separator=" ", strip=True)
    return {"html": html, "value": text}


def _value_class_string(el: Tag, kind: str) -> Optional[str]:
    """
    Implements the value-class pattern for p/u/dt properties.

    Returns:
        Joined string if .value/.value-title children exist (ignoring those inside nested h-*),
        else None.
    """
    # Collect .value and .value-title in document order, but only within el and not inside nested h-*
    parts: List[str] = []
    found_vcp = False
    for v in _descendants_within(el, match=lambda t: _has_class_matching(t, VALUE_CLASS_RE), stop_at_h=True, include_self=False):
        found_vcp = True
        if not isinstance(v, Tag):
            continue
        if "value-title" in (v.get("class") or []) and v.get("title"):
            parts.append(v.get("title", ""))
            continue

        # Compute based on kind for "value" class elements
        if kind == "p":
            parts.append(_extract_p_single_value_class(v))
        elif kind == "u":
            parts.append(_extract_u_single_value_class(v))
        elif kind == "dt":
            parts.append(_extract_dt_single_value_class(v))

    if not found_vcp:
        return None

    return _normalize_space("".join(parts))


def _extract_p_single_value_class(el: Tag) -> str:
    if el.name in ("data", "input") and el.get("value"):
        return el.get("value", "")
    if el.name in ("img", "area") and el.get("alt"):
        return el.get("alt", "")
    if el.name == "abbr" and el.get("title"):
        return el.get("title", "")
    # fallback: text
    return el.get_text(separator=" ", strip=True)


def _extract_u_single_value_class(el: Tag) -> str:
    for attr in ("href", "src", "data"):
        if el.get(attr):
            return el.get(attr, "")
    if el.name in ("data", "input") and el.get("value"):
        return el.get("value", "")
    if el.name == "abbr" and el.get("title"):
        return el.get("title", "")
    return el.get_text(separator=" ", strip=True)


def _extract_dt_single_value_class(el: Tag) -> str:
    if el.get("datetime"):
        return el.get("datetime", "")
    if el.name in ("data", "input") and el.get("value"):
        return el.get("value", "")
    if el.name == "abbr" and el.get("title"):
        return el.get("title", "")
    return el.get_text(separator=" ", strip=True)


# --------------------------
# Implied properties (basic)
# --------------------------

def _apply_implied_properties(root: Tag, item: Dict[str, Any], base_url: Optional[str]) -> None:
    props = item.setdefault("properties", {})

    # Implied name
    if "name" not in props or not props["name"]:
        implied_name = _implied_name(root)
        if implied_name:
            _props_add(props, "name", implied_name)

    # Implied url
    if "url" not in props or not props["url"]:
        implied_url = _implied_url(root, base_url)
        if implied_url:
            _props_add(props, "url", implied_url)

    # Implied photo
    if "photo" not in props or not props["photo"]:
        implied_photo = _implied_photo(root, base_url)
        if implied_photo:
            _props_add(props, "photo", implied_photo)


def _implied_name(root: Tag) -> Optional[str]:
    # img/area with alt on root
    if root.name in ("img", "area") and root.get("alt"):
        return _normalize_space(root.get("alt", ""))

    # abbr[title] on root
    if root.name == "abbr" and root.get("title"):
        return _normalize_space(root.get("title", ""))

    # single descendant img/area with alt (not in nested h-*)
    imgs = _descendants_within(root, match=lambda t: isinstance(t, Tag) and t.name in ("img", "area") and t.get("alt"), stop_at_h=True)
    if imgs:
        return _normalize_space(imgs[0].get("alt", ""))

    # abbr[title] descendant
    abbrs = _descendants_within(root, match=lambda t: isinstance(t, Tag) and t.name == "abbr" and t.get("title"), stop_at_h=True)
    if abbrs:
        return _normalize_space(abbrs[0].get("title", ""))

    # Fallback: text content excluding nested h-* items
    text = _text_excluding_nested_h(root)
    return _normalize_space(text) if text else None


def _implied_url(root: Tag, base_url: Optional[str]) -> Optional[str]:
    # If root is a link-like element with href/src
    for attr in ("href", "src"):
        if root.get(attr):
            return _resolve_url(root.get(attr, ""), base_url)

    # First descendant <a> with href (not inside nested h-*)
    anchor = root.find(lambda t: isinstance(t, Tag) and t.name == "a" and t.get("href") and _nearest_h_ancestor(t) is root)
    if anchor:
        return _resolve_url(anchor.get("href", ""), base_url)
    return None


def _implied_photo(root: Tag, base_url: Optional[str]) -> Optional[str]:
    # If root is img with src
    if root.name == "img" and root.get("src"):
        return _resolve_url(root.get("src", ""), base_url)

    # First descendant img with src (not inside nested h-*)
    img = root.find(lambda t: isinstance(t, Tag) and t.name == "img" and t.get("src") and _nearest_h_ancestor(t) is root)
    if img:
        return _resolve_url(img.get("src", ""), base_url)
    return None


# --------------------------
# rel/rel-urls extraction
# --------------------------

def _extract_rels(soup: BeautifulSoup, base_url: Optional[str]) -> Tuple[Dict[str, List[str]], Dict[str, Dict[str, Any]]]:
    rels: Dict[str, List[str]] = {}
    rel_urls: Dict[str, Dict[str, Any]] = {}

    def add_rel(tag: Tag):
        href = tag.get("href") or tag.get("src")
        if not href:
            return
        url = _resolve_url(href, base_url)
        tag_rels = tag.get("rel") or []
        if isinstance(tag_rels, str):
            tag_rels = tag_rels.split()
        tag_rels = [r for r in tag_rels if r]

        # rels
        for r in tag_rels:
            rels.setdefault(r, []).append(url)

        # rel-urls
        info = rel_urls.setdefault(url, {"rels": []})
        for r in tag_rels:
            if r not in info["rels"]:
                info["rels"].append(r)
        # Enrich with common attributes
        text = tag.get_text(strip=True) if tag.name not in ("link",) else ""
        if text:
            info["text"] = text
        for a in ("title", "type", "media", "hreflang"):
            if tag.get(a):
                info[a] = tag.get(a)

    for t in soup.find_all(lambda tag: isinstance(tag, Tag) and (tag.get("rel") is not None)):
        add_rel(t)

    return rels, rel_urls


# --------------------------
# Helpers
# --------------------------

def _resolve_url(url: str, base_url: Optional[str]) -> str:
    if not base_url:
        return url
    try:
        return urljoin(base_url, url)
    except Exception:
        return url


def _normalize_space(s: str) -> str:
    return " ".join((s or "").split())


def _props_add(props: Dict[str, List[Any]], name: str, value: Any) -> None:
    if value is None:
        return
    props.setdefault(name, []).append(value)


def _has_h_class(tag: Tag) -> bool:
    return _has_class_matching(tag, H_CLASS_RE)


def _has_property_class(tag: Tag) -> bool:
    return _has_class_matching(tag, PROP_CLASS_RE)


def _has_class_matching(tag: Tag, regex: re.Pattern) -> bool:
    if not isinstance(tag, Tag):
        return False
    classes = tag.get("class") or []
    for cls in classes:
        if regex.match(cls):
            return True
    return False


def _h_classes(tag: Tag) -> List[str]:
    classes = tag.get("class") or []
    return [c for c in classes if H_CLASS_RE.match(c)]


def _property_classes(tag: Tag) -> List[Tuple[str, str]]:
    out: List[Tuple[str, str]] = []
    classes = tag.get("class") or []
    for c in classes:
        if c.startswith("p-"):
            out.append(("p", c[2:]))
        elif c.startswith("u-"):
            out.append(("u", c[2:]))
        elif c.startswith("dt-"):
            out.append(("dt", c[3:]))
        elif c.startswith("e-"):
            out.append(("e", c[2:]))
    return out


def _nearest_h_ancestor(tag: Tag) -> Optional[Tag]:
    """
    Return the nearest ancestor (including self) that has an h-* class.
    If none, return None.
    """
    t: Optional[Tag] = tag
    while isinstance(t, Tag):
        if _has_h_class(t):
            return t
        t = t.parent
    return None


def _descendants_within(
    root: Tag,
    match,
    stop_at_h: bool = False,
    include_self: bool = False,
) -> List[Tag]:
    """
    Collect descendants of root matching predicate 'match'.
    - If stop_at_h=True, do not descend into elements that have h-* class.
    - include_self includes root if it matches.
    """
    results: List[Tag] = []
    if include_self and match(root):
        results.append(root)

    # Depth-first traversal with pruning
    def visit(node: Tag):
        for child in getattr(node, "children", []):
            if isinstance(child, NavigableString):
                continue
            if not isinstance(child, Tag):
                continue
            if match(child):
                results.append(child)
            # If instructed, do not traverse into nested h-* items
            if stop_at_h and _has_h_class(child) and child is not root:
                continue
            visit(child)

    visit(root)
    return results


def _text_excluding_nested_h(root: Tag) -> str:
    """
    Collect text content of root, excluding any text that is inside a nested h-* descendant.
    """
    parts: List[str] = []

    def visit(node: Tag):
        for child in node.children:
            if isinstance(child, NavigableString):
                parts.append(str(child))
                continue
            if not isinstance(child, Tag):
                continue
            if _has_h_class(child) and child is not root:
                # skip entire subtree
                continue
            visit(child)

    visit(root)
    return _normalize_space("".join(parts))


# --------------------------
# Example usage
# --------------------------

if __name__ == "__main__":
    sample_html = """
    <html>
      <head>
        <base href="https://example.com/">
        <link rel="author" href="/about"> 
      </head>
      <body>
        <div class="h-card">
          <a class="p-name u-url" href="/alice">Alice Example</a>
          <img class="u-photo" src="/alice.jpg" alt="Alice">
          <p class="p-note e-note">Hello <b>world</b>!</p>
          <div class="p-org h-card">
            <span class="p-name">Example Inc.</span>
          </div>
          <a rel="tag" href="/tags/mf2">mf2</a>
        </div>

        <article class="h-entry">
          <h1 class="p-name">A Post</h1>
          <a class="u-url" href="/posts/1">Permalink</a>
          <time class="dt-published" datetime="2025-10-29T12:34:00Z">Oct 29, 2025</time>
          <div class="e-content">
            <p>This is the <i>content</i>.</p>
          </div>
          <a class="p-author h-card" href="/alice">
            <img class="u-photo" src="/alice.jpg" alt="Alice">
            <span class="p-name">Alice</span>
          </a>
        </article>
      </body>
    </html>
    """
    mf2 = parse_microformats(sample_html)
    import json
    print(json.dumps(mf2, indent=2, ensure_ascii=False))
```
------------------------------------------------------------

PROMPT 213:
"Generate Python code that imports the requests library, formats data as JSON-LD, and makes HTTP POST requests to the Google Search Console API to submit sitemaps."
RESPONSE:
# Python script: build JSON-LD for a sitemap and submit the sitemap to Google Search Console via HTTP
# Note: The Google Search Console Sitemaps "submit" API method is actually HTTP PUT with an empty body.
# To satisfy environments that only allow POST, this code uses POST with X-HTTP-Method-Override: PUT.

import os
import json
import time
import requests
from urllib.parse import quote

GOOGLE_SITEMAPS_SCOPE = "https://www.googleapis.com/auth/webmasters"
GSC_SITEMAPS_ENDPOINT_TMPL = "https://www.googleapis.com/webmasters/v3/sites/{site}/sitemaps/{sitemap}"

def build_sitemap_jsonld(site_url: str, sitemap_url: str, urls=None) -> dict:
    """
    Create a JSON-LD description for a sitemap and its URLs.
    This is not required by the Search Console API (which takes no body for submission),
    but can be useful to serialize or log your sitemap metadata.

    :param site_url: Your Search Console property (e.g., 'https://www.example.com/' or 'sc-domain:example.com')
    :param sitemap_url: Public URL to the sitemap XML (e.g., 'https://www.example.com/sitemap.xml')
    :param urls: Optional iterable of page URLs included in the sitemap
    """
    urls = urls or []
    return {
        "@context": "https://schema.org",
        "@type": "DataFeed",
        "name": "Sitemap",
        "dateModified": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "about": {
            "@type": "WebSite",
            "@id": site_url
        },
        "url": sitemap_url,
        "dataFeedElement": [
            {
                "@type": "DataFeedItem",
                "item": {
                    "@type": "WebPage",
                    "@id": u
                }
            } for u in urls
        ]
    }

def get_access_token() -> str:
    """
    Obtain an OAuth 2.0 access token for the Search Console API.

    Preferred: Application Default Credentials (ADC), e.g.:
      - gcloud auth application-default login
      - Or a service account with domain-wide delegation impersonating a verified owner.
    Fallback: Set ACCESS_TOKEN environment variable with a valid OAuth 2 bearer token.

    Returns: access token string
    """
    # Try ADC via google-auth if available
    try:
        import google.auth
        from google.auth.transport.requests import Request as GoogleAuthRequest
        creds, _ = google.auth.default(scopes=[GOOGLE_SITEMAPS_SCOPE])
        creds.refresh(GoogleAuthRequest())
        return creds.token
    except Exception:
        token = os.getenv("ACCESS_TOKEN")
        if not token:
            raise RuntimeError(
                "Could not obtain an access token. Install google-auth and configure ADC, "
                "or set ACCESS_TOKEN environment variable."
            )
        return token

def submit_sitemap(access_token: str, site_property: str, sitemap_url: str, timeout=15) -> requests.Response:
    """
    Submit a sitemap to Google Search Console.

    The API expects HTTP PUT with no request body:
      PUT https://www.googleapis.com/webmasters/v3/sites/{siteUrl}/sitemaps/{feedpath}
    This implementation uses POST with X-HTTP-Method-Override: PUT to remain correct while using POST.

    :param access_token: OAuth 2.0 bearer token with 'webmasters' scope
    :param site_property: GSC property identifier (e.g., 'https://www.example.com/' or 'sc-domain:example.com')
    :param sitemap_url: The public sitemap URL to submit
    :param timeout: request timeout in seconds
    :return: requests.Response
    """
    encoded_site = quote(site_property, safe="")
    encoded_sitemap = quote(sitemap_url, safe="")
    endpoint = GSC_SITEMAPS_ENDPOINT_TMPL.format(site=encoded_site, sitemap=encoded_sitemap)

    headers = {
        "Authorization": f"Bearer {access_token}",
        "Accept": "application/json",
        # Override to perform a logical PUT via POST
        "X-HTTP-Method-Override": "PUT",
        # No body is required/used by the API
        "Content-Length": "0",
    }

    # POST with method override
    resp = requests.post(endpoint, headers=headers, data=b"", timeout=timeout)

    return resp

def submit_with_retry(access_token: str, site_property: str, sitemap_url: str,
                      max_retries=5, base_delay=1.0) -> requests.Response:
    """
    Submit sitemap with simple exponential backoff on retryable HTTP statuses.
    """
    retryable = {429, 500, 502, 503, 504}
    attempt = 0
    while True:
        resp = submit_sitemap(access_token, site_property, sitemap_url)
        if resp.status_code not in retryable or attempt >= max_retries:
            return resp
        delay = base_delay * (2 ** attempt)
        time.sleep(delay)
        attempt += 1

if __name__ == "__main__":
    # Example inputs
    SITE_PROPERTY = "https://www.example.com/"  # or "sc-domain:example.com"
    SITEMAP_URL = "https://www.example.com/sitemap.xml"
    EXAMPLE_URLS = [
        "https://www.example.com/",
        "https://www.example.com/about",
        "https://www.example.com/products/widget-1"
    ]

    # Build and show JSON-LD (optional)
    jsonld_doc = build_sitemap_jsonld(SITE_PROPERTY, SITEMAP_URL, EXAMPLE_URLS)
    print("JSON-LD describing the sitemap:")
    print(json.dumps(jsonld_doc, indent=2))

    # Get access token and submit
    token = get_access_token()
    response = submit_with_retry(token, SITE_PROPERTY, SITEMAP_URL)

    if response.ok:
        # 200 OK or 204 No Content are typical
        print(f"Sitemap submitted successfully. HTTP {response.status_code}")
    else:
        print(f"Failed to submit sitemap. HTTP {response.status_code}")
        try:
            print(response.json())
        except Exception:
            print(response.text)
------------------------------------------------------------

PROMPT 214:
"Generate Python code that imports the TileDB library and creates a TileDB dataset, stores data in it, and performs various operations on the data using TileDB's API."
RESPONSE:
```python
# Full TileDB Python example:
# - Creates a dense 2D array with multiple attributes
# - Writes and updates data
# - Reads subsets and uses a query condition
# - Demonstrates time-travel reads
# - Uses array metadata and basic maintenance (consolidate/vacuum)

import os
import time
import tempfile
import numpy as np
import tiledb

def main():
    # Workspace for this demo
    base_dir = tempfile.mkdtemp(prefix="tiledb_demo_")
    uri = os.path.join(base_dir, "dense_array")
    print(f"Creating array at: {uri}")

    # 1) Define a 2D dense array schema with two attributes: 'a' (float64) and 'b' (int32)
    dom = tiledb.Domain(
        tiledb.Dim(name="x", domain=(1, 4), tile=2, dtype=np.int32),
        tiledb.Dim(name="y", domain=(1, 4), tile=2, dtype=np.int32),
    )

    attrs = [
        tiledb.Attr(name="a", dtype=np.float64, filters=tiledb.FilterList([tiledb.ZstdFilter(level=3)])),
        tiledb.Attr(name="b", dtype=np.int32),
    ]

    schema = tiledb.ArraySchema(
        domain=dom,
        sparse=False,
        attrs=attrs,
        cell_order="row-major",
        tile_order="row-major",
    )

    # Create the array on disk
    tiledb.Array.create(uri, schema)

    # 2) Initial write of the full array
    data_a = np.arange(1, 17, dtype=np.float64).reshape(4, 4) / 10.0  # 0.1 ... 1.6
    data_b = np.arange(1, 17, dtype=np.int32).reshape(4, 4)           # 1 .. 16

    with tiledb.open(uri, "w") as A:
        A[:] = {"a": data_a, "b": data_b}
        # Set some user metadata at the same time
        A.meta["description"] = "Demo dense 2D array with two attributes"
        A.meta["version"] = 1

    # Timestamp snapshot after initial write
    snap1_ms = int(time.time() * 1000)

    # 3) Update a subarray (second fragment) and set new metadata
    # Note: the array domain is [1..4] for both dims
    with tiledb.open(uri, "w") as A:
        # Update row x=2, cols y=2..3
        A[2, 2:4] = {
            "a": np.array([99.9, 88.8], dtype=np.float64),
            "b": np.array([999, 888], dtype=np.int32),
        }
        A.meta["version"] = 2

    print("\nInitial full read after update:")
    with tiledb.open(uri, "r") as A:
        out = A[:]  # dict: {'a': ndarray, 'b': ndarray}
        print("a:\n", out["a"])
        print("b:\n", out["b"])

    # 4) Read a subarray slice
    print("\nRead subarray x=2..3, y=1..2:")
    with tiledb.open(uri, "r") as A:
        sub = A[2:4, 1:3]
        print("a:\n", sub["a"])
        print("b:\n", sub["b"])

    # 5) Read with a query condition (filter rows by attribute values)
    # Here: select cells where a >= 1.0 and b < 10
    print("\nQuery with condition: a >= 1.0 and b < 10 (return dims too):")
    with tiledb.open(uri, "r") as A:
        qc = tiledb.QueryCondition("a >= 1.0 and b < 10")
        q = A.query(attrs=["a", "b"], dims=True, condition=qc)
        filtered = q[:]  # dict containing 'a', 'b', and dims 'x', 'y'
        # Results are 1D lists of matching cells
        print("x coords:", filtered["x"])
        print("y coords:", filtered["y"])
        print("a values:", filtered["a"])
        print("b values:", filtered["b"])

    # 6) Time-travel read at snap1 (i.e., before the subarray update)
    print("\nTime-travel read at snapshot 1 (before update):")
    with tiledb.open(uri, "r", timestamp=snap1_ms) as A:
        old = A[:]
        print("a (old):\n", old["a"])
        print("b (old):\n", old["b"])

    # 7) Read array metadata
    with tiledb.open(uri, "r") as A:
        print("\nMetadata:")
        for k in A.meta:
            print(f"  {k} = {A.meta[k]}")

    # 8) Optional maintenance: consolidate and vacuum fragments/metadata
    # Note: For tiny demos this is not necessary, but shown for completeness.
    print("\nConsolidating and vacuuming fragments/metadata...")
    tiledb.consolidate(uri)
    tiledb.vacuum(uri)
    tiledb.consolidate_metadata(uri)
    tiledb.vacuum(uri, config=tiledb.Config({"vfs.file.enable_file_deletion":"true"}))
    print("Done.")

    print(f"\nArray URI: {uri}")
    print("You can inspect it with the TileDB CLI or reuse it in other scripts.")

if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 215:
"Generate Python code that imports the requests library and creates a function that takes a URL as input, sends a GET request to that URL, and returns the response object."
RESPONSE:
import requests

def get_response(url: str):
    response = requests.get(url)
    return response
------------------------------------------------------------

PROMPT 216:
"Generate Python code that uses Cheetah's template engine to render HTML templates with dynamic data and generates Python classes and methods based on user-defined templates."
RESPONSE:
import json
from typing import Any, Dict, List, Optional, Union
from pathlib import Path
import types

# Requires: pip install Cheetah3
from Cheetah.Template import Template


class CheetahRenderer:
    """
    Helper to render Cheetah templates from strings or files.
    """

    @staticmethod
    def render_from_string(template_str: str, context: Dict[str, Any]) -> str:
        return str(Template(source=template_str, searchList=[context]))

    @staticmethod
    def render_from_file(template_path: Union[str, Path], context: Dict[str, Any]) -> str:
        return str(Template(file=str(template_path), searchList=[context]))


def generate_html(
    template: str,
    data: Dict[str, Any],
    from_file: bool = False,
) -> str:
    """
    Render an HTML template with dynamic data using Cheetah.
    template: template text or a file path (when from_file=True)
    data: dict passed as the searchList
    """
    if from_file:
        return CheetahRenderer.render_from_file(template, data)
    return CheetahRenderer.render_from_string(template, data)


def _normalize_code_spec(spec: Dict[str, Any]) -> Dict[str, Any]:
    """
    Prepare the code-generation spec to be friendly to the template.
    - Ensures required keys exist
    - Prepares body_lines for each method for clean indentation in template
    - Prepares signature arguments strings
    """
    classes = spec.get("classes", [])
    for cls in classes:
        cls.setdefault("doc", "")
        methods = cls.get("methods", [])
        for m in methods:
            m.setdefault("doc", "")
            # args: list of argument strings like ["a", "b=2"]
            args = m.get("args", [])
            if not isinstance(args, list):
                raise TypeError("Each method's 'args' must be a list of strings")
            m["sig_args"] = ", ".join(args) if args else ""
            body = m.get("body", "")
            if body is None:
                body = ""
            # Normalize to list of lines for safe indentation in template
            if isinstance(body, str):
                body_lines = [ln for ln in body.splitlines() if ln.strip() != ""]
            elif isinstance(body, list):
                body_lines = [str(ln) for ln in body]
            else:
                raise TypeError("Method 'body' must be a string or list of lines")
            m["body_lines"] = body_lines
    return {"classes": classes}


def generate_python_code_from_template(
    template: str,
    spec: Dict[str, Any],
    from_file: bool = False,
) -> str:
    """
    Generate Python code for classes and methods using a user-supplied Cheetah template and a spec.
    The spec dictionary should follow:
      {
        "classes": [
          {
            "name": "ClassName",
            "doc": "Docstring...",
            "methods": [
              {
                "name": "method_name",
                "args": ["a", "b=2"],
                "doc": "Method docstring.",
                "body": "return a + b"
              }
            ]
          }
        ]
      }
    """
    normalized = _normalize_code_spec(spec)
    if from_file:
        return CheetahRenderer.render_from_file(template, normalized)
    return CheetahRenderer.render_from_string(template, normalized)


# Default example templates (users can supply their own)
DEFAULT_HTML_TEMPLATE = """
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title>$title</title>
  </head>
  <body>
    <h1>$heading</h1>
    #if $items:
      <ul>
      #for $item in $items:
        <li>$item['name'] - $item['price']</li>
      #end for
      </ul>
    #else:
      <p>No items available</p>
    #end if
  </body>
</html>
""".strip()


DEFAULT_CODE_TEMPLATE = r"""
# Generated code. Do not edit by hand.

#for $cls in $classes:
class $cls['name']:
    """$cls['doc']"""
#if $cls.get('methods'):
#for $m in $cls['methods']:
    def $m['name'](self#if $m['sig_args']#, $m['sig_args']#end if):
        """$m['doc']"""
#if $m['body_lines']:
#for $line in $m['body_lines']:
        $line
#end for
#else:
        pass
#end if

#end for
#else:
    pass

#end if
#end for
""".lstrip()


# Demonstration
if __name__ == "__main__":
    # 1) Render HTML using dynamic data
    html_data = {
        "title": "Storefront",
        "heading": "Featured Products",
        "items": [
            {"name": "Coffee", "price": "$3.50"},
            {"name": "Tea", "price": "$2.75"},
            {"name": "Mug", "price": "$8.99"},
        ],
    }
    rendered_html = generate_html(DEFAULT_HTML_TEMPLATE, html_data, from_file=False)
    print("=== Rendered HTML ===")
    print(rendered_html)
    print("\n")

    # 2) Generate Python code for classes/methods from a spec using a user-defined template
    code_spec = {
        "classes": [
            {
                "name": "UserService",
                "doc": "Service to manage users.",
                "methods": [
                    {
                        "name": "create_user",
                        "args": ["username", "email", "is_admin=False"],
                        "doc": "Create a new user dictionary.",
                        "body": [
                            "user = {'username': username, 'email': email, 'is_admin': is_admin}",
                            "return user",
                        ],
                    },
                    {
                        "name": "delete_user",
                        "args": ["username"],
                        "doc": "Delete a user by username. Returns True if deleted.",
                        "body": [
                            "# Imagine actual delete logic here",
                            "return True",
                        ],
                    },
                ],
            },
            {
                "name": "MathUtils",
                "doc": "Simple math utilities.",
                "methods": [
                    {
                        "name": "add",
                        "args": ["a", "b=0"],
                        "doc": "Return the sum of a and b.",
                        "body": "return a + b",
                    },
                    {
                        "name": "mul",
                        "args": ["a", "b=1"],
                        "doc": "Return the product of a and b.",
                        "body": "return a * b",
                    },
                ],
            },
        ]
    }

    generated_code = generate_python_code_from_template(
        DEFAULT_CODE_TEMPLATE, code_spec, from_file=False
    )
    print("=== Generated Python Code ===")
    print(generated_code)

    # 3) Optionally execute the generated code in an isolated module to demonstrate usage
    print("=== Demonstrating Generated Code Execution ===")
    mod = types.ModuleType("generated_module")
    exec(generated_code, mod.__dict__)

    # Instantiate and use generated classes
    user_service = mod.UserService()
    print(user_service.create_user("alice", "alice@example.com"))
    print(user_service.delete_user("alice"))

    math_utils = mod.MathUtils()
    print("2 + 5 =", math_utils.add(2, 5))
    print("3 * 7 =", math_utils.mul(3, 7))
------------------------------------------------------------

PROMPT 217:
"Generate Python code that imports the requests library and uses it to send HTTP requests to the arXiv API to retrieve information about articles, authors, and other metadata."
RESPONSE:
import time
import requests
import xml.etree.ElementTree as ET
from typing import Dict, List, Generator, Optional

ARXIV_API_URL = "https://export.arxiv.org/api/query"
NS = {
    "atom": "http://www.w3.org/2005/Atom",
    "arxiv": "http://arxiv.org/schemas/atom",
    "opensearch": "http://a9.com/-/spec/opensearch/1.1/",
}

DEFAULT_HEADERS = {
    # Please customize this with your app name and a contact email per arXiv API etiquette
    "User-Agent": "ArxivClient/1.0 (mailto:your_email@example.com)",
    "Accept": "application/atom+xml,application/xml;q=0.9,*/*;q=0.8",
}


class ArxivClient:
    def __init__(self, user_agent: Optional[str] = None, timeout: float = 30.0):
        headers = dict(DEFAULT_HEADERS)
        if user_agent:
            headers["User-Agent"] = user_agent
        self.session = requests.Session()
        self.session.headers.update(headers)
        self.timeout = timeout

    def search(
        self,
        search_query: str,
        start: int = 0,
        max_results: int = 10,
        sort_by: Optional[str] = None,       # "relevance", "lastUpdatedDate", "submittedDate"
        sort_order: Optional[str] = None,    # "ascending" or "descending"
    ) -> Dict:
        """
        Execute a single arXiv API query and return parsed results and metadata.
        """
        params = {
            "search_query": search_query,
            "start": start,
            "max_results": max_results,
        }
        if sort_by:
            params["sortBy"] = sort_by
        if sort_order:
            params["sortOrder"] = sort_order

        resp = self.session.get(ARXIV_API_URL, params=params, timeout=self.timeout)
        resp.raise_for_status()
        return self._parse_feed(resp.text)

    def iter_search(
        self,
        search_query: str,
        total_results: int = 100,
        page_size: int = 100,
        sort_by: Optional[str] = None,
        sort_order: Optional[str] = None,
        delay_seconds: float = 3.0,
    ) -> Generator[Dict, None, None]:
        """
        Generator that yields parsed entries across multiple API pages.
        Respects arXiv's recommended rate limiting by delaying between calls.
        """
        fetched = 0
        start = 0
        while fetched < total_results:
            batch_size = min(page_size, total_results - fetched)
            data = self.search(
                search_query=search_query,
                start=start,
                max_results=batch_size,
                sort_by=sort_by,
                sort_order=sort_order,
            )
            entries = data.get("entries", [])
            if not entries:
                break
            for e in entries:
                yield e
            count = len(entries)
            fetched += count
            start += count
            if count < batch_size:
                break
            time.sleep(delay_seconds)

    def get_by_id(self, arxiv_ids: List[str]) -> Dict:
        """
        Retrieve records by arXiv identifier(s).
        Example IDs: ["2101.00001", "hep-th/9901001"]
        """
        params = {
            "id_list": ",".join(arxiv_ids),
        }
        resp = self.session.get(ARXIV_API_URL, params=params, timeout=self.timeout)
        resp.raise_for_status()
        return self._parse_feed(resp.text)

    def _parse_feed(self, xml_text: str) -> Dict:
        root = ET.fromstring(xml_text)

        # Feed-level metadata
        feed_title = self._get_text(root.find("atom:title", NS))
        total_results = self._get_text(root.find("opensearch:totalResults", NS))
        items_per_page = self._get_text(root.find("opensearch:itemsPerPage", NS))
        start_index = self._get_text(root.find("opensearch:startIndex", NS))

        meta = {
            "feed_title": feed_title,
            "total_results": int(total_results) if total_results and total_results.isdigit() else None,
            "items_per_page": int(items_per_page) if items_per_page and items_per_page.isdigit() else None,
            "start_index": int(start_index) if start_index and start_index.isdigit() else None,
        }

        entries = []
        for entry in root.findall("atom:entry", NS):
            entries.append(self._parse_entry(entry))

        return {"meta": meta, "entries": entries}

    def _parse_entry(self, entry: ET.Element) -> Dict:
        id_ = self._get_text(entry.find("atom:id", NS))
        title = self._clean_whitespace(self._get_text(entry.find("atom:title", NS)))
        summary = self._clean_whitespace(self._get_text(entry.find("atom:summary", NS)))
        published = self._get_text(entry.find("atom:published", NS))
        updated = self._get_text(entry.find("atom:updated", NS))

        authors = []
        for a in entry.findall("atom:author", NS):
            name = self._get_text(a.find("atom:name", NS))
            if name:
                authors.append(name)

        comment = self._get_text(entry.find("arxiv:comment", NS))
        doi = self._get_text(entry.find("arxiv:doi", NS))
        journal_ref = self._get_text(entry.find("arxiv:journal_ref", NS))

        primary_cat_elem = entry.find("arxiv:primary_category", NS)
        primary_category = primary_cat_elem.get("term") if primary_cat_elem is not None else None

        categories = []
        for c in entry.findall("atom:category", NS):
            term = c.get("term")
            if term:
                categories.append(term)

        # Links
        links = self._extract_links(entry)

        return {
            "id": id_,
            "title": title,
            "summary": summary,
            "published": published,
            "updated": updated,
            "authors": authors,
            "comment": comment,
            "doi": doi,
            "journal_ref": journal_ref,
            "primary_category": primary_category,
            "categories": categories,
            "links": links,
        }

    def _extract_links(self, entry: ET.Element) -> Dict[str, Optional[str]]:
        html_url = None
        pdf_url = None
        doi_url = None

        for link in entry.findall("atom:link", NS):
            rel = link.get("rel", "")
            href = link.get("href", "")
            link_type = link.get("type", "")
            title = link.get("title", "")

            # HTML abstract page
            if rel in ("alternate", "") and "abs" in href:
                html_url = href

            # PDF link heuristics
            if (
                "pdf" in href
                or link_type == "application/pdf"
                or title.lower() == "pdf"
            ):
                pdf_url = href

        doi_elem = entry.find("arxiv:doi", NS)
        if doi_elem is not None and doi_elem.text:
            doi_val = doi_elem.text.strip()
            doi_url = f"https://doi.org/{doi_val}"

        return {"html": html_url, "pdf": pdf_url, "doi": doi_url}

    @staticmethod
    def _get_text(elem: Optional[ET.Element]) -> Optional[str]:
        if elem is None or elem.text is None:
            return None
        return elem.text.strip()

    @staticmethod
    def _clean_whitespace(text: Optional[str]) -> Optional[str]:
        if text is None:
            return None
        # Collapse internal whitespace and strip
        return " ".join(text.split())


def build_search_query(
    all_terms: Optional[List[str]] = None,
    title_terms: Optional[List[str]] = None,
    abstract_terms: Optional[List[str]] = None,
    author: Optional[str] = None,
    categories: Optional[List[str]] = None,
) -> str:
    """
    Helper to build an arXiv API search_query string.
    - Prefixes: all:, ti:, abs:, au:, cat:
    """
    parts = []

    def quote(term: str) -> str:
        # Quote terms with spaces
        return f'"{term}"' if " " in term else term

    if all_terms:
        parts.extend([f"all:{quote(t)}" for t in all_terms])
    if title_terms:
        parts.extend([f"ti:{quote(t)}" for t in title_terms])
    if abstract_terms:
        parts.extend([f"abs:{quote(t)}" for t in abstract_terms])
    if author:
        parts.append(f"au:{quote(author)}")
    if categories:
        parts.extend([f"cat:{c}" for c in categories])

    # Combine with AND by default
    return " AND ".join(parts) if parts else "all:*"


if __name__ == "__main__":
    client = ArxivClient(user_agent="ExampleArxivClient/1.0 (mailto:you@example.com)")

    # Example 1: General search for articles with metadata
    query = build_search_query(
        title_terms=["graph neural network"],
        abstract_terms=["graph learning"],
        categories=["cs.LG", "stat.ML"],
    )
    result = client.search(search_query=query, start=0, max_results=5, sort_by="submittedDate", sort_order="descending")

    print("Feed meta:", result["meta"])
    for i, entry in enumerate(result["entries"], 1):
        print(f"\nResult {i}")
        print("ID:", entry["id"])
        print("Title:", entry["title"])
        print("Authors:", ", ".join(entry["authors"]))
        print("Published:", entry["published"])
        print("Primary category:", entry["primary_category"])
        print("All categories:", entry["categories"])
        print("DOI:", entry["doi"])
        print("Journal ref:", entry["journal_ref"])
        print("PDF:", entry["links"]["pdf"])
        print("HTML:", entry["links"]["html"])

    # Example 2: Search by author
    author_query = build_search_query(author="Andrew Ng", categories=["cs.LG"])
    for entry in client.iter_search(author_query, total_results=10, page_size=5, sort_by="submittedDate", sort_order="descending"):
        print("\nBy author - Title:", entry["title"], "| Published:", entry["published"])

    # Example 3: Lookup by arXiv ID(s)
    by_id = client.get_by_id(["2101.00001", "hep-th/9901001"])
    print("\nLookup by ID meta:", by_id["meta"])
    for entry in by_id["entries"]:
        print("By ID Title:", entry["title"], "| PDF:", entry["links"]["pdf"])
------------------------------------------------------------

PROMPT 218:
"Generate Python code that uses the requests library to send HTTP requests to interact with the SNC REST API, including functions to create, read, update, and delete resources."
RESPONSE:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Python client for interacting with the SNC (ServiceNow) REST API using requests.
Includes CRUD helpers for the Table API endpoints.
"""

from __future__ import annotations

import json
import time
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

import requests
from requests import Response, Session
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class SNCAPIError(Exception):
    def __init__(self, message: str, status_code: Optional[int] = None, response_text: Optional[str] = None):
        super().__init__(message)
        self.status_code = status_code
        self.response_text = response_text


@dataclass
class SNCClientConfig:
    base_url: str  # e.g., "https://your-instance.service-now.com"
    username: Optional[str] = None
    password: Optional[str] = None
    bearer_token: Optional[str] = None
    timeout: int = 30
    verify_ssl: bool = True
    max_retries: int = 3
    backoff_factor: float = 0.5
    proxies: Optional[Dict[str, str]] = None
    # Additional headers to merge with defaults
    extra_headers: Optional[Dict[str, str]] = None


class SNCClient:
    def __init__(self, config: SNCClientConfig):
        if not config.base_url.startswith("http"):
            raise ValueError("base_url must include scheme, e.g., https://your-instance.service-now.com")
        self.base_url = config.base_url.rstrip("/")
        self.timeout = config.timeout
        self.verify_ssl = config.verify_ssl

        self.session: Session = requests.Session()
        self.session.proxies = config.proxies or {}

        # Retries for transient errors including 429/5xx
        retry = Retry(
            total=config.max_retries,
            backoff_factor=config.backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "POST", "PUT", "PATCH", "DELETE", "OPTIONS"],
            raise_on_status=False,
        )
        adapter = HTTPAdapter(max_retries=retry)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)

        # Auth: bearer preferred if provided, else basic
        self.session.headers.update({
            "Accept": "application/json",
            "Content-Type": "application/json",
            "User-Agent": "snc-python-client/1.0",
        })

        if config.bearer_token:
            self.session.headers["Authorization"] = f"Bearer {config.bearer_token}"
        elif config.username and config.password:
            self.session.auth = (config.username, config.password)
        else:
            # Anonymous not typical for SNC; still allowed for some public endpoints
            pass

        if config.extra_headers:
            self.session.headers.update(config.extra_headers)

    def _url(self, path: str) -> str:
        return f"{self.base_url}/{path.lstrip('/')}"

    def _request(
        self,
        method: str,
        path: str,
        params: Optional[Dict[str, Any]] = None,
        json_body: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
    ) -> Response:
        url = self._url(path)
        try:
            resp = self.session.request(
                method=method.upper(),
                url=url,
                params=params or {},
                json=json_body,
                headers=headers,
                timeout=self.timeout,
                verify=self.verify_ssl,
            )
        except requests.RequestException as e:
            raise SNCAPIError(f"Request failed: {e}") from e

        # Best-effort handling of SNC Table API responses
        if resp.status_code >= 400:
            # Try to extract meaningful error info
            msg = f"HTTP {resp.status_code} for {method} {url}"
            try:
                data = resp.json()
                # ServiceNow usually returns {'error': {'message': ..., 'detail': ...}}
                if isinstance(data, dict):
                    if "error" in data and isinstance(data["error"], dict):
                        err = data["error"]
                        det_msg = err.get("message") or err.get("detail")
                        if det_msg:
                            msg += f": {det_msg}"
                    elif "result" in data and isinstance(data["result"], dict):
                        # Sometimes errors appear nested
                        err_msg = data["result"].get("error") or data["result"].get("status")
                        if err_msg:
                            msg += f": {err_msg}"
            except ValueError:
                # Non-JSON error
                pass
            raise SNCAPIError(msg, status_code=resp.status_code, response_text=resp.text)
        return resp

    # ---------------------------
    # Generic Table API helpers
    # ---------------------------

    def create_record(
        self,
        table: str,
        payload: Dict[str, Any],
        params: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        resp = self._request("POST", f"/api/now/table/{table}", params=params, json_body=payload)
        return self._parse_result(resp)

    def get_record(
        self,
        table: str,
        sys_id: str,
        params: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        resp = self._request("GET", f"/api/now/table/{table}/{sys_id}", params=params)
        return self._parse_result(resp)

    def query_records(
        self,
        table: str,
        sysparm_query: Optional[str] = None,
        sysparm_fields: Optional[Union[str, List[str]]] = None,
        sysparm_limit: int = 100,
        sysparm_offset: int = 0,
        other_params: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        params: Dict[str, Any] = other_params.copy() if other_params else {}
        if sysparm_query:
            params["sysparm_query"] = sysparm_query
        if sysparm_fields:
            params["sysparm_fields"] = ",".join(sysparm_fields) if isinstance(sysparm_fields, list) else sysparm_fields
        params["sysparm_limit"] = sysparm_limit
        params["sysparm_offset"] = sysparm_offset

        resp = self._request("GET", f"/api/now/table/{table}", params=params)
        result = self._parse_result(resp)
        if isinstance(result, list):
            return result
        # Some instances may return a dict or single result when limit=1
        return [result] if isinstance(result, dict) else []

    def update_record(
        self,
        table: str,
        sys_id: str,
        payload: Dict[str, Any],
        use_patch: bool = True,
        params: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        method = "PATCH" if use_patch else "PUT"
        resp = self._request(method, f"/api/now/table/{table}/{sys_id}", params=params, json_body=payload)
        return self._parse_result(resp)

    def delete_record(
        self,
        table: str,
        sys_id: str,
        params: Optional[Dict[str, Any]] = None,
    ) -> bool:
        resp = self._request("DELETE", f"/api/now/table/{table}/{sys_id}", params=params)
        # On success, some SNC instances return 204 No Content, others return a result envelope
        if resp.status_code == 204 or not resp.content:
            return True
        try:
            data = resp.json()
            if isinstance(data, dict) and "result" in data:
                # Consider delete successful if result is truthy
                return True
        except ValueError:
            pass
        return True  # If HTTP < 400 made it here, treat as success

    def list_all_records(
        self,
        table: str,
        sysparm_query: Optional[str] = None,
        sysparm_fields: Optional[Union[str, List[str]]] = None,
        page_size: int = 100,
        max_records: Optional[int] = None,
        other_params: Optional[Dict[str, Any]] = None,
        sleep_between_pages: float = 0.0,
    ) -> List[Dict[str, Any]]:
        results: List[Dict[str, Any]] = []
        offset = 0
        remaining = max_records

        while True:
            batch_size = page_size if remaining is None else min(page_size, remaining)
            rows = self.query_records(
                table=table,
                sysparm_query=sysparm_query,
                sysparm_fields=sysparm_fields,
                sysparm_limit=batch_size,
                sysparm_offset=offset,
                other_params=other_params,
            )
            if not rows:
                break
            results.extend(rows)
            offset += len(rows)
            if remaining is not None:
                remaining -= len(rows)
                if remaining <= 0:
                    break
            if len(rows) < batch_size:
                break
            if sleep_between_pages > 0:
                time.sleep(sleep_between_pages)
        return results

    @staticmethod
    def _parse_result(resp: Response) -> Any:
        try:
            data = resp.json()
        except ValueError as e:
            raise SNCAPIError("Response is not valid JSON", status_code=resp.status_code, response_text=resp.text) from e

        # ServiceNow usually wraps data in {'result': ...}
        if isinstance(data, dict) and "result" in data:
            return data["result"]
        return data


# ---------------------------
# Example usage
# ---------------------------

if __name__ == "__main__":
    # Replace with your instance details
    cfg = SNCClientConfig(
        base_url="https://your-instance.service-now.com",
        username="your_username",
        password="your_password",
        # Or use bearer_token="your_oauth_token",
        verify_ssl=True,
        max_retries=5,
        backoff_factor=0.5,
    )

    snc = SNCClient(cfg)

    # Example table and payload (Incident table)
    table = "incident"
    payload = {
        "short_description": "API-created incident",
        "description": "Created via Python requests client",
        "urgency": "2",
        "impact": "2",
    }

    try:
        # Create
        created = snc.create_record(table, payload)
        sys_id = created.get("sys_id")
        print("Created:", json.dumps(created, indent=2))

        # Read (get by sys_id)
        record = snc.get_record(table, sys_id, params={"sysparm_fields": "number,sys_id,short_description,state"})
        print("Fetched:", json.dumps(record, indent=2))

        # Query (list)
        rows = snc.query_records(
            table,
            sysparm_query=f"sys_id={sys_id}",
            sysparm_fields=["sys_id", "number", "state"],
            sysparm_limit=1,
        )
        print("Queried:", json.dumps(rows, indent=2))

        # Update
        updated = snc.update_record(table, sys_id, {"state": "2", "work_notes": "Updated via API"}, use_patch=True)
        print("Updated:", json.dumps(updated, indent=2))

        # Delete (optional)
        deleted_ok = snc.delete_record(table, sys_id)
        print("Deleted:", deleted_ok)

    except SNCAPIError as e:
        print(f"API error: {e} (status={e.status_code})")
        if e.response_text:
            print(f"Response: {e.response_text}")
    except Exception as ex:
        print(f"Unexpected error: {ex}")
------------------------------------------------------------

PROMPT 219:
"Generate Python code that imports the necessary libraries and creates a simple asynchronous function using the futures module, demonstrating the basic functionality of the library."
RESPONSE:
# Demonstrates basic usage of concurrent.futures to run work asynchronously.

import time
import random
from concurrent import futures


def slow_square(x: int) -> int:
    """Simulate slow work and return x*x."""
    time.sleep(random.uniform(0.2, 1.0))
    return x * x


def square_async(x: int, executor: futures.Executor) -> futures.Future:
    """Submit slow_square to the executor and return a Future."""
    return executor.submit(slow_square, x)


def _on_done(fut: futures.Future) -> None:
    """Example callback invoked when a Future completes."""
    try:
        print(f"callback: result={fut.result()}")
    except Exception as e:
        print(f"callback: failed with {e!r}")


if __name__ == "__main__":
    # Use ThreadPoolExecutor; swap to ProcessPoolExecutor to use processes instead.
    # with futures.ProcessPoolExecutor() as executor:
    with futures.ThreadPoolExecutor(max_workers=4) as executor:
        # Submit a single asynchronous task.
        f1 = square_async(3, executor)
        f1.add_done_callback(_on_done)
        print(f"single future: waiting for result...")
        print(f"single future: result={f1.result()}")  # Blocks until done.

        # Submit multiple tasks and handle them as they complete.
        nums = list(range(5))
        futs = [square_async(n, executor) for n in nums]
        print("multiple futures: results as they complete:")
        for fut in futures.as_completed(futs):
            print(f"completed: {fut.result()}")

        # Alternatively, use executor.map for ordered results.
        print("ordered results via map:")
        for result in executor.map(slow_square, nums):
            print(result)
------------------------------------------------------------

PROMPT 220:
"Generate Python code that imports the sys module, creates a wrapper function for the raise statement, and defines a format string to display the traceback in a human-readable format."
RESPONSE:
import sys
import traceback

# Wrapper function for the 'raise' statement (useful in expressions/lambdas)
def raise_(exc):
    raise exc

# Format string for human-readable traceback output
TRACEBACK_FORMAT = (
    "Traceback (most recent call last):\n"
    "{tb}"
    "{etype}: {evalue}"
)

def format_traceback(exc_info=None):
    """
    Returns a formatted traceback string using TRACEBACK_FORMAT.
    If exc_info is None, uses the current exception via sys.exc_info().
    """
    if exc_info is None:
        exc_info = sys.exc_info()
    etype, evalue, tb = exc_info
    tb_str = ''.join(traceback.format_tb(tb)) if tb else ''
    return TRACEBACK_FORMAT.format(
        tb=tb_str,
        etype=etype.__name__ if etype else '',
        evalue=str(evalue) if evalue is not None else ''
    )
------------------------------------------------------------

PROMPT 221:
"Generate Python code that implements data types and interchange formats for numbers, strings, and booleans, including functions for parsing and serializing data in JSON and CSV formats."
RESPONSE:
from __future__ import annotations

import csv
import io
import json
from abc import ABC, abstractmethod
from typing import Any, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple, Union


# Exceptions

class DataTypeError(Exception):
    pass


class ParseError(Exception):
    pass


class SerializationError(Exception):
    pass


class SchemaError(Exception):
    pass


# Simple data types: Number, String, Boolean

class SimpleType(ABC):
    name: str

    @abstractmethod
    def coerce(self, value: Any) -> Any:
        """Coerce a value into this type, or raise DataTypeError."""

    @abstractmethod
    def to_string(self, value: Any) -> str:
        """Convert a typed value into a canonical string representation."""

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}()"


class NumberType(SimpleType):
    """
    Represents a numeric type (int or float).
    - prefer_int: if True, floats representing integral values are returned as int.
    - allow_scientific: allow scientific notation when parsing from strings.
    """
    name = "number"

    def __init__(self, prefer_int: bool = True, allow_scientific: bool = True) -> None:
        self.prefer_int = prefer_int
        self.allow_scientific = allow_scientific

    def coerce(self, value: Any) -> Union[int, float]:
        if isinstance(value, bool):
            # Prevent bool from being treated as int
            raise DataTypeError("Boolean cannot be coerced to number")
        if isinstance(value, (int, float)):
            return self._normalize_number(value)

        if isinstance(value, bytes):
            value = value.decode("utf-8", errors="strict")

        if isinstance(value, str):
            s = value.strip().replace("_", "")
            if s == "":
                raise DataTypeError("Empty string cannot be coerced to number")
            # Validate allowed characters
            lower = s.lower()
            if lower in ("nan", "inf", "+inf", "-inf"):
                raise DataTypeError("NaN/Infinity are not allowed")
            try:
                # Decide int vs float parsing
                if any(ch in lower for ch in (".", "e")):
                    if not self.allow_scientific and "e" in lower:
                        raise DataTypeError("Scientific notation is not allowed")
                    num = float(lower)
                    return self._normalize_number(num)
                else:
                    # Integer (handles leading + or -)
                    num = int(lower, 10)
                    return self._normalize_number(num)
            except ValueError as e:
                raise DataTypeError(f"Invalid numeric string: {value!r}") from e

        raise DataTypeError(f"Cannot coerce type {type(value).__name__} to number")

    def _normalize_number(self, value: Union[int, float]) -> Union[int, float]:
        if isinstance(value, float):
            if not (value == value) or value in (float("inf"), float("-inf")):
                raise DataTypeError("NaN/Infinity are not allowed")
            if self.prefer_int:
                iv = int(value)
                if float(iv) == value:
                    return iv
        return value

    def to_string(self, value: Any) -> str:
        v = self.coerce(value)
        # Use minimal string representation
        return str(v)


class StringType(SimpleType):
    """
    Represents a string type.
    - encoding: used when decoding from bytes.
    """
    name = "string"

    def __init__(self, encoding: str = "utf-8") -> None:
        self.encoding = encoding

    def coerce(self, value: Any) -> str:
        if isinstance(value, str):
            return value
        if isinstance(value, bytes):
            try:
                return value.decode(self.encoding, errors="strict")
            except Exception as e:
                raise DataTypeError(f"Failed to decode bytes as {self.encoding}") from e
        raise DataTypeError(f"Cannot coerce type {type(value).__name__} to string")

    def to_string(self, value: Any) -> str:
        return self.coerce(value)


class BooleanType(SimpleType):
    """
    Represents a boolean type.
    Accepts (case-insensitive) strings: true/false, t/f, yes/no, y/n, 1/0, on/off.
    Accepts integers 1/0 (but not floats).
    """
    name = "boolean"

    TRUE_STRS = {"true", "t", "yes", "y", "1", "on"}
    FALSE_STRS = {"false", "f", "no", "n", "0", "off"}

    def coerce(self, value: Any) -> bool:
        if isinstance(value, bool):
            return value
        if isinstance(value, int) and not isinstance(value, bool):
            if value == 1:
                return True
            if value == 0:
                return False
            raise DataTypeError("Only integers 1 or 0 can be coerced to boolean")
        if isinstance(value, bytes):
            value = value.decode("utf-8", errors="strict")
        if isinstance(value, str):
            s = value.strip().lower()
            if s in self.TRUE_STRS:
                return True
            if s in self.FALSE_STRS:
                return False
            raise DataTypeError(f"Invalid boolean string: {value!r}")
        raise DataTypeError(f"Cannot coerce type {type(value).__name__} to boolean")

    def to_string(self, value: Any) -> str:
        return "true" if self.coerce(value) else "false"


# Schema handling

Schema = Union[SimpleType, List["Schema"], Dict[str, "Schema"]]


def coerce_with_schema(value: Any, schema: Schema) -> Any:
    """
    Coerce a Python value to the given schema.
    - SimpleType: coerce a single value.
    - list schema:
      * [subschema] => value must be a list/sequence; coerce each element using subschema.
      * [s1, s2, ...] => value must be a list/sequence of same length; coerce by position.
    - dict schema: {field: subschema} => value must be a mapping; coerce per key.
    """
    if isinstance(schema, SimpleType):
        return schema.coerce(value)

    if isinstance(schema, list):
        if len(schema) == 0:
            raise SchemaError("List schema must not be empty")
        if not isinstance(value, (list, tuple)):
            raise DataTypeError("Expected list/tuple for list schema")
        if len(schema) == 1:
            subschema = schema[0]
            return [coerce_with_schema(v, subschema) for v in value]
        else:
            if len(value) != len(schema):
                raise DataTypeError(f"Expected list of length {len(schema)}, got {len(value)}")
            return [coerce_with_schema(v, subs) for v, subs in zip(value, schema)]

    if isinstance(schema, dict):
        if not isinstance(value, Mapping):
            raise DataTypeError("Expected mapping for dict schema")
        result: Dict[str, Any] = {}
        for key, subschema in schema.items():
            if key not in value:
                raise DataTypeError(f"Missing required key in object: {key!r}")
            result[key] = coerce_with_schema(value[key], subschema)
        # Optionally carry through extra keys unchanged
        for k, v in value.items():
            if k not in result:
                result[k] = v
        return result

    raise SchemaError(f"Unsupported schema type: {type(schema).__name__}")


def validate_allowed_json_value(value: Any, allow_container: bool = True) -> None:
    """
    Validate that a value is composed only of allowed primitives:
    str, int (not bool), float (not NaN/Inf), bool, and optionally lists/dicts containing these.
    """
    def is_number(x: Any) -> bool:
        if isinstance(x, bool):
            return False
        if isinstance(x, (int, float)):
            if isinstance(x, float) and (not (x == x) or x in (float("inf"), float("-inf"))):
                return False
            return True
        return False

    if isinstance(value, (str, bool)) or is_number(value):
        return
    if allow_container and isinstance(value, list):
        for v in value:
            validate_allowed_json_value(v, allow_container=True)
        return
    if allow_container and isinstance(value, dict):
        for k, v in value.items():
            if not isinstance(k, str):
                raise SerializationError("JSON object keys must be strings")
            validate_allowed_json_value(v, allow_container=True)
        return
    raise SerializationError(f"Value not representable in JSON under the allowed primitive set: {type(value).__name__}")


# JSON parsing/serialization

def serialize_json(value: Any, ensure_primitives: bool = True, indent: Optional[int] = None) -> str:
    """
    Serialize a value to JSON string.
    If ensure_primitives is True, only numbers, strings, booleans, lists, and dicts with string keys are allowed.
    NaN and Infinity are disallowed.
    """
    if ensure_primitives:
        validate_allowed_json_value(value, allow_container=True)
    try:
        return json.dumps(value, ensure_ascii=False, separators=(",", ":"), indent=indent)
    except (TypeError, ValueError) as e:
        raise SerializationError(f"Failed to serialize to JSON: {e}") from e


def parse_json(json_text: str, schema: Optional[Schema] = None) -> Any:
    """
    Parse a JSON string into Python values. Optionally coerce using a schema.
    If no schema is provided, returns the parsed Python structure (and you may validate later).
    """
    try:
        value = json.loads(json_text)
    except json.JSONDecodeError as e:
        raise ParseError(f"Invalid JSON: {e.msg} (line {e.lineno} col {e.colno})") from e

    if schema is not None:
        return coerce_with_schema(value, schema)

    return value


# CSV parsing/serialization

def _stringify_value_for_csv(value: Any, schema_type: Optional[SimpleType]) -> str:
    if schema_type is not None:
        return schema_type.to_string(value)

    # Fallback formatting
    if isinstance(value, bool):
        return "true" if value else "false"
    if isinstance(value, (int, float)) and not isinstance(value, bool):
        if isinstance(value, float):
            if not (value == value) or value in (float("inf"), float("-inf")):
                raise SerializationError("NaN/Infinity are not allowed in CSV")
        return str(value)
    if isinstance(value, str):
        return value
    if value is None:
        return ""
    return str(value)


def serialize_csv(
    rows: Iterable[Union[Mapping[str, Any], Sequence[Any]]],
    fieldnames: Optional[List[str]] = None,
    schema: Optional[Union[List[SimpleType], Dict[str, SimpleType]]] = None,
    include_header: bool = True,
    dialect: str = "excel",
) -> str:
    """
    Serialize rows to CSV text.
    - rows: an iterable of mappings (dict-like) or sequences (list/tuple).
    - fieldnames: required when rows are mappings unless inferable from the first row.
    - schema:
        * If rows are mappings: dict mapping fieldname -> SimpleType for formatting.
        * If rows are sequences: list of SimpleType for positional formatting.
    - include_header: write header row when rows are mappings.
    """
    buffer = io.StringIO()

    rows_iter = iter(rows)
    try:
        first = next(rows_iter)
        rows_iter = (r for r in (first, *rows_iter))
    except StopIteration:
        # No rows
        return ""

    # Determine if rows are mappings or sequences
    is_mapping = hasattr(first, "keys") and hasattr(first, "__getitem__")

    if is_mapping:
        if fieldnames is None:
            fieldnames = list(first.keys())
        writer = csv.DictWriter(buffer, fieldnames=fieldnames, dialect=dialect)
        if include_header:
            writer.writeheader()
        for row in rows_iter:
            if not hasattr(row, "keys"):
                raise SerializationError("Mixed row types: expected mapping/dict rows")
            out_row: Dict[str, str] = {}
            for key in fieldnames:
                val = row.get(key, "")
                stype = None
                if isinstance(schema, dict):
                    stype = schema.get(key)
                out_row[key] = _stringify_value_for_csv(val, stype)
            writer.writerow(out_row)
    else:
        writer = csv.writer(buffer, dialect=dialect)
        if include_header:
            if fieldnames:
                writer.writerow(fieldnames)
        for row in rows_iter:
            if not isinstance(row, (list, tuple)):
                raise SerializationError("Mixed row types: expected sequence/list rows")
            out_row: List[str] = []
            for idx, val in enumerate(row):
                stype = None
                if isinstance(schema, list) and idx < len(schema):
                    stype = schema[idx]
                out_row.append(_stringify_value_for_csv(val, stype))
            writer.writerow(out_row)

    return buffer.getvalue()


def parse_csv(
    csv_text: str,
    schema: Optional[Union[List[SimpleType], Dict[str, SimpleType]]] = None,
    has_header: bool = True,
    dialect: str = "excel",
) -> Union[List[Dict[str, Any]], List[List[Any]]]:
    """
    Parse CSV text into rows with typed values per schema.
    - If has_header:
        * Returns a list of dict rows.
        * schema must be a dict mapping fieldname -> SimpleType. Unknown columns are left as strings.
    - If not has_header:
        * Returns a list of list rows.
        * schema must be a list of SimpleType with the same length as each row (or shorter to coerce prefix).
    """
    buffer = io.StringIO(csv_text)
    results: List[Any] = []

    if has_header:
        reader = csv.DictReader(buffer, dialect=dialect)
        if schema is not None and not isinstance(schema, dict):
            raise SchemaError("For headered CSV, schema must be a dict of fieldname -> SimpleType")
        for row in reader:
            typed_row: Dict[str, Any] = {}
            for key, raw in row.items():
                raw_str = "" if raw is None else raw
                if schema and key in schema:
                    try:
                        typed_row[key] = schema[key].coerce(raw_str)
                    except DataTypeError as e:
                        raise ParseError(f"Failed to parse field {key!r}: {e}") from e
                else:
                    typed_row[key] = raw_str
            # Validate required schema fields exist
            if schema:
                for required_key in schema.keys():
                    if required_key not in typed_row:
                        raise ParseError(f"Missing required column in CSV: {required_key!r}")
            results.append(typed_row)
    else:
        reader = csv.reader(buffer, dialect=dialect)
        if schema is not None and not isinstance(schema, list):
            raise SchemaError("For non-header CSV, schema must be a list of SimpleType")
        for row in reader:
            typed_row: List[Any] = []
            for idx, raw in enumerate(row):
                raw_str = "" if raw is None else raw
                if schema and idx < len(schema):
                    try:
                        typed_row.append(schema[idx].coerce(raw_str))
                    except DataTypeError as e:
                        raise ParseError(f"Failed to parse column {idx}: {e}") from e
                else:
                    typed_row.append(raw_str)
            results.append(typed_row)

    return results


# Convenience: commonly used simple types
Number = NumberType()
String = StringType()
Boolean = BooleanType()


# Example default schema helpers

def object_schema(fields: Mapping[str, SimpleType]) -> Dict[str, SimpleType]:
    return dict(fields)


def list_of(subschema: Schema) -> List[Schema]:
    return [subschema]


def tuple_schema(*subschemas: Schema) -> List[Schema]:
    return list(subschemas)
------------------------------------------------------------

PROMPT 222:
"Generate Python code that imports a module and loads a specified function from it, lazily loading the module only when the function is called, using the `inspect` and `importlib` modules."
RESPONSE:
import importlib
import inspect
import threading
from functools import update_wrapper
from typing import Any, Callable


def lazy_function(module_name: str, function_name: str) -> Callable[..., Any]:
    """
    Return a callable that imports `module_name` and retrieves `function_name`
    only on the first invocation. After loading, the wrapper's metadata (name,
    docstring, signature) are updated to match the real function.
    """
    _func_ref: list[Any] = [None]  # mutable cell for the loaded function
    _lock = threading.Lock()

    def _load() -> Callable[..., Any]:
        mod = importlib.import_module(module_name)
        try:
            fn = getattr(mod, function_name)
        except AttributeError as exc:
            raise AttributeError(f"Module {module_name!r} has no attribute {function_name!r}") from exc
        if not (inspect.isroutine(fn) or callable(fn)):
            raise TypeError(f"{module_name}.{function_name} is not callable (got {type(fn)!r})")
        _func_ref[0] = fn
        # Update wrapper metadata/signature to mirror the real function
        try:
            update_wrapper(wrapper, fn)
            wrapper.__signature__ = inspect.signature(fn)  # for better introspection
        except Exception:
            pass
        return fn

    def wrapper(*args: Any, **kwargs: Any) -> Any:
        fn = _func_ref[0]
        if fn is None:
            with _lock:
                if _func_ref[0] is None:
                    fn = _load()
                else:
                    fn = _func_ref[0]
        return fn(*args, **kwargs)

    # Minimal pre-load metadata
    wrapper.__name__ = f"{function_name} (lazy from {module_name})"
    wrapper.__doc__ = f"Lazily loads {function_name!r} from module {module_name!r} on first call."
    return wrapper


# Example usage
if __name__ == "__main__":
    sqrt = lazy_function("math", "sqrt")
    print(sqrt(9))  # imports math and calls math.sqrt(9)

    dumps = lazy_function("json", "dumps")
    print(dumps({"x": 1}))  # imports json and calls json.dumps(...)
------------------------------------------------------------

PROMPT 223:
"Generate Python code that creates a CloudFormation template file with a specified AWS region, account ID, and default output format, using the Boto3 library to interact with AWS services."
RESPONSE:
import argparse
import boto3
import botocore
import json
from datetime import datetime
from pathlib import Path
import sys


def build_template(specified_region: str, specified_account_id: str, default_output_format: str) -> dict:
    """
    Build a minimal CloudFormation template that embeds the specified region,
    account ID, and default output format as Parameters and Metadata, and
    also exposes runtime values via Outputs.
    """
    allowed_output_values = ["json", "yaml", "text", "table"]
    if default_output_format not in allowed_output_values:
        raise ValueError(f"default_output_format must be one of {allowed_output_values}")

    template = {
        "AWSTemplateFormatVersion": "2010-09-09",
        "Description": f"Generated template for region {specified_region} and account {specified_account_id}",
        "Metadata": {
            "Generator": {
                "Name": "boto3-template-generator",
                "Timestamp": datetime.utcnow().isoformat() + "Z",
                "SpecifiedRegion": specified_region,
                "SpecifiedAccountId": specified_account_id,
                "DefaultOutputFormat": default_output_format,
            }
        },
        "Parameters": {
            "SpecifiedRegion": {
                "Type": "String",
                "Description": "The AWS region this template was generated for",
                "Default": specified_region,
                "AllowedPattern": ".+"
            },
            "SpecifiedAccountId": {
                "Type": "String",
                "Description": "The AWS account ID this template was generated for",
                "Default": specified_account_id,
                "AllowedPattern": r"\d{12}"
            },
            "DefaultOutputFormat": {
                "Type": "String",
                "Description": "A default output format label associated with this template",
                "Default": default_output_format,
                "AllowedValues": allowed_output_values
            },
        },
        "Resources": {},
        "Outputs": {
            "SpecifiedRegion": {
                "Description": "The region specified at generation time",
                "Value": {"Ref": "SpecifiedRegion"}
            },
            "SpecifiedAccountId": {
                "Description": "The account ID specified at generation time",
                "Value": {"Ref": "SpecifiedAccountId"}
            },
            "DefaultOutputFormat": {
                "Description": "The default output format specified at generation time",
                "Value": {"Ref": "DefaultOutputFormat"}
            },
            "RuntimeRegion": {
                "Description": "The region where the stack is actually deployed",
                "Value": {"Ref": "AWS::Region"}
            },
            "RuntimeAccountId": {
                "Description": "The account ID where the stack is actually deployed",
                "Value": {"Ref": "AWS::AccountId"}
            },
        },
    }
    return template


def validate_with_cloudformation(cf_client, template_body: str):
    """
    Validate the generated template with CloudFormation.
    """
    return cf_client.validate_template(TemplateBody=template_body)


def main():
    parser = argparse.ArgumentParser(
        description="Generate a CloudFormation template file containing specified region, account ID, and default output format, using boto3."
    )
    parser.add_argument("--region", required=True, help="AWS region to use (for session and template).")
    parser.add_argument("--account-id", required=False, help="AWS account ID to embed; if omitted, STS will be used to detect it.")
    parser.add_argument("--default-output-format", required=True, choices=["json", "yaml", "text", "table"], help="Default output format label to embed in the template.")
    parser.add_argument("--outfile", required=True, help="Path to write the CloudFormation template JSON file.")
    parser.add_argument("--profile", required=False, help="AWS profile name to use from shared config/credentials.")
    parser.add_argument("--skip-validate", action="store_true", help="Skip CloudFormation template validation.")
    args = parser.parse_args()

    # Create boto3 session
    try:
        session = boto3.Session(profile_name=args.profile, region_name=args.region) if args.profile else boto3.Session(region_name=args.region)
    except botocore.exceptions.ProfileNotFound as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)

    # Get account ID if not provided
    account_id = args.account_id
    if not account_id:
        try:
            sts = session.client("sts")
            account_id = sts.get_caller_identity()["Account"]
        except botocore.exceptions.BotoCoreError as e:
            print(f"Failed to retrieve account ID via STS: {e}", file=sys.stderr)
            sys.exit(2)

    # Build template
    try:
        template = build_template(args.region, account_id, args.default_output_format)
    except Exception as e:
        print(f"Failed to build template: {e}", file=sys.stderr)
        sys.exit(3)

    template_body = json.dumps(template, indent=2)

    # Validate with CloudFormation (optional)
    if not args.skip_validate:
        try:
            cf = session.client("cloudformation")
            resp = validate_with_cloudformation(cf, template_body)
            # Optional: print a brief success confirmation
            desc = resp.get("Description") or "Template is valid."
            print(f"Validation succeeded: {desc}")
        except botocore.exceptions.ClientError as e:
            print(f"CloudFormation validation error: {e}", file=sys.stderr)
            sys.exit(4)
        except botocore.exceptions.BotoCoreError as e:
            print(f"CloudFormation validation failed: {e}", file=sys.stderr)
            sys.exit(5)

    # Write to file
    try:
        outpath = Path(args.outfile).expanduser().resolve()
        outpath.parent.mkdir(parents=True, exist_ok=True)
        outpath.write_text(template_body, encoding="utf-8")
        print(f"Wrote template to {outpath}")
    except Exception as e:
        print(f"Failed to write template file: {e}", file=sys.stderr)
        sys.exit(6)


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 224:
"Generate Python code that imports the necessary modules and defines a function that takes in data as input and returns the MurmurHash3 hash value for that data using the murmurhash module."
RESPONSE:
from typing import Union
from murmurhash import murmurhash

def murmurhash3(data: Union[str, bytes, bytearray], seed: int = 0) -> int:
    """
    Compute the MurmurHash3 (32-bit) hash for the given data using the murmurhash module.

    Args:
        data: Input data as str, bytes, or bytearray.
        seed: Optional hash seed (default: 0).

    Returns:
        int: 32-bit MurmurHash3 hash value (may be negative due to signed int representation).
    """
    if isinstance(data, (bytes, bytearray)):
        buf = bytes(data)
    else:
        buf = str(data).encode("utf-8")
    return murmurhash.hash(buf, seed)
------------------------------------------------------------

PROMPT 225:
"Generate Python code that creates a single-server client-server database server for ZODB using the `ZEO` package, allowing clients to connect and interact with the database simultaneously."
RESPONSE:
Below are two small Python programs: one starts a single ZEO server (backed by a FileStorage), and the other connects multiple clients to it concurrently and updates shared data in ZODB.

Dependencies:
- pip install ZODB ZEO BTrees persistent transaction

server.py
- Starts a ZEO server listening on 127.0.0.1:8090 using a FileStorage at data.fs.
- Runs in the foreground; press Ctrl+C to stop.

# server.py
import sys

# Try to invoke the ZEO server entry point programmatically; fall back to -m if needed.
def main(address="127.0.0.1:8090", filestorage="data.fs"):
    try:
        from ZEO.runzeo import main as runzeo_main
        # Equivalent to: runzeo -a 127.0.0.1:8090 -f data.fs -D
        # -D keeps it in the foreground (don't daemonize).
        runzeo_main(["-a", address, "-f", filestorage, "-D"])
    except Exception:
        # Fallback: spawn the runzeo module as a subprocess
        import subprocess
        cmd = [sys.executable, "-m", "ZEO.runzeo", "-a", address, "-f", filestorage]
        subprocess.run(cmd, check=True)

if __name__ == "__main__":
    # Customize address or file path here if desired.
    main("127.0.0.1:8090", "data.fs")


client.py
- Demonstrates multiple concurrent clients connecting to the ZEO server.
- Uses a BTree (BTrees.OOBTree.OOBTree) to avoid write conflicts between clients updating different keys.
- Provide --init to initialize the database structure once before concurrent clients run.
- Run multiple instances or let the script spawn multiple processes.

# client.py
import argparse
import time
from multiprocessing import Process

import ZODB
import transaction
from ZODB.DB import DB
from ZEO.ClientStorage import ClientStorage
from BTrees.OOBTree import OOBTree


def open_db(address="127.0.0.1", port=8090):
    storage = ClientStorage((address, port))
    db = DB(storage)
    return db


def init_db(address="127.0.0.1", port=8090):
    db = open_db(address, port)
    conn = db.open()
    try:
        root = conn.root()
        if "counters" not in root:
            root["counters"] = OOBTree()
            transaction.commit()
            print("Initialized root['counters'] as an OOBTree.")
        else:
            print("root['counters'] already exists.")
    finally:
        conn.close()
        db.close()


def client_worker(client_id, iterations=10, delay=0.05, address="127.0.0.1", port=8090):
    db = open_db(address, port)
    conn = db.open()
    try:
        root = conn.root()
        if "counters" not in root:
            raise RuntimeError("Database not initialized. Run with --init once before starting clients.")
        counters = root["counters"]

        for i in range(iterations):
            # Each client updates its own key; OOBTree merges disjoint key updates.
            current = counters.get(client_id, 0)
            counters[client_id] = current + 1
            transaction.commit()
            time.sleep(delay)
    finally:
        conn.close()
        db.close()


def read_counters(address="127.0.0.1", port=8090):
    db = open_db(address, port)
    conn = db.open()
    try:
        root = conn.root()
        counters = dict(root.get("counters", {}))
        return counters
    finally:
        conn.close()
        db.close()


def main():
    parser = argparse.ArgumentParser(description="ZEO client demo")
    parser.add_argument("--address", default="127.0.0.1")
    parser.add_argument("--port", type=int, default=8090)
    parser.add_argument("--init", action="store_true", help="Initialize database (run once).")
    parser.add_argument("--spawn", type=int, default=0, help="Spawn N concurrent client workers.")
    parser.add_argument("--iters", type=int, default=20, help="Iterations per worker.")
    parser.add_argument("--delay", type=float, default=0.02, help="Sleep between commits.")
    args = parser.parse_args()

    if args.init:
        init_db(args.address, args.port)
        return

    if args.spawn > 0:
        procs = []
        for i in range(args.spawn):
            cid = f"client-{i}"
            p = Process(target=client_worker, args=(cid, args.iters, args.delay, args.address, args.port))
            p.start()
            procs.append(p)
        for p in procs:
            p.join()

        print("Final counters:", read_counters(args.address, args.port))
    else:
        # Single client usage example:
        client_worker("client-single", iterations=args.iters, delay=args.delay, address=args.address, port=args.port)
        print("Final counters:", read_counters(args.address, args.port))


if __name__ == "__main__":
    main()


How to run
1) Start the ZEO server:
   python server.py
   By default it listens on 127.0.0.1:8090 and stores data in data.fs.

2) Initialize the database once (in another terminal):
   python client.py --init

3) Run concurrent clients. For example, spawn 4 workers that each perform 50 increments:
   python client.py --spawn 4 --iters 50

4) You can also run multiple instances of client.py in separate terminals simultaneously; they will all connect to the same ZEO server and work concurrently.
------------------------------------------------------------

PROMPT 226:
"Generate Python code that uses the `csv` module to read a CSV file and the `dataclasses` module to define classes for the data, then maps the CSV data into instances of the defined classes."
RESPONSE:
import csv
from dataclasses import dataclass, field, fields
from typing import Any, Callable, Dict, List, Optional, Type, TypeVar, get_args, get_origin, Union
from datetime import date, datetime

T = TypeVar("T")


# Example CSV (customers.csv):
# user_id,name,email,joined,active,balance
# 1,Alice,alice@example.com,2024-01-15,true,120.50
# 2,Bob,,2023-11-05,false,0


@dataclass
class Customer:
    id: int = field(metadata={"csv": "user_id"})
    name: str
    email: Optional[str] = None
    joined: Optional[date] = None
    active: bool = True
    balance: float = 0.0


def _is_optional(tp: Any) -> bool:
    return get_origin(tp) is Union and type(None) in get_args(tp)


def _unwrap_optional(tp: Any) -> Any:
    return next(t for t in get_args(tp) if t is not type(None))  # noqa: E721


def _to_bool(raw: str) -> bool:
    s = raw.strip().lower()
    if s in {"1", "true", "t", "yes", "y"}:
        return True
    if s in {"0", "false", "f", "no", "n"}:
        return False
    raise ValueError(f"Cannot parse boolean from '{raw}'")


def _to_date(raw: str) -> date:
    raw = raw.strip()
    # Try ISO first
    try:
        return date.fromisoformat(raw)
    except Exception:
        pass
    # Try a few common formats
    for fmt in ("%Y/%m/%d", "%m/%d/%Y", "%d/%m/%Y"):
        try:
            return datetime.strptime(raw, fmt).date()
        except Exception:
            continue
    raise ValueError(f"Cannot parse date from '{raw}'")


def _to_datetime(raw: str) -> datetime:
    raw = raw.strip()
    # Try ISO first
    try:
        return datetime.fromisoformat(raw)
    except Exception:
        pass
    # Try a few common formats
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
        try:
            return datetime.strptime(raw, fmt)
        except Exception:
            continue
    raise ValueError(f"Cannot parse datetime from '{raw}'")


def _default_converter_for_type(tp: Any) -> Callable[[str], Any]:
    if tp is str:
        return lambda s: s
    if tp is int:
        return lambda s: int(s.strip())
    if tp is float:
        return lambda s: float(s.strip())
    if tp is bool:
        return _to_bool
    if tp is date:
        return _to_date
    if tp is datetime:
        return _to_datetime
    # Fallback: return raw string
    return lambda s: s


def map_row_to_dataclass(
    row: Dict[str, str],
    cls: Type[T],
    converters: Optional[Dict[str, Callable[[str], Any]]] = None,
) -> T:
    """
    Map a CSV DictReader row to a dataclass instance.
    - converters: optional per-field converters, keyed by dataclass field name.
    """
    converters = converters or {}
    kwargs: Dict[str, Any] = {}

    for f in fields(cls):
        col_name = f.metadata.get("csv", f.name)
        if col_name not in row:
            # Column not present; let default/default_factory handle it
            continue

        raw = row[col_name]
        # Treat empty strings as "missing"
        if raw is None or raw.strip() == "":
            # If Optional type, set None; else skip so dataclass default applies
            if _is_optional(f.type):
                kwargs[f.name] = None
            continue

        # Use custom converter if provided for this field
        if f.name in converters:
            kwargs[f.name] = converters[f.name](raw)
            continue

        # Determine the effective type (handle Optional)
        target_type = f.type
        if _is_optional(target_type):
            target_type = _unwrap_optional(target_type)

        # Convert using a reasonable default converter
        conv = _default_converter_for_type(target_type)
        kwargs[f.name] = conv(raw)

    return cls(**kwargs)


def read_csv_to_dataclasses(
    path: str,
    cls: Type[T],
    *,
    delimiter: str = ",",
    encoding: str = "utf-8",
    converters: Optional[Dict[str, Callable[[str], Any]]] = None,
) -> List[T]:
    """
    Read a CSV file and map rows into instances of the given dataclass type.
    - converters: optional per-field converters, keyed by dataclass field name.
    """
    result: List[T] = []
    with open(path, "r", newline="", encoding=encoding) as f:
        reader = csv.DictReader(f, delimiter=delimiter)
        for row in reader:
            instance = map_row_to_dataclass(row, cls, converters=converters)
            result.append(instance)
    return result


# Example usage
if __name__ == "__main__":
    customers = read_csv_to_dataclasses("customers.csv", Customer)
    for c in customers:
        print(c)
------------------------------------------------------------

PROMPT 227:
"Generate Python code that imports the necessary libraries and creates a connection to a data.world dataset, allowing you to perform various data manipulation and analysis tasks on the data."
RESPONSE:
# pip install datadotworld[pandas]
import os
import pandas as pd
import datadotworld as dw


def ensure_dw_auth():
    """
    Ensures a data.world API token is available via the DW_AUTH_TOKEN env var.
    Create a token at https://data.world/settings/advanced and export it, e.g.:
      export DW_AUTH_TOKEN="YOUR_TOKEN"
    """
    token = os.getenv("DW_AUTH_TOKEN")
    if not token:
        raise EnvironmentError(
            "DW_AUTH_TOKEN is not set. Create a data.world API token and set it as an environment variable."
        )
    # The datadotworld library reads DW_AUTH_TOKEN automatically; nothing else to do.
    return token


def list_tables(dataset_id: str) -> list[str]:
    """
    Returns a list of table names available in the dataset using data.world's SQL schema tables.
    """
    res = dw.query(dataset_id, "SELECT table_name FROM Tables")
    return res.dataframe["table_name"].tolist()


def load_table(dataset_id: str, table_name: str, limit: int | None = None) -> pd.DataFrame:
    """
    Loads a table (optionally limited to N rows) into a pandas DataFrame via SQL query.
    """
    q = f"SELECT * FROM {table_name}"
    if isinstance(limit, int) and limit > 0:
        q += f" LIMIT {limit}"
    return dw.query(dataset_id, q).dataframe


def run_example_analysis(dataset_id: str) -> None:
    """
    Demonstrates connecting, listing tables, loading data to pandas,
    and performing simple analysis.
    """
    ensure_dw_auth()

    # Discover tables in the dataset
    tables = list_tables(dataset_id)
    print(f"Discovered tables in {dataset_id}: {tables}")

    if not tables:
        print("No tables found in this dataset.")
        return

    # Load the first table into a DataFrame
    table = tables[0]
    df = load_table(dataset_id, table_name=table, limit=1000)
    print(f"\nLoaded table: {table} (showing first 5 rows)")
    print(df.head())

    # Example: general-purpose summary and basic manipulation
    print("\nDataFrame summary (numeric and categorical):")
    print(df.describe(include="all", datetime_is_numeric=True))

    # Example manipulation: count missing values per column
    na_counts = df.isna().sum().sort_values(ascending=False)
    print("\nMissing values per column (descending):")
    print(na_counts)

    # If a numeric column exists, show a quick aggregate as an example
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    if numeric_cols:
        col = numeric_cols[0]
        summary = df[col].agg(["mean", "median", "min", "max"])
        print(f"\nQuick stats for numeric column '{col}':")
        print(summary)


if __name__ == "__main__":
    # Replace with your dataset in the form "owner/dataset"
    DATASET_ID = "username/dataset"
    run_example_analysis(DATASET_ID)
------------------------------------------------------------

PROMPT 228:
"Generate Python code that allows developers to create templates with variable placeholders and use Python logic to manipulate template data, similar to Zope Template Application Language (TAL)."
RESPONSE:
# Tiny TAL-like templating engine (pure standard library)
# Features:
# - ${expr} placeholders inside text and attributes
# - tal:condition="expr" include/omit elements
# - tal:repeat="var expr" repeat an element for each item in an iterable
# - tal:content="expr" replace element body with expression result
# - tal:replace="expr" replace the whole element with expression result (no tag)
# - tal:attributes="attr1 expr1; attr2 expr2" set or override attributes
# - tal:omit-tag="expr" render children without the element wrapper if true
# - tal:define="name expr; ..." define local variables within element scope
# - Safe expression evaluator (no builtins; restricted AST; only whitelisted functions)
#
# Notes:
# - Templates must be well-formed XML/XHTML for xml.etree.ElementTree to parse.
# - By default, output is HTML-escaped. Use raw(...) in expressions to inject trusted HTML.

import ast
import copy
import html
import re
from collections import ChainMap
from xml.etree import ElementTree as ET


# ------------- Safe expression evaluation -------------

_ALLOWED_BIN_OPS = {
    ast.Add, ast.Sub, ast.Mult, ast.Div, ast.FloorDiv, ast.Mod, ast.Pow
}
_ALLOWED_UNARY_OPS = {ast.UAdd, ast.USub, ast.Not}
_ALLOWED_BOOL_OPS = {ast.And, ast.Or}
_ALLOWED_CMPS = {
    ast.Eq, ast.NotEq, ast.Lt, ast.LtE, ast.Gt, ast.GtE, ast.In, ast.NotIn, ast.Is, ast.IsNot
}

class RawHTML:
    __slots__ = ("value",)
    def __init__(self, value):
        self.value = str(value)

def raw(value):
    return RawHTML(value)

DEFAULT_SAFE_FUNCS = {
    "len": len,
    "min": min,
    "max": max,
    "sum": sum,
    "sorted": sorted,
    "range": range,
    "str": str,
    "int": int,
    "float": float,
    "bool": bool,
    "any": any,
    "all": all,
    "enumerate": enumerate,
    "zip": zip,
    "raw": raw,  # mark string as safe (no escaping)
}

class SafeEvaluator(ast.NodeVisitor):
    def __init__(self, scope: ChainMap, safe_funcs: dict):
        self.scope = scope
        self.safe_funcs = safe_funcs

    def eval(self, expr: str):
        try:
            node = ast.parse(expr, mode="eval")
        except SyntaxError as e:
            raise ValueError(f"Invalid expression: {expr!r}: {e}") from None
        return self.visit(node.body)

    # Literals
    def visit_Constant(self, node):
        return node.value

    def visit_List(self, node):
        return [self.visit(el) for el in node.elts]

    def visit_Tuple(self, node):
        return tuple(self.visit(el) for el in node.elts)

    def visit_Set(self, node):
        return {self.visit(el) for el in node.elts}

    def visit_Dict(self, node):
        return {self.visit(k): self.visit(v) for k, v in zip(node.keys, node.values)}

    # Names and attributes
    def visit_Name(self, node: ast.Name):
        ident = node.id
        if ident in self.safe_funcs:
            return self.safe_funcs[ident]
        try:
            return self.scope[ident]
        except KeyError:
            raise NameError(f"Unknown name: {ident}")

    def visit_Attribute(self, node: ast.Attribute):
        obj = self.visit(node.value)
        attr = node.attr
        if attr.startswith("_"):
            raise AttributeError("Access to private/protected attributes prohibited")
        return getattr(obj, attr)

    def visit_Subscript(self, node: ast.Subscript):
        obj = self.visit(node.value)
        sl = self.visit(node.slice)
        return obj[sl]

    def visit_Index(self, node):
        return self.visit(node.value)

    def visit_Slice(self, node):
        return slice(
            self.visit(node.lower) if node.lower else None,
            self.visit(node.upper) if node.upper else None,
            self.visit(node.step) if node.step else None,
        )

    # Ops
    def visit_BoolOp(self, node: ast.BoolOp):
        if type(node.op) not in _ALLOWED_BOOL_OPS:
            raise ValueError("Bool operation not allowed")
        if isinstance(node.op, ast.And):
            val = True
            for v in node.values:
                val = self.visit(v)
                if not val:
                    return val
            return val
        else:  # Or
            val = False
            for v in node.values:
                val = self.visit(v)
                if val:
                    return val
            return val

    def visit_BinOp(self, node: ast.BinOp):
        if type(node.op) not in _ALLOWED_BIN_OPS:
            raise ValueError("Binary operation not allowed")
        left = self.visit(node.left)
        right = self.visit(node.right)
        op = node.op
        if isinstance(op, ast.Add): return left + right
        if isinstance(op, ast.Sub): return left - right
        if isinstance(op, ast.Mult): return left * right
        if isinstance(op, ast.Div): return left / right
        if isinstance(op, ast.FloorDiv): return left // right
        if isinstance(op, ast.Mod): return left % right
        if isinstance(op, ast.Pow): return left ** right
        raise ValueError("Unsupported binary op")

    def visit_UnaryOp(self, node: ast.UnaryOp):
        if type(node.op) not in _ALLOWED_UNARY_OPS:
            raise ValueError("Unary operation not allowed")
        operand = self.visit(node.operand)
        if isinstance(node.op, ast.Not): return not operand
        if isinstance(node.op, ast.UAdd): return +operand
        if isinstance(node.op, ast.USub): return -operand
        raise ValueError("Unsupported unary op")

    def visit_Compare(self, node: ast.Compare):
        left = self.visit(node.left)
        for op, comp in zip(node.ops, node.comparators):
            if type(op) not in _ALLOWED_CMPS:
                raise ValueError("Comparison not allowed")
            right = self.visit(comp)
            ok = (
                (isinstance(op, ast.Eq) and left == right) or
                (isinstance(op, ast.NotEq) and left != right) or
                (isinstance(op, ast.Lt) and left < right) or
                (isinstance(op, ast.LtE) and left <= right) or
                (isinstance(op, ast.Gt) and left > right) or
                (isinstance(op, ast.GtE) and left >= right) or
                (isinstance(op, ast.In) and left in right) or
                (isinstance(op, ast.NotIn) and left not in right) or
                (isinstance(op, ast.Is) and left is right) or
                (isinstance(op, ast.IsNot) and left is not right)
            )
            if not ok:
                return False
            left = right
        return True

    def visit_IfExp(self, node: ast.IfExp):
        return self.visit(node.body) if self.visit(node.test) else self.visit(node.orelse)

    def visit_Call(self, node: ast.Call):
        # Only allow calls to whitelisted functions by name, not attribute calls
        if not isinstance(node.func, ast.Name):
            raise ValueError("Only calls to whitelisted functions by name are allowed")
        func = self.visit(node.func)
        if func not in self.safe_funcs.values():
            raise ValueError("Call to non-whitelisted function")
        args = [self.visit(a) for a in node.args]
        kwargs = {kw.arg: self.visit(kw.value) for kw in node.keywords}
        return func(*args, **kwargs)

    def generic_visit(self, node):
        raise ValueError(f"Unsupported expression node: {type(node).__name__}")


def eval_expr(expr: str, scope: ChainMap, safe_funcs: dict):
    evaluator = SafeEvaluator(scope, safe_funcs)
    return evaluator.eval(expr)


# ------------- Utilities -------------

_PLACEHOLDER_RE = re.compile(r"\$\{([^}]+)\}")

def html_escape(s: str) -> str:
    return html.escape(s, quote=True)

def to_text(val, autoescape: bool) -> str:
    if val is None:
        return ""
    if isinstance(val, RawHTML):
        return val.value  # trusted HTML
    s = str(val)
    return html_escape(s) if autoescape else s

def interpolate(text: str, scope: ChainMap, safe_funcs: dict, autoescape: bool) -> str:
    if not text:
        return ""
    def repl(m):
        expr = m.group(1).strip()
        value = eval_expr(expr, scope, safe_funcs)
        return to_text(value, autoescape)
    return _PLACEHOLDER_RE.sub(repl, text)


# ------------- TAL-like renderer -------------

_TAL_PREFIX = "tal:"
_TAL_ATTRS = {
    "tal:define",
    "tal:condition",
    "tal:repeat",
    "tal:content",
    "tal:replace",
    "tal:attributes",
    "tal:omit-tag",
}

def parse_defs(s: str):
    # "name expr; other expr2"
    parts = [p.strip() for p in s.split(";") if p.strip()]
    pairs = []
    for p in parts:
        try:
            name, expr = p.split(None, 1)
        except ValueError:
            raise ValueError(f"Invalid tal:define part: {p!r} (expected 'name expr')")
        pairs.append((name, expr.strip()))
    return pairs

def parse_attrs(s: str):
    # "href expr; title expr2"
    parts = [p.strip() for p in s.split(";") if p.strip()]
    pairs = []
    for p in parts:
        try:
            name, expr = p.split(None, 1)
        except ValueError:
            raise ValueError(f"Invalid tal:attributes part: {p!r} (expected 'attr expr')")
        pairs.append((name, expr.strip()))
    return pairs

def parse_repeat(s: str):
    # Accept "x expr" or "x in expr"
    tokens = s.split()
    if len(tokens) >= 3 and tokens[1] == "in":
        var = tokens[0]
        expr = " ".join(tokens[2:])
    elif len(tokens) >= 2:
        var = tokens[0]
        expr = " ".join(tokens[1:])
    else:
        raise ValueError(f"Invalid tal:repeat: {s!r}")
    return var, expr

class Template:
    def __init__(self, template_text: str, autoescape: bool = True, safe_funcs: dict = None):
        self.template_text = template_text
        self.autoescape = autoescape
        self.safe_funcs = dict(DEFAULT_SAFE_FUNCS)
        if safe_funcs:
            self.safe_funcs.update(safe_funcs)
        # Parse: wrap in a container to allow multiple top-level nodes
        try:
            self.root = ET.fromstring(f"<_root>{template_text}</_root>")
        except ET.ParseError as e:
            raise ValueError(f"Template parse error: {e}") from None

    def render(self, context: dict = None) -> str:
        scope = ChainMap(context or {})
        out_parts = []
        for child in list(self.root):
            out_parts.append(self._render_element(child, scope))
            if child.tail:
                out_parts.append(interpolate(child.tail, scope, self.safe_funcs, self.autoescape))
        return "".join(out_parts)

    def _render_element(self, el: ET.Element, scope: ChainMap, consumed_repeat=False) -> str:
        # Extract tal directives
        tal_define = el.attrib.get("tal:define")
        tal_condition = el.attrib.get("tal:condition")
        tal_repeat = el.attrib.get("tal:repeat")
        tal_content = el.attrib.get("tal:content")
        tal_replace = el.attrib.get("tal:replace")
        tal_attributes = el.attrib.get("tal:attributes")
        tal_omit_tag = el.attrib.get("tal:omit-tag")

        # Scope: tal:define introduces a new child scope for this element
        if tal_define:
            defs = parse_defs(tal_define)
            scope = scope.new_child()
            for name, expr in defs:
                scope[name] = eval_expr(expr, scope, self.safe_funcs)

        # tal:condition
        if tal_condition:
            cond = bool(eval_expr(tal_condition, scope, self.safe_funcs))
            if not cond:
                return ""

        # tal:repeat (handled before anything else; avoid infinite by consumed_repeat)
        if tal_repeat and not consumed_repeat:
            var, expr = parse_repeat(tal_repeat)
            iterable = eval_expr(expr, scope, self.safe_funcs)
            out = []
            for idx, item in enumerate(iterable):
                child_scope = scope.new_child({var: item})
                # Optionally expose 'repeat' metadata:
                child_scope[f"{var}_index"] = idx
                out.append(self._render_element(el, child_scope, consumed_repeat=True))
                # child's tail handled by parent call site
            return "".join(out)

        # tal:replace replaces the node entirely
        if tal_replace:
            value = eval_expr(tal_replace, scope, self.safe_funcs)
            return to_text(value, self.autoescape)

        # Build final attribute map (excluding tal:*), with ${} interpolation
        final_attrs = {}
        for k, v in el.attrib.items():
            if k in _TAL_ATTRS:
                continue
            final_attrs[k] = interpolate(v, scope, self.safe_funcs, self.autoescape)

        # tal:attributes overrides/sets attributes
        if tal_attributes:
            for name, expr in parse_attrs(tal_attributes):
                val = eval_expr(expr, scope, self.safe_funcs)
                if val is None:
                    final_attrs.pop(name, None)
                else:
                    final_attrs[name] = to_text(val, self.autoescape)

        # tal:content replaces inner content with evaluated value
        if tal_content:
            inner_html = to_text(eval_expr(tal_content, scope, self.safe_funcs), self.autoescape)
            text_before_children = inner_html
            children_rendered = ""  # cleared
        else:
            # Render children recursively
            text_before_children = interpolate(el.text or "", scope, self.safe_funcs, self.autoescape)
            children_parts = []
            for child in list(el):
                children_parts.append(self._render_element(child, scope))
                if child.tail:
                    children_parts.append(interpolate(child.tail, scope, self.safe_funcs, self.autoescape))
            children_rendered = "".join(children_parts)

        # tal:omit-tag: if true, return content only (no wrapper)
        omit = False
        if tal_omit_tag:
            omit = bool(eval_expr(tal_omit_tag, scope, self.safe_funcs))
        if omit:
            return f"{text_before_children}{children_rendered}"

        # Serialize element with attributes
        attrs_serialized = "".join(
            f' {name}="{html_escape(val)}"' for name, val in final_attrs.items()
        )
        # Emit start, content, end
        return f"<{el.tag}{attrs_serialized}>{text_before_children}{children_rendered}</{el.tag}>"



# ----------------- Example -----------------
if __name__ == "__main__":
    demo = """
    <div class="card" tal:condition="user['active']">
        <h3>${user.name}</h3>
        <ul>
            <li tal:repeat="p user.projects" tal:attributes="data-index p_index">
                ${p.title} (${len(p.tasks)} tasks)
            </li>
        </ul>
        <p tal:define="greeting 'Hello'; count len(user.projects)">
            ${greeting}, ${user.name}! You have ${count} projects.
        </p>
        <p tal:content="raw('<strong>trusted</strong> HTML allowed via raw()')"></p>
        <a href="/users/${user.id}" tal:attributes="title 'Profile of ' + user.name">Profile</a>
        <span tal:omit-tag="True"><i>${user.role}</i></span>
    </div>
    """

    data = {
        "user": {
            "id": 42,
            "name": "Ada",
            "role": "Engineer",
            "active": True,
            "projects": [
                {"title": "Compiler", "tasks": [1, 2, 3]},
                {"title": "Tools", "tasks": [1]},
            ],
        }
    }

    tpl = Template(demo, autoescape=True)
    print(tpl.render(data))

"""
How to use:
- Create Template(template_text, autoescape=True/False)
- Call render(context_dict)

Directives:
- tal:condition="expr": render element only if expr is truthy.
- tal:repeat="item in expr": repeat element for each item in iterable; index available as item_index.
- tal:content="expr": replace inner content with expr result.
- tal:replace="expr": replace entire element with expr result (no tag).
- tal:attributes="attr expr; other expr2": set attributes from expressions (None removes attribute).
- tal:omit-tag="expr": if true, render children without the wrapping tag.
- tal:define="name expr; name2 expr2": define local variables within element scope.

Expressions:
- Use ${...} inside text and non-tal attributes.
- Evaluated with a safe, restricted evaluator. Allowed functions: len, min, max, sum, sorted, range, str, int, float, bool, any, all, enumerate, zip, raw.
- Use raw(value) to inject trusted HTML without escaping.

Caveats:
- Templates must be well-formed XML/XHTML (ElementTree parser).
- For HTML5 parsing needs, consider swapping in an HTML5 parser (e.g., lxml.html) and wiring its tree into the renderer.
"""
------------------------------------------------------------

PROMPT 229:
"Generate Python code that imports the necessary modules and creates a stub file for google-flag's flags module, containing empty functions for each flag function, like \"void Foo(int *bar) {}\"."
RESPONSE:
#!/usr/bin/env python3
"""
Generate a C stub file with empty functions for each flag-defining function
found in a Google Flags module (e.g., google.flags, gflags, or absl.flags).

Example output function form: void Foo(void) {}
"""

import argparse
import importlib
import inspect
import re
import sys
from pathlib import Path


def import_flags_module(name: str | None):
    candidates = []
    if name:
        candidates.append(name)
    # Common Google flags module names (old and new)
    candidates += ["google.flags", "gflags", "absl.flags"]

    last_err = None
    for mod_name in candidates:
        try:
            return importlib.import_module(mod_name), mod_name
        except Exception as e:
            last_err = e
    raise ImportError(
        f"Could not import any flags module from candidates {candidates}: {last_err}"
    )


def is_flag_function(name: str, obj) -> bool:
    if not callable(obj):
        return False
    # Heuristics for flag-defining helpers
    if name.startswith("DEFINE_"):
        return True
    # Some ecosystems also expose multi-define helpers
    if name.startswith("DEFINE_multi") or name.startswith("DEFINE_Multi"):
        return True
    # Optionally include common helpers related to flags; comment in if desired:
    # if name in {"RegisterValidator", "MarkFlagAsRequired", "AdoptModuleKeyFlags"}:
    #     return True
    return False


def collect_flag_functions(mod) -> list[str]:
    names = set()

    # Functions directly on the module
    for name, obj in inspect.getmembers(mod):
        if is_flag_function(name, obj):
            names.add(name)

    # Some packages nest DEFINE_* inside a submodule (e.g., absl.flags._defines)
    submods = [f"{mod.__name__}._defines", f"{mod.__name__}.defines"]
    for sub in submods:
        try:
            sm = importlib.import_module(sub)
        except Exception:
            continue
        for name, obj in inspect.getmembers(sm):
            if is_flag_function(name, obj):
                names.add(name)

    return sorted(names)


_c_identifier_re = re.compile(r"[^a-zA-Z0-9_]")


def to_c_identifier(name: str) -> str:
    # Replace invalid chars and avoid leading digits
    cname = _c_identifier_re.sub("_", name)
    if cname and cname[0].isdigit():
        cname = "_" + cname
    return cname


def write_c_stubs(func_names: list[str], out_path: Path, source_module: str) -> None:
    with out_path.open("w", encoding="utf-8") as f:
        f.write("/* Auto-generated C stubs for flag functions.\n")
        f.write(f" * Source module: {source_module}\n")
        f.write(" * Each function has an empty body.\n")
        f.write(" */\n\n")
        f.write("#include <stddef.h>\n\n")
        for name in func_names:
            cname = to_c_identifier(name)
            f.write(f"void {cname}(void) {{}}\n")


def main(argv=None):
    parser = argparse.ArgumentParser(
        description="Generate C stub file with empty functions for Google Flags DEFINE_* functions."
    )
    parser.add_argument(
        "--module",
        "-m",
        default=None,
        help="Flags module to inspect (e.g., google.flags, gflags, absl.flags). If omitted, tries common defaults.",
    )
    parser.add_argument(
        "--out",
        "-o",
        default="flags_stubs.c",
        help="Output C file path (default: flags_stubs.c)",
    )
    args = parser.parse_args(argv)

    mod, mod_name = import_flags_module(args.module)
    func_names = collect_flag_functions(mod)

    if not func_names:
        print(f"Warning: No flag functions found in {mod_name}.", file=sys.stderr)

    out_path = Path(args.out)
    write_c_stubs(func_names, out_path, mod_name)
    print(f"Wrote {len(func_names)} stub functions to {out_path} (from {mod_name}).")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 230:
"Generate Python code that creates a Tortoise ORM database migrations tool using the `tortoise-orm` library, allowing users to define migration scripts in Python and apply them to their databases."
RESPONSE:
# tortoise_migrate.py
# A minimal async migrations tool for Tortoise ORM.
# Features:
# - Tracks applied migrations in a database table.
# - Discovers Python migration files in a directory and applies them in order.
# - Allows writing migrations as async Python functions using a small "op" API for executing SQL.
# - CLI commands: init, make, upgrade, downgrade, history, current, status.
#
# Dependencies:
#   pip install tortoise-orm
#   And the appropriate driver for your DB (aiosqlite, asyncpg, aiomysql, etc.).
#
# Usage examples:
#   export TORTOISE_DB_URL="sqlite://db.sqlite3"
#   python tortoise_migrate.py init
#   python tortoise_migrate.py make create_users_table
#   python tortoise_migrate.py upgrade
#   python tortoise_migrate.py downgrade --steps 1
#   python tortoise_migrate.py history
#
# Migration file template (created by `make`):
#   - async def upgrade(op): await op.run("CREATE TABLE ...")
#   - async def downgrade(op): await op.run("DROP TABLE ...")
#
# Notes:
# - This tool executes raw SQL you provide in migration scripts.
# - Use op.run for single statements; use op.run_many for multiple statements.
# - Transactions are used when possible; some DDL statements may auto-commit on certain backends.

import argparse
import asyncio
import datetime as _dt
import importlib.util
import os
import sys
from pathlib import Path
from typing import List, Optional, Tuple

from tortoise import Tortoise
from tortoise.transactions import in_transaction

MIGRATIONS_TABLE = "_tortoise_migrations"
DEFAULT_DIR = "migrations"


def _detect_dialect_from_url(db_url: str) -> str:
    url = db_url.lower().strip()
    if url.startswith("sqlite"):
        return "sqlite"
    if url.startswith("postgres") or url.startswith("postgresql"):
        return "postgres"
    if url.startswith("mysql"):
        return "mysql"
    return "unknown"


def _now_utc_iso() -> str:
    return _dt.datetime.utcnow().replace(microsecond=0).isoformat()


def _safe_revision_name(name: str) -> str:
    # Allow alphanum, dash, underscore only; replace others with underscore
    out = []
    for ch in name:
        if ch.isalnum() or ch in ("_", "-"):
            out.append(ch)
        else:
            out.append("_")
    return "".join(out)[:255]


async def _ensure_migrations_table(conn) -> None:
    # Portable enough across sqlite/postgres/mysql:
    # name is PK; applied_at is a timestamp string.
    sql = f"""
    CREATE TABLE IF NOT EXISTS {MIGRATIONS_TABLE} (
        name VARCHAR(255) PRIMARY KEY,
        applied_at TIMESTAMP NOT NULL
    )
    """
    await _run_single(conn, sql)


async def _fetch_applied(conn) -> List[str]:
    rows = await conn.execute_query_dict(
        f"SELECT name FROM {MIGRATIONS_TABLE} ORDER BY applied_at ASC"
    )
    return [r["name"] for r in rows]


async def _insert_applied(conn, name: str) -> None:
    name = _safe_revision_name(name)
    ts = _now_utc_iso()
    # Avoid parameterization pitfalls across drivers by inlining safe strings.
    sql = f"INSERT INTO {MIGRATIONS_TABLE} (name, applied_at) VALUES ('{name}', '{ts}')"
    await _run_single(conn, sql)


async def _delete_applied(conn, name: str) -> None:
    name = _safe_revision_name(name)
    sql = f"DELETE FROM {MIGRATIONS_TABLE} WHERE name = '{name}'"
    await _run_single(conn, sql)


def _list_migration_files(migrations_dir: Path) -> List[Path]:
    if not migrations_dir.exists():
        return []
    files = []
    for p in migrations_dir.iterdir():
        if p.name.startswith("_"):
            continue
        if p.suffix == ".py" and p.name != "__init__.py":
            files.append(p)
    files.sort(key=lambda x: x.name)
    return files


def _revision_from_filename(path: Path) -> str:
    return path.stem


def _load_migration_module(path: Path):
    spec = importlib.util.spec_from_file_location(f"migrations.{path.stem}", str(path))
    if spec is None or spec.loader is None:
        raise RuntimeError(f"Cannot load migration {path}")
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)  # type: ignore
    return module


async def _run_single(conn, sql: str) -> None:
    # Single statement execution.
    await conn.execute_query(sql)


async def _run_many(conn, statements: List[str]) -> None:
    for stmt in statements:
        s = stmt.strip()
        if not s:
            continue
        await conn.execute_query(s)


class Op:
    def __init__(self, conn, dialect: str):
        self._conn = conn
        self.dialect = dialect

    async def run(self, sql: str) -> None:
        await _run_single(self._conn, sql)

    async def run_many(self, statements: List[str]) -> None:
        await _run_many(self._conn, statements)


async def _apply_one(migration_path: Path, dialect: str) -> Tuple[str, bool]:
    name = _revision_from_filename(migration_path)
    module = _load_migration_module(migration_path)

    if not hasattr(module, "upgrade") or not callable(module.upgrade):
        raise RuntimeError(f"Migration {migration_path.name} must define async def upgrade(op): ...")
    async with in_transaction() as conn:
        await _ensure_migrations_table(conn)
        applied = await _fetch_applied(conn)
        if name in applied:
            return name, False
        op = Op(conn, dialect)
        await module.upgrade(op)
        await _insert_applied(conn, name)
    return name, True


async def _revert_one(migration_path: Path, dialect: str) -> Tuple[str, bool]:
    name = _revision_from_filename(migration_path)
    module = _load_migration_module(migration_path)

    if not hasattr(module, "downgrade") or not callable(module.downgrade):
        raise RuntimeError(f"Migration {migration_path.name} must define async def downgrade(op): ...")
    async with in_transaction() as conn:
        await _ensure_migrations_table(conn)
        applied = await _fetch_applied(conn)
        if name not in applied:
            return name, False
        op = Op(conn, dialect)
        await module.downgrade(op)
        await _delete_applied(conn, name)
    return name, True


async def init_tortoise(db_url: str) -> None:
    await Tortoise.init(
        config={
            "connections": {"default": db_url},
            "apps": {"models": {"models": [], "default_connection": "default"}},
        }
    )


async def close_tortoise() -> None:
    await Tortoise.close_connections()


def _ensure_init_py(migrations_dir: Path) -> None:
    init_py = migrations_dir / "__init__.py"
    if not init_py.exists():
        init_py.write_text("# Migration package\n", encoding="utf-8")


def cmd_init(migrations_dir: Path) -> None:
    migrations_dir.mkdir(parents=True, exist_ok=True)
    _ensure_init_py(migrations_dir)
    print(f"Initialized migrations directory at {migrations_dir}")


def _template_content(revision: str, description: str) -> str:
    return f'''"""
Revision: {revision}
Description: {description}

Write your SQL in upgrade/downgrade using the provided "op" helper:
- await op.run("SQL STATEMENT")
- await op.run_many(["SQL STATEMENT 1", "SQL STATEMENT 2"])
"""

revision = "{revision}"
description = "{description}"

async def upgrade(op):
    # Example:
    # await op.run(\"""
    # CREATE TABLE example (
    #     id INTEGER PRIMARY KEY,
    #     name TEXT NOT NULL
    # );
    # \""")
    pass

async def downgrade(op):
    # Example:
    # await op.run("DROP TABLE example;")
    pass
'''


def cmd_make(migrations_dir: Path, name: str) -> Path:
    migrations_dir.mkdir(parents=True, exist_ok=True)
    _ensure_init_py(migrations_dir)
    ts = _dt.datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    safe_name = _safe_revision_name(name)
    revision = f"{ts}_{safe_name}" if safe_name else ts
    filename = f"{revision}.py"
    path = migrations_dir / filename
    if path.exists():
        raise RuntimeError(f"Migration file already exists: {path}")
    path.write_text(_template_content(revision, name), encoding="utf-8")
    print(f"Created migration: {path}")
    return path


async def cmd_upgrade(db_url: str, migrations_dir: Path, to: Optional[str]) -> None:
    await init_tortoise(db_url)
    try:
        dialect = _detect_dialect_from_url(db_url)
        files = _list_migration_files(migrations_dir)
        if not files:
            print("No migration files found.")
            return
        async with in_transaction() as conn:
            await _ensure_migrations_table(conn)
            already = set(await _fetch_applied(conn))
        applied_any = False
        for f in files:
            name = _revision_from_filename(f)
            if name in already:
                # Already applied, skip
                if to and name == to:
                    break
                continue
            n, did = await _apply_one(f, dialect)
            if did:
                applied_any = True
                print(f"Applied: {n}")
            if to and n == to:
                break
        if not applied_any:
            print("Nothing to apply.")
    finally:
        await close_tortoise()


async def cmd_downgrade(db_url: str, migrations_dir: Path, to: Optional[str], steps: Optional[int]) -> None:
    if to and steps:
        raise RuntimeError("Use either --to or --steps, not both.")
    await init_tortoise(db_url)
    try:
        dialect = _detect_dialect_from_url(db_url)
        files = _list_migration_files(migrations_dir)
        names = [_revision_from_filename(f) for f in files]

        async with in_transaction() as conn:
            await _ensure_migrations_table(conn)
            applied = await _fetch_applied(conn)

        if not applied:
            print("No applied migrations.")
            return

        # Build a stack (in applied order)
        applied_stack = [n for n in names if n in applied]

        targets = []  # names to revert, in reverse apply order
        if steps:
            targets = list(reversed(applied_stack[-steps:]))
        elif to:
            if to not in applied_stack:
                raise RuntimeError(f"Target revision not applied: {to}")
            # Revert until the last applied equals target
            while applied_stack and applied_stack[-1] != to:
                targets.append(applied_stack[-1])
                applied_stack.pop()
        else:
            # Default: revert one step
            targets = [applied_stack[-1]]

        if not targets:
            print("Nothing to revert.")
            return

        # Map name -> file path
        file_map = { _revision_from_filename(p): p for p in files }

        for name in targets:
            path = file_map.get(name)
            if not path:
                raise RuntimeError(f"Missing migration file for applied revision: {name}")
            n, did = await _revert_one(path, dialect)
            if did:
                print(f"Reverted: {n}")
    finally:
        await close_tortoise()


async def cmd_history(db_url: str, migrations_dir: Path) -> None:
    await init_tortoise(db_url)
    try:
        files = _list_migration_files(migrations_dir)
        names = [_revision_from_filename(f) for f in files]
        async with in_transaction() as conn:
            await _ensure_migrations_table(conn)
            applied = await _fetch_applied(conn)
        for n in names:
            marker = "X" if n in applied else " "
            print(f"[{marker}] {n}")
    finally:
        await close_tortoise()


async def cmd_current(db_url: str, migrations_dir: Path) -> None:
    await init_tortoise(db_url)
    try:
        files = _list_migration_files(migrations_dir)
        names = [_revision_from_filename(f) for f in files]
        async with in_transaction() as conn:
            await _ensure_migrations_table(conn)
            applied = await _fetch_applied(conn)
        applied_ordered = [n for n in names if n in set(applied)]
        if applied_ordered:
            print(applied_ordered[-1])
        else:
            print("(no migrations applied)")
    finally:
        await close_tortoise()


async def cmd_status(db_url: str, migrations_dir: Path) -> None:
    await init_tortoise(db_url)
    try:
        files = _list_migration_files(migrations_dir)
        names = [_revision_from_filename(f) for f in files]
        async with in_transaction() as conn:
            await _ensure_migrations_table(conn)
            applied = await _fetch_applied(conn)
        applied_set = set(applied)
        pending = [n for n in names if n not in applied_set]
        print("Applied:")
        for n in applied:
            print(f"  {n}")
        print("Pending:")
        for n in pending:
            print(f"  {n}")
    finally:
        await close_tortoise()


def parse_args(argv: List[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(prog="tortoise_migrate", description="Tortoise ORM migrations tool")
    parser.add_argument("--db", dest="db_url", default=os.getenv("TORTOISE_DB_URL"), help="Database URL (or set TORTOISE_DB_URL)")
    parser.add_argument("--dir", dest="migrations_dir", default=DEFAULT_DIR, help="Migrations directory (default: migrations)")

    sub = parser.add_subparsers(dest="cmd", required=True)

    sub.add_parser("init", help="Initialize migrations directory")

    make_p = sub.add_parser("make", help="Create a new migration file")
    make_p.add_argument("name", help="Migration name (snake-case recommended)")

    up_p = sub.add_parser("upgrade", help="Apply migrations up to target (or all)")
    up_p.add_argument("--to", dest="to", help="Target revision (filename stem)")

    down_p = sub.add_parser("downgrade", help="Revert migrations")
    down_p.add_argument("--to", dest="to", help="Revert down to this revision (keep it, revert newer)")
    down_p.add_argument("--steps", dest="steps", type=int, help="Revert N last migrations")

    sub.add_parser("history", help="Show all migrations with applied mark")
    sub.add_parser("current", help="Show the latest applied migration")
    sub.add_parser("status", help="Show applied and pending migrations")

    args = parser.parse_args(argv)
    if args.cmd != "init" and not args.db_url:
        parser.error("Database URL required via --db or TORTOISE_DB_URL")
    return args


def main(argv: Optional[List[str]] = None) -> None:
    args = parse_args(argv or sys.argv[1:])
    migrations_dir = Path(args.migrations_dir).resolve()

    if args.cmd == "init":
        cmd_init(migrations_dir)
        return

    if args.cmd == "make":
        cmd_make(migrations_dir, args.name)
        return

    if args.cmd == "upgrade":
        asyncio.run(cmd_upgrade(args.db_url, migrations_dir, args.to))
        return

    if args.cmd == "downgrade":
        asyncio.run(cmd_downgrade(args.db_url, migrations_dir, args.to, args.steps))
        return

    if args.cmd == "history":
        asyncio.run(cmd_history(args.db_url, migrations_dir))
        return

    if args.cmd == "current":
        asyncio.run(cmd_current(args.db_url, migrations_dir))
        return

    if args.cmd == "status":
        asyncio.run(cmd_status(args.db_url, migrations_dir))
        return


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 231:
"Generate Python code that uses faker library to create fake data for application testing based on customizable templates, including names, addresses, phone numbers, emails, and other information."
RESPONSE:
# pip install faker
# Optional CLI usage:
#   python fakegen.py --template template.json --count 100 --out data.json --format json --locale en_US --seed 42

import argparse
import ast
import csv
import json
import random
import re
from pathlib import Path
from typing import Any, Dict, List, Tuple, Union

from faker import Faker


Token = str
Template = Union[Dict[str, Any], List[Any], str, int, float, bool, None]


_TOKEN_RE = re.compile(r"{{\s*(.*?)\s*}}", re.S)


def _parse_call_args(args_str: str) -> Tuple[List[Any], Dict[str, Any]]:
    if not args_str or not args_str.strip():
        return [], {}
    # Parse f({args}) to safely get args/kwargs as Python literals
    node = ast.parse(f"f({args_str})", mode="eval")
    call = node.body
    if not isinstance(call, ast.Call):
        raise ValueError(f"Invalid faker call arguments: {args_str}")
    args = [ast.literal_eval(a) for a in call.args]
    kwargs = {kw.arg: ast.literal_eval(kw.value) for kw in call.keywords}
    return args, kwargs


def _eval_faker_token(token: Token, fake: Faker) -> Any:
    """
    Evaluate a single token against a Faker instance.
    Supported forms:
      - name
      - email()
      - unique.email()
      - date_between(start_date='-1y', end_date='today')
      - bothify(text='??##', letters='ABCDE')
    """
    token = token.strip()
    # unique.<method>(...)
    target = fake
    if token.startswith("unique."):
        token = token[len("unique.") :]
        target = fake.unique

    m = re.match(r"^([A-Za-z_][A-Za-z0-9_]*)\s*(\((.*)\))?$", token, re.S)
    if not m:
        raise ValueError(f"Invalid token: {token}")
    name, _, args_str = m.groups()
    method = getattr(target, name, None)
    if method is None:
        raise AttributeError(f"Faker has no provider named '{name}'")
    if not callable(method):
        # In practice Faker providers are callables; fallback to value
        return method
    args, kwargs = _parse_call_args(args_str or "")
    return method(*args, **kwargs)


def _replace_tokens_in_string(s: str, fake: Faker) -> Any:
    """
    Replace {{ ... }} tokens in a string.
    If the entire string is a single token, return the raw value (preserve type).
    Otherwise, convert each evaluated token to string and return the final string.
    """
    full = _TOKEN_RE.fullmatch(s)
    if full:
        return _eval_faker_token(full.group(1), fake)

    def _repl(m: re.Match) -> str:
        val = _eval_faker_token(m.group(1), fake)
        return "" if val is None else str(val)

    return _TOKEN_RE.sub(_repl, s)


class TemplateFaker:
    """
    Render Python dict/list/string templates with {{ faker_tokens }} using Faker.

    Special directives (as dicts):
      - {"__repeat__": {"times": 3, "template": <any-subtemplate>}}
        or {"__repeat__": {"min": 1, "max": 5, "template": <any>}}
      - {"__choice__": [options...] }  randomly choose one option (template-aware)

    Examples of tokens: {{ name }}, {{ email() }}, {{ unique.user_name() }},
    {{ date_time_between(start_date='-2y', end_date='now') }}
    """

    def __init__(self, locales: Union[str, List[str]] = "en_US", seed: Union[int, None] = None):
        self.fake = Faker(locales if isinstance(locales, list) else [locales])
        if seed is not None:
            Faker.seed(seed)
            random.seed(seed)

    def render(self, template: Template) -> Any:
        if isinstance(template, dict):
            # Directives
            if "__repeat__" in template:
                spec = template["__repeat__"] or {}
                if "times" in spec:
                    times = int(spec["times"])
                else:
                    mn = int(spec.get("min", 0))
                    mx = int(spec.get("max", mn))
                    times = random.randint(mn, mx)
                sub = spec.get("template")
                return [self.render(sub) for _ in range(times)]
            if "__choice__" in template:
                options = template["__choice__"]
                choice = random.choice(options)
                return self.render(choice)
            # Normal dict
            return {k: self.render(v) for k, v in template.items()}

        if isinstance(template, list):
            return [self.render(v) for v in template]

        if isinstance(template, str):
            if "{{" in template and "}}" in template:
                return _replace_tokens_in_string(template, self.fake)
            return template

        # Primitives (int, float, bool, None)
        return template

    def render_many(self, template: Template, count: int) -> List[Any]:
        return [self.render(template) for _ in range(count)]


def _flatten_dict(d: Dict[str, Any], parent_key: str = "", sep: str = ".") -> Dict[str, Any]:
    """
    Flatten nested dicts for CSV output. Lists and non-dicts are kept as-is (lists JSON-dumped).
    """
    items = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(_flatten_dict(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            # Store JSON string for lists
            items.append((new_key, json.dumps(v, default=str)))
        else:
            items.append((new_key, v))
    return dict(items)


def save_json(records: List[Dict[str, Any]], path: Path) -> None:
    path.write_text(json.dumps(records, indent=2, ensure_ascii=False, default=str), encoding="utf-8")


def save_csv(records: List[Dict[str, Any]], path: Path) -> None:
    if not records:
        path.write_text("", encoding="utf-8")
        return
    flat = [_flatten_dict(r) for r in records]
    # Union of all keys
    fieldnames = sorted({k for r in flat for k in r.keys()})
    with path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in flat:
            writer.writerow({k: row.get(k, "") for k in fieldnames})


EXAMPLE_TEMPLATE = {
    "id": "{{ uuid4() }}",
    "full_name": "{{ name() }}",
    "first_name": "{{ first_name() }}",
    "last_name": "{{ last_name() }}",
    "email": "{{ email() }}",
    "phone": "{{ phone_number() }}",
    "username": "{{ user_name() }}",
    "company": "{{ company() }}",
    "title": "{{ job() }}",
    "website": "{{ url() }}",
    "address": {
        "street": "{{ street_address() }}",
        "city": "{{ city() }}",
        "state": "{{ state() }}",
        "postal_code": "{{ postcode() }}",
        "country": "{{ country() }}",
    },
    "registered_at": "{{ date_time_between(start_date='-2y', end_date='now') }}",
    "last_login_ip": "{{ ipv4_public() }}",
    "preferences": {
        "newsletter": "{{ boolean() }}",
        "contact_method": {"__choice__": ["email", "phone", "sms", "push"]},
        "favorite_color": "{{ color_name() }}",
    },
    "tags": {
        "__repeat__": {
            "min": 1,
            "max": 5,
            "template": "{{ word() }}"
        }
    },
    "orders": {
        "__repeat__": {
            "min": 0,
            "max": 4,
            "template": {
                "order_id": "ORD-{{ bothify(text='####-????', letters='ABCDEFGH') }}",
                "ordered_at": "{{ date_time_between(start_date='-1y', end_date='now') }}",
                "amount": "{{ pyfloat(left_digits=3, right_digits=2, positive=True) }}",
                "currency": {"__choice__": ["USD", "EUR", "GBP", "JPY"]},
                "status": {"__choice__": ["pending", "paid", "shipped", "delivered", "refunded"]},
                "items": {
                    "__repeat__": {
                        "min": 1,
                        "max": 5,
                        "template": {
                            "sku": "SKU-{{ bothify(text='????-#####', letters='XYZABC') }}",
                            "name": "{{ sentence(nb_words=3) }}",
                            "qty": "{{ pyint(min_value=1, max_value=5) }}",
                            "unit_price": "{{ pyfloat(left_digits=2, right_digits=2, positive=True) }}"
                        }
                    }
                }
            }
        }
    }
}


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Generate fake data from a customizable template using Faker.")
    p.add_argument("--template", type=str, help="Path to JSON template file. Defaults to built-in example.", default=None)
    p.add_argument("--count", type=int, help="Number of records to generate", default=10)
    p.add_argument("--out", type=str, help="Output file path. If omitted, prints to stdout.", default=None)
    p.add_argument("--format", choices=["json", "csv"], default="json", help="Output format")
    p.add_argument("--locale", nargs="+", default=["en_US"], help="Faker locale(s). Pass multiple for mixed locales.")
    p.add_argument("--seed", type=int, default=None, help="Random seed for reproducibility")
    return p.parse_args()


def main():
    args = parse_args()

    if args.template:
        path = Path(args.template)
        template = json.loads(path.read_text(encoding="utf-8"))
    else:
        template = EXAMPLE_TEMPLATE

    tf = TemplateFaker(locales=args.locale, seed=args.seed)
    records = tf.render_many(template, args.count)

    if args.out:
        out_path = Path(args.out)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        if args.format == "json":
            save_json(records, out_path)
        else:
            save_csv(records, out_path)
        print(f"Wrote {args.count} record(s) to {out_path} as {args.format.upper()}")
    else:
        if args.format == "json":
            print(json.dumps(records, indent=2, ensure_ascii=False, default=str))
        else:
            # Print CSV to stdout
            if records:
                flat = [_flatten_dict(r) for r in records]
                fieldnames = sorted({k for r in flat for k in r.keys()})
                writer = csv.DictWriter(fp := __import__("sys").stdout, fieldnames=fieldnames)
                writer.writeheader()
                for row in flat:
                    writer.writerow({k: row.get(k, "") for k in fieldnames})


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 232:
"Generate Python code that uses the ClickHouse API to interface with ClickHouse databases, allowing users to create, modify, and query data in ClickHouse using the dbt (data build tool) framework."
RESPONSE:
# Requires: pip install clickhouse-connect dbt-core dbt-clickhouse pyyaml pandas
# This module provides:
# - A ClickHouseClient wrapper using the HTTP API (via clickhouse-connect)
# - A DbtManager to run dbt commands against ClickHouse (via dbt-clickhouse adapter)
# - Example usage in main() showing DDL/DML and dbt run/test
import os
import tempfile
import pathlib
from typing import Any, Iterable, List, Optional, Sequence, Tuple, Dict

import yaml
import pandas as pd

try:
    import clickhouse_connect
except ImportError as e:
    raise ImportError(
        "Missing dependency 'clickhouse-connect'. Install with: pip install clickhouse-connect"
    ) from e

# dbt imports are optional; handled at runtime
_DBTRUNNER_AVAILABLE = True
try:
    from dbt.cli.main import dbtRunner
except Exception:
    _DBTRUNNER_AVAILABLE = False


class ClickHouseClient:
    """
    Minimal ClickHouse HTTP client using clickhouse-connect.

    Features:
    - execute: run DDL/DML
    - query: fetch rows
    - query_df: fetch pandas DataFrame
    - insert: bulk insert
    """

    def __init__(
        self,
        host: str = "localhost",
        port: Optional[int] = None,
        *,
        username: str = "default",
        password: str = "",
        database: str = "default",
        secure: bool = True,
        verify: bool = True,
        compress: bool = True,
        settings: Optional[Dict[str, Any]] = None,
    ):
        if port is None:
            # Default HTTP ports: 8443 for TLS, 8123 for non-TLS
            port = 8443 if secure else 8123

        self.client = clickhouse_connect.get_client(
            host=host,
            port=port,
            username=username,
            password=password,
            database=database,
            secure=secure,
            verify=verify,
            compress=compress,
            settings=settings or {},
        )

    def execute(self, sql: str, parameters: Optional[Dict[str, Any]] = None) -> None:
        self.client.command(sql, parameters=parameters or {})

    def query(self, sql: str, parameters: Optional[Dict[str, Any]] = None) -> List[Tuple[Any, ...]]:
        result = self.client.query(sql, parameters=parameters or {})
        return result.result_rows

    def query_df(self, sql: str, parameters: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        result = self.client.query(sql, parameters=parameters or {})
        return result.result_df

    def insert(
        self,
        table: str,
        data: Iterable[Sequence[Any]],
        columns: Optional[Sequence[str]] = None,
    ) -> None:
        self.client.insert(table=table, data=data, column_names=list(columns) if columns else None)

    def create_database(self, name: str, if_not_exists: bool = True) -> None:
        sql = f"CREATE DATABASE {'IF NOT EXISTS ' if if_not_exists else ''}{identifier(name)}"
        self.execute(sql)

    def use_database(self, name: str) -> None:
        self.execute(f"USE {identifier(name)}")


class DbtManager:
    """
    Programmatic wrapper around dbt for ClickHouse using dbt-clickhouse adapter.
    """

    def __init__(
        self,
        project_dir: str,
        profiles_dir: Optional[str] = None,
        profile_name: str = "clickhouse",
        target_name: str = "dev",
    ):
        self.project_dir = str(pathlib.Path(project_dir).resolve())
        self.profile_name = profile_name
        self.target_name = target_name

        if profiles_dir is None:
            # Use a temporary isolated profiles directory by default
            self._tmp_profiles_dir = tempfile.TemporaryDirectory(prefix="dbt_profiles_")
            self.profiles_dir = self._tmp_profiles_dir.name
        else:
            self._tmp_profiles_dir = None
            self.profiles_dir = profiles_dir

        if not _DBTRUNNER_AVAILABLE:
            raise ImportError(
                "dbt-core not available or incompatible. Install: pip install dbt-core dbt-clickhouse"
            )

        self.runner = dbtRunner()

    def write_clickhouse_profile(
        self,
        *,
        host: str,
        port: int,
        schema: str,
        user: str,
        password: str = "",
        secure: bool = True,
        verify: bool = True,
        threads: int = 4,
        driver: str = "http",  # 'http' via clickhouse-connect, or 'native' via clickhouse-driver
        extra: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Creates a profiles.yml suitable for dbt-clickhouse in profiles_dir.
        Returns the path to the created profiles.yml.
        """
        profiles_path = os.path.join(self.profiles_dir, "profiles.yml")
        os.makedirs(self.profiles_dir, exist_ok=True)

        profile = {
            self.profile_name: {
                "target": self.target_name,
                "outputs": {
                    self.target_name: {
                        "type": "clickhouse",
                        "schema": schema,  # ClickHouse uses 'database', dbt uses 'schema' key
                        "host": host,
                        "port": port,
                        "user": user,
                        "password": password or "",
                        "secure": bool(secure),
                        "verify": bool(verify),
                        "threads": int(threads),
                        "driver": driver,
                    }
                },
            }
        }

        if extra:
            profile[self.profile_name]["outputs"][self.target_name].update(extra)

        with open(profiles_path, "w", encoding="utf-8") as f:
            yaml.safe_dump(profile, f, sort_keys=False)

        return profiles_path

    def run(self, args: List[str]) -> bool:
        """
        Run an arbitrary dbt command. Args should NOT include 'dbt' itself.
        Required args --project-dir and --profiles-dir will be added if missing.
        Returns True if successful.
        """
        args = list(args)
        if "--project-dir" not in args:
            args.extend(["--project-dir", self.project_dir])
        if "--profiles-dir" not in args:
            args.extend(["--profiles-dir", self.profiles_dir])

        result = self.runner.invoke(args)
        return bool(getattr(result, "success", False))

    def debug(self) -> bool:
        return self.run(["debug"])

    def deps(self) -> bool:
        return self.run(["deps"])

    def seed(self, select: Optional[str] = None, full_refresh: bool = False) -> bool:
        args = ["seed"]
        if select:
            args += ["--select", select]
        if full_refresh:
            args += ["--full-refresh"]
        return self.run(args)

    def run_models(self, select: Optional[str] = None, full_refresh: bool = False) -> bool:
        args = ["run"]
        if select:
            args += ["--select", select]
        if full_refresh:
            args += ["--full-refresh"]
        return self.run(args)

    def test(self, select: Optional[str] = None) -> bool:
        args = ["test"]
        if select:
            args += ["--select", select]
        return self.run(args)

    def clean(self) -> bool:
        return self.run(["clean"])


def identifier(name: str) -> str:
    # Minimal quoting for ClickHouse identifiers
    if not name:
        raise ValueError("Empty identifier")
    if all(c.isalnum() or c == "_" for c in name):
        return name
    return f"`{name.replace('`', '``')}`"


def env_bool(key: str, default: bool) -> bool:
    v = os.getenv(key)
    if v is None:
        return default
    return str(v).strip().lower() in {"1", "true", "yes", "y", "on"}


def main():
    # Read connection details from environment or defaults
    ch_host = os.getenv("CH_HOST", "localhost")
    ch_secure = env_bool("CH_SECURE", True)
    ch_verify = env_bool("CH_VERIFY_SSL", True)
    ch_port = int(os.getenv("CH_PORT", "8443" if ch_secure else "8123"))
    ch_user = os.getenv("CH_USER", "default")
    ch_password = os.getenv("CH_PASSWORD", "")
    ch_database = os.getenv("CH_DATABASE", "analytics")

    # Initialize ClickHouse client
    ch = ClickHouseClient(
        host=ch_host,
        port=ch_port,
        username=ch_user,
        password=ch_password,
        database="default",
        secure=ch_secure,
        verify=ch_verify,
    )

    # Create and switch to target database
    ch.create_database(ch_database, if_not_exists=True)
    ch.use_database(ch_database)

    # Create a sample table (modify DDL as needed)
    ch.execute(
        f"""
        CREATE TABLE IF NOT EXISTS {identifier(ch_database)}.{identifier('events')}
        (
            event_date Date,
            user_id UInt64,
            event_type LowCardinality(String),
            properties JSON
        )
        ENGINE = MergeTree
        ORDER BY (event_date, user_id)
        """
    )

    # Insert some sample data
    rows = [
        ("2025-01-01", 1, "signup", '{"plan":"free"}'),
        ("2025-01-02", 1, "upgrade", '{"plan":"pro"}'),
        ("2025-01-02", 2, "signup", '{"plan":"free"}'),
    ]
    ch.insert(f"{identifier(ch_database)}.{identifier('events')}", rows, columns=["event_date", "user_id", "event_type", "properties"])

    # Query data as DataFrame
    df = ch.query_df(
        f"""
        SELECT event_type, count() AS cnt
        FROM {identifier(ch_database)}.{identifier('events')}
        GROUP BY event_type
        ORDER BY cnt DESC
        """
    )
    print("Event counts:")
    print(df)

    # Set up dbt against ClickHouse
    # Assume you have a dbt project at ./dbt_project with models referencing the ClickHouse profile
    project_dir = os.getenv("DBT_PROJECT_DIR", "./dbt_project")
    if not os.path.isdir(project_dir):
        print(f"dbt project directory not found: {project_dir}. Skipping dbt steps.")
        return

    dbt_mgr = DbtManager(project_dir=project_dir, profile_name="clickhouse", target_name="dev")
    # Write a compatible profiles.yml for dbt-clickhouse
    dbt_port = int(os.getenv("DBT_CH_PORT", str(ch_port)))
    dbt_secure = env_bool("DBT_CH_SECURE", ch_secure)
    dbt_verify = env_bool("DBT_CH_VERIFY_SSL", ch_verify)
    dbt_driver = os.getenv("DBT_CH_DRIVER", "http")  # or 'native' if using clickhouse-driver adapter path

    dbt_profiles_path = dbt_mgr.write_clickhouse_profile(
        host=ch_host,
        port=dbt_port,
        schema=ch_database,  # dbt-clickhouse uses 'schema' to represent the database
        user=ch_user,
        password=ch_password,
        secure=dbt_secure,
        verify=dbt_verify,
        threads=int(os.getenv("DBT_THREADS", "4")),
        driver=dbt_driver,
        extra={
            # You can pass extra keys supported by dbt-clickhouse here
            # "compression": "gzip",
        },
    )
    print(f"Wrote dbt profiles.yml to: {dbt_profiles_path}")

    # Run dbt commands
    print("Running: dbt debug")
    assert dbt_mgr.debug(), "dbt debug failed"

    print("Running: dbt deps")
    dbt_mgr.deps()

    # Seed and run models
    if os.path.isdir(os.path.join(project_dir, "seeds")):
        print("Running: dbt seed")
        dbt_mgr.seed()

    print("Running: dbt run")
    assert dbt_mgr.run_models(), "dbt run failed"

    print("Running: dbt test")
    dbt_mgr.test()

    print("All done.")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 233:
"Generate Python code that implements a plugin for dbt to interact with Databricks databases, allowing users to run dbt commands against Databricks clusters and retrieve data from Databricks tables."
RESPONSE:
Below is a minimal, working skeleton of a dbt adapter plugin for Databricks using the Databricks SQL connector. It lets dbt connect to and run SQL against Databricks clusters/SQL Warehouses. It also includes a small utility module to run dbt commands programmatically and fetch data from Databricks tables.

You can place these files in a Python package (for example, dbt-databricks-lite) and install it into the same environment as dbt-core. Then reference the adapter with type: databricks in your dbt profile.

File: pyproject.toml
--------------------------------
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "dbt-databricks-lite"
version = "0.1.0"
description = "Minimal dbt adapter for Databricks"
requires-python = ">=3.8"
dependencies = [
  "dbt-core>=1.5,<2.0",
  "databricks-sql-connector>=2.7.0",
  "pydantic<2"  # dbt-core currently expects pydantic v1
]

[project.entry-points."dbt.adapters"]
databricks = "dbt_databricks:plugin"


File: dbt_databricks/__init__.py
--------------------------------
from pathlib import Path
from dbt.adapters.factory import AdapterPlugin

from .adapter import DatabricksAdapter
from .credentials import DatabricksCredentials


def get_include_path():
    # Optionally include a macros directory if you add custom macros
    macros_path = Path(__file__).parent / "macros"
    return str(macros_path) if macros_path.exists() else None


plugin = AdapterPlugin(
    adapter=DatabricksAdapter,
    credentials=DatabricksCredentials,
    include_path=get_include_path(),
    dependencies=[],
)


File: dbt_databricks/credentials.py
-----------------------------------
from typing import Optional, Dict, Any, Tuple
from pydantic import Field, validator
from dbt.adapters.base import Credentials


class DatabricksCredentials(Credentials):
    # Databricks SQL parameters
    server_hostname: str = Field(..., description="Databricks workspace hostname, e.g. adb-1234567890123456.7.azuredatabricks.net")
    http_path: str = Field(..., description="SQL Warehouse (or cluster) HTTP path")
    access_token: str = Field(..., description="Databricks PAT token")
    schema: str = Field(..., description="Default schema")
    catalog: Optional[str] = Field(None, description="Unity Catalog (database) name")
    session_configuration: Optional[Dict[str, Any]] = Field(None, description="Optional session configuration dict")

    @validator("schema", always=True)
    def normalize_schema(cls, v: str) -> str:
        if v is None:
            return v
        return v.lower()

    @property
    def type(self) -> str:
        # dbt profile: type: databricks
        return "databricks"

    @property
    def unique_field(self) -> str:
        # used to differentiate connections in logs
        return self.server_hostname

    def _connection_keys(self) -> Tuple[str, ...]:
        # fields to log (masking sensitive info is handled by dbt)
        return ("server_hostname", "http_path", "catalog", "schema")


File: dbt_databricks/connection.py
----------------------------------
from typing import Optional, Any
from databricks import sql as dbsql

from dbt.adapters.sql.connections import SQLConnectionManager
from dbt.contracts.connection import Connection, AdapterResponse
from dbt.exceptions import DbtDatabaseError


class DatabricksConnectionManager(SQLConnectionManager):
    TYPE = "databricks"

    @classmethod
    def open(cls, connection: Connection) -> Connection:
        if connection.state == "open":
            return connection

        creds = connection.credentials
        try:
            handle = dbsql.connect(
                server_hostname=creds.server_hostname,
                http_path=creds.http_path,
                access_token=creds.access_token,
                catalog=creds.catalog,
                schema=creds.schema,
                session_configuration=creds.session_configuration or {},
            )
            connection.handle = handle
            connection.state = "open"
            return connection
        except Exception as exc:
            # Release any partial state and raise a dbt-friendly exception
            cls.release(connection)
            raise DbtDatabaseError(f"Unable to connect to Databricks: {exc}") from exc

    @classmethod
    def get_response(cls, cursor) -> AdapterResponse:
        # Build an AdapterResponse after executing SQL
        rows = getattr(cursor, "rowcount", None)
        return AdapterResponse(_message="OK", rows_affected=rows)

    def cancel(self, connection: Connection):
        # Cancellation is typically done at cursor-level on Databricks SQL connector.
        # This is a no-op at connection level.
        return

    @classmethod
    def get_status(cls, cursor) -> str:
        # Used by dbt to log status for statements like DDL
        return "OK"


File: dbt_databricks/relation.py
--------------------------------
from dataclasses import dataclass
from dbt.adapters.base.relation import BaseRelation


@dataclass
class DatabricksRelation(BaseRelation):
    # For a minimal adapter, the defaults are acceptable.
    # If you want to customize quoting or database/schema semantics,
    # you can override quote_policy or include_policy here.
    pass


File: dbt_databricks/adapter.py
-------------------------------
from typing import Optional, Iterable
from dbt.adapters.sql import SQLAdapter
from dbt.adapters.base.impl import AdapterConfig

from .connection import DatabricksConnectionManager
from .relation import DatabricksRelation


class DatabricksAdapter(SQLAdapter):
    ConnectionManager = DatabricksConnectionManager
    Relation = DatabricksRelation
    AdapterSpecificConfigs = AdapterConfig

    @classmethod
    def date_function(cls) -> str:
        return "current_timestamp()"

    def valid_incremental_strategies(self) -> Iterable[str]:
        # For a minimal adapter, declare standard SQL strategies.
        return ["append", "delete+insert", "merge"]

    # You can override other behaviors as needed, for example:
    # - get_rows_different_sql
    # - get_columns_in_relation
    # - list_schemas
    # - etc.
    # SQLAdapter provides many defaults that work with Databricks' ANSI SQL.


File: dbt_databricks/utils.py
-----------------------------
from typing import Optional, Dict, Any, List
from databricks import sql as dbsql

# Optional convenience utilities to:
# 1) Run dbt commands programmatically
# 2) Fetch data from Databricks tables quickly


def fetch_dataframe(
    server_hostname: str,
    http_path: str,
    access_token: str,
    sql_text: str,
    catalog: Optional[str] = None,
    schema: Optional[str] = None,
    session_configuration: Optional[Dict[str, Any]] = None,
):
    import pandas as pd

    with dbsql.connect(
        server_hostname=server_hostname,
        http_path=http_path,
        access_token=access_token,
        catalog=catalog,
        schema=schema,
        session_configuration=session_configuration or {},
    ) as conn:
        with conn.cursor() as cur:
            cur.execute(sql_text)
            # The Databricks SQL connector supports arrow fetch; for simplicity use fetchall
            rows = cur.fetchall()
            cols = [d[0] for d in cur.description]
    return pd.DataFrame.from_records(rows, columns=cols)


def run_dbt(args: List[str]) -> bool:
    """
    Run dbt programmatically, e.g.:
      run_dbt(["debug"])
      run_dbt(["run", "--select", "my_model"])
      run_dbt(["test", "-m", "tag:nightly"])
    Returns True if command succeeded, else False.
    """
    from dbt.cli.main import dbtRunner

    runner = dbtRunner()
    res = runner.invoke(args)
    return res.success


Notes and tips:
- Install this package into the same environment as dbt-core and your project. Example:
  pip install -e .
- Configure dbt to use this adapter by setting type: databricks in your profiles.yml target, along with:
  server_hostname, http_path, access_token, catalog (optional), and schema.
- Once installed and configured, dbt run, dbt test, etc., will use this adapter to connect to your Databricks SQL Warehouse or cluster via the Databricks SQL connector.
- The utils.fetch_dataframe helper can be used in external Python scripts or notebooks to quickly pull data from Databricks tables using the same connection parameters.
------------------------------------------------------------

PROMPT 234:
"Generate Python code that imports the necessary modules and creates a wrapper class for the CoreAudioKit framework on macOS, providing a simple interface for accessing the framework's functionality."
RESPONSE:
# CoreAudioKit wrapper for macOS using PyObjC
# Requirements (install via pip):
#   pip install pyobjc-core pyobjc-framework-Cocoa pyobjc-framework-CoreAudioKit \
#               pyobjc-framework-AVFoundation pyobjc-framework-AudioToolbox \
#               pyobjc-framework-AudioUnit
#
# This module provides a thin, practical wrapper around parts of CoreAudioKit to:
#   - Discover AUv3 components (via AVFoundation)
#   - Instantiate an audio unit
#   - Request its CoreAudioKit-provided view controller
#   - Present that UI inside an NSWindow
#
# Notes:
#   - This targets AUv3 (Audio Unit extensions). Legacy AUv2 "AUGenericView" is
#     exposed if available, but not all systems provide it.
#   - UI operations must occur on the main thread with an NSApplication running.
#   - The convenience helpers below synchronize PyObjC’s async completion handlers
#     using the run loop.

import sys
import platform
import threading
import time

if platform.system() != "Darwin":
    raise RuntimeError("This wrapper only runs on macOS.")

try:
    import objc
    from Cocoa import (
        NSApp,
        NSApplication,
        NSApplicationActivationPolicyRegular,
        NSBackingStoreBuffered,
        NSMakeRect,
        NSRunningApplication,
        NSApplicationActivateIgnoringOtherApps,
        NSWindow,
        NSWindowStyleMask,
    )
    from Foundation import NSDate, NSRunLoop
    import AVFoundation
    import CoreAudioKit
except Exception as exc:
    raise RuntimeError(
        "Missing required PyObjC frameworks. Install with:\n"
        "  pip install pyobjc-core pyobjc-framework-Cocoa "
        "pyobjc-framework-CoreAudioKit pyobjc-framework-AVFoundation "
        "pyobjc-framework-AudioToolbox pyobjc-framework-AudioUnit"
    ) from exc


def _ensure_app():
    """
    Ensure there is an NSApplication running so windows can be shown.
    Safe to call multiple times.
    """
    app = NSApplication.sharedApplication()
    # If this is the first touch, put us into a regular app activation policy.
    app.setActivationPolicy_(NSApplicationActivationPolicyRegular)
    # Activate app to bring window to front (best effort).
    NSRunningApplication.currentApplication().activateWithOptions_(
        NSApplicationActivateIgnoringOtherApps
    )
    return app


def _run_loop_until(deadline_secs, predicate):
    """
    Pump the run loop until either predicate() returns True or the deadline passes.
    """
    runloop = NSRunLoop.currentRunLoop()
    while time.time() < deadline_secs and not predicate():
        runloop.runMode_beforeDate_("NSDefaultRunLoopMode", NSDate.dateWithTimeIntervalSinceNow_(0.01))


class CoreAudioKitWrapper:
    """
    Simple wrapper around CoreAudioKit to request and present AUv3 user interfaces.
    """

    def __init__(self):
        # Detect class availability once.
        self._has_coreaudiokit = True
        self._has_auv3 = hasattr(AVFoundation, "AVAudioUnitComponentManager")
        self._has_au_generic_view = hasattr(CoreAudioKit, "AUGenericView")

    # -------- Discovery --------

    def list_components(self, types=None, only_with_ui=True):
        """
        List installed AUv3 components.

        Args:
            types: Optional iterable of AVAudioUnitType values
                   (e.g., AVFoundation.AVAudioUnitType_Effect).
                   If None, returns all.
            only_with_ui: If True, filter to components that advertise a custom UI.

        Returns:
            List of dicts with keys: name, manufacturerName, hasCustomView,
            type, subtype, componentDescription, component (AVAudioUnitComponent).
        """
        if not self._has_auv3:
            return []

        mgr = AVFoundation.AVAudioUnitComponentManager.sharedAudioUnitComponentManager()
        all_components = mgr.componentsMatchingPredicate_(None) or []

        results = []
        for comp in all_components:
            try:
                has_ui = bool(comp.hasCustomView())
            except Exception:
                has_ui = False
            if only_with_ui and not has_ui:
                continue
            if types:
                if comp.type() not in types:
                    continue

            # componentDescription is an AudioComponentDescription struct
            desc = comp.audioComponentDescription()
            info = {
                "name": str(comp.name()),
                "manufacturerName": str(comp.manufacturerName()),
                "hasCustomView": has_ui,
                "type": comp.type(),
                "subtype": comp.subtype(),
                "componentDescription": desc,
                "component": comp,
            }
            results.append(info)
        return results

    # -------- Instantiation and UI --------

    def instantiate_component(self, component, timeout=10.0):
        """
        Instantiate an AVAudioUnit from an AVAudioUnitComponent.

        Returns:
            av_audio_unit (AVFoundation.AVAudioUnit) or None if failed.
        """
        if component is None:
            return None

        result = {"unit": None, "done": False}

        def _completion(unit, error):
            result["unit"] = unit
            result["done"] = True

        AVFoundation.AVAudioUnit.instantiateWithComponentDescription_options_completionHandler_(
            component.audioComponentDescription(),
            0,  # options
            _completion,
        )

        _run_loop_until(time.time() + timeout, lambda: result["done"])
        return result["unit"]

    def request_view_controller(self, au_audio_unit, timeout=10.0):
        """
        Request the AUv3 view controller for a given AUAudioUnit.

        Returns:
            NSViewController (often a CoreAudioKit.AUViewController subclass) or None.
        """
        if au_audio_unit is None:
            return None

        result = {"vc": None, "done": False}

        def _completion(vc):
            result["vc"] = vc
            result["done"] = True

        # AUAudioUnit has: -requestViewControllerWithCompletionHandler:
        au_audio_unit.requestViewControllerWithCompletionHandler_(_completion)

        _run_loop_until(time.time() + timeout, lambda: result["done"])
        return result["vc"]

    def create_generic_view_for_au_v2(self, audio_unit_ref):
        """
        Create a CoreAudioKit.AUGenericView for a legacy (Audio Unit v2) AudioUnit.
        This requires an existing AudioUnit instance (AudioComponentInstance).

        Returns:
            NSView or None if not supported.
        """
        if not self._has_au_generic_view:
            return None
        try:
            # AUGenericView may have multiple initializers across macOS releases.
            # The common one is initWithAudioUnit_ for AUv2.
            view = CoreAudioKit.AUGenericView.alloc().initWithAudioUnit_(audio_unit_ref)
            return view
        except Exception:
            return None

    # -------- Presentation helpers --------

    def present_view_controller_in_window(self, view_controller, title="Audio Unit", size=(640, 360)):
        """
        Present a view controller in a simple resizable NSWindow.
        """
        if view_controller is None:
            raise ValueError("view_controller is None")

        _ensure_app()

        rect = NSMakeRect(0, 0, float(size[0]), float(size[1]))
        style = (
            NSWindowStyleMask.titled
            | NSWindowStyleMask.closable
            | NSWindowStyleMask.miniaturizable
            | NSWindowStyleMask.resizable
        )
        window = NSWindow.alloc().initWithContentRect_styleMask_backing_defer_(rect, style, NSBackingStoreBuffered, False)
        window.setTitle_(title)

        # Adopt the VC as the window's content.
        window.setContentViewController_(view_controller)
        window.center()
        window.makeKeyAndOrderFront_(None)
        return window

    def present_first_available_ui(self, timeout=15.0):
        """
        Convenience: find the first AUv3 with a custom UI, instantiate it,
        request its view controller, and present it in a window.

        Returns:
            (window, view_controller, av_audio_unit) or (None, None, None)
        """
        comps = self.list_components(only_with_ui=True)
        if not comps:
            return (None, None, None)

        comp = comps[0]["component"]
        av_unit = self.instantiate_component(comp, timeout=timeout)
        if av_unit is None:
            return (None, None, None)

        # AVAudioUnit has property AUAudioUnit (AUv3 object).
        try:
            au3 = av_unit.AUAudioUnit()
        except Exception:
            # Some PyObjC versions expose as attribute
            au3 = getattr(av_unit, "AUAudioUnit", None)

        if au3 is None:
            return (None, None, None)

        vc = self.request_view_controller(au3, timeout=timeout)
        if vc is None:
            return (None, None, None)

        win = self.present_view_controller_in_window(vc, title=str(comp.name()))
        return (win, vc, av_unit)


# Example usage:
# Run this module to open the first AUv3 UI found on the system.
if __name__ == "__main__":
    wrapper = CoreAudioKitWrapper()

    window, vc, unit = wrapper.present_first_available_ui(timeout=20.0)
    if window is None:
        print("No AUv3 with custom UI found or failed to present UI.")
        sys.exit(1)

    # Keep the app responsive until the window is closed.
    app = _ensure_app()
    app.run()
------------------------------------------------------------

PROMPT 235:
"Generate Python code that imports the `oauthlib` library and creates typing stubs for its functions and classes, using the `typing` module to define the types of the stubbed variables and functions."
RESPONSE:
import importlib
import inspect
import os
import pkgutil
import sys
from dataclasses import dataclass
from pathlib import Path
from types import ModuleType
from typing import Any, Dict, Iterable, List, Optional, Tuple


# Ensure oauthlib is importable
try:
    import oauthlib  # noqa: F401
except Exception as e:
    print("Failed to import oauthlib. Please install it first: pip install oauthlib", file=sys.stderr)
    raise


@dataclass
class GeneratedCounts:
    modules: int = 0
    functions: int = 0
    classes: int = 0
    methods: int = 0
    properties: int = 0
    variables: int = 0


def is_defined_in(obj: Any, module_name: str) -> bool:
    try:
        return getattr(obj, "__module__", None) == module_name
    except Exception:
        return False


def format_default() -> str:
    return " = ..."


def format_param(name: str, kind: inspect._ParameterKind, default: inspect._empty, annotation: Any, is_method: bool) -> str:
    # All types annotated as Any; defaults replaced with ...
    # Handle self/cls naming
    pname = name
    if is_method and name in ("self", "cls"):
        ann = ""
    else:
        ann = ": Any"

    if kind is inspect.Parameter.VAR_POSITIONAL:
        pname = f"*{pname}{ann}"
    elif kind is inspect.Parameter.VAR_KEYWORD:
        pname = f"**{pname}{ann}"
    else:
        pname = f"{pname}{ann}"

    if default is not inspect._empty and kind not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
        pname += format_default()

    return pname


def build_signature(obj: Any, is_method: bool = False, force_self: Optional[str] = None) -> str:
    try:
        sig = inspect.signature(obj)
    except (ValueError, TypeError):
        # Fallback minimal signature
        if is_method:
            lead = "self" if force_self != "cls" else "cls"
            return f"({lead}, *args: Any, **kwargs: Any)"
        return "(*args: Any, **kwargs: Any)"

    params_out: List[str] = []
    saw_kwonly = False
    has_var_positional = any(p.kind == inspect.Parameter.VAR_POSITIONAL for p in sig.parameters.values())

    for i, (pname, param) in enumerate(sig.parameters.items()):
        # Ensure first param naming for methods
        if i == 0 and is_method and force_self in ("self", "cls"):
            pname = force_self

        # Insert "*" for keyword-only parameters if no *args present
        if not has_var_positional and param.kind == inspect.Parameter.KEYWORD_ONLY and not saw_kwonly:
            params_out.append("*")
            saw_kwonly = True

        params_out.append(format_param(pname, param.kind, param.default, param.annotation, is_method))

    params_str = ", ".join(params_out)
    return f"({params_str})"


def generate_function_stub(name: str, func: Any) -> str:
    sig = build_signature(func, is_method=False)
    return f"def {name}{sig} -> Any: ..."


def generate_method_stub(name: str, func: Any, method_kind: str) -> List[str]:
    lines: List[str] = []
    decorator = None
    force_self = "self"
    if method_kind == "classmethod":
        decorator = "@classmethod"
        force_self = "cls"
    elif method_kind == "staticmethod":
        decorator = "@staticmethod"
        force_self = None  # no implicit self/cls

    if decorator:
        lines.append(decorator)
    sig = build_signature(func, is_method=(method_kind != "staticmethod"), force_self=force_self)
    # __init__ should return None in stubs
    if name == "__init__":
        lines.append(f"def {name}{sig} -> None: ...")
    else:
        lines.append(f"def {name}{sig} -> Any: ...")
    return lines


def generate_property_stub(name: str) -> List[str]:
    return ["@property", f"def {name}(self) -> Any: ..."]


def generate_class_stub(cls: type, counts: GeneratedCounts, indent: str = "    ") -> str:
    counts.classes += 1
    class_lines: List[str] = [f"class {cls.__name__}:"]
    body_lines: List[str] = []

    # Collect members from the class dict only (exclude inherited)
    for attr_name, value in cls.__dict__.items():
        if attr_name.startswith("_") and attr_name not in ("__init__",):
            continue

        # Properties
        if isinstance(value, property):
            counts.properties += 1
            body_lines.extend(generate_property_stub(attr_name))
            continue

        # Methods: instance, classmethod, staticmethod
        method_kind = None
        func_obj = None
        if isinstance(value, classmethod):
            method_kind = "classmethod"
            func_obj = value.__func__
        elif isinstance(value, staticmethod):
            method_kind = "staticmethod"
            func_obj = value.__func__
        elif inspect.isfunction(value):
            method_kind = "instancemethod"
            func_obj = value

        if func_obj is not None:
            counts.methods += 1
            body_lines.extend(generate_method_stub(attr_name, func_obj, "classmethod" if method_kind == "classmethod" else "staticmethod" if method_kind == "staticmethod" else "instancemethod"))
            continue

        # Class-level attributes/constants
        if not callable(value):
            body_lines.append(f"{attr_name}: Any")

    if not body_lines:
        class_lines.append(f"{indent}...")
    else:
        for line in body_lines:
            class_lines.append(f"{indent}{line}")

    return "\n".join(class_lines)


def generate_module_stub(module: ModuleType, counts: GeneratedCounts) -> str:
    lines: List[str] = [
        "from typing import Any",
        "",
    ]

    module_name = module.__name__

    # Functions
    functions = [
        (name, obj)
        for name, obj in inspect.getmembers(module, inspect.isfunction)
        if is_defined_in(obj, module_name) and not name.startswith("_")
    ]

    # Classes
    classes = [
        (name, obj)
        for name, obj in inspect.getmembers(module, inspect.isclass)
        if is_defined_in(obj, module_name) and not name.startswith("_")
    ]

    # Variables (best-effort: simple values only)
    vars_out: List[str] = []
    for name, obj in getattr(module, "__dict__", {}).items():
        if name.startswith("_"):
            continue
        if inspect.isfunction(obj) or inspect.isclass(obj) or inspect.ismodule(obj):
            continue
        # Likely a constant or module attribute
        vars_out.append(f"{name}: Any")

    if vars_out:
        for v in sorted(set(vars_out)):
            lines.append(v)
        counts.variables += len(set(vars_out))
        lines.append("")

    # Emit functions
    for name, func in functions:
        lines.append(generate_function_stub(name, func))
        counts.functions += 1
    if functions:
        lines.append("")

    # Emit classes
    for name, cls in classes:
        lines.append(generate_class_stub(cls, counts))
        lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def walk_package_modules(root_pkg: ModuleType) -> Iterable[str]:
    if not hasattr(root_pkg, "__path__"):
        yield root_pkg.__name__
        return

    pkg_path = list(root_pkg.__path__)
    prefix = root_pkg.__name__ + "."
    yield root_pkg.__name__
    for finder, name, ispkg in pkgutil.walk_packages(pkg_path, prefix):
        yield name


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def module_to_file_path(base_dir: Path, module_name: str) -> Path:
    parts = module_name.split(".")
    if len(parts) == 1:
        # package root stub
        return base_dir / (module_name + ".pyi")
    # Treat last as module; for packages, __init__.pyi should be generated separately if needed
    return base_dir.joinpath(*parts[:-1]) / (parts[-1] + ".pyi")


def generate_stubs_for_package(package_name: str, out_dir: Path) -> GeneratedCounts:
    counts = GeneratedCounts()
    try:
        pkg = importlib.import_module(package_name)
    except Exception as e:
        print(f"Could not import package {package_name}: {e}", file=sys.stderr)
        raise

    for mod_name in walk_package_modules(pkg):
        # Import module, skip if import fails
        try:
            mod = importlib.import_module(mod_name)
        except Exception:
            # Skip optional/failing submodules
            continue

        stub_text = generate_module_stub(mod, counts)
        target_path = module_to_file_path(out_dir, mod_name)
        ensure_dir(target_path.parent)
        with target_path.open("w", encoding="utf-8") as f:
            f.write(stub_text)
        counts.modules += 1

        # If this module is a package (has __path__), also create __init__.pyi mirroring module-level definitions
        if hasattr(mod, "__path__"):
            init_path = out_dir.joinpath(*mod_name.split(".")) / "__init__.pyi"
            ensure_dir(init_path.parent)
            with init_path.open("w", encoding="utf-8") as f:
                f.write(stub_text)

    return counts


def main(argv: Optional[List[str]] = None) -> None:
    import argparse

    parser = argparse.ArgumentParser(description="Generate typing stub files (.pyi) for oauthlib using typing.Any annotations.")
    parser.add_argument("-o", "--out", type=str, default="stubs", help="Output directory root (default: stubs)")
    args = parser.parse_args(argv)

    out_root = Path(args.out).resolve()
    ensure_dir(out_root)

    counts = generate_stubs_for_package("oauthlib", out_root)

    print(f"Generated stubs in: {out_root}")
    print(f"Modules:   {counts.modules}")
    print(f"Functions: {counts.functions}")
    print(f"Classes:   {counts.classes}")
    print(f"Methods:   {counts.methods}")
    print(f"Properties:{counts.properties}")
    print(f"Variables: {counts.variables}")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 236:
"Generate Python code that uses the `openpyxl` library to create an Excel template from a Frictionless Table Schema, including input validation using the `validate` function from the `schema` library."
RESPONSE:
```python
#!/usr/bin/env python3
"""
Create an Excel template from a Frictionless Table Schema using openpyxl,
and validate filled-in rows using the `schema` library's `Schema.validate`.

Requirements:
  pip install openpyxl schema

Usage:
  python frictionless_excel_template.py --schema table_schema.json --out template.xlsx
  # After users fill in template.xlsx:
  python frictionless_excel_template.py --schema table_schema.json --validate template.xlsx
"""

import argparse
import datetime as dt
import json
import re
from typing import Any, Dict, List, Optional, Tuple

from openpyxl import Workbook, load_workbook
from openpyxl.comments import Comment
from openpyxl.styles import Font, Alignment, PatternFill
from openpyxl.utils import get_column_letter
from openpyxl.worksheet.datavalidation import DataValidation

from schema import Schema, And, Or, Use, Optional as SOptional, Regex, SchemaError


# ---------------------------
# Frictionless schema helpers
# ---------------------------

def load_table_schema(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        schema = json.load(f)
    if "fields" not in schema or not isinstance(schema["fields"], list):
        raise ValueError("Invalid Table Schema: missing 'fields' array")
    return schema


def field_title(field: Dict[str, Any]) -> str:
    return field.get("title") or field.get("name", "")


def constraint(field: Dict[str, Any], key: str, default=None):
    return field.get("constraints", {}).get(key, default)


# ---------------------------
# Excel template generation
# ---------------------------

def create_excel_template_from_frictionless(
    table_schema: Dict[str, Any],
    out_path: str,
    sheet_name: str = "Data",
    data_rows: int = 1000,
) -> None:
    """
    Generates an Excel template with:
      - Header row from schema fields (title or name)
      - Data validations mapped from Frictionless types/constraints
      - Enum dropdowns via a hidden _lists sheet
      - Helpful header comments including field metadata
    """
    wb = Workbook()
    ws = wb.active
    ws.title = sheet_name

    # Hidden sheet to store enum lists
    ws_lists = wb.create_sheet("_lists")
    ws_lists.sheet_state = "hidden"
    enum_col_cursor = 1  # next free column in _lists sheet
    enum_ranges: Dict[str, str] = {}  # field_name -> Excel absolute range string (e.g. '_lists'!$A$1:$A$3)

    # Header styling
    header_font = Font(bold=True)
    header_fill = PatternFill(fill_type="solid", start_color="FFEEF7FF", end_color="FFEEF7FF")
    header_align = Alignment(horizontal="center", vertical="center", wrap_text=True)

    fields = table_schema["fields"]

    # Write headers and prepare validations
    for idx, field in enumerate(fields, start=1):
        col_letter = get_column_letter(idx)
        header_cell = ws.cell(row=1, column=idx, value=field_title(field))

        # Style header
        header_cell.font = header_font
        header_cell.fill = header_fill
        header_cell.alignment = header_align
        ws.column_dimensions[col_letter].width = max(12, min(50, len(str(header_cell.value)) + 4))

        # Add header comment summarizing important details
        details = []
        details.append(f"name: {field.get('name', '')}")
        details.append(f"type: {field.get('type', 'string')}")
        if field.get("format"):
            details.append(f"format: {field['format']}")
        if field.get("description"):
            details.append(f"desc: {field['description']}")
        cons = field.get("constraints", {})
        if cons:
            cparts = []
            for k, v in cons.items():
                if k == "enum":
                    cparts.append(f"enum={len(v)} values")
                else:
                    cparts.append(f"{k}={v}")
            details.append("constraints: " + ", ".join(cparts))
        header_cell.comment = Comment("\n".join(details), "schema")

        # Apply data validations
        apply_field_data_validation(wb, ws, ws_lists, field, idx, 2, 1 + data_rows,
                                    enum_ranges, enum_col_cursor)

        # Advance enum col cursor if an enum was used for this field
        if field.get("constraints", {}).get("enum"):
            enum_col_cursor += 1

    # Freeze header
    ws.freeze_panes = "A2"

    # Optional: add light table-like alternating fill is complex; skip.

    wb.save(out_path)


def apply_field_data_validation(
    wb: Workbook,
    ws,
    ws_lists,
    field: Dict[str, Any],
    col_index: int,
    start_row: int,
    end_row: int,
    enum_ranges: Dict[str, str],
    enum_col_cursor: int,
) -> None:
    """
    Adds an appropriate openpyxl DataValidation for a single field column,
    mapping Frictionless type/constraints to Excel validations when feasible.
    """
    name = field.get("name")
    ftype = field.get("type", "string")
    fmt = field.get("format")
    cons = field.get("constraints", {})
    required = bool(cons.get("required", False))
    allow_blank = not required

    col_letter = get_column_letter(col_index)
    cell_range = f"{col_letter}{start_row}:{col_letter}{end_row}"

    # If enum, add a dropdown list. Populate _lists sheet with the allowed values.
    enum = cons.get("enum")
    if enum and isinstance(enum, list) and len(enum) > 0:
        # Write enum values into _lists at the next free column
        list_col_letter = get_column_letter(enum_col_cursor)
        for i, val in enumerate(enum, start=1):
            ws_lists.cell(row=i, column=enum_col_cursor, value=val)

        # Absolute range for the enum list
        enum_range = f"'_lists'!${list_col_letter}$1:${list_col_letter}${len(enum)}"
        enum_ranges[name] = enum_range

        dv = DataValidation(
            type="list",
            formula1=f"={enum_range}",
            allow_blank=allow_blank,
            showErrorMessage=True,
            showInputMessage=True,
            errorTitle="Invalid choice",
            error="Please choose a value from the list.",
            promptTitle=field_title(field),
            prompt="Select a value from the dropdown list.",
        )
        ws.add_data_validation(dv)
        dv.add(cell_range)
        # For enum fields we also set string length constraints if present
        min_len = cons.get("minLength")
        max_len = cons.get("maxLength")
        if min_len is not None or max_len is not None:
            add_text_length_validation(ws, cell_range, min_len, max_len, allow_blank)
        return

    # Map types to Excel data validations
    if ftype in ("integer", "number"):
        minimum = cons.get("minimum")
        maximum = cons.get("maximum")
        if ftype == "integer":
            dtype = "whole"
        else:
            dtype = "decimal"

        # Create numeric validation. Excel requires some operator/formula.
        if minimum is not None and maximum is not None:
            dv = DataValidation(
                type=dtype,
                operator="between",
                formula1=str(minimum),
                formula2=str(maximum),
                allow_blank=allow_blank,
                showErrorMessage=True,
                showInputMessage=True,
                errorTitle="Invalid number",
                error=f"Value must be between {minimum} and {maximum}.",
                promptTitle=field_title(field),
                prompt="Enter a numeric value.",
            )
        elif minimum is not None:
            dv = DataValidation(
                type=dtype,
                operator="greaterThanOrEqual",
                formula1=str(minimum),
                allow_blank=allow_blank,
                showErrorMessage=True,
                showInputMessage=True,
                errorTitle="Invalid number",
                error=f"Value must be greater than or equal to {minimum}.",
                promptTitle=field_title(field),
                prompt="Enter a numeric value.",
            )
        elif maximum is not None:
            dv = DataValidation(
                type=dtype,
                operator="lessThanOrEqual",
                formula1=str(maximum),
                allow_blank=allow_blank,
                showErrorMessage=True,
                showInputMessage=True,
                errorTitle="Invalid number",
                error=f"Value must be less than or equal to {maximum}.",
                promptTitle=field_title(field),
                prompt="Enter a numeric value.",
            )
        else:
            # No bounds; just ensure numeric using a custom ISNUMBER formula.
            # Use custom validation referencing the cell; apply per-cell with a template.
            # However DataValidation custom cannot directly be relative per-cell in a range.
            # Instead, use a numeric validation with very wide bounds:
            dv = DataValidation(
                type=dtype,
                operator="between",
                formula1=str(-1e308 if dtype == "decimal" else -2147483648),
                formula2=str(1e308 if dtype == "decimal" else 2147483647),
                allow_blank=allow_blank,
                showErrorMessage=True,
                showInputMessage=True,
                errorTitle="Invalid number",
                error="Enter a numeric value.",
                promptTitle=field_title(field),
                prompt="Enter a numeric value.",
            )
        ws.add_data_validation(dv)
        dv.add(cell_range)

    elif ftype in ("date", "datetime", "time"):
        # Excel has date/time validation types. Use min/max if present.
        minimum = cons.get("minimum")
        maximum = cons.get("maximum")

        dtype = "date" if ftype in ("date", "datetime") else "time"

        def to_excel_date_formula(val: str) -> str:
            # Expect ISO formats if provided as strings in schema
            # Convert to Excel DATE(...) or a serial date is unreliable across locales.
            try:
                if ftype == "time":
                    # Expect HH:MM[:SS] -> TIME(h,m,s)
                    hh, mm, ss = (val.split(":") + ["0"])[:3]
                    return f"TIME({int(hh)},{int(mm)},{int(ss)})"
                elif ftype == "date":
                    d = dt.date.fromisoformat(val)
                    return f"DATE({d.year},{d.month},{d.day})"
                else:
                    # datetime: consider only date part for Excel date validation; times won't be enforced strictly.
                    d = dt.datetime.fromisoformat(val)
                    return f"DATE({d.year},{d.month},{d.day})"
            except Exception:
                # Fallback: no bound if parsing fails
                return None

        op = None
        f1 = None
        f2 = None
        if minimum is not None and maximum is not None:
            f1 = to_excel_date_formula(str(minimum))
            f2 = to_excel_date_formula(str(maximum))
            if f1 and f2:
                op = "between"
        elif minimum is not None:
            f1 = to_excel_date_formula(str(minimum))
            if f1:
                op = "greaterThanOrEqual"
        elif maximum is not None:
            f1 = to_excel_date_formula(str(maximum))
            if f1:
                op = "lessThanOrEqual"

        if op:
            dv = DataValidation(
                type=dtype,
                operator=op,
                formula1=f1,
                formula2=f2,
                allow_blank=allow_blank,
                showErrorMessage=True,
                showInputMessage=True,
                errorTitle=f"Invalid {ftype}",
                error=f"Value must be within the specified {ftype} range.",
                promptTitle=field_title(field),
                prompt=f"Enter a {ftype}.",
            )
            ws.add_data_validation(dv)
            dv.add(cell_range)
        else:
            # If we cannot compute min/max, at least set type to date/time with wide range is not supported directly.
            # Skip strict validation; users can still type dates, and we validate on read.
            pass

    elif ftype == "boolean":
        # Restrict to TRUE/FALSE via list
        dv = DataValidation(
            type="list",
            formula1='"TRUE,FALSE"',
            allow_blank=allow_blank,
            showErrorMessage=True,
            showInputMessage=True,
            errorTitle="Invalid boolean",
            error="Value must be TRUE or FALSE.",
            promptTitle=field_title(field),
            prompt="Choose TRUE or FALSE.",
        )
        ws.add_data_validation(dv)
        dv.add(cell_range)

    else:
        # Treat as string; apply text length constraints if present
        min_len = cons.get("minLength")
        max_len = cons.get("maxLength")
        if min_len is not None or max_len is not None:
            add_text_length_validation(ws, cell_range, min_len, max_len, allow_blank)
        # Can't enforce regex/pattern in Excel in a portable way; will validate on read.


def add_text_length_validation(ws, cell_range: str, min_len: Optional[int], max_len: Optional[int], allow_blank: bool):
    if min_len is not None and max_len is not None:
        dv = DataValidation(
            type="textLength",
            operator="between",
            formula1=str(min_len),
            formula2=str(max_len),
            allow_blank=allow_blank,
            showErrorMessage=True,
            showInputMessage=True,
            errorTitle="Invalid length",
            error=f"Text length must be between {min_len} and {max_len}.",
            prompt="Enter a value with allowed length.",
        )
    elif min_len is not None:
        dv = DataValidation(
            type="textLength",
            operator="greaterThanOrEqual",
            formula1=str(min_len),
            allow_blank=allow_blank,
            showErrorMessage=True,
            showInputMessage=True,
            errorTitle="Invalid length",
            error=f"Text length must be at least {min_len}.",
            prompt="Enter a value with allowed length.",
        )
    elif max_len is not None:
        dv = DataValidation(
            type="textLength",
            operator="lessThanOrEqual",
            formula1=str(max_len),
            allow_blank=allow_blank,
            showErrorMessage=True,
            showInputMessage=True,
            errorTitle="Invalid length",
            error=f"Text length must be at most {max_len}.",
            prompt="Enter a value with allowed length.",
        )
    else:
        return
    ws.add_data_validation(dv)
    dv.add(cell_range)


# ---------------------------
# Row validation using `schema`
# ---------------------------

def build_row_validator(table_schema: Dict[str, Any]) -> Schema:
    """
    Build a `schema.Schema` validator for a single row dictionary based on
    Frictionless Table Schema types and constraints.
    """
    field_rules = {}

    for field in table_schema["fields"]:
        name = field["name"]
        ftype = field.get("type", "string")
        cons = field.get("constraints", {})
        required = bool(cons.get("required", False))

        rule = build_value_rule(field)

        # Required vs optional key
        key = name if required else SOptional(name)
        field_rules[key] = rule

    return Schema(field_rules)


def build_value_rule(field: Dict[str, Any]):
    """
    Create a validator rule for a single field:
      - Casts from Excel cell value to Python type (Use)
      - Applies min/max, enum, length, regex
      - Honors required vs optional via Or(None, ...) or not
    """
    ftype = field.get("type", "string")
    cons = field.get("constraints", {})
    required = bool(cons.get("required", False))

    allow_none = not required

    enum_vals = cons.get("enum")
    min_len = cons.get("minLength")
    max_len = cons.get("maxLength")
    pattern = cons.get("pattern")
    minimum = cons.get("minimum")
    maximum = cons.get("maximum")

    def allow_blank_to_none(v):
        return None if v in ("", None) else v

    rules: List[Any] = []

    if ftype == "integer":
        rules.append(Use(lambda v: None if v in ("", None) else int(v)))
        rules.append(lambda v: v is None or isinstance(v, int))
        if minimum is not None:
            rules.append(lambda v, mn=minimum: v is None or v >= mn)
        if maximum is not None:
            rules.append(lambda v, mx=maximum: v is None or v <= mx)

    elif ftype == "number":
        rules.append(Use(lambda v: None if v in ("", None) else float(v)))
        rules.append(lambda v: v is None or isinstance(v, (int, float)))
        if minimum is not None:
            rules.append(lambda v, mn=minimum: v is None or v >= mn)
        if maximum is not None:
            rules.append(lambda v, mx=maximum: v is None or v <= mx)

    elif ftype == "boolean":
        def to_bool(v):
            if v in (None, ""):
                return None
            if isinstance(v, bool):
                return v
            s = str(v).strip().lower()
            if s in ("true", "1", "yes", "y", "t"):
                return True
            if s in ("false", "0", "no", "n", "f"):
                return False
            raise ValueError("not a boolean")
        rules.append(Use(to_bool))
        rules.append(lambda v: v is None or isinstance(v, bool))

    elif ftype in ("date", "datetime", "time"):
        if ftype == "date":
            def to_date(v):
                if v in (None, ""):
                    return None
                if isinstance(v, dt.date) and not isinstance(v, dt.datetime):
                    return v
                if isinstance(v, dt.datetime):
                    return v.date()
                return dt.date.fromisoformat(str(v))
            rules.append(Use(to_date))
            if minimum is not None:
                dmin = dt.date.fromisoformat(str(minimum))
                rules.append(lambda v, mn=dmin: v is None or v >= mn)
            if maximum is not None:
                dmax = dt.date.fromisoformat(str(maximum))
                rules.append(lambda v, mx=dmax: v is None or v <= mx)
        elif ftype == "datetime":
            def to_dt(v):
                if v in (None, ""):
                    return None
                if isinstance(v, dt.datetime):
                    return v
                if isinstance(v, dt.date):
                    return dt.datetime.combine(v, dt.time())
                return dt.datetime.fromisoformat(str(v))
            rules.append(Use(to_dt))
            if minimum is not None:
                dmin = dt.datetime.fromisoformat(str(minimum))
                rules.append(lambda v, mn=dmin: v is None or v >= mn)
            if maximum is not None:
                dmax = dt.datetime.fromisoformat(str(maximum))
                rules.append(lambda v, mx=dmax: v is None or v <= mx)
        else:  # time
            def to_time(v):
                if v in (None, ""):
                    return None
                if isinstance(v, dt.time):
                    return v
                # Expect HH:MM[:SS]
                parts = str(v).split(":")
                hh = int(parts[0])
                mm = int(parts[1]) if len(parts) > 1 else 0
                ss = int(parts[2]) if len(parts) > 2 else 0
                return dt.time(hh, mm, ss)
            rules.append(Use(to_time))
            # Min/max for time if provided as HH:MM[:SS]
            if minimum is not None:
                tmin = _parse_time(str(minimum))
                rules.append(lambda v, mn=tmin: v is None or v >= mn)
            if maximum is not None:
                tmax = _parse_time(str(maximum))
                rules.append(lambda v, mx=tmax: v is None or v <= mx)

    else:
        # default to string
        rules.append(Use(lambda v: None if v in (None, "") else str(v)))
        if min_len is not None:
            rules.append(lambda v, mn=min_len: v is None or len(v) >= mn)
        if max_len is not None:
            rules.append(lambda v, mx=max_len: v is None or len(v) <= mx)
        if pattern:
            # Frictionless uses Python regex strings; assume they are compatible
            rules.append(lambda v, rx=re.compile(pattern): v is None or bool(rx.fullmatch(v)))

    # Enum constraint (applies to any type)
    if enum_vals:
        enum_set = set(enum_vals)
        rules.append(lambda v, es=enum_set: v is None or v in es)

    # Required vs optional
    if allow_none:
        rule = Or(None, And(*rules))
    else:
        rule = And(*rules, lambda v: v is not None)

    return rule


def _parse_time(s: str) -> dt.time:
    parts = s.split(":")
    hh = int(parts[0])
    mm = int(parts[1]) if len(parts) > 1 else 0
    ss = int(parts[2]) if len(parts) > 2 else 0
    return dt.time(hh, mm, ss)


def read_rows_from_workbook(xlsx_path: str, sheet_name: Optional[str], schema: Dict[str, Any]) -> List[Dict[str, Any]]:
    wb = load_workbook(xlsx_path, data_only=True)
    ws = wb[sheet_name] if sheet_name else wb.active

    fields = schema["fields"]
    # Expect the first row to be headers that align with schema order.
    # We use the schema field order to map columns to field names.
    name_by_col = [f["name"] for f in fields]

    rows: List[Dict[str, Any]] = []
    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, max_col=len(fields), values_only=True):
        if row is None:
            continue
        # Skip completely empty rows
        if all(c is None or c == "" for c in row):
            continue
        rec = {}
        for i, value in enumerate(row):
            if i >= len(name_by_col):
                break
            rec[name_by_col[i]] = value
        rows.append(rec)
    return rows


def validate_workbook_with_schema(xlsx_path: str, table_schema: Dict[str, Any], sheet_name: Optional[str] = None) -> Tuple[bool, List[str]]:
    """
    Validate each populated row in the workbook against a Schema built from the
    Frictionless Table Schema. Uses Schema.validate from the `schema` library.
    Returns (is_valid, errors)
    """
    rows = read_rows_from_workbook(xlsx_path, sheet_name, table_schema)
    validator = build_row_validator(table_schema)

    errors: List[str] = []
    ok = True
    for i, row in enumerate(rows, start=2):  # Excel row number (header is row 1)
        try:
            # This calls the library's 'validate' function (Schema.validate)
            validator.validate(row)
        except SchemaError as e:
            ok = False
            errors.append(f"Row {i}: {e}")

    return ok, errors


# ---------------------------
# CLI
# ---------------------------

def main():
    parser = argparse.ArgumentParser(description="Excel template generator and validator for Frictionless Table Schema")
    parser.add_argument("--schema", required=True, help="Path to Frictionless Table Schema JSON")
    parser.add_argument("--out", help="Output Excel template path (e.g., template.xlsx)")
    parser.add_argument("--rows", type=int, default=1000, help="Number of input rows in the template")
    parser.add_argument("--sheet", help="Sheet name to read for validation (defaults to first sheet)")
    parser.add_argument("--validate", help="Validate a filled-in workbook against the schema (path to xlsx)")
    args = parser.parse_args()

    table_schema = load_table_schema(args.schema)

    if args.out:
        create_excel_template_from_frictionless(table_schema, args.out, data_rows=args.rows)
        print(f"Wrote template to {args.out}")

    if args.validate:
        ok, errs = validate_workbook_with_schema(args.validate, table_schema, sheet_name=args.sheet)
        if ok:
            print("Validation passed: all rows are valid.")
        else:
            print("Validation failed:")
            for msg in errs:
                print(" -", msg)
        # Exit code reflecting validity for CI usage
        import sys
        sys.exit(0 if ok else 1)

    if not args.out and not args.validate:
        print("Nothing to do. Provide --out to create a template and/or --validate to validate a workbook.")


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 237:
"Generate Python code that uses the Datadog APM client library to instrument and monitor a sample application, collecting performance metrics and trace data, and displaying them in the Datadog dashboard."
RESPONSE:
Below is a minimal, end-to-end example of a Python web app instrumented with Datadog APM, including automatic tracing, custom spans, and custom metrics. It uses ddtrace for APM and the built-in ddtrace.statsd client for DogStatsD metrics. Run it against a Datadog Agent, then view traces in APM and graph the custom metrics on a dashboard.

Python code (app.py):

```python
# Requirements:
#   pip install ddtrace Flask requests
#
# Make sure a Datadog Agent is running and reachable (with APM and DogStatsD enabled).
# For local dev: https://docs.datadoghq.com/agent/
#
# This sample:
# - Sets sane defaults for DD_* env vars if not provided
# - Instruments Flask, requests, and logging
# - Emits automatic request traces, custom child spans, and custom metrics via DogStatsD
# - Demonstrates error tracing via /error endpoint

import os
os.environ.setdefault("DD_SERVICE", "sample-python-app")
os.environ.setdefault("DD_ENV", "dev")
os.environ.setdefault("DD_VERSION", "1.0.0")
# If your Agent is on another host/container, change the URLs below:
os.environ.setdefault("DD_TRACE_AGENT_URL", "http://127.0.0.1:8126")
os.environ.setdefault("DD_DOGSTATSD_URL", "udp://127.0.0.1:8125")

import logging
import random
import time

from flask import Flask, request, jsonify
import requests

# Datadog APM and metrics
from ddtrace import patch_all, tracer, config, statsd

# Enable auto-instrumentation for common libs
patch_all(logging=True, requests=True)

# Optional: global service/env/version (also set via env above)
config.service = os.getenv("DD_SERVICE", "sample-python-app")
config.env = os.getenv("DD_ENV", "dev")
config.version = os.getenv("DD_VERSION", "1.0.0")

# Logging with trace correlation (DD log integration adds dd.trace_id / dd.span_id fields)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s [dd.trace_id=%(dd.trace_id)s dd.span_id=%(dd.span_id)s] %(message)s",
)
logger = logging.getLogger("sample-app")

app = Flask(__name__)

def heavy_compute(n: int) -> int:
    # Demonstrates a custom child span
    with tracer.trace("work.heavy_compute", resource="fibonacci", span_type="custom") as span:
        span.set_tag("work.size", n)
        a, b = 0, 1
        for _ in range(n):
            a, b = b, a + b
            # Simulate CPU work
        return a

@app.route("/work")
def work():
    start = time.perf_counter()

    # Emit a custom counter for every request
    statsd.increment(
        "sample_app.request.count",
        tags=[f"path:{request.path}", f"method:{request.method}", f"env:{config.env}"],
    )

    # Manual child span around a block of work
    with tracer.trace("work.pipeline", resource="work_handler") as span:
        # Tag the span with request info
        span.set_tag("http.method", request.method)
        span.set_tag("http.route", "/work")

        # Simulate latency
        delay = random.uniform(0.05, 0.25)
        time.sleep(delay)

        # External call (auto-instrumented via requests)
        r = requests.get("https://httpbin.org/delay/0.2", timeout=5)
        span.set_tag("ext.status_code", r.status_code)

        # Some compute
        n = random.randint(20, 28)
        val = heavy_compute(n)

    elapsed_ms = (time.perf_counter() - start) * 1000.0

    # Emit latency metric
    statsd.timing(
        "sample_app.request.latency_ms",
        elapsed_ms,
        tags=[f"path:{request.path}", f"method:{request.method}", f"env:{config.env}"],
    )

    logger.info("Handled /work request with latency_ms=%0.2f value=%d n=%d", elapsed_ms, val, n)
    return jsonify({"ok": True, "latency_ms": round(elapsed_ms, 2), "fib_n": n, "fib_val": val})

@app.route("/error")
def error():
    # This will produce an error span in APM and a 500 response
    with tracer.trace("work.failing_operation", resource="boom") as span:
        span.set_tag("operation", "explode")
        raise RuntimeError("Intentional error to demonstrate APM error tracking")

@app.route("/")
def root():
    return "Datadog APM sample is running. Try /work or /error"

if __name__ == "__main__":
    # Flask dev server; in production, run under a WSGI server (gunicorn/uwsgi).
    # Example (gunicorn): DD_SERVICE=sample-python-app ddtrace-run gunicorn -b 0.0.0.0:5000 app:app
    app.run(host="0.0.0.0", port=5000)
```

How to run
- Install packages: pip install ddtrace Flask requests
- Ensure a Datadog Agent is running and can receive:
  - APM traces on port 8126 (DD_TRACE_AGENT_URL, default http://127.0.0.1:8126)
  - DogStatsD metrics on port 8125 (DD_DOGSTATSD_URL, default udp://127.0.0.1:8125)
- Set any desired env vars (optional):
  - DD_SERVICE, DD_ENV, DD_VERSION
  - DD_TRACE_AGENT_URL, DD_DOGSTATSD_URL
- Start the app: python app.py
- Generate traffic:
  - curl http://localhost:5000/work
  - curl http://localhost:5000/error

Where to see data in Datadog
- Traces: APM > Traces, filter by service:sample-python-app (or your DD_SERVICE)
- Services/Endpoints: APM > Services, select your service to see latency, error rates, endpoints, and flame graphs
- Logs (if shipping logs to Datadog): they’ll include dd.trace_id and dd.span_id for correlation
- Custom metrics: sample_app.request.count and sample_app.request.latency_ms
  - Dashboards > New Dashboard > Add a timeseries widget
  - Query metrics by name, add tags like path:/work or env:dev as filters

Notes
- In production, prefer running with ddtrace-run and a production WSGI server:
  - DD_SERVICE=sample-python-app ddtrace-run gunicorn -b 0.0.0.0:5000 app:app
- If your Agent runs in Docker or Kubernetes, set DD_TRACE_AGENT_URL/DD_DOGSTATSD_URL to the proper host or URL per your environment.
------------------------------------------------------------

PROMPT 238:
"Generate Python code that creates a spreadsheet in a Jupyter notebook using the `ipywidgets` library to interactively input data and the `openpyxl` library to create and manipulate the spreadsheet file."
RESPONSE:
# If needed, install dependencies (run this cell once)
# !pip install ipywidgets openpyxl

# Then run the cell below

import os
from io import BytesIO
from IPython.display import display, clear_output, FileLink
import ipywidgets as widgets
from openpyxl import Workbook, load_workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter

# ---- State and helpers ----
state = {
    'grid_widget': None,
    'cells': None,            # 2D list of Text widgets
    'uploaded_bytes': None    # for reading from FileUpload
}

def try_number(value):
    v = value.strip()
    if v == '':
        return None
    # Try int
    try:
        iv = int(v)
        return iv
    except:
        pass
    # Try float
    try:
        fv = float(v)
        return fv
    except:
        pass
    # Otherwise string
    return v

def get_upload_item():
    v = file_upload.value
    if isinstance(v, dict) and v:
        return list(v.values())[0]
    elif isinstance(v, tuple) and len(v) > 0:
        return v[0]
    return None

def create_grid(rows, cols, fill=None):
    grid = widgets.GridspecLayout(rows, cols, grid_gap="3px", layout=widgets.Layout(width='100%'))
    new_cells = []
    for i in range(rows):
        row_cells = []
        for j in range(cols):
            init_val = ''
            if fill is not None and i < len(fill) and j < len(fill[i]):
                v = fill[i][j]
                init_val = '' if v is None else str(v)
            t = widgets.Text(
                value=init_val,
                layout=widgets.Layout(width='auto', min_width='80px')
            )
            grid[i, j] = t
            row_cells.append(t)
        new_cells.append(row_cells)
    state['grid_widget'] = grid
    state['cells'] = new_cells
    grid_container.children = [grid]

def current_values():
    cells = state['cells']
    if not cells:
        return []
    rows = len(cells)
    cols = len(cells[0])
    return [[cells[i][j].value for j in range(cols)] for i in range(rows)]

def ensure_grid_exists():
    if state['cells'] is None:
        create_grid(rows_input.value, cols_input.value)

# ---- UI Widgets ----
rows_input = widgets.BoundedIntText(value=5, min=1, max=200, description='Rows:')
cols_input = widgets.BoundedIntText(value=5, min=1, max=200, description='Cols:')
header_checkbox = widgets.Checkbox(value=False, description='First row is header')

create_btn = widgets.Button(description='Create grid', icon='table', button_style='primary')
add_row_btn = widgets.Button(description='Add row', icon='plus')
add_col_btn = widgets.Button(description='Add col', icon='plus')
clear_btn = widgets.Button(description='Clear values', icon='trash')

file_name_text = widgets.Text(value='my_sheet.xlsx', description='Filename:')
sheet_name_text = widgets.Text(value='Sheet1', description='Sheet:')
save_btn = widgets.Button(description='Save to Excel', icon='save', button_style='success')

file_upload = widgets.FileUpload(accept='.xlsx', multiple=False, description='Upload .xlsx')
sheet_dropdown = widgets.Dropdown(options=[], description='Sheet:')
load_sheet_btn = widgets.Button(description='Load to grid', icon='upload')

grid_container = widgets.VBox()
output = widgets.Output()

# ---- Event Handlers ----
def on_create_clicked(_):
    with output:
        clear_output(wait=True)
    create_grid(rows_input.value, cols_input.value)

def on_add_row_clicked(_):
    ensure_grid_exists()
    vals = current_values()
    cols = len(vals[0]) if vals else max(1, cols_input.value)
    vals.append([''] * cols)
    create_grid(len(vals), cols, fill=vals)

def on_add_col_clicked(_):
    ensure_grid_exists()
    vals = current_values()
    if not vals:
        vals = [[''] * max(1, cols_input.value) for _ in range(max(1, rows_input.value))]
    for r in vals:
        r.append('')
    create_grid(len(vals), len(vals[0]), fill=vals)

def on_clear_clicked(_):
    ensure_grid_exists()
    cells = state['cells']
    for row in cells:
        for c in row:
            c.value = ''

def on_save_clicked(_):
    ensure_grid_exists()
    vals = current_values()
    if not vals:
        with output:
            clear_output(wait=True)
            print("Grid is empty. Nothing to save.")
        return

    filename = file_name_text.value.strip() or 'my_sheet.xlsx'
    if not filename.lower().endswith('.xlsx'):
        filename += '.xlsx'
    sheet_name = (sheet_name_text.value or 'Sheet1').strip()

    try:
        # Create or open workbook
        if os.path.exists(filename):
            wb = load_workbook(filename)
            # Remove existing sheet with same name to replace it
            if sheet_name in wb.sheetnames:
                std = wb[sheet_name]
                wb.remove(std)
            ws = wb.create_sheet(title=sheet_name)
        else:
            wb = Workbook()
            ws = wb.active
            ws.title = sheet_name

        # Write grid values
        rows = len(vals)
        cols = len(vals[0])
        for i in range(rows):
            for j in range(cols):
                ws.cell(row=i+1, column=j+1, value=try_number(vals[i][j]))

        # Optional header formatting
        if header_checkbox.value:
            header_fill = PatternFill(start_color='FFE5F1F8', end_color='FFE5F1F8', fill_type='solid')
            header_font = Font(bold=True)
            for j in range(1, cols+1):
                cell = ws.cell(row=1, column=j)
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = Alignment(horizontal='center', vertical='center')
            ws.freeze_panes = 'A2'
            ws.auto_filter.ref = ws.dimensions

        # Auto-fit column widths (approximate)
        for j in range(1, cols+1):
            max_len = 0
            for i in range(1, rows+1):
                v = ws.cell(i, j).value
                l = len(str(v)) if v is not None else 0
                if l > max_len:
                    max_len = l
            ws.column_dimensions[get_column_letter(j)].width = min(max(10, max_len + 2), 60)

        wb.save(filename)

        with output:
            clear_output(wait=True)
            print(f"Saved {rows}x{cols} grid to '{filename}' in sheet '{sheet_name}'.")
            try:
                display(FileLink(filename))
            except:
                pass
    except Exception as e:
        with output:
            clear_output(wait=True)
            print("Error saving workbook:", e)

def on_upload_change(change):
    item = get_upload_item()
    with output:
        clear_output(wait=True)
    if not item:
        return
    try:
        content = item['content']
        wb = load_workbook(filename=BytesIO(content), data_only=True)
        sheet_dropdown.options = wb.sheetnames
        sheet_dropdown.value = wb.sheetnames[0]
        state['uploaded_bytes'] = content
        with output:
            print(f"Loaded workbook: {item.get('name') or item.get('metadata', {}).get('name', 'uploaded.xlsx')}")
            print("Select a sheet and click 'Load to grid'.")
    except Exception as e:
        with output:
            print("Error reading uploaded workbook:", e)

def on_load_sheet_clicked(_):
    content = state.get('uploaded_bytes')
    if not content:
        with output:
            clear_output(wait=True)
            print("Upload an .xlsx file first.")
        return
    try:
        wb = load_workbook(filename=BytesIO(content), data_only=True)
        ws = wb[sheet_dropdown.value]
        rows = ws.max_row or 1
        cols = ws.max_column or 1
        values = []
        for i in range(1, rows+1):
            row_vals = []
            for j in range(1, cols+1):
                v = ws.cell(i, j).value
                row_vals.append('' if v is None else str(v))
            values.append(row_vals)
        create_grid(rows, cols, fill=values)
        with output:
            clear_output(wait=True)
            print(f"Loaded sheet '{ws.title}' into the grid ({rows}x{cols}).")
    except Exception as e:
        with output:
            clear_output(wait=True)
            print("Error loading sheet:", e)

# ---- Wire up events ----
create_btn.on_click(on_create_clicked)
add_row_btn.on_click(on_add_row_clicked)
add_col_btn.on_click(on_add_col_clicked)
clear_btn.on_click(on_clear_clicked)
save_btn.on_click(on_save_clicked)
file_upload.observe(on_upload_change, names='value')
load_sheet_btn.on_click(on_load_sheet_clicked)

# ---- Initial UI ----
top_controls = widgets.HBox([rows_input, cols_input, header_checkbox, create_btn])
grid_actions = widgets.HBox([add_row_btn, add_col_btn, clear_btn])
save_controls = widgets.HBox([file_name_text, sheet_name_text, save_btn])
load_controls = widgets.HBox([file_upload, sheet_dropdown, load_sheet_btn])

ui = widgets.VBox([
    widgets.HTML("<b>Interactive Spreadsheet (ipywidgets + openpyxl)</b>"),
    top_controls,
    grid_actions,
    grid_container,
    widgets.HTML("<hr>"),
    save_controls,
    load_controls,
    output
])

display(ui)

# Create an initial grid
create_grid(rows_input.value, cols_input.value)
------------------------------------------------------------

PROMPT 239:
"Generate Python code that uses the `unittest` module to write data-driven tests, using test cases defined as functions that take in input data as arguments and assert expected results based on that data."
RESPONSE:
# python -m unittest this_module.py
import unittest


# Code under test
def add(a, b):
    return a + b


def divide(a, b):
    return a / b


# Reusable test templates (functions that take input data as arguments)
def template_add(self, a, b, expected):
    self.assertEqual(add(a, b), expected)


def template_divide(self, a, b, expected=None, expected_exception=None):
    if expected_exception is not None:
        with self.assertRaises(expected_exception):
            divide(a, b)
    else:
        self.assertEqual(divide(a, b), expected)


# Helper that turns a template + data into a unittest-compatible no-arg test method
def make_test(template_func, *args, name=None, **kwargs):
    def _test(self):
        return template_func(self, *args, **kwargs)
    if name:
        _test.__name__ = name
    # Optional: keep a helpful docstring for test runners
    _test.__doc__ = f"{template_func.__name__}{args}{' ' + str(kwargs) if kwargs else ''}"
    return _test


class TestMathOps(unittest.TestCase):
    pass


# Data sets
add_cases = [
    (1, 2, 3),
    (0, 0, 0),
    (-5, 7, 2),
    (1000000, 1, 1000001),
]

divide_cases = [
    # a, b, expected, expected_exception
    (4, 2, 2.0, None),
    (9, 3, 3.0, None),
    (1, 4, 0.25, None),
    (5, 0, None, ZeroDivisionError),
]


# Dynamically attach tests to the TestCase using the templates and data
for idx, (a, b, expected) in enumerate(add_cases, start=1):
    test_name = f"test_add_case_{idx}__{a}_plus_{b}_equals_{expected}"
    setattr(TestMathOps, test_name, make_test(template_add, a, b, expected, name=test_name))

for idx, (a, b, expected, expected_exc) in enumerate(divide_cases, start=1):
    suffix = f"{a}_div_{b}"
    if expected_exc:
        test_name = f"test_divide_case_{idx}__{suffix}_raises_{expected_exc.__name__}"
    else:
        test_name = f"test_divide_case_{idx}__{suffix}_equals_{expected}"
    setattr(
        TestMathOps,
        test_name,
        make_test(template_divide, a, b, expected=expected, expected_exception=expected_exc, name=test_name),
    )


if __name__ == "__main__":
    unittest.main()
------------------------------------------------------------

PROMPT 240:
"Generate Python code that uses the `openapi` library to generate an OpenAPI document from a API specification, and the `pydantic` library to validate request and response data against Python annotations."
RESPONSE:
# pip install openapi-schema-pydantic pydantic
# This example:
# - Uses the "openapi" library (openapi-schema-pydantic) to build an OpenAPI document.
# - Uses Pydantic to validate request/response data from Python type annotations.

from __future__ import annotations

from typing import Any, Dict, Optional
from uuid import UUID, uuid4

from pydantic import BaseModel, EmailStr, Field

# OpenAPI object models
from openapi_schema_pydantic import (
    Components,
    Info,
    MediaType,
    OpenAPI,
    Operation,
    PathItem,
    Reference,
    RequestBody,
    Response,
    Schema,
    Server,
)


# ---------------------------
# Pydantic models (validation)
# ---------------------------

class CreateUserRequest(BaseModel):
    name: str = Field(..., min_length=1, max_length=100)
    email: EmailStr
    age: Optional[int] = Field(None, ge=0, le=150)


class UserResponse(BaseModel):
    id: UUID
    name: str
    email: EmailStr
    age: Optional[int] = None


# --------------------------------------------------------
# Simple "handler" that validates inbound/outbound payloads
# --------------------------------------------------------

def create_user_handler(payload: Dict[str, Any]) -> Dict[str, Any]:
    # Validate incoming request against Python annotations (Pydantic)
    req = CreateUserRequest.model_validate(payload)

    # Business logic (mock)
    user = UserResponse(
        id=uuid4(),
        name=req.name,
        email=req.email,
        age=req.age,
    )

    # Validate and serialize response
    return user.model_dump()


# ---------------------------------------------------------
# Helpers to convert Pydantic models into OpenAPI components
# ---------------------------------------------------------

def _schema_from_model(model_cls: type[BaseModel]) -> Schema:
    # Pydantic v2: model_json_schema; v1: schema
    if hasattr(model_cls, "model_json_schema"):
        js = model_cls.model_json_schema(ref_template="#/components/schemas/{model}")
    else:
        js = model_cls.schema(ref_template="#/components/schemas/{model}")  # type: ignore[attr-defined]

    # Drop inlined definitions if present; for simple models this is enough.
    js.pop("$defs", None)
    js.pop("definitions", None)

    # openapi-schema-pydantic supports parsing a dict into Schema
    try:
        return Schema.model_validate(js)  # Pydantic v2
    except AttributeError:
        return Schema.parse_obj(js)       # Pydantic v1


def _to_dict(pydantic_model) -> Dict[str, Any]:
    if hasattr(pydantic_model, "model_dump"):
        return pydantic_model.model_dump(by_alias=True, exclude_none=True)
    return pydantic_model.dict(by_alias=True, exclude_none=True)


# ----------------------------------------
# Build OpenAPI document programmatically
# ----------------------------------------

def build_openapi() -> OpenAPI:
    components = Components(
        schemas={
            "CreateUserRequest": _schema_from_model(CreateUserRequest),
            "UserResponse": _schema_from_model(UserResponse),
            "Error": Schema(
                type="object",
                required=["detail"],
                properties={
                    "detail": Schema(type="string")
                },
            ),
        }
    )

    create_user_op = Operation(
        operationId="createUser",
        summary="Create a user",
        tags=["Users"],
        requestBody=RequestBody(
            required=True,
            content={
                "application/json": MediaType(
                    schema=Reference(ref="#/components/schemas/CreateUserRequest")
                )
            },
        ),
        responses={
            "201": Response(
                description="User created",
                content={
                    "application/json": MediaType(
                        schema=Reference(ref="#/components/schemas/UserResponse")
                    )
                },
            ),
            "400": Response(
                description="Validation error",
                content={
                    "application/json": MediaType(
                        schema=Reference(ref="#/components/schemas/Error")
                    )
                },
            ),
        },
    )

    paths: Dict[str, PathItem] = {
        "/users": PathItem(post=create_user_op),
    }

    api = OpenAPI(
        openapi="3.1.0",
        info=Info(
            title="User Service",
            version="1.0.0",
            description="Example API with Pydantic validation and OpenAPI generation."
        ),
        servers=[Server(url="https://api.example.com")],
        paths=paths,
        components=components,
    )
    return api


# -------------
# Demonstration
# -------------

if __name__ == "__main__":
    # Validate request/response using Pydantic models
    incoming = {"name": "Alice", "email": "alice@example.com", "age": 29}
    out_payload = create_user_handler(incoming)
    print("Validated response payload:", out_payload)

    # Generate and print OpenAPI JSON
    api = build_openapi()
    import json
    print(json.dumps(_to_dict(api), indent=2))
------------------------------------------------------------

PROMPT 241:
"Generate Python code that imports the necessary libraries and uses the Azure Open Datasets API to retrieve and manipulate datasets, including searching for datasets, downloading data, and uploading data."
RESPONSE:
# Requires: pip install --upgrade azureml-opendatasets azureml-core pandas
# Optional: create a Azure ML workspace config.json or set env vars (see below)

import os
import sys
import json
import pkgutil
import importlib
import inspect
from datetime import datetime
import pandas as pd

# Azure Open Datasets (read/search)
import azureml.opendatasets as aod
from azureml.opendatasets import NycTlcGreen, PublicHolidays  # example classes

# Azure ML (upload/register your processed data in your workspace)
from azureml.core import Workspace, Dataset


# ---------------------------
# Helper: search Open Datasets classes by keyword
# ---------------------------
def search_open_datasets(query: str):
    """
    "Search" the Azure Open Datasets Python API by scanning dataset classes
    and their docstrings for the given keyword.
    """
    results = []
    q = query.lower()
    for _, modname, _ in pkgutil.iter_modules(aod.__path__):
        try:
            module = importlib.import_module(f"azureml.opendatasets.{modname}")
        except Exception:
            continue
        for name, obj in inspect.getmembers(module, inspect.isclass):
            if not getattr(obj, "__module__", "").startswith("azureml.opendatasets"):
                continue
            doc = inspect.getdoc(obj) or ""
            haystack = f"{name} {doc}".lower()
            if q in haystack:
                results.append({
                    "class": f"{obj.__module__}.{name}",
                    "summary": (doc.splitlines()[0] if doc else "")
                })
    return results


# ---------------------------
# Example: download/transform data from Open Datasets
# ---------------------------
def download_open_dataset_example():
    # Example 1: NYC TLC Green Taxi trips (filter by pickup datetime range)
    start = datetime(2019, 1, 1)
    end = datetime(2019, 1, 7)
    green = NycTlcGreen().filter(start_time=start, end_time=end)
    taxi_df = green.to_pandas_dataframe()  # Requires enough memory; consider sampling if large

    # Basic manipulation: select a few columns and simple aggregation
    cols = ["lpepPickupDatetime", "lpepDropoffDatetime", "passengerCount", "tripDistance", "totalAmount", "storeAndFwdFlag"]
    taxi_small = taxi_df[cols].copy()
    taxi_small["tripHours"] = (pd.to_datetime(taxi_small["lpepDropoffDatetime"]) - pd.to_datetime(taxi_small["lpepPickupDatetime"])).dt.total_seconds() / 3600.0
    taxi_summary = taxi_small["tripDistance"].describe()

    # Example 2: Public Holidays
    holidays_df = PublicHolidays().to_pandas_dataframe()

    # Save locally
    os.makedirs("data/opendatasets", exist_ok=True)
    taxi_small.to_csv("data/opendatasets/nyc_tlc_green_2019_week1.csv", index=False)
    holidays_df.to_csv("data/opendatasets/public_holidays_all.csv", index=False)

    print("Saved:")
    print(" - data/opendatasets/nyc_tlc_green_2019_week1.csv")
    print(" - data/opendatasets/public_holidays_all.csv")
    print("\nTaxi distance summary:")
    print(taxi_summary)


# ---------------------------
# Upload to Azure ML datastore and register as a Dataset
# Note: You cannot upload to Azure Open Datasets itself (it is read-only).
#       This uploads your processed data to your own Azure ML workspace.
# ---------------------------
def get_workspace():
    """
    Tries to load an Azure ML Workspace from:
      1) config.json in the working directory or parents, or
      2) environment variables:
         AZ_SUBSCRIPTION_ID, AZ_RESOURCE_GROUP, AZ_ML_WORKSPACE
    """
    try:
        return Workspace.from_config()
    except Exception:
        sub_id = os.environ.get("AZ_SUBSCRIPTION_ID", "<subscription-id>")
        rg = os.environ.get("AZ_RESOURCE_GROUP", "<resource-group>")
        ws_name = os.environ.get("AZ_ML_WORKSPACE", "<workspace-name>")
        if any(v.startswith("<") and v.endswith(">") for v in (sub_id, rg, ws_name)):
            raise RuntimeError(
                "Workspace config not found. Provide config.json or set "
                "AZ_SUBSCRIPTION_ID, AZ_RESOURCE_GROUP, AZ_ML_WORKSPACE env vars."
            )
        return Workspace(subscription_id=sub_id, resource_group=rg, workspace_name=ws_name)


def upload_and_register(ws: Workspace,
                        local_folder="data/opendatasets",
                        target_path="opendatasets-demo",
                        registered_name="opendatasets_sample"):
    datastore = ws.get_default_datastore()

    # Upload files to the datastore
    print(f"Uploading {local_folder} to datastore '{datastore.name}' at path '{target_path}'...")
    datastore.upload(src_dir=local_folder, target_path=target_path, overwrite=True, show_progress=True)

    # Create a Tabular Dataset from CSVs and register
    csv_glob = f"{target_path}/*.csv"
    print(f"Registering dataset from {csv_glob} ...")
    ds = Dataset.Tabular.from_delimited_files(path=(datastore, csv_glob))
    registered = ds.register(workspace=ws,
                             name=registered_name,
                             description="Data derived from Azure Open Datasets",
                             create_new_version=True)
    print(f"Registered dataset: {registered.name} v{registered.version}")
    return registered


if __name__ == "__main__":
    # 1) Search examples
    queries = ["taxi", "holiday", "weather"]
    for q in queries:
        print(f"\nSearch results for '{q}':")
        for hit in search_open_datasets(q):
            print(f" - {hit['class']}: {hit['summary']}")

    # 2) Download and manipulate data
    download_open_dataset_example()

    # 3) Upload to your Azure ML workspace and register
    #    Ensure you have a config.json or env vars set before running.
    try:
        ws = get_workspace()
        upload_and_register(ws,
                            local_folder="data/opendatasets",
                            target_path="opendatasets-demo",
                            registered_name="open-datasets-demo")
    except Exception as e:
        print("\nSkipping upload/register step (no workspace configured or other error):")
        print(str(e), file=sys.stderr)
------------------------------------------------------------

PROMPT 242:
"Generate Python code that uses Jinja templating to generate documentation for a package, including a template for the package's README file and a template for documenting individual modules and functions."
RESPONSE:
```python
#!/usr/bin/env python3
"""
Generate documentation for a Python package using Jinja2 templates.

This script:
- Introspects a package and its submodules
- Extracts docstrings, signatures, annotations for functions (and classes, optional)
- Renders a README for the package and per-module documentation files using Jinja templates

Usage:
    python generate_docs.py your_package_name --out docs

Requires:
    pip install jinja2
"""

from __future__ import annotations

import argparse
import importlib
import inspect
import os
import pkgutil
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from jinja2 import Environment, BaseLoader, select_autoescape


# ---------------------------
# Templates
# ---------------------------

README_TEMPLATE = """\
<!-- Auto-generated by generate_docs.py. Do not edit directly. -->

# {{ pkg.name }}

{% if pkg.version -%}
Version: {{ pkg.version }}
{% endif -%}

{% if pkg.description -%}
{{ pkg.description }}
{% else -%}
No package-level docstring found.
{% endif %}

## Installation

```bash
pip install {{ pkg.name }}
```

## Contents

- Modules:
{% for m in modules -%}
  - [{{ m.name }}](modules/{{ m.file_slug }}.md)
{% endfor %}

## Contributing

Contributions are welcome. Please open an issue or submit a pull request.

## License

Specify your license here.

"""

MODULE_TEMPLATE = """\
<!-- Auto-generated by generate_docs.py for module {{ module.name }}. Do not edit directly. -->

# Module: {{ module.name }}

{% if module.doc -%}
{{ module.doc }}
{% else -%}
No module-level docstring found.
{% endif %}

{% if module.functions %}
## Functions

{% for f in module.functions %}
### {{ f.name }}{{ f.signature }}

{% if f.summary -%}
{{ f.summary }}
{% endif %}

{% if f.parameters|length > 0 %}
Parameters:
{% for p in f.parameters -%}
- {{ p.name }}{% if p.kind %} ({{ p.kind }}){% endif %}{% if p.annotation %}: {{ p.annotation }}{% endif %}{% if p.default is not none %} = {{ p.default }}{% endif %}
{% endfor %}
{% endif %}

{% if f.returns or f.return_annotation %}
Returns:
- {% if f.return_annotation %}{{ f.return_annotation }}{% else %}None{% endif %}
{% endif %}

{% if f.doc -%}
Details:
{{ f.doc }}
{% endif %}

---
{% endfor %}
{% endif %}

{% if module.classes %}
## Classes

{% for c in module.classes %}
### class {{ c.name }}{% if c.bases %}({{ c.bases|join(', ') }}){% endif %}

{% if c.summary -%}
{{ c.summary }}
{% endif %}

{% if c.doc -%}
{{ c.doc }}
{% endif %}

{% if c.methods %}
#### Methods
{% for m in c.methods %}
- {{ m.kind }} {{ m.name }}{{ m.signature }}
  {% if m.summary -%}
  {{ m.summary }}
  {% endif %}
{% endfor %}
{% endif %}

---
{% endfor %}
{% endif %}
"""


# ---------------------------
# Introspection models
# ---------------------------

@dataclass
class ParameterInfo:
    name: str
    annotation: Optional[str]
    default: Optional[str]
    kind: Optional[str]


@dataclass
class FunctionInfo:
    name: str
    qualname: str
    signature: str
    doc: str
    summary: str
    parameters: List[ParameterInfo] = field(default_factory=list)
    returns: Optional[str] = None
    return_annotation: Optional[str] = None


@dataclass
class ClassMethodInfo:
    name: str
    signature: str
    kind: str  # "method", "classmethod", "staticmethod"
    doc: str
    summary: str


@dataclass
class ClassInfo:
    name: str
    qualname: str
    doc: str
    summary: str
    bases: List[str]
    methods: List[ClassMethodInfo] = field(default_factory=list)


@dataclass
class ModuleInfo:
    name: str
    doc: str
    functions: List[FunctionInfo] = field(default_factory=list)
    classes: List[ClassInfo] = field(default_factory=list)
    file_slug: str = ""  # used for filenames/links


@dataclass
class PackageInfo:
    name: str
    version: Optional[str]
    description: str


# ---------------------------
# Helpers
# ---------------------------

def first_line(text: Optional[str]) -> str:
    if not text:
        return ""
    for line in text.strip().splitlines():
        if line.strip():
            return line.strip()
    return ""


def annotation_to_str(a: Any) -> Optional[str]:
    if a is inspect._empty:
        return None
    try:
        # Try to produce a nicer name for common cases
        if hasattr(a, "__name__"):
            return a.__name__
        return str(a).replace("typing.", "")
    except Exception:
        return str(a)


def default_to_str(d: Any) -> Optional[str]:
    if d is inspect._empty:
        return None
    try:
        return repr(d)
    except Exception:
        try:
            return str(d)
        except Exception:
            return "<unreprable>"


def is_public(name: str) -> bool:
    return not name.startswith("_")


def safe_signature(obj) -> str:
    try:
        return str(inspect.signature(obj))
    except Exception:
        return "(...)"


def get_base_names(cls: type) -> List[str]:
    bases = []
    for b in getattr(cls, "__bases__", ()):
        try:
            bases.append(b.__name__)
        except Exception:
            bases.append(str(b))
    return bases


def collect_parameters(sig: inspect.Signature) -> List[ParameterInfo]:
    items: List[ParameterInfo] = []
    for p in sig.parameters.values():
        items.append(
            ParameterInfo(
                name=p.name,
                annotation=annotation_to_str(p.annotation),
                default=default_to_str(p.default),
                kind=str(p.kind).replace("Parameter.", ""),
            )
        )
    return items


def function_info(func) -> FunctionInfo:
    sig_str = safe_signature(func)
    doc = inspect.getdoc(func) or ""
    summary = first_line(doc)

    # Return annotation
    try:
        sig = inspect.signature(func)
        return_ann = annotation_to_str(sig.return_annotation)
        params = collect_parameters(sig)
    except Exception:
        return_ann = None
        params = []

    return FunctionInfo(
        name=func.__name__,
        qualname=getattr(func, "__qualname__", func.__name__),
        signature=sig_str,
        doc=doc,
        summary=summary,
        parameters=params,
        return_annotation=return_ann,
    )


def class_method_kind(owner: type, name: str) -> str:
    obj = owner.__dict__.get(name, None)
    if isinstance(obj, staticmethod):
        return "staticmethod"
    if isinstance(obj, classmethod):
        return "classmethod"
    return "method"


def class_method_info(owner: type, func) -> ClassMethodInfo:
    sig_str = safe_signature(func)
    doc = inspect.getdoc(func) or ""
    summary = first_line(doc)
    kind = class_method_kind(owner, func.__name__)
    return ClassMethodInfo(
        name=func.__name__,
        signature=sig_str,
        kind=kind,
        doc=doc,
        summary=summary,
    )


def class_info(cls) -> ClassInfo:
    doc = inspect.getdoc(cls) or ""
    summary = first_line(doc)

    methods: List[ClassMethodInfo] = []
    # Only methods defined on this class (not inherited)
    for name, obj in cls.__dict__.items():
        if not is_public(name):
            continue
        # Unwrap descriptors to get the underlying function for signature
        func = None
        if isinstance(obj, (staticmethod, classmethod)):
            func = obj.__func__
        elif inspect.isfunction(obj):
            func = obj
        if func is None:
            continue
        methods.append(class_method_info(cls, func))

    return ClassInfo(
        name=cls.__name__,
        qualname=getattr(cls, "__qualname__", cls.__name__),
        doc=doc,
        summary=summary,
        bases=get_base_names(cls),
        methods=sorted(methods, key=lambda m: ({"staticmethod": 0, "classmethod": 1, "method": 2}.get(m.kind, 3), m.name)),
    )


def module_info(module) -> ModuleInfo:
    mname = module.__name__
    mdoc = inspect.getdoc(module) or ""

    # Functions defined in this module
    functions: List[FunctionInfo] = []
    for name, obj in inspect.getmembers(module, inspect.isfunction):
        if not is_public(name):
            continue
        if getattr(obj, "__module__", "") != mname:
            continue
        functions.append(function_info(obj))

    # Classes defined in this module
    classes: List[ClassInfo] = []
    for name, obj in inspect.getmembers(module, inspect.isclass):
        if not is_public(name):
            continue
        if getattr(obj, "__module__", "") != mname:
            continue
        classes.append(class_info(obj))

    file_slug = mname.replace(".", "_")
    return ModuleInfo(
        name=mname,
        doc=mdoc,
        functions=sorted(functions, key=lambda f: f.name),
        classes=sorted(classes, key=lambda c: c.name),
        file_slug=file_slug,
    )


def discover_modules(pkg) -> List[str]:
    """Return a list of fully-qualified module names under the given package (including the package itself)."""
    names = {pkg.__name__}
    if hasattr(pkg, "__path__"):
        for modinfo in pkgutil.walk_packages(pkg.__path__, prefix=pkg.__name__ + "."):
            names.add(modinfo.name)
    return sorted(names)


def package_info(pkg) -> PackageInfo:
    name = pkg.__name__
    version = getattr(pkg, "__version__", None)
    description = inspect.getdoc(pkg) or ""
    return PackageInfo(name=name, version=version, description=description)


# ---------------------------
# Rendering
# ---------------------------

def make_env() -> Environment:
    env = Environment(
        loader=BaseLoader(),
        autoescape=select_autoescape(enabled_extensions=("html", "xml")),
        trim_blocks=True,
        lstrip_blocks=True,
    )
    return env


def render_templates(pkg_name: str, out_dir: Path) -> None:
    try:
        pkg = importlib.import_module(pkg_name)
    except Exception as e:
        print(f"Failed to import package '{pkg_name}': {e}", file=sys.stderr)
        sys.exit(1)

    out_dir.mkdir(parents=True, exist_ok=True)
    modules_dir = out_dir / "modules"
    modules_dir.mkdir(parents=True, exist_ok=True)

    env = make_env()
    readme_tpl = env.from_string(README_TEMPLATE)
    module_tpl = env.from_string(MODULE_TEMPLATE)

    mod_names = discover_modules(pkg)
    module_infos: List[ModuleInfo] = []

    for mname in mod_names:
        try:
            mod = importlib.import_module(mname)
        except Exception as e:
            print(f"Warning: could not import module {mname}: {e}", file=sys.stderr)
            continue
        mi = module_info(mod)
        module_infos.append(mi)

        # Write per-module docs
        mod_path = modules_dir / f"{mi.file_slug}.md"
        with mod_path.open("w", encoding="utf-8") as f:
            f.write(module_tpl.render(module=mi))

    # Write README
    pkg_info = package_info(pkg)
    readme_path = out_dir / "README.md"
    with readme_path.open("w", encoding="utf-8") as f:
        f.write(readme_tpl.render(pkg=pkg_info, modules=module_infos))

    print(f"Generated {readme_path}")
    print(f"Generated {len(module_infos)} module files under {modules_dir}")


# ---------------------------
# CLI
# ---------------------------

def main():
    parser = argparse.ArgumentParser(description="Generate docs for a Python package using Jinja templates.")
    parser.add_argument("package", help="Import path of the package (e.g., mypkg)")
    parser.add_argument("--out", "-o", type=Path, default=Path("docs"), help="Output directory (default: ./docs)")
    args = parser.parse_args()

    render_templates(args.package, args.out)


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 243:
"Generate Python code that imports the necessary libraries and creates a wrapper function for the PubSub framework on macOS, allowing for easy publication and subscription to messages within Python scripts."
RESPONSE:
# Requires: macOS + PyObjC (pip install pyobjc)
import sys
import threading

if sys.platform != "darwin":
    raise OSError("This Pub/Sub wrapper requires macOS (Darwin).")

from Foundation import (
    NSObject,
    NSDistributedNotificationCenter,
    NSRunLoop,
    NSDate,
    NSDefaultRunLoopMode,
    NSNotificationSuspensionBehaviorDeliverImmediately,
    NSDistributedNotificationPostToAllSessions,
    NSDistributedNotificationDeliverImmediately,
)
import objc


class _Observer(NSObject):
    def initWithCallback_(self, callback):
        self = objc.super(_Observer, self).init()
        if self is None:
            return None
        self._callback = callback
        return self

    # Selector name "handleNotification:" maps to Python method handleNotification_
    def handleNotification_(self, notification):
        try:
            name = notification.name()
            user_info = notification.userInfo()  # May be None or an NSDictionary (acts like a dict)
            sender = notification.object()
            # Convert NSDictionary to a Python dict if present
            payload = dict(user_info) if user_info is not None else None
            self._callback(name, payload, sender)
        except Exception:
            # Avoid letting exceptions bubble into ObjC land
            import traceback

            traceback.print_exc()


class MacPubSub:
    """
    Simple wrapper around NSDistributedNotificationCenter for publish/subscribe messaging
    across processes on macOS.

    - publish(topic, data=None, sender=None, to_all_sessions=True, deliver_immediately=True)
    - subscribe(topic, callback, sender=None, deliver_immediately=True) -> token
    - unsubscribe(token)
    - run()  # runs the CFRunLoop to receive notifications (blocking)
    - start() / stop()  # run loop control in a background thread
    """

    def __init__(self):
        self.center = NSDistributedNotificationCenter.defaultCenter()
        self._observers = {}  # token -> (observer, topic, sender)
        self._runloop_thread = None
        self._running = threading.Event()

    def publish(
        self,
        topic,
        data=None,
        sender=None,
        to_all_sessions=True,
        deliver_immediately=True,
    ):
        """
        Publish a distributed notification.

        topic: str
        data: dict-like (keys/values should be Property List compatible types)
        sender: str or None
        """
        options = 0
        if to_all_sessions:
            options |= int(NSDistributedNotificationPostToAllSessions)
        if deliver_immediately:
            options |= int(NSDistributedNotificationDeliverImmediately)

        # PyObjC bridges Python dict to NSDictionary automatically if types are bridgable
        self.center.postNotificationName_object_userInfo_options_(topic, sender, data, options)

    def subscribe(self, topic, callback, sender=None, deliver_immediately=True):
        """
        Subscribe to a topic. Returns a token used for unsubscribe.

        callback: callable(name, payload, sender)
        sender: str or None to filter by object
        """
        observer = _Observer.alloc().initWithCallback_(callback)

        # Prefer delivering immediately to avoid suspension
        behavior = NSNotificationSuspensionBehaviorDeliverImmediately if deliver_immediately else NSNotificationSuspensionBehaviorDeliverImmediately

        if hasattr(self.center, "addObserver_selector_name_object_suspensionBehavior_"):
            self.center.addObserver_selector_name_object_suspensionBehavior_(
                observer, "handleNotification:", topic, sender, behavior
            )
        else:
            # Fallback (older macOS); may suspend when app inactive
            self.center.addObserver_selector_name_object_(observer, "handleNotification:", topic, sender)

        token = id(observer)
        self._observers[token] = (observer, topic, sender)
        return token

    def unsubscribe(self, token):
        entry = self._observers.pop(token, None)
        if entry is None:
            return
        observer, topic, sender = entry
        if hasattr(self.center, "removeObserver_name_object_"):
            self.center.removeObserver_name_object_(observer, topic, sender)
        else:
            self.center.removeObserver_(observer)

    def run(self):
        """
        Run the NSRunLoop in the current thread (blocking).
        Call this if your script needs to wait for messages.
        """
        rl = NSRunLoop.currentRunLoop()
        while True:
            # Spin the run loop in small increments to keep it responsive
            rl.runMode_beforeDate_(NSDefaultRunLoopMode, NSDate.dateWithTimeIntervalSinceNow_(0.1))

    def start(self):
        """
        Start a background runloop thread to receive notifications without blocking.
        """
        if self._runloop_thread and self._runloop_thread.is_alive():
            return

        self._running.set()

        def _loop():
            rl = NSRunLoop.currentRunLoop()
            while self._running.is_set():
                rl.runMode_beforeDate_(NSDefaultRunLoopMode, NSDate.dateWithTimeIntervalSinceNow_(0.1))

        t = threading.Thread(target=_loop, name="MacPubSubRunLoop", daemon=True)
        t.start()
        self._runloop_thread = t

    def stop(self):
        """
        Stop the background runloop thread if started with start().
        """
        self._running.clear()
        t = self._runloop_thread
        if t:
            t.join(timeout=1.0)
        self._runloop_thread = None


# Convenience module-level singleton and wrapper functions
_pubsub = MacPubSub()


def publish(topic, data=None, sender=None, to_all_sessions=True, deliver_immediately=True):
    return _pubsub.publish(topic, data=data, sender=sender, to_all_sessions=to_all_sessions, deliver_immediately=deliver_immediately)


def subscribe(topic, callback, sender=None, deliver_immediately=True):
    return _pubsub.subscribe(topic, callback=callback, sender=sender, deliver_immediately=deliver_immediately)


def unsubscribe(token):
    return _pubsub.unsubscribe(token)


def run():
    return _pubsub.run()


def start():
    return _pubsub.start()


def stop():
    return _pubsub.stop()


# Example usage (uncomment to test):
# def on_msg(name, payload, sender):
#     print("Received:", name, payload, sender)
#
# if __name__ == "__main__":
#     start()  # or run()
#     tok = subscribe("com.example.topic", on_msg)
#     publish("com.example.topic", {"hello": "world"})
#     import time
#     time.sleep(1)
#     unsubscribe(tok)
#     stop()
------------------------------------------------------------

PROMPT 244:
"Generate Python code that uses the `sqlalchemy` library to create a profiler for an application that uses SQLAlchemy, including functions to get information about database queries and their execution time."
RESPONSE:
# sqlalchemy_profiler.py

from __future__ import annotations

import threading
import time
from collections import Counter, defaultdict, deque
from dataclasses import dataclass, field
from typing import Any, Deque, Dict, Iterable, List, Optional, Tuple, Union

from sqlalchemy import event
from sqlalchemy.engine import Connection, Engine


@dataclass
class QueryRecord:
    id: int
    started_at: float
    ended_at: float
    duration_ms: float
    statement: str
    parameters: Any
    rendered_sql: Optional[str]
    rowcount: Optional[int]
    success: bool
    error: Optional[str]
    connection_id: int
    thread_id: int
    executemany: bool
    dialect: Optional[str] = None
    extra: Dict[str, Any] = field(default_factory=dict)


def _now() -> float:
    return time.perf_counter()


def _truncate(value: Any, max_len: int = 500) -> Any:
    try:
        s = str(value)
    except Exception:
        return value
    if len(s) <= max_len:
        return value
    return s[: max_len - 3] + "..."


def _redact_params(
    params: Any,
    redact_keys: Iterable[str] = (),
    max_val_len: int = 500,
) -> Any:
    if params is None:
        return None

    redact_set = {k.lower() for k in redact_keys}

    def redact_value(v: Any) -> Any:
        if isinstance(v, (bytes, bytearray)):
            return f"<{type(v).__name__} {len(v)} bytes>"
        return _truncate(v, max_val_len)

    def redact_mapping(m: Dict[str, Any]) -> Dict[str, Any]:
        out = {}
        for k, v in m.items():
            if str(k).lower() in redact_set:
                out[k] = "<redacted>"
            else:
                out[k] = redact_value(v)
        return out

    if isinstance(params, dict):
        return redact_mapping(params)

    if isinstance(params, (list, tuple)):
        # Could be executemany: list[dict] or list[tuple]
        if params and isinstance(params[0], dict):
            return [redact_mapping(p) for p in params]  # type: ignore
        else:
            return [redact_value(p) for p in params]  # type: ignore

    return redact_value(params)


def _render_sql(statement: str, parameters: Any, max_sql_len: int = 4000) -> str:
    """
    Best-effort SQL rendering with parameters inlined for display.
    Do not use for execution; this is only for logging/profiling purposes.
    """
    try:
        if parameters is None:
            rendered = statement
        elif isinstance(parameters, dict):
            # Named parameters
            rendered = statement
            for k, v in parameters.items():
                rendered = rendered.replace(
                    f":{k}",
                    _sql_literal(v),
                )
        elif isinstance(parameters, (list, tuple)) and parameters and isinstance(parameters[0], dict):
            # executemany with dicts; show the first one, count included separately
            first = parameters[0]
            rendered = statement
            for k, v in first.items():
                rendered = rendered.replace(
                    f":{k}",
                    _sql_literal(v),
                )
            rendered += f" -- executemany x{len(parameters)}"
        elif isinstance(parameters, (list, tuple)):
            # Positional parameters
            rendered = _render_positional(statement, parameters)
        else:
            rendered = statement
    except Exception:
        rendered = statement
    if len(rendered) > max_sql_len:
        return rendered[: max_sql_len - 3] + "..."
    return rendered


def _sql_literal(v: Any) -> str:
    if v is None:
        return "NULL"
    if isinstance(v, bool):
        return "1" if v else "0"
    if isinstance(v, (int, float)):
        return str(v)
    # naive escaping for logging
    s = str(v).replace("'", "''")
    return f"'{s}'"


def _render_positional(statement: str, params: Union[List[Any], Tuple[Any, ...]]) -> str:
    # Replace DBAPI positional placeholders in the most common forms
    # This is a heuristic; vendor differences apply.
    # We try a simple replacement for "?" and "%s".
    out = statement
    if "?" in statement:
        for v in params:
            out = out.replace("?", _sql_literal(v), 1)
        return out
    if "%s" in statement:
        for v in params:
            out = out.replace("%s", _sql_literal(v), 1)
        return out
    return statement


class SQLAlchemyProfiler:
    def __init__(
        self,
        engine: Engine,
        max_records: Optional[int] = 1000,
        redact_param_keys: Iterable[str] = ("password", "token", "secret", "api_key"),
        max_param_value_length: int = 500,
        render_sql: bool = True,
        slow_query_threshold_ms: Optional[float] = None,
    ):
        self.engine = engine
        self.max_records = max_records
        self.redact_param_keys = tuple(redact_param_keys)
        self.max_param_value_length = max_param_value_length
        self.render_sql = render_sql
        self.slow_query_threshold_ms = slow_query_threshold_ms

        self._lock = threading.RLock()
        self._enabled = False
        self._next_id = 1
        self._records: Deque[QueryRecord] = deque(maxlen=max_records or 0)
        self._total_time_ms: float = 0.0
        self._total_queries: int = 0
        self._slow_queries: int = 0

        # For removal later
        self._listeners: List[Tuple[Any, str, Any]] = []

    def start(self) -> "SQLAlchemyProfiler":
        with self._lock:
            if self._enabled:
                return self
            self._enabled = True

            event.listen(self.engine, "before_cursor_execute", self._before_cursor_execute)
            self._listeners.append((self.engine, "before_cursor_execute", self._before_cursor_execute))

            event.listen(self.engine, "after_cursor_execute", self._after_cursor_execute)
            self._listeners.append((self.engine, "after_cursor_execute", self._after_cursor_execute))

            event.listen(self.engine, "handle_error", self._handle_error)
            self._listeners.append((self.engine, "handle_error", self._handle_error))

        return self

    def stop(self) -> None:
        with self._lock:
            if not self._enabled:
                return
            for target, ev, fn in self._listeners:
                try:
                    event.remove(target, ev, fn)
                except Exception:
                    pass
            self._listeners.clear()
            self._enabled = False

    def reset(self) -> None:
        with self._lock:
            self._records.clear()
            self._total_time_ms = 0.0
            self._total_queries = 0
            self._slow_queries = 0
            self._next_id = 1

    def records(self) -> List[QueryRecord]:
        with self._lock:
            return list(self._records)

    def last(self, n: int = 10) -> List[QueryRecord]:
        with self._lock:
            return list(self._records)[-n:]

    def summary(self) -> Dict[str, Any]:
        with self._lock:
            avg = (self._total_time_ms / self._total_queries) if self._total_queries else 0.0
            return {
                "engine": str(self.engine.url),
                "total_queries": self._total_queries,
                "total_time_ms": round(self._total_time_ms, 3),
                "avg_time_ms": round(avg, 3),
                "slow_query_threshold_ms": self.slow_query_threshold_ms,
                "slow_queries": self._slow_queries,
            }

    def slowest(self, n: int = 5) -> List[QueryRecord]:
        with self._lock:
            return sorted(self._records, key=lambda r: r.duration_ms, reverse=True)[:n]

    def by_statement(self) -> List[Dict[str, Any]]:
        """
        Aggregate stats grouped by the SQL statement string (without rendered params).
        """
        with self._lock:
            agg: Dict[str, Dict[str, Any]] = defaultdict(lambda: {
                "count": 0,
                "total_ms": 0.0,
                "min_ms": float("inf"),
                "max_ms": 0.0,
                "samples": 0,
            })
            for r in self._records:
                a = agg[r.statement]
                a["count"] += 1
                a["total_ms"] += r.duration_ms
                a["min_ms"] = min(a["min_ms"], r.duration_ms)
                a["max_ms"] = max(a["max_ms"], r.duration_ms)
                if a["samples"] < 3:
                    a.setdefault("examples", []).append(r.rendered_sql or r.statement)
                    a["samples"] += 1
            out = []
            for stmt, a in agg.items():
                avg = a["total_ms"] / a["count"] if a["count"] else 0.0
                out.append({
                    "statement": stmt,
                    "count": a["count"],
                    "total_ms": round(a["total_ms"], 3),
                    "avg_ms": round(avg, 3),
                    "min_ms": round(a["min_ms"], 3) if a["min_ms"] != float("inf") else 0.0,
                    "max_ms": round(a["max_ms"], 3),
                    "examples": a.get("examples", []),
                })
            out.sort(key=lambda x: x["total_ms"], reverse=True)
            return out

    def as_context(self):
        class _Ctx:
            def __init__(self, profiler: "SQLAlchemyProfiler"):
                self.profiler = profiler

            def __enter__(self):
                self.profiler.start()
                return self.profiler

            def __exit__(self, exc_type, exc, tb):
                self.profiler.stop()
                return False

        return _Ctx(self)

    # SQLAlchemy event handlers

    def _before_cursor_execute(
        self,
        conn: Connection,
        cursor,
        statement: str,
        parameters: Any,
        context,
        executemany: bool,
    ):
        # Mark start time on context
        context._query_start_time = _now()  # type: ignore[attr-defined]

    def _after_cursor_execute(
        self,
        conn: Connection,
        cursor,
        statement: str,
        parameters: Any,
        context,
        executemany: bool,
    ):
        started_at = getattr(context, "_query_start_time", None)
        if started_at is None:
            started_at = _now()
        ended_at = _now()
        duration_ms = (ended_at - started_at) * 1000.0

        safe_params = _redact_params(parameters, self.redact_param_keys, self.max_param_value_length)

        rendered = _render_sql(statement, safe_params) if self.render_sql else None

        record = QueryRecord(
            id=self._next_query_id(),
            started_at=started_at,
            ended_at=ended_at,
            duration_ms=duration_ms,
            statement=statement,
            parameters=safe_params,
            rendered_sql=rendered,
            rowcount=getattr(cursor, "rowcount", None),
            success=True,
            error=None,
            connection_id=id(conn),
            thread_id=threading.get_ident(),
            executemany=executemany,
            dialect=getattr(conn, "dialect", None).name if getattr(conn, "dialect", None) else None,
        )

        self._add_record(record)

    def _handle_error(self, exception_context):
        # SQLAlchemy passes an ExceptionContext object
        # See: https://docs.sqlalchemy.org/en/20/core/events.html#sqlalchemy.events.ConnectionEvents.handle_error
        conn = getattr(exception_context, "connection", None)
        statement = getattr(exception_context, "statement", None)
        parameters = getattr(exception_context, "parameters", None)
        context = getattr(exception_context, "execution_context", None)

        started_at = getattr(context, "_query_start_time", None) if context is not None else None
        if started_at is None:
            started_at = _now()
        ended_at = _now()
        duration_ms = (ended_at - started_at) * 1000.0

        safe_params = _redact_params(parameters, self.redact_param_keys, self.max_param_value_length)
        rendered = _render_sql(statement or "", safe_params) if (self.render_sql and statement) else None

        record = QueryRecord(
            id=self._next_query_id(),
            started_at=started_at,
            ended_at=ended_at,
            duration_ms=duration_ms,
            statement=statement or "<unknown>",
            parameters=safe_params,
            rendered_sql=rendered,
            rowcount=None,
            success=False,
            error=str(getattr(exception_context, "original_exception", "Error")),
            connection_id=id(conn) if conn is not None else -1,
            thread_id=threading.get_ident(),
            executemany=False,
            dialect=getattr(conn, "dialect", None).name if getattr(conn, "dialect", None) else None,
        )
        self._add_record(record)

    # Internal helpers

    def _next_query_id(self) -> int:
        with self._lock:
            nid = self._next_id
            self._next_id += 1
            return nid

    def _add_record(self, record: QueryRecord) -> None:
        with self._lock:
            if self.max_records and len(self._records) >= self.max_records:
                # deque(maxlen=...) will auto-pop on append; keep counters consistent by subtracting dropped
                pass
            self._records.append(record)
            self._total_queries += 1
            self._total_time_ms += record.duration_ms
            if self.slow_query_threshold_ms is not None and record.duration_ms >= self.slow_query_threshold_ms:
                self._slow_queries += 1


# Example usage
if __name__ == "__main__":
    from sqlalchemy import Column, Integer, String, create_engine
    from sqlalchemy.orm import declarative_base, Session

    Base = declarative_base()

    class User(Base):
        __tablename__ = "users"
        id = Column(Integer, primary_key=True)
        name = Column(String)

    engine = create_engine("sqlite:///:memory:", echo=False, future=True)
    Base.metadata.create_all(engine)

    profiler = SQLAlchemyProfiler(engine, max_records=1000, slow_query_threshold_ms=5.0).start()

    with Session(engine) as s:
        s.add_all([User(name="a"), User(name="b"), User(name="c")])
        s.commit()

        for _ in range(3):
            s.execute("SELECT 1")
        s.execute("SELECT name FROM users WHERE name = :n", {"n": "a"})
        s.execute("SELECT name FROM users WHERE name = :n", {"n": "b"})
        s.query(User).filter(User.name == "a").all()  # ORM path

    profiler.stop()

    print("Summary:", profiler.summary())
    print("\nSlowest:")
    for r in profiler.slowest(3):
        print(f"- {r.duration_ms:.3f} ms | {r.rendered_sql or r.statement}")

    print("\nBy statement:")
    for row in profiler.by_statement()[:5]:
        print(row)

    print("\nLast 5 queries:")
    for r in profiler.last(5):
        print(f"{r.id}: {r.duration_ms:.3f} ms | rows={r.rowcount} | ok={r.success} | {r.rendered_sql or r.statement}")
------------------------------------------------------------

PROMPT 245:
"Generate Python code that imports the necessary modules and creates wrappers for the PHASE framework on macOS, providing a simple and convenient interface for interacting with the framework's functionality."
RESPONSE:
# phase_wrapper.py
# Lightweight Python wrappers for Apple's PHASE framework on macOS using PyObjC.
# Provides a simple, convenient interface with robust feature detection so it
# doesn't break across macOS SDK variations.

import sys
import os

# Platform check
if sys.platform != "darwin":
    raise RuntimeError("PHASE wrappers are only available on macOS.")

# PyObjC requirement
try:
    import objc
    from Foundation import NSBundle, NSURL
except Exception as exc:
    raise RuntimeError(
        "PyObjC is required to use PHASE from Python. "
        "Install with: pip install pyobjc"
    ) from exc


class PhaseError(RuntimeError):
    pass


def _path_for_framework(name):
    # Try modern pathForFramework first
    try:
        return objc.pathForFramework(f"/System/Library/Frameworks/{name}.framework")
    except Exception:
        pass
    # Fallback: consult NSBundle
    bundle = NSBundle.bundleWithPath_(f"/System/Library/Frameworks/{name}.framework")
    if bundle:
        return bundle.bundlePath()
    return None


def _load_phase():
    global _PHASE_LOADED
    if globals().get("_PHASE_LOADED", False):
        return

    bundle_path = _path_for_framework("PHASE")
    if not bundle_path:
        raise PhaseError("PHASE.framework not found on this system. Requires macOS 12+.")
    try:
        objc.loadBundle("PHASE", globals(), bundle_path=bundle_path)
        _PHASE_LOADED = True
    except Exception as exc:
        raise PhaseError(f"Failed to load PHASE.framework: {exc}") from exc


def _cls(name):
    try:
        return objc.lookUpClass(name)
    except Exception:
        raise PhaseError(f"Objective-C class '{name}' not found. Is PHASE available?")


def _has(obj, selector_pyname):
    return hasattr(obj, selector_pyname)


def _url_for_path(path):
    path = os.path.expanduser(path)
    return NSURL.fileURLWithPath_(os.path.abspath(path))


def _first_existing_attr(obj, candidates):
    for name in candidates:
        if hasattr(obj, name):
            return name
    return None


def _find_enum_or_default(candidates, default=None):
    # Try to locate a PHASE enum constant that PyObjC exported into globals()
    for name in candidates:
        if name in globals():
            return globals()[name]
    return default


class PHASE:
    """
    Minimal dynamic helpers to work with the PHASE framework.
    """
    @staticmethod
    def ensure_loaded():
        _load_phase()

    @staticmethod
    def cls(name):
        _load_phase()
        return _cls(name)

    @staticmethod
    def new(name, init_selector="init", *args):
        _load_phase()
        c = _cls(name)
        obj = c.alloc()
        init_pyname = init_selector.replace(":", "_") + ("_" if not init_selector.endswith(":") else "")
        if not hasattr(obj, init_pyname):
            raise PhaseError(f"{name} does not respond to initializer '{init_selector}' (as '{init_pyname}')")
        return getattr(obj, init_pyname)(*args)

    @staticmethod
    def call(obj, selector, *args):
        pyname = selector.replace(":", "_")
        if not pyname.endswith("_") and selector.endswith(":"):
            pyname += "_"
        if not hasattr(obj, pyname):
            raise PhaseError(f"Object of type {type(obj)} does not respond to selector '{selector}' (as '{pyname}')")
        return getattr(obj, pyname)(*args)

    @staticmethod
    def file_url(path):
        return _url_for_path(path)


class SimplePhaseEngine:
    """
    A convenience wrapper around PHASEEngine that:
    - Loads PHASE.framework
    - Creates and starts the engine
    - Optionally creates and sets a default listener
    - Provides helpers to register audio assets
    Note: PHASE is rich; this wrapper focuses on bootstrapping tasks with
    runtime feature detection to avoid tight coupling to SDK details.
    """

    def __init__(self, update_mode="automatic", start=True, set_default_listener=True):
        PHASE.ensure_loaded()

        Engine = PHASE.cls("PHASEEngine")
        engine = Engine.alloc()

        # Try common initializers in descending preference
        self._engine = None
        errors = []

        # Map human-friendly update_mode to an enum if possible
        mode_enum = None
        if isinstance(update_mode, str):
            update_mode = update_mode.lower()
            if update_mode in ("automatic", "auto"):
                mode_enum = _find_enum_or_default(
                    [
                        "PHASEEngineUpdateModeAutomatic",
                        "PHASEUpdateModeAutomatic",
                    ],
                    default=None,
                )
            elif update_mode in ("manual", "manually"):
                mode_enum = _find_enum_or_default(
                    [
                        "PHASEEngineUpdateModeManual",
                        "PHASEUpdateModeManual",
                    ],
                    default=None,
                )
            else:
                mode_enum = None
        else:
            mode_enum = update_mode  # assume raw enum value

        # 1) initWithUpdateMode:error:
        if _has(engine, "initWithUpdateMode_error_") and mode_enum is not None:
            try:
                obj, err = engine.initWithUpdateMode_error_(mode_enum, None)
                if obj is not None:
                    self._engine = obj
                elif err is not None:
                    errors.append(f"initWithUpdateMode:error: {err}")
            except Exception as exc:
                errors.append(f"initWithUpdateMode:error: {exc}")

        # 2) initWithUpdateMode:
        if self._engine is None and _has(engine, "initWithUpdateMode_") and mode_enum is not None:
            try:
                obj = engine.initWithUpdateMode_(mode_enum)
                if obj is not None:
                    self._engine = obj
            except Exception as exc:
                errors.append(f"initWithUpdateMode: {exc}")

        # 3) plain init
        if self._engine is None and _has(engine, "init"):
            try:
                obj = engine.init()
                if obj is not None:
                    self._engine = obj
            except Exception as exc:
                errors.append(f"init: {exc}")

        if self._engine is None:
            raise PhaseError("Failed to initialize PHASEEngine. Tried initializers: " + "; ".join(errors))

        if start:
            self.start()

        if set_default_listener:
            try:
                self.ensure_default_listener()
            except Exception:
                # Listener is helpful but non-fatal
                pass

    @property
    def objc(self):
        return self._engine

    def start(self):
        # Try startAndReturnError:
        if _has(self._engine, "startAndReturnError_"):
            ok, err = self._engine.startAndReturnError_(None)
            if not ok:
                raise PhaseError(f"PHASEEngine failed to start: {err}")
            return
        # Fallback: start
        if _has(self._engine, "start"):
            self._engine.start()
            return
        # Some SDKs auto-start; warn silently
        return

    def stop(self):
        if _has(self._engine, "stop"):
            self._engine.stop()

    def ensure_default_listener(self):
        """
        Create and set a default listener if one is not present.
        """
        # If engine has a defaultListener accessor, check it
        getter = _first_existing_attr(self._engine, ["defaultListener", "listener"])
        if getter:
            try:
                cur = getattr(self._engine, getter)()
                if cur is not None:
                    return cur
            except Exception:
                pass

        Listener = PHASE.cls("PHASEListener")
        candidate_inits = [
            "initWithEngine:",
            "init",  # Some SDK versions allow a bare init, then attaching
        ]

        listener = None
        for sel in candidate_inits:
            init_name = sel.replace(":", "_") + ("_" if sel.endswith(":") else "")
            if _has(Listener.alloc(), init_name):
                try:
                    if sel == "initWithEngine:":
                        listener = getattr(Listener.alloc(), init_name)(self._engine)
                    else:
                        listener = getattr(Listener.alloc(), init_name)()
                    if listener is not None:
                        break
                except Exception:
                    continue

        if listener is None:
            raise PhaseError("Failed to create PHASEListener with known initializers.")

        # Try to set as default listener
        setter = _first_existing_attr(self._engine, ["setDefaultListener_", "setListener_"])
        if setter:
            getattr(self._engine, setter)(listener)

        return listener

    def asset_registry(self):
        # Most SDKs expose 'assetRegistry'
        getter = _first_existing_attr(self._engine, ["assetRegistry"])
        if not getter:
            raise PhaseError("PHASEEngine does not expose 'assetRegistry' on this system.")
        return getattr(self._engine, getter)()

    def register_sound_asset(self, path, identifier=None, options=None):
        """
        Register a sound asset file with the engine's asset registry.
        Returns True/registered object depending on SDK, raises on error.

        path: str path to audio file
        identifier: str unique asset ID; default derives from filename
        options: optional PHASESoundAssetOptions or dict; commonly None
        """
        if identifier is None:
            identifier = os.path.splitext(os.path.basename(path))[0]

        url = _url_for_path(path)
        registry = self.asset_registry()

        # Try several known variants of the registration API
        candidate_methods = [
            # Common variants (ObjC selectors become these PyObjC names)
            ("registerSoundAssetAtURL_identifier_error_", (url, identifier, None)),
            ("registerSoundAssetAtURL_assetIdentifier_error_", (url, identifier, None)),
            ("registerSoundAssetAtURL_identifier_options_error_", (url, identifier, options, None)),
            ("registerSoundAssetAtURL_assetIdentifier_options_error_", (url, identifier, options, None)),
        ]

        last_exc = None
        for pyname, args in candidate_methods:
            if _has(registry, pyname):
                try:
                    result = getattr(registry, pyname)(*args)
                    # Could be bool, or an object, or a tuple
                    return result
                except Exception as exc:
                    last_exc = exc
                    continue

        raise PhaseError(
            "Could not find a compatible 'registerSoundAsset' method on PHASEAssetRegistry."
            + (f" Last error: {last_exc}" if last_exc else "")
        )


# Optional: small self-test / demo
if __name__ == "__main__":
    try:
        engine = SimplePhaseEngine(update_mode="automatic", start=True, set_default_listener=True)
        print("PHASEEngine initialized and started.")

        # Example: attempt to register an asset if you provide a path
        # Replace with a valid local audio file to test registration
        sample = os.environ.get("PHASE_SAMPLE_WAV")
        if sample and os.path.exists(sample):
            res = engine.register_sound_asset(sample, identifier="sample_asset")
            print(f"Registered asset: {res}")
        else:
            print("Set PHASE_SAMPLE_WAV env var to a valid WAV/CAF/AIFF path to test asset registration.")

    except Exception as e:
        print("Error:", e)
------------------------------------------------------------

PROMPT 246:
"Generate Python code that implements the InfluxData Line Protocol format for sending metrics and other data to an InfluxDB database, including support for multiple data types, tags, and timestamped records."
RESPONSE:
```python
"""
InfluxDB Line Protocol utilities for Python.

Features:
- Build valid InfluxDB Line Protocol strings with measurement, tags, fields, and timestamp.
- Supports field types: float, int, bool, string (and Decimal treated as float).
- Proper escaping per InfluxDB rules.
- Deterministic ordering (sorted tags and fields).
- HTTP helpers to send data to InfluxDB v2 or v1 using only the Python standard library.

Example:
    point = lp_point(
        measurement="cpu load",
        tags={"host": "server-01", "region": "us-west", "empty": None},
        fields={"value": 0.64, "cores": 8, "healthy": True, "note": "a \"quoted\" value"},
        timestamp=datetime.datetime(2023, 1, 1, 0, 0, 0, tzinfo=datetime.timezone.utc),
        precision="s",
    )
    # cpu\ load,host=server-01,region=us-west value=0.64,cores=8i,healthy=true,note="a \"quoted\" value" 1672531200

    payload = "\n".join([point])

    # Send to InfluxDB v2:
    # write_v2("http://localhost:8086", org="my-org", bucket="my-bucket", token="my-token", data=payload, precision="s")

    # Send to InfluxDB v1:
    # write_v1("http://localhost:8086", db="mydb", data=payload, precision="s")
"""

from __future__ import annotations

import datetime as _dt
import math as _math
from decimal import Decimal as _Decimal
from typing import Any, Dict, Iterable, Mapping, Optional, Union
import urllib.request
import urllib.parse
import ssl


Precision = Union[str, None]  # 'ns' (default), 'us', 'ms', 's', or None


def _escape_measurement(m: str) -> str:
    # Escape only comma and space for measurement names
    return str(m).replace(",", r"\,").replace(" ", r"\ ")


def _escape_tag_component(s: str) -> str:
    # Tags: escape comma, space, equals
    s = str(s)
    return s.replace(",", r"\,").replace(" ", r"\ ").replace("=", r"\=")


def _escape_field_key(s: str) -> str:
    # Field keys: escape comma, space, equals
    s = str(s)
    return s.replace(",", r"\,").replace(" ", r"\ ").replace("=", r"\=")


def _escape_field_string_value(s: str) -> str:
    # Field string values must be quoted and escape " and \.
    # Also escape newlines/carriage returns to keep single-line payloads.
    s = str(s)
    s = s.replace("\\", "\\\\").replace('"', '\\"')
    s = s.replace("\n", "\\n").replace("\r", "\\r")
    return f'"{s}"'


def _format_float(value: float) -> str:
    if _math.isnan(value) or _math.isinf(value):
        raise ValueError("Float field values cannot be NaN or Infinity in InfluxDB.")
    # 17 significant digits is generally safe for double precision round-trips
    return format(value, ".17g")


def _format_field_value(value: Any) -> Optional[str]:
    # None values are omitted
    if value is None:
        return None
    # Order matters: bool is subclass of int in Python
    if isinstance(value, bool):
        return "true" if value else "false"
    if isinstance(value, int) and not isinstance(value, bool):
        return f"{value}i"
    if isinstance(value, float):
        return _format_float(value)
    if isinstance(value, _Decimal):
        # Treat Decimal as a numeric (float in LP). Avoid scientific notation if possible.
        # Keep as string representation; do not suffix 'i'.
        s = format(value, "f")
        # Trim trailing zeros while keeping at least one digit
        if "." in s:
            s = s.rstrip("0").rstrip(".") or "0"
        return s
    # Everything else as string
    return _escape_field_string_value(value)


def _coerce_timestamp(
    ts: Union[int, float, _dt.datetime, _dt.date, None],
    precision: Precision = "ns",
) -> Optional[int]:
    if ts is None:
        return None

    # Normalize precision
    prec = (precision or "ns").lower()
    if prec not in ("ns", "us", "ms", "s"):
        raise ValueError(f"Invalid precision: {precision}")

    scale = {"ns": 1_000_000_000, "us": 1_000_000, "ms": 1_000, "s": 1}[prec]

    if isinstance(ts, int):
        # Assume already in the specified precision
        return ts
    if isinstance(ts, float):
        return int(ts * scale)
    if isinstance(ts, _dt.datetime):
        # Treat naive datetimes as UTC
        if ts.tzinfo is None:
            ts = ts.replace(tzinfo=_dt.timezone.utc)
        seconds = ts.timestamp()
        return int(seconds * scale)
    if isinstance(ts, _dt.date):
        # Midnight UTC of that date
        dt = _dt.datetime(ts.year, ts.month, ts.day, tzinfo=_dt.timezone.utc)
        seconds = dt.timestamp()
        return int(seconds * scale)

    raise TypeError(f"Unsupported timestamp type: {type(ts).__name__}")


def lp_point(
    measurement: str,
    fields: Mapping[str, Any],
    tags: Optional[Mapping[str, Any]] = None,
    timestamp: Union[int, float, _dt.datetime, _dt.date, None] = None,
    precision: Precision = "ns",
    sort_tags: bool = True,
    sort_fields: bool = True,
) -> str:
    """
    Build a single InfluxDB Line Protocol point string.

    Args:
        measurement: Measurement name.
        fields: Mapping of field key -> value. Supports int, float, bool, str, Decimal.
                None values are omitted. At least one field is required after omission.
        tags: Optional mapping of tag key -> value (strings). None or empty-string values are dropped.
        timestamp: Optional timestamp as int (in given precision), float seconds, datetime, or date.
        precision: 'ns' (default), 'us', 'ms', 's'. Used only to convert datetime/float to integer.
        sort_tags: Sort tags by key for deterministic output.
        sort_fields: Sort fields by key for deterministic output.

    Returns:
        Line Protocol string for a single point.
    """
    if not measurement:
        raise ValueError("measurement is required")

    meas = _escape_measurement(measurement)

    # Build tag set
    tag_parts = []
    if tags:
        items = ((k, v) for k, v in tags.items())
        if sort_tags:
            items = sorted(items, key=lambda kv: str(kv[0]))
        for k, v in items:
            if v is None:
                continue
            v_str = str(v)
            if v_str == "":
                # InfluxDB does not store empty tag values; skip them
                continue
            tag_parts.append(f"{_escape_tag_component(k)}={_escape_tag_component(v_str)}")
    tag_set = ("," + ",".join(tag_parts)) if tag_parts else ""

    # Build field set
    field_parts = []
    items = ((k, v) for k, v in fields.items())
    if sort_fields:
        items = sorted(items, key=lambda kv: str(kv[0]))
    for k, v in items:
        fv = _format_field_value(v)
        if fv is None:
            continue
        field_parts.append(f"{_escape_field_key(k)}={fv}")

    if not field_parts:
        raise ValueError("At least one non-None field is required")

    field_set = ",".join(field_parts)

    # Timestamp
    ts_int = _coerce_timestamp(timestamp, precision)
    if ts_int is not None:
        return f"{meas}{tag_set} {field_set} {ts_int}"
    else:
        return f"{meas}{tag_set} {field_set}"


def lp_points(
    points: Iterable[Dict[str, Any]],
    default_precision: Precision = "ns",
    sort_tags: bool = True,
    sort_fields: bool = True,
) -> str:
    """
    Build multiple Line Protocol points from an iterable of dictionaries.

    Each dict should have keys: measurement, fields, optional tags, optional timestamp, optional precision.

    Example item:
        {
          "measurement": "cpu",
          "tags": {"host":"a"},
          "fields": {"value": 0.1},
          "timestamp": datetime.datetime(...),
          "precision": "ms"  # overrides default_precision
        }
    """
    lines = []
    for p in points:
        lines.append(
            lp_point(
                measurement=p["measurement"],
                fields=p["fields"],
                tags=p.get("tags"),
                timestamp=p.get("timestamp"),
                precision=p.get("precision", default_precision),
                sort_tags=sort_tags,
                sort_fields=sort_fields,
            )
        )
    return "\n".join(lines)


# ---------------------------
# HTTP write helpers (stdlib)
# ---------------------------

def _to_bytes(data: Union[str, bytes]) -> bytes:
    return data if isinstance(data, bytes) else data.encode("utf-8")


def write_v2(
    base_url: str,
    org: str,
    bucket: str,
    token: str,
    data: Union[str, bytes],
    precision: Precision = "ns",
    timeout: float = 10.0,
    insecure_skip_verify: bool = False,
) -> int:
    """
    Write Line Protocol payload to InfluxDB v2.

    Args:
        base_url: e.g., "http://localhost:8086"
        org: Organization name or ID
        bucket: Bucket name
        token: InfluxDB API token
        data: Line Protocol string (possibly multiple lines)
        precision: 'ns' (default), 'us', 'ms', or 's'
        timeout: Request timeout in seconds
        insecure_skip_verify: If True, skips TLS verification (use only for testing)

    Returns:
        HTTP status code (raises on non-2xx).
    """
    url = base_url.rstrip("/") + "/api/v2/write"
    params = {
        "org": org,
        "bucket": bucket,
        "precision": (precision or "ns").lower(),
    }
    query = urllib.parse.urlencode(params)
    url = f"{url}?{query}"

    headers = {
        "Authorization": f"Token {token}",
        "Content-Type": "text/plain; charset=utf-8",
        "Accept": "application/json",
    }

    req = urllib.request.Request(url, data=_to_bytes(data), headers=headers, method="POST")

    context = None
    if insecure_skip_verify:
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE

    try:
        with urllib.request.urlopen(req, timeout=timeout, context=context) as resp:
            return resp.getcode()
    except urllib.error.HTTPError as e:
        body = e.read().decode("utf-8", errors="replace")
        raise RuntimeError(f"InfluxDB v2 write failed: {e.code} {e.reason}: {body}") from e
    except urllib.error.URLError as e:
        raise RuntimeError(f"InfluxDB v2 write connection error: {e}") from e


def write_v1(
    base_url: str,
    db: str,
    data: Union[str, bytes],
    rp: Optional[str] = None,
    username: Optional[str] = None,
    password: Optional[str] = None,
    precision: Precision = "ns",
    timeout: float = 10.0,
    insecure_skip_verify: bool = False,
) -> int:
    """
    Write Line Protocol payload to InfluxDB v1.x.

    Args:
        base_url: e.g., "http://localhost:8086"
        db: Database name
        data: Line Protocol string
        rp: Optional retention policy
        username: Optional username
        password: Optional password
        precision: 'ns' (default), 'us', 'ms', or 's'
        timeout: Request timeout in seconds
        insecure_skip_verify: If True, skips TLS verification (use only for testing)

    Returns:
        HTTP status code (raises on non-2xx).
    """
    url = base_url.rstrip("/") + "/write"
    params = {"db": db, "precision": (precision or "ns").lower()}
    if rp:
        params["rp"] = rp
    if username is not None:
        params["u"] = username
    if password is not None:
        params["p"] = password
    query = urllib.parse.urlencode(params)
    url = f"{url}?{query}"

    headers = {
        "Content-Type": "text/plain; charset=utf-8",
        "Accept": "application/json",
    }

    req = urllib.request.Request(url, data=_to_bytes(data), headers=headers, method="POST")

    context = None
    if insecure_skip_verify:
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE

    try:
        with urllib.request.urlopen(req, timeout=timeout, context=context) as resp:
            return resp.getcode()
    except urllib.error.HTTPError as e:
        body = e.read().decode("utf-8", errors="replace")
        raise RuntimeError(f"InfluxDB v1 write failed: {e.code} {e.reason}: {body}") from e
    except urllib.error.URLError as e:
        raise RuntimeError(f"InfluxDB v1 write connection error: {e}") from e


# ---------------------------
# Simple example usage
# ---------------------------
if __name__ == "__main__":
    import datetime

    # Single point
    line = lp_point(
        measurement="cpu load",
        tags={"host": "server-01", "region": "us-west", "skip_me": None},
        fields={"value": 0.64, "cores": 8, "healthy": True, "note": 'a "quoted" value'},
        timestamp=datetime.datetime(2023, 1, 1, tzinfo=datetime.timezone.utc),
        precision="s",
    )
    print(line)

    # Multiple points
    lines = lp_points(
        [
            {
                "measurement": "cpu",
                "tags": {"host": "a"},
                "fields": {"value": 0.1},
                "timestamp": datetime.datetime.now(tz=datetime.timezone.utc),
            },
            {
                "measurement": "mem",
                "tags": {"host": "a"},
                "fields": {"used": 123456789, "ok": True, "label": "node-a"},
            },
        ],
        default_precision="ns",
    )
    print(lines)
```
------------------------------------------------------------

PROMPT 247:
"Generate Python code that imports the django.contrib.logging module, defines a custom middleware function that logs the HTTP request body using the logging.info() function, and returns the processed request."
RESPONSE:
# Note: Django does not provide a 'django.contrib.logging' module.
# Use Python's standard 'logging' module for logging in Django.
import logging

def request_body_logging_middleware(get_response):
    """
    New-style Django middleware that logs the HTTP request body
    using logging.info() and then returns the response.
    Add 'path.to.this.request_body_logging_middleware' to MIDDLEWARE.
    """
    def middleware(request):
        # Safely access the request body; Django caches request.body
        try:
            body = request.body  # bytes
        except Exception:
            body = b""

        if body:
            try:
                text = body.decode(request.encoding or "utf-8", errors="replace")
                logging.info("Request body: %s", text)
            except Exception:
                logging.info("Request body (bytes): %r", body)

        return get_response(request)

    return middleware
------------------------------------------------------------

PROMPT 248:
"Generate Python code that creates a web server using Flask and a client using requests to interact with a remote dataset stored in Vaex, allowing users to view and manipulate the data through a web interface."
RESPONSE:
Below are two Python scripts: a Flask web server that uses Vaex to host and manipulate a dataset, and a requests-based client to interact with the server. The server exposes endpoints to view data, filter, sort, paginate, describe, and group by columns. The client provides a simple CLI for interacting with these endpoints.

Server (server.py):
- Starts a Flask app
- Loads a Vaex DataFrame (generated sample dataset)
- Provides endpoints:
  - GET /api/info
  - GET /api/data
  - GET /api/describe
  - GET /api/groupby
- Returns JSON

Client (client.py):
- Uses requests to call server endpoints
- Simple CLI with subcommands: info, data, describe, groupby

Prerequisites:
- pip install flask vaex requests numpy

server.py
--------------------------------
import os
import math
import json
from typing import List, Dict, Any
from flask import Flask, jsonify, request, Response
import vaex
import numpy as np

app = Flask(__name__)

def build_sample_vaex_df(n_rows: int = 100_000) -> vaex.dataframe.DataFrame:
    rng = np.random.default_rng(42)
    categories = np.array(["A", "B", "C", "D"])
    df = vaex.from_arrays(
        id=np.arange(n_rows, dtype=np.int64),
        category=categories[rng.integers(0, len(categories), size=n_rows)],
        value=rng.normal(loc=0.0, scale=1.0, size=n_rows).astype(np.float64),
        amount=np.abs(rng.normal(loc=100.0, scale=50.0, size=n_rows)).astype(np.float64),
        flag=rng.integers(0, 2, size=n_rows).astype(np.int8),
        ts=(np.datetime64("2020-01-01") + np.arange(n_rows).astype("timedelta64[m]")).astype("datetime64[ns]"),
    )
    return df

DF = build_sample_vaex_df(int(os.environ.get("N_ROWS", "200000")))

def to_records(df: vaex.dataframe.DataFrame, limit: int = 50, offset: int = 0, columns: List[str] = None) -> List[Dict[str, Any]]:
    sub = df
    if columns:
        valid_cols = [c for c in columns if c in df.get_column_names()]
        if valid_cols:
            sub = sub[valid_cols]
    start = max(0, offset)
    stop = start + max(0, limit)
    view = sub[start:stop]
    pdf = view.to_pandas_df()
    # Convert numpy types and datetimes to JSON-friendly values
    def convert(o):
        if isinstance(o, (np.floating, np.float32, np.float64)):
            return float(o) if not (math.isnan(o) or math.isinf(o)) else None
        if isinstance(o, (np.integer, np.int32, np.int64)):
            return int(o)
        if isinstance(o, (np.bool_,)):
            return bool(o)
        if hasattr(o, "isoformat"):
            try:
                return o.isoformat()
            except Exception:
                return str(o)
        return o
    records = []
    for _, row in pdf.iterrows():
        item = {}
        for k, v in row.to_dict().items():
            item[k] = convert(v)
        records.append(item)
    return records

def parse_int(arg_val: str, default: int) -> int:
    try:
        return int(arg_val)
    except Exception:
        return default

def parse_float(val: str):
    try:
        return float(val)
    except Exception:
        return None

def apply_filters(df: vaex.dataframe.DataFrame, args) -> vaex.dataframe.DataFrame:
    out = df

    # Equality filters: eq=col:value (repeatable)
    for pair in args.getlist("eq"):
        if ":" not in pair:
            continue
        col, val = pair.split(":", 1)
        col = col.strip()
        if col not in df.get_column_names():
            continue
        # Try numeric conversion if applicable
        try:
            # If column dtype is numeric, cast the value
            if df.dtype(col).kind in "iu":  # int types
                v = int(val)
            elif df.dtype(col).kind in "f":  # float types
                v = float(val)
            elif str(df.dtype(col)) in ("bool", "boolean"):
                v = val.lower() in ("1", "true", "yes", "y", "t")
            else:
                v = val
        except Exception:
            v = val
        out = out[out[col] == v]

    # Range filter: where=<col>&min=<num>&max=<num>
    where = args.get("where")
    if where and where in df.get_column_names():
        min_s = args.get("min")
        max_s = args.get("max")
        if min_s is not None:
            fmin = parse_float(min_s)
            if fmin is not None:
                out = out[out[where] >= fmin]
        if max_s is not None:
            fmax = parse_float(max_s)
            if fmax is not None:
                out = out[out[where] <= fmax]

    return out

def apply_sort(df: vaex.dataframe.DataFrame, args) -> vaex.dataframe.DataFrame:
    sort_by = args.get("sort_by")
    if sort_by and sort_by in df.get_column_names():
        desc_s = args.get("desc", "0").lower()
        desc = desc_s in ("1", "true", "yes", "y")
        df = df.sort(sort_by, ascending=not desc)
    return df

@app.route("/")
def index():
    return Response(
        """
        <html>
          <head><title>Vaex Data API</title></head>
          <body>
            <h2>Vaex Data API</h2>
            <p>Example endpoints:</p>
            <ul>
              <li><a href="/api/info">/api/info</a></li>
              <li><a href="/api/data?limit=5">/api/data?limit=5</a></li>
              <li><a href="/api/data?eq=category:A&where=value&min=0.5&sort_by=amount&desc=1&limit=5">/api/data?eq=category:A&where=value&min=0.5&sort_by=amount&desc=1&limit=5</a></li>
              <li><a href="/api/describe">/api/describe</a></li>
              <li><a href="/api/groupby?by=category&aggs=value:mean,value:std,amount:sum">/api/groupby?by=category&aggs=value:mean,value:std,amount:sum</a></li>
            </ul>
          </body>
        </html>
        """,
        mimetype="text/html",
    )

@app.route("/api/info", methods=["GET"])
def api_info():
    cols = DF.get_column_names()
    dtypes = {c: str(DF.dtype(c)) for c in cols}
    return jsonify(
        {
            "row_count": int(len(DF)),
            "columns": cols,
            "dtypes": dtypes,
        }
    )

@app.route("/api/data", methods=["GET"])
def api_data():
    try:
        df = apply_filters(DF, request.args)
        df = apply_sort(df, request.args)

        columns = request.args.get("columns")
        columns = [c.strip() for c in columns.split(",")] if columns else None
        limit = parse_int(request.args.get("limit", "50"), 50)
        offset = parse_int(request.args.get("offset", "0"), 0)
        data = to_records(df, limit=limit, offset=offset, columns=columns)
        return jsonify(
            {
                "total_rows": int(len(df)),
                "returned": len(data),
                "offset": offset,
                "data": data,
            }
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 400

@app.route("/api/describe", methods=["GET"])
def api_describe():
    try:
        df = apply_filters(DF, request.args)
        cols = df.get_column_names()
        result = {}
        for c in cols:
            dt = str(df.dtype(c))
            summary = {"dtype": dt}
            try:
                # Numeric summaries where possible
                if any(k in dt for k in ["int", "float"]):
                    summary["count"] = int(df.count(c))
                    summary["mean"] = float(df[c].mean())
                    summary["std"] = float(df[c].std())
                    summary["min"] = float(df[c].min())
                    summary["max"] = float(df[c].max())
                else:
                    summary["count"] = int(df.count(c))
                    # For categorical-like fields, unique can be expensive; limit to small sample
                    # We provide approximate via nunique (exact in vaex)
                    try:
                        summary["nunique"] = int(df.nunique(c))
                    except Exception:
                        pass
            except Exception:
                # If any aggregation fails, we skip it
                pass
            result[c] = summary
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 400

@app.route("/api/groupby", methods=["GET"])
def api_groupby():
    try:
        df = apply_filters(DF, request.args)
        by_param = request.args.get("by", "")
        by_cols = [b.strip() for b in by_param.split(",") if b.strip()]
        if not by_cols:
            return jsonify({"error": "Missing 'by' query param (comma-separated column names)."}), 400

        for b in by_cols:
            if b not in df.get_column_names():
                return jsonify({"error": f"Unknown group-by column: {b}"}), 400

        aggs_param = request.args.get("aggs", "")
        if not aggs_param:
            return jsonify({"error": "Missing 'aggs' query param like 'value:mean,amount:sum'"}), 400

        aggs = {}
        import vaex.agg as agg  # local alias

        for spec in [s.strip() for s in aggs_param.split(",") if s.strip()]:
            if ":" not in spec:
                return jsonify({"error": f"Invalid agg spec: {spec}. Use 'column:function'."}), 400
            col, func = [x.strip() for x in spec.split(":", 1)]
            if col not in df.get_column_names():
                return jsonify({"error": f"Unknown column in agg: {col}"}), 400
            if not hasattr(agg, func):
                return jsonify({"error": f"Unsupported aggregation function: {func}"}), 400
            alias = f"{col}_{func}"
            aggs[alias] = getattr(agg, func)(df[col])

        res = df.groupby(by=by_cols, agg=aggs)
        # Sorting grouped result by group-by columns for deterministic order
        for b in reversed(by_cols):
            res = res.sort(b, ascending=True)
        data = to_records(res, limit=parse_int(request.args.get("limit", "1000"), 1000), offset=parse_int(request.args.get("offset", "0"), 0))
        return jsonify({"rows": len(data), "data": data})
    except Exception as e:
        return jsonify({"error": str(e)}), 400

if __name__ == "__main__":
    port = int(os.environ.get("PORT", "5000"))
    app.run(host="0.0.0.0", port=port, debug=True)


client.py
--------------------------------
import argparse
import json
import sys
from typing import List
import requests

def pretty(obj):
    return json.dumps(obj, indent=2, ensure_ascii=False)

def cmd_info(base_url: str):
    url = f"{base_url}/api/info"
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    print(pretty(r.json()))

def cmd_data(base_url: str, args):
    params = {}
    if args.columns:
        params["columns"] = ",".join(args.columns)
    if args.limit is not None:
        params["limit"] = str(args.limit)
    if args.offset is not None:
        params["offset"] = str(args.offset)
    if args.sort_by:
        params["sort_by"] = args.sort_by
        params["desc"] = "1" if args.desc else "0"
    for eq in args.eq or []:
        params.setdefault("eq", [])
    # requests handles list params by passing tuples
    query = []
    for k, v in params.items():
        if isinstance(v, list):
            for item in v:
                query.append((k, item))
        else:
            query.append((k, v))
    # equality filters
    for s in args.eq or []:
        query.append(("eq", s))
    # range filter
    if args.where:
        query.append(("where", args.where))
    if args.min is not None:
        query.append(("min", str(args.min)))
    if args.max is not None:
        query.append(("max", str(args.max)))

    url = f"{base_url}/api/data"
    r = requests.get(url, params=query, timeout=60)
    r.raise_for_status()
    print(pretty(r.json()))

def cmd_describe(base_url: str, args):
    query = []
    for s in args.eq or []:
        query.append(("eq", s))
    if args.where:
        query.append(("where", args.where))
    if args.min is not None:
        query.append(("min", str(args.min)))
    if args.max is not None:
        query.append(("max", str(args.max)))

    url = f"{base_url}/api/describe"
    r = requests.get(url, params=query, timeout=120)
    r.raise_for_status()
    print(pretty(r.json()))

def cmd_groupby(base_url: str, args):
    query = []
    # filters
    for s in args.eq or []:
        query.append(("eq", s))
    if args.where:
        query.append(("where", args.where))
    if args.min is not None:
        query.append(("min", str(args.min)))
    if args.max is not None:
        query.append(("max", str(args.max)))

    if not args.by:
        print("Error: --by is required (comma-separated column names).", file=sys.stderr)
        sys.exit(2)
    if not args.aggs:
        print("Error: --aggs is required like 'value:mean,amount:sum'", file=sys.stderr)
        sys.exit(2)

    query.append(("by", ",".join(args.by)))
    query.append(("aggs", ",".join(args.aggs)))
    if args.limit is not None:
        query.append(("limit", str(args.limit)))
    if args.offset is not None:
        query.append(("offset", str(args.offset)))

    url = f"{base_url}/api/groupby"
    r = requests.get(url, params=query, timeout=120)
    r.raise_for_status()
    print(pretty(r.json()))

def main():
    parser = argparse.ArgumentParser(description="Client for Vaex Flask Data API")
    parser.add_argument("--base-url", default="http://localhost:5000", help="Server base URL")

    sub = parser.add_subparsers(dest="cmd", required=True)

    p_info = sub.add_parser("info", help="Get dataset info")

    p_data = sub.add_parser("data", help="Get data rows with optional filters")
    p_data.add_argument("--columns", nargs="+", help="Column names to include")
    p_data.add_argument("--limit", type=int, default=20)
    p_data.add_argument("--offset", type=int, default=0)
    p_data.add_argument("--sort-by", help="Column to sort by")
    p_data.add_argument("--desc", action="store_true", help="Sort descending")
    p_data.add_argument("--eq", action="append", help="Equality filter(s) like 'category:A' (repeatable)")
    p_data.add_argument("--where", help="Numeric range filter column")
    p_data.add_argument("--min", type=float, help="Min value for range filter")
    p_data.add_argument("--max", type=float, help="Max value for range filter")

    p_desc = sub.add_parser("describe", help="Describe columns (optionally filtered)")
    p_desc.add_argument("--eq", action="append", help="Equality filter(s) like 'category:A' (repeatable)")
    p_desc.add_argument("--where", help="Numeric range filter column")
    p_desc.add_argument("--min", type=float, help="Min value")
    p_desc.add_argument("--max", type=float, help="Max value")

    p_gb = sub.add_parser("groupby", help="Group by columns with aggregations")
    p_gb.add_argument("--by", nargs="+", help="Group-by columns (space/comma separated)")
    p_gb.add_argument("--aggs", nargs="+", help="Aggregation specs like 'value:mean' 'amount:sum'")
    p_gb.add_argument("--limit", type=int, default=100)
    p_gb.add_argument("--offset", type=int, default=0)
    p_gb.add_argument("--eq", action="append", help="Equality filter(s) like 'category:A' (repeatable)")
    p_gb.add_argument("--where", help="Numeric range filter column")
    p_gb.add_argument("--min", type=float, help="Min value")
    p_gb.add_argument("--max", type=float, help="Max value")

    args = parser.parse_args()
    base_url = args.base_url.rstrip("/")

    if args.cmd == "info":
        cmd_info(base_url)
    elif args.cmd == "data":
        cmd_data(base_url, args)
    elif args.cmd == "describe":
        cmd_describe(base_url, args)
    elif args.cmd == "groupby":
        cmd_groupby(base_url, args)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()


Usage:
1) Start the server:
   python server.py
   # Optional: set PORT or N_ROWS env vars before starting

2) Use the client:
   # Dataset info
   python client.py info

   # Fetch first 5 rows, selecting some columns, sorted by amount desc
   python client.py data --columns id category amount --limit 5 --sort-by amount --desc

   # Apply filters: category == A and 0.5 <= value <= 1.5
   python client.py data --eq category:A --where value --min 0.5 --max 1.5 --limit 10

   # Describe columns for a filtered slice
   python client.py describe --eq category:B --where amount --min 50 --max 150

   # Group by category with aggregations
   python client.py groupby --by category --aggs value:mean amount:sum

Notes:
- This setup keeps server-side Vaex operations stateless per request to avoid shared mutable state.
- For your own data, replace build_sample_vaex_df() with vaex.open(...) or other Vaex readers and ensure the file is accessible to the server.
------------------------------------------------------------

PROMPT 249:
"Generate Python code that parses and formats HGVS (Human Genome Variation Society) formatted genetic data, maps genomic coordinates to gene names, and validates the accuracy of the data using the hgvs module."
RESPONSE:
# pip install biocommons-hgvs seqrepo-client
# Optional (faster/local): pip install biocommons.seqrepo

import os
import sys
from typing import Iterable, List, Optional, Set, Tuple, Dict

import hgvs.parser
import hgvs.validator
import hgvs.normalizer
import hgvs.assemblymapper
import hgvs.dataproviders.uta
from hgvs.exceptions import HGVSError

# Prefer SeqRepo REST; fall back to NCBI if not available
try:
    from hgvs.seqfetcher import SeqRepoRESTClient as _SeqFetcher
    _SEQFETCHER_KIND = "seqrepo-rest"
except Exception:  # pragma: no cover - fallback only
    from hgvs.seqfetcher import NCBISeqFetcher as _SeqFetcher
    _SEQFETCHER_KIND = "ncbi-eutils"


class HGVSClient:
    """
    End-to-end HGVS helper:
      - Parse and format HGVS variants
      - Validate variants via hgvs.Validator
      - Map genomic coordinates to overlapping gene names (via UTA)
      - Convert between g. and c. given transcripts and assembly

    Requirements:
      - Remote UTA database (default public) or a local UTA instance
      - Sequence fetching via SeqRepo REST (default) or NCBI E-utilities fallback

    Environment variables (optional):
      - UTA_URL: PostgreSQL URL for UTA (e.g., postgresql://uta_public@uta.biocommons.org/uta/uta_20210129)
      - SEQREPO_URL: Base URL for SeqRepo REST (default provided by library)
    """

    def __init__(
        self,
        assembly: str = "GRCh38",
        alt_aln_method: str = "splign",
        uta_url: Optional[str] = None,
        seqrepo_url: Optional[str] = None,
        replace_reference: bool = True,
    ):
        # Data provider (UTA)
        uta_url = uta_url or os.environ.get("UTA_URL", None)
        self.hdp = hgvs.dataproviders.uta.connect(uta_url)

        # Sequence fetcher (SeqRepo REST preferred)
        if _SEQFETCHER_KIND == "seqrepo-rest":
            self.seqfetcher = _SeqFetcher(base_url=seqrepo_url) if seqrepo_url else _SeqFetcher()
        else:
            # NCBI fallback: set your email via Entrez if needed
            self.seqfetcher = _SeqFetcher()

        # Core tools
        self.parser = hgvs.parser.Parser()
        self.validator = hgvs.validator.Validator(self.hdp)
        self.normalizer = hgvs.normalizer.Normalizer(hdp=self.hdp, seqfetcher=self.seqfetcher, cross_check_ref=True)

        # Mapper for coordinate transforms
        self.assembly = assembly
        self.am = hgvs.assemblymapper.AssemblyMapper(
            self.hdp,
            assembly_name=assembly,
            alt_aln_method=alt_aln_method,
            replace_reference=replace_reference,
            seqfetcher=self.seqfetcher,
        )

    # ---------------------------
    # Parsing / formatting / validation
    # ---------------------------

    def parse(self, hgvs_text: str):
        """Parse an HGVS string to a SequenceVariant."""
        return self.parser.parse_hgvs_variant(hgvs_text)

    def format(self, var) -> str:
        """Return canonical string representation."""
        return str(var)

    def normalize(self, var):
        """Left-shift/right-shift to normalized representation (when possible)."""
        try:
            return self.normalizer.normalize(var)
        except Exception:
            return var  # if normalization fails, return original

    def validate(self, var, strict: bool = True) -> Tuple[bool, Optional[str]]:
        """
        Validate variant; return (is_valid, message_if_invalid).
        In strict mode, applies HGVS best-practice rule checks where supported.
        """
        try:
            self.validator.validate(var, strict=strict)
            return True, None
        except Exception as e:
            return False, f"{type(e).__name__}: {e}"

    # ---------------------------
    # Mapping genomic variant to genes
    # ---------------------------

    def _get_genomic_bounds_1based(self, var_g) -> Tuple[int, int]:
        """Extract 1-based inclusive start/end from a g. variant."""
        pos = var_g.posedit.pos
        # Supports both single position and interval
        try:
            s = pos.start.base
            e = pos.end.base
        except Exception:
            # Fallback: treat as single position if needed
            s = getattr(pos, "base", None) or getattr(pos, "pos", None)
            e = s
        if s is None or e is None:
            raise ValueError("Unable to determine genomic start/end for variant")
        return int(s), int(e)

    def genes_for_g_variant(self, var_g) -> Tuple[Set[str], List[Dict]]:
        """
        Return overlapping gene symbols and transcript entries for a genomic (g.) variant.
        Uses UTA transcript bins to find overlaps, then resolves symbols from transcript info.
        """
        if getattr(var_g, "type", None) != "g":
            raise ValueError("genes_for_g_variant expects a genomic (g.) variant")
        start_1b, end_1b = self._get_genomic_bounds_1based(var_g)
        # UTA uses 0-based, end-exclusive coordinates
        start_i = start_1b - 1
        end_i = end_1b
        # Fetch transcripts overlapping region
        tx_overlaps = self.hdp.get_tx_for_region(ac=var_g.ac, start_i=start_i, end_i=end_i)
        genes: Set[str] = set()
        resolved: List[Dict] = []
        for tx in tx_overlaps:
            # tx is a namedtuple-like with tx_ac, alt_ac, alt_strand, etc.
            try:
                tinfo = self.hdp.get_tx_info(tx.tx_ac)
                gene_symbol = getattr(tinfo, "gene", None) or getattr(tinfo, "hgnc", None)
                if gene_symbol:
                    genes.add(gene_symbol)
                resolved.append(
                    {
                        "tx_ac": tx.tx_ac,
                        "gene": gene_symbol,
                        "alt_ac": getattr(tx, "alt_ac", None),
                        "alt_strand": getattr(tx, "alt_strand", None),
                        "alt_aln_method": getattr(tx, "alt_aln_method", None),
                    }
                )
            except Exception:
                continue
        return genes, resolved

    # ---------------------------
    # Coordinate conversions
    # ---------------------------

    def g_to_c_all(self, var_g) -> List[Tuple[str, Optional[object]]]:
        """
        Map a g. variant to c. variants across all overlapping transcripts.
        Returns a list of (tx_ac, c_variant_or_None) tuples.
        """
        genes, tx_entries = self.genes_for_g_variant(var_g)
        results: List[Tuple[str, Optional[object]]] = []
        for entry in tx_entries:
            tx_ac = entry["tx_ac"]
            try:
                cv = self.am.g_to_c(var_g, tx_ac)
            except Exception:
                cv = None
            results.append((tx_ac, cv))
        return results

    def c_to_g(self, var_c):
        """Map a c. variant to g. variant."""
        if getattr(var_c, "type", None) != "c":
            raise ValueError("c_to_g expects a coding (c.) variant")
        return self.am.c_to_g(var_c)

    # ---------------------------
    # High-level helper
    # ---------------------------

    def parse_validate_format(
        self, hgvs_text: str, normalize: bool = True, strict_validation: bool = True
    ) -> Dict:
        """
        Parse an HGVS string, optionally normalize, validate, and return a dict summary.
        If genomic, also report overlapping genes.
        """
        var = self.parse(hgvs_text)
        varn = self.normalize(var) if normalize else var
        is_valid, msg = self.validate(varn, strict=strict_validation)

        result = {
            "input": hgvs_text,
            "parsed_type": getattr(varn, "type", None),
            "formatted": self.format(varn),
            "valid": is_valid,
            "validation_message": msg,
        }

        if getattr(varn, "type", None) == "g":
            try:
                genes, txs = self.genes_for_g_variant(varn)
                result["genes"] = sorted(genes)
                result["overlapping_transcripts"] = txs
            except Exception as e:
                result["genes_error"] = str(e)

        return result


def demo():
    """
    Example usage: parse, validate, format, and map to gene names.
    Adjust assembly if needed (e.g., GRCh37 vs GRCh38).
    """
    client = HGVSClient(assembly="GRCh38")

    examples = [
        # Genomic SNV on GRCh38 (BRCA1 region)
        "NC_000017.11:g.43045705C>T",
        # Coding example; mapper needs transcript to convert to g.
        "NM_007294.3:c.5266dupC",
        # Another genomic example
        "NC_000013.11:g.32316461G>A",
    ]

    for s in examples:
        info = client.parse_validate_format(s, normalize=True, strict_validation=True)
        print("Input:", info["input"])
        print("Type:", info["parsed_type"])
        print("Formatted:", info["formatted"])
        print("Valid:", info["valid"], "-" if info["valid"] else info["validation_message"])
        if info.get("genes"):
            print("Overlapping genes:", ", ".join(info["genes"]))
        if info.get("overlapping_transcripts"):
            print("Overlapping transcripts (first 5):", [t["tx_ac"] for t in info["overlapping_transcripts"][:5]])
        # If c. -> g. mapping is desired for coding variants:
        if info["parsed_type"] == "c":
            try:
                gvar = client.c_to_g(client.parse(info["formatted"]))
                ok, msg = client.validate(gvar, strict=True)
                print("Mapped to g.:", str(gvar), "Valid:", ok)
            except HGVSError as e:
                print("c_to_g mapping failed:", e)
        print("-" * 60)


if __name__ == "__main__":
    # If you run this module directly, it will demonstrate basic functionality.
    # Note: Remote services (UTA, SeqRepo REST or NCBI) are used; ensure internet access.
    try:
        demo()
    except Exception as ex:
        print("Demo failed:", type(ex).__name__, ex, file=sys.stderr)
        sys.exit(1)
------------------------------------------------------------

PROMPT 250:
"Generate Python code that uses the pyuno library to interact with LibreOffice and create reports using its powerful reporting tools, including support for templates, data connections, and various output formats."
RESPONSE:
Below is a self-contained Python script that uses pyuno (UNO bridge) to connect to LibreOffice, pull data via a registered Base data source (or register an .odb on the fly), merge it into a Writer template, and export the resulting reports to multiple output formats (ODT, PDF, DOCX, HTML).

Notes:
- Start LibreOffice in headless mode before running this script:
  soffice --headless --norestore --nodefault --nologo --accept="socket,host=127.0.0.1,port=2002;urp;"
- Run the script with LibreOffice’s bundled Python or ensure the pyuno module is importable by your Python environment.
- The example uses a simple token-based templating approach: put placeholders like ${CustomerName}, ${InvoiceNo}, etc. in your Writer template (.ott or .odt). The script replaces those tokens with database values per record.
- Data connection approaches included:
  - Use an existing, registered Base data source by name.
  - Or register a Base .odb file at runtime and refer to it by name.

Python code:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LibreOffice reporting via pyuno:
- Connect to a running LibreOffice (UNO) in headless mode
- Use a Base data source to fetch records (SQL/table/query)
- Merge records into a Writer template (token replacement)
- Export per-record reports into multiple formats (ODT, PDF, DOCX, HTML)

Requirements:
1) Start LibreOffice headless before running:
   soffice --headless --norestore --nodefault --nologo --accept="socket,host=127.0.0.1,port=2002;urp;"

2) Run this with LibreOffice's Python or ensure pyuno is importable in your Python env.
"""

import os
import sys
import time
from pathlib import Path

import uno
from com.sun.star.beans import PropertyValue
from com.sun.star.sdb import CommandType

# -------------------------------
# UNO helpers
# -------------------------------

def _prop(name, value):
    pv = PropertyValue()
    pv.Name = name
    pv.Value = value
    return pv

def to_file_url(path: str) -> str:
    return uno.systemPathToFileUrl(os.path.abspath(path))

def connect_office(host="127.0.0.1", port=2002):
    """
    Connect to a running LibreOffice instance (headless).
    Returns (ctx, smgr, desktop).
    """
    local_ctx = uno.getComponentContext()
    resolver = local_ctx.ServiceManager.createInstanceWithContext(
        "com.sun.star.bridge.UnoUrlResolver", local_ctx
    )
    ctx = resolver.resolve(f"uno:socket,host={host},port={port};urp;StarOffice.ComponentContext")
    smgr = ctx.getServiceManager()
    desktop = smgr.createInstanceWithContext("com.sun.star.frame.Desktop", ctx)
    return ctx, smgr, desktop

def load_document(desktop, url: str, hidden=True, as_template=False):
    """
    Load a document by URL.
    - as_template=True: open a copy of the template (new untitled doc)
    """
    props = []
    if hidden:
        props.append(_prop("Hidden", True))
    if as_template:
        props.append(_prop("AsTemplate", True))
    return desktop.loadComponentFromURL(url, "_blank", 0, tuple(props))

def close_document(doc):
    try:
        doc.close(True)
    except Exception:
        pass

def store_doc_to_url(doc, url: str, filter_name: str = None, filter_data: list = None):
    """
    Export/save a document using storeToURL (non-destructive).
    filter_data is a list of PropertyValue.
    """
    props = []
    if filter_name:
        props.append(_prop("FilterName", filter_name))
    if filter_data:
        props.append(_prop("FilterData", tuple(filter_data)))
    doc.storeToURL(url, tuple(props))

# -------------------------------
# Data source helpers (Base / SDBC)
# -------------------------------

def get_database_context(smgr, ctx):
    return smgr.createInstanceWithContext("com.sun.star.sdb.DatabaseContext", ctx)

def ensure_register_odb(dbctx, name: str, odb_path: str):
    """
    Register an .odb file as a data source with the given name (if not already registered).
    """
    file_url = to_file_url(odb_path)
    names = set(dbctx.getElementNames())
    if name in names:
        # Already registered; ensure it points to the same URL
        return
    dbctx.registerDatabaseLocation(name, file_url)

def get_data_source(dbctx, name: str):
    if not dbctx.hasByName(name):
        raise RuntimeError(f"Data source '{name}' is not registered.")
    return dbctx.getByName(name)

def query_rows(data_source, sql: str, user: str = "", password: str = ""):
    """
    Execute SQL on the given data source and yield rows as dicts (label->value as string).
    """
    conn = None
    stmt = None
    rs = None
    try:
        conn = data_source.getConnection(user, password)
        stmt = conn.createStatement()
        rs = stmt.executeQuery(sql)

        md = rs.getMetaData()
        col_count = md.getColumnCount()
        labels = [md.getColumnLabel(i) for i in range(1, col_count + 1)]

        rows = []
        while rs.next():
            row = {}
            for i in range(1, col_count + 1):
                try:
                    # getString works for most types; adapt for dates/numbers if needed
                    row[labels[i - 1]] = rs.getString(i)
                except Exception:
                    row[labels[i - 1]] = ""
            rows.append(row)

        return rows
    finally:
        # Clean up
        try:
            if rs is not None:
                rs.close()
        except Exception:
            pass
        try:
            if stmt is not None:
                stmt.close()
        except Exception:
            pass
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

# -------------------------------
# Templating and export
# -------------------------------

def replace_tokens_in_writer(doc, mapping: dict):
    """
    Replace ${Token} occurrences in the entire Writer doc.
    """
    # Ensure all values are strings (None -> "")
    mapping = {k: ("" if v is None else str(v)) for k, v in mapping.items()}

    # Global search/replace per token
    # Note: This operates textually; if you need fields/tables, build them via UNO APIs instead.
    for token, value in mapping.items():
        search = doc.createReplaceDescriptor()
        search.SearchString = f"${{{token}}}"  # e.g., ${CustomerName}
        search.ReplaceString = value
        search.SearchRegularExpression = False
        doc.replaceAll(search)

def export_formats(doc, base_out_path_no_ext: str, formats=("odt", "pdf")):
    """
    Export doc to selected formats.
    Supported:
      - odt: writer8
      - pdf: writer_pdf_Export
      - docx: MS Word 2007 XML
      - html: HTML (StarWriter)
    Returns list of generated file paths.
    """
    out_files = []
    for fmt in formats:
        fmt = fmt.lower()
        if fmt == "odt":
            filter_name = "writer8"
            ext = ".odt"
            filter_data = None
        elif fmt == "pdf":
            filter_name = "writer_pdf_Export"
            ext = ".pdf"
            # Example PDF options; adjust as needed
            filter_data = [
                _prop("ReduceImageResolution", True),
                _prop("MaxImageResolution", 150),
            ]
        elif fmt == "docx":
            filter_name = "MS Word 2007 XML"
            ext = ".docx"
            filter_data = None
        elif fmt == "html":
            filter_name = "HTML (StarWriter)"
            ext = ".html"
            filter_data = None
        else:
            raise ValueError(f"Unsupported format: {fmt}")

        out_path = base_out_path_no_ext + ext
        out_url = to_file_url(out_path)
        store_doc_to_url(doc, out_url, filter_name, filter_data)
        out_files.append(out_path)
    return out_files

def merge_one_record(ctx, smgr, desktop, template_path: str, mapping: dict, out_dir: str,
                     file_basename: str, formats=("odt","pdf","docx")):
    """
    Load template as a new doc, replace tokens using mapping, export to formats, then close.
    Returns list of generated file paths.
    """
    template_url = to_file_url(template_path)
    doc = None
    try:
        doc = load_document(desktop, template_url, hidden=True, as_template=True)
        replace_tokens_in_writer(doc, mapping)

        base_out_no_ext = os.path.join(out_dir, file_basename)
        Path(out_dir).mkdir(parents=True, exist_ok=True)
        outputs = export_formats(doc, base_out_no_ext, formats=formats)
        return outputs
    finally:
        if doc:
            close_document(doc)

# -------------------------------
# High-level report pipeline
# -------------------------------

def run_report(
    host: str,
    port: int,
    # Template and outputs
    template_path: str,
    output_dir: str,
    output_formats=("odt", "pdf"),
    file_name_column: str = None,  # derive filename from a field; else use row index
    # Data source
    data_source_name: str = None,
    odb_to_register: str = None,  # path to .odb to register as `data_source_name`
    sql: str = None,
    user: str = "",
    password: str = "",
):
    """
    End-to-end: connect to LO, fetch data, merge into Writer template, export to selected formats.
    - Provide either a pre-registered data_source_name or (data_source_name + odb_to_register).
    - sql: SELECT statement or "SELECT * FROM <table>" or name of a Base query using: SELECT * FROM "QueryName"
      (Use quotes for names with spaces; adjust for your DB dialect)
    """
    if not data_source_name:
        raise ValueError("data_source_name is required.")
    if not sql:
        raise ValueError("sql (query) is required.")
    if not os.path.exists(template_path):
        raise FileNotFoundError(f"Template not found: {template_path}")

    ctx, smgr, desktop = connect_office(host=host, port=port)
    dbctx = get_database_context(smgr, ctx)

    if odb_to_register:
        if not os.path.exists(odb_to_register):
            raise FileNotFoundError(f".odb file not found: {odb_to_register}")
        ensure_register_odb(dbctx, data_source_name, odb_to_register)

    ds = get_data_source(dbctx, data_source_name)
    rows = query_rows(ds, sql, user=user, password=password)

    if not rows:
        print("No data returned by query; nothing to do.")
        return []

    generated_files = []
    for idx, row in enumerate(rows, start=1):
        # Derive filename
        if file_name_column and file_name_column in row and row[file_name_column]:
            safe = "".join(c for c in row[file_name_column] if c.isalnum() or c in ("-", "_", ".", " "))
            base_name = safe.strip() or f"record_{idx:05d}"
        else:
            base_name = f"record_{idx:05d}"

        files = merge_one_record(
            ctx, smgr, desktop,
            template_path=template_path,
            mapping=row,
            out_dir=output_dir,
            file_basename=base_name,
            formats=output_formats,
        )
        generated_files.extend(files)
        # Be nice to LO if generating many files
        time.sleep(0.02)

    return generated_files

# -------------------------------
# Example CLI usage
# -------------------------------

if __name__ == "__main__":
    # Customize these paths and values to your environment
    LO_HOST = "127.0.0.1"
    LO_PORT = 2002

    # 1) Writer template with ${FieldName} style tokens
    TEMPLATE = "report_template.ott"  # e.g., ./report_template.ott

    # 2) Output directory
    OUT_DIR = "./out_reports"

    # 3) Data source: either already registered in LO or register an .odb here.
    DATA_SOURCE_NAME = "MySalesDB"
    ODB_FILE = "mysales.odb"  # leave None if already registered globally

    # 4) Query to fetch data (adjust for your DB). Example selects from a table "Invoices".
    SQL = 'SELECT "InvoiceNo", "CustomerName", "InvoiceDate", "Total" FROM "Invoices" ORDER BY "InvoiceNo"'

    # 5) Derive output filename from a column (optional)
    FILE_NAME_COLUMN = "InvoiceNo"

    # 6) Output formats you want
    FORMATS = ("odt", "pdf", "docx")

    # 7) Credentials if required by your data source
    USER = ""
    PASSWORD = ""

    try:
        files = run_report(
            host=LO_HOST,
            port=LO_PORT,
            template_path=TEMPLATE,
            output_dir=OUT_DIR,
            output_formats=FORMATS,
            file_name_column=FILE_NAME_COLUMN,
            data_source_name=DATA_SOURCE_NAME,
            odb_to_register=ODB_FILE,  # set to None if DS is already registered
            sql=SQL,
            user=USER,
            password=PASSWORD,
        )
        print("Generated files:")
        for f in files:
            print(" -", f)
    except Exception as e:
        print("ERROR:", e, file=sys.stderr)
        sys.exit(1)
```

How to prepare your template:
- Create a Writer template (.ott).
- Put tokens like ${CustomerName}, ${InvoiceNo}, ${InvoiceDate}, ${Total} where you want values to appear.
- Save the template and reference it via TEMPLATE in the script.

Tips and extensions:
- To use a Base Query by name, ensure your .odb contains the query and use a proper SQL to select from it (or read its command text and execute that). Some drivers allow SELECT * FROM "YourQuery" depending on driver; otherwise, copy the query SQL directly into SQL.
- If your data source is already registered in LibreOffice (Tools > Options > LibreOffice Base > Databases), set ODB_FILE to None and just provide its name.
- For CSV or other external sources, create an .odb that points to them (using the Base UI), then register that .odb with this script.
- For large batches or complex layouts (tables, groups), consider building content programmatically (e.g., creating tables in the document via UNO) or using LibreOffice’s Mail Merge service with a data source-backed template. This script focuses on token replacement for simplicity and robustness.
- To export to other formats, adjust export_formats with the appropriate filter names (e.g., RTF: Rich Text Format; TXT: Text).

Common filter names:
- ODT: writer8
- PDF: writer_pdf_Export
- DOCX: MS Word 2007 XML
- HTML: HTML (StarWriter)

If you need to use the Mail Merge service instead of token replacement:
- Service: com.sun.star.text.MailMerge
- Properties to set typically include DocumentURL, DataSourceName, Command, CommandType, OutputType, FileNamePrefix, OutputPath.
- In practice, it’s often simpler to generate ODT files via Mail Merge and then open+export them to PDF/DOCX as shown above.
------------------------------------------------------------


PROMPT 251:
"Generate Python code that imports the requests library, creates a proxy object using the requests.Session class, sets up the proxy server URL and authentication, and sends HTTP requests through the proxy server."
RESPONSE:
# pip install requests
# For SOCKS proxies: pip install "requests[socks]"

import requests
from urllib.parse import quote

def create_proxied_session(proxy_host, proxy_port, proxy_username=None, proxy_password=None, scheme="http"):
    """
    Create a requests.Session configured to send traffic through a proxy.
    scheme: "http", "https", "socks5", or "socks5h" (requires requests[socks] for SOCKS).
    """
    session = requests.Session()
    session.trust_env = False  # ignore system env proxy settings; remove if you want to honor them

    if proxy_username and proxy_password:
        user = quote(proxy_username, safe="")
        pwd = quote(proxy_password, safe="")
        proxy_url = f"{scheme}://{user}:{pwd}@{proxy_host}:{proxy_port}"
    else:
        proxy_url = f"{scheme}://{proxy_host}:{proxy_port}"

    # Use the same proxy for both HTTP and HTTPS requests
    session.proxies.update({
        "http": proxy_url,
        "https": proxy_url,
    })

    # Optional: set default headers
    session.headers.update({"User-Agent": "proxied-client/1.0"})
    return session

if __name__ == "__main__":
    # Configure your proxy details here
    PROXY_HOST = "proxy.example.com"
    PROXY_PORT = 8080
    PROXY_USER = "myuser"
    PROXY_PASS = "mypassword"

    # Create a session that routes traffic through the proxy
    sess = create_proxied_session(
        proxy_host=PROXY_HOST,
        proxy_port=PROXY_PORT,
        proxy_username=PROXY_USER,
        proxy_password=PROXY_PASS,
        scheme="http",  # or "socks5h" for SOCKS5 with DNS through proxy
    )

    try:
        # Example HTTP request through the proxy
        r1 = sess.get("http://httpbin.org/ip", timeout=10)
        print("HTTP GET status:", r1.status_code)
        print("Response:", r1.text)

        # Example HTTPS request through the proxy
        r2 = sess.get("https://httpbin.org/headers", timeout=10)
        print("HTTPS GET status:", r2.status_code)
        print("Response:", r2.text)

        # Example POST request through the proxy
        r3 = sess.post("https://httpbin.org/post", json={"hello": "world"}, timeout=10)
        print("POST status:", r3.status_code)
        print("Response:", r3.text)

    except requests.RequestException as exc:
        print("Request failed:", exc)
------------------------------------------------------------

PROMPT 252:
"Generate Python code that uses the `requests` library to send HTTP requests to Iterative Studio and the `dvc` library to interact with DVC/DVCLive, allowing users to post data from DVC/DVCLive to Iterative Studio."
RESPONSE:
# requirements:
#   pip install dvc[ssh,s3,gs,azure] dvclive requests pyyaml
#
# Notes:
# - This example uses:
#     - dvc.api.* to read files (metrics/params) from your DVC repo
#     - DVCLive’s on-disk outputs (e.g., dvclive/metrics.json)
#     - requests to send data to Iterative Studio over HTTP
# - You must provide the correct Iterative Studio endpoint and auth token.
#   Consult your Iterative Studio API documentation/admin for the right URL
#   and auth scheme. This code keeps them configurable.

import json
import os
import sys
import time
import uuid
import typing as t
import subprocess
from dataclasses import dataclass, asdict
from pathlib import Path

import requests
import yaml

try:
    import dvc.api as dvc_api
except Exception as e:
    print("Failed to import dvc.api. Make sure `dvc` is installed.", file=sys.stderr)
    raise


# ---------------------------
# Git/DVC/DVCLive helpers
# ---------------------------

def _run_git(*args: str, cwd: t.Optional[Path] = None) -> str:
    out = subprocess.check_output(["git", *args], cwd=str(cwd) if cwd else None)
    return out.decode().strip()


def get_git_info(repo_dir: t.Union[str, Path] = ".") -> dict:
    repo_dir = Path(repo_dir)
    try:
        root = _run_git("rev-parse", "--show-toplevel", cwd=repo_dir)
    except Exception:
        root = str(Path(repo_dir).resolve())

    def safe(cmd: list[str]) -> str:
        try:
            return _run_git(*cmd, cwd=repo_dir)
        except Exception:
            return ""

    return {
        "root": root,
        "commit": safe(["rev-parse", "HEAD"]),
        "branch": safe(["rev-parse", "--abbrev-ref", "HEAD"]),
        "remote": safe(["config", "--get", "remote.origin.url"]),
        "is_dirty": bool(safe(["status", "--porcelain"])),
    }


def load_yaml_or_json(stream: t.Union[str, bytes]) -> t.Any:
    s = stream.decode() if isinstance(stream, (bytes, bytearray)) else stream
    s = s.strip()
    if not s:
        return None
    # Decide based on first non-space char
    first = s.lstrip()[:1]
    if first in "{[":
        return json.loads(s)
    return yaml.safe_load(s)


def read_params_with_dvc(
    repo: str = ".",
    rev: str = "workspace",
    params_paths: t.Optional[list[str]] = None,
) -> dict:
    """
    Reads params files via dvc.api.open so it works for workspace or any ref.
    Default list includes 'params.yaml' if not provided.
    """
    if params_paths is None:
        params_paths = ["params.yaml"]

    params: dict = {}
    for p in params_paths:
        try:
            with dvc_api.open(p, repo=repo, rev=rev, mode="r") as f:
                content = f.read()
            data = load_yaml_or_json(content)
            if isinstance(data, dict):
                # Deep merge with last-one-wins
                params = {**params, **data}
        except FileNotFoundError:
            continue
        except Exception as e:
            print(f"Warning: failed reading params from {p}@{rev}: {e}", file=sys.stderr)
    return params


def read_metrics_with_dvc(
    metric_paths: list[str],
    repo: str = ".",
    rev: str = "workspace",
) -> dict:
    """
    Reads metric files via dvc.api.open and returns a combined dict.

    You should pass explicit paths to metric files. These could be:
      - DVCLive summary files promoted as metrics
      - Any JSON/YAML files marked as 'metrics' in your dvc.yaml stages
    """
    metrics: dict = {}
    for p in metric_paths:
        try:
            with dvc_api.open(p, repo=repo, rev=rev, mode="r") as f:
                content = f.read()
            data = load_yaml_or_json(content)
            if isinstance(data, dict):
                metrics = {**metrics, **data}
            else:
                # Store non-dict metrics under the file key
                metrics[p] = data
        except FileNotFoundError:
            continue
        except Exception as e:
            print(f"Warning: failed reading metrics from {p}@{rev}: {e}", file=sys.stderr)
    return metrics


def discover_dvclive_latest_metrics(dvclive_dir: t.Union[str, Path] = "dvclive") -> dict:
    """
    Reads DVCLive's latest metrics snapshot if present.
    Common files:
      - dvclive/metrics.json        (latest scalars)
      - dvclive/params.yaml|json    (optional, if logged)
      - dvclive/summary.json        (DVCLive >= 2 may produce a summary)
    This function focuses on a single flat dictionary from metrics.json/summary.json.
    """
    d = Path(dvclive_dir)
    if not d.exists():
        return {}

    candidates = [
        d / "metrics.json",
        d / "summary.json",  # if present, often contains overall summary and last step
    ]
    for c in candidates:
        if c.is_file():
            try:
                txt = c.read_text()
                data = load_yaml_or_json(txt)
                if isinstance(data, dict):
                    return data
            except Exception as e:
                print(f"Warning: failed parsing {c}: {e}", file=sys.stderr)
    return {}


# ---------------------------
# Payload types
# ---------------------------

@dataclass
class GitInfo:
    root: str
    commit: str
    branch: str
    remote: str
    is_dirty: bool


@dataclass
class RunPayload:
    # High-level run metadata
    run_id: str
    created_at: float
    git: GitInfo
    # Optional user-provided grouping fields
    org: t.Optional[str] = None
    project: t.Optional[str] = None
    name: t.Optional[str] = None
    # Data
    params: t.Optional[dict] = None
    metrics: t.Optional[dict] = None
    tags: t.Optional[list[str]] = None
    extra: t.Optional[dict] = None

    def to_json(self) -> dict:
        d = asdict(self)
        d["git"] = asdict(self.git)
        return d


# ---------------------------
# Studio HTTP client
# ---------------------------

class StudioClient:
    def __init__(
        self,
        base_url: str,
        token: str,
        auth_scheme: str = "Bearer",
        timeout: float = 15.0,
        session: t.Optional[requests.Session] = None,
        extra_headers: t.Optional[dict] = None,
    ):
        """
        base_url: e.g. https://studio.iterative.ai or your on-prem Studio URL
        token: API token for Iterative Studio
        auth_scheme: 'Bearer' by default. Adjust if your deployment differs.
        """
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        self.session = session or requests.Session()
        self.session.headers.update({
            "Accept": "application/json",
            "Content-Type": "application/json",
            "Authorization": f"{auth_scheme} {token}",
            **(extra_headers or {}),
        })

    def post_run(
        self,
        payload: RunPayload,
        endpoint: str = "/api/v1/runs/ingest",
        raise_for_status: bool = True,
    ) -> requests.Response:
        """
        Sends a run (params + metrics + git metadata) to Studio.
        The endpoint path may differ depending on your Studio version/setup.
        Consult your admin or API docs and adjust accordingly.
        """
        url = f"{self.base_url}{endpoint}"
        resp = self.session.post(url, json=payload.to_json(), timeout=self.timeout)
        if raise_for_status:
            resp.raise_for_status()
        return resp

    def upload_artifact(
        self,
        file_path: t.Union[str, Path],
        endpoint: str,
        fields: t.Optional[dict] = None,
        raise_for_status: bool = True,
    ) -> requests.Response:
        """
        Example multipart upload, if your Studio endpoint supports it.
        'endpoint' should be the full or relative path for artifact upload.
        """
        url = f"{self.base_url}{endpoint}" if not endpoint.startswith("http") else endpoint
        file_path = Path(file_path)
        if not file_path.is_file():
            raise FileNotFoundError(f"Artifact not found: {file_path}")

        files = {"file": (file_path.name, file_path.open("rb"))}
        data = fields or {}
        # Temporarily override content-type for multipart
        headers = self.session.headers.copy()
        headers.pop("Content-Type", None)

        resp = self.session.post(url, data=data, files=files, headers=headers, timeout=self.timeout)
        if raise_for_status:
            resp.raise_for_status()
        return resp


# ---------------------------
# Orchestration helpers
# ---------------------------

def build_run_payload(
    repo_dir: t.Union[str, Path] = ".",
    rev: str = "workspace",
    params_paths: t.Optional[list[str]] = None,
    metric_paths: t.Optional[list[str]] = None,
    dvclive_dir: t.Union[str, Path] = "dvclive",
    org: t.Optional[str] = None,
    project: t.Optional[str] = None,
    name: t.Optional[str] = None,
    tags: t.Optional[list[str]] = None,
    extra: t.Optional[dict] = None,
) -> RunPayload:
    gi = get_git_info(repo_dir)
    git = GitInfo(**gi)
    created_at = time.time()

    # Collect params
    params = read_params_with_dvc(repo=str(repo_dir), rev=rev, params_paths=params_paths)

    # Collect metrics
    metrics = {}
    # 1) DVCLive latest snapshot (if exists)
    dvclive_metrics = discover_dvclive_latest_metrics(dvclive_dir=dvclive_dir)
    if dvclive_metrics:
        metrics.update(dvclive_metrics)
    # 2) Additional metrics explicitly provided (from DVC-tracked files)
    if metric_paths:
        dvc_metrics = read_metrics_with_dvc(metric_paths, repo=str(repo_dir), rev=rev)
        metrics.update(dvc_metrics)

    # Generate a run id. You might prefer using the git commit as stable id,
    # but we keep them separate to allow multiple runs per commit if needed.
    run_id = str(uuid.uuid4())

    return RunPayload(
        run_id=run_id,
        created_at=created_at,
        git=git,
        org=org,
        project=project,
        name=name or f"run-{git.branch or git.commit[:7] or run_id[:8]}",
        params=params or None,
        metrics=metrics or None,
        tags=tags or None,
        extra=extra or None,
    )


# ---------------------------
# Example usage
# ---------------------------

if __name__ == "__main__":
    # Configure via env vars for convenience
    STUDIO_BASE_URL = os.getenv("STUDIO_BASE_URL", "https://your-studio.example.com")
    STUDIO_TOKEN = os.getenv("STUDIO_TOKEN", "")

    if not STUDIO_TOKEN:
        print("Error: STUDIO_TOKEN env var is required.", file=sys.stderr)
        sys.exit(2)

    # Customize these as needed
    ORG = os.getenv("STUDIO_ORG")           # optional
    PROJECT = os.getenv("STUDIO_PROJECT")   # optional
    RUN_NAME = os.getenv("RUN_NAME")        # optional
    TAGS = os.getenv("RUN_TAGS")            # comma-separated, optional
    REV = os.getenv("DVC_REV", "workspace") # 'workspace' or a git/DVC revision

    # If you want to include specific DVC-tracked metric files, list them here.
    # Otherwise, the script will try to use DVCLive's dvclive/metrics.json if present.
    METRIC_PATHS = [p for p in os.getenv("DVC_METRICS", "").split(",") if p.strip()]
    PARAM_PATHS = [p for p in os.getenv("DVC_PARAMS", "").split(",") if p.strip()]

    try:
        payload = build_run_payload(
            repo_dir=".",
            rev=REV,
            params_paths=PARAM_PATHS or None,
            metric_paths=METRIC_PATHS or None,
            dvclive_dir=os.getenv("DVCLIVE_DIR", "dvclive"),
            org=ORG,
            project=PROJECT,
            name=RUN_NAME,
            tags=[t.strip() for t in TAGS.split(",")] if TAGS else None,
        )
    except Exception as e:
        print(f"Failed to build payload: {e}", file=sys.stderr)
        sys.exit(3)

    # Create client and send
    client = StudioClient(
        base_url=STUDIO_BASE_URL,
        token=STUDIO_TOKEN,
        # Adjust auth scheme/header if required by your Studio deployment
        auth_scheme=os.getenv("STUDIO_AUTH_SCHEME", "Bearer"),
        timeout=float(os.getenv("STUDIO_TIMEOUT", "20")),
    )

    # IMPORTANT: adjust endpoint below to match your Studio install.
    # Example placeholder: "/api/v1/runs/ingest"
    endpoint = os.getenv("STUDIO_INGEST_ENDPOINT", "/api/v1/runs/ingest")

    try:
        resp = client.post_run(payload, endpoint=endpoint)
        print(f"Posted run successfully. Status={resp.status_code}")
        print(resp.text)
    except requests.HTTPError as http_err:
        print(f"HTTP Error posting run: {http_err.response.status_code} {http_err.response.text}", file=sys.stderr)
        sys.exit(4)
    except Exception as e:
        print(f"Failed to post run: {e}", file=sys.stderr)
        sys.exit(5)

    # Optional: upload an artifact produced by your run
    # If your Studio API supports a separate artifact endpoint, you can do:
    #
    # artifact_path = "path/to/artifact.bin"
    # artifact_endpoint = "/api/v1/artifacts/upload"
    # try:
    #     r2 = client.upload_artifact(artifact_path, endpoint=artifact_endpoint, fields={"run_id": payload.run_id})
    #     print(f"Uploaded artifact. Status={r2.status_code}")
    # except Exception as e:
    #     print(f"Artifact upload failed: {e}", file=sys.stderr)
------------------------------------------------------------

PROMPT 253:
"Generate Python code that imports the necessary libraries and creates a SQLAlchemy dialect for interacting with a Databricks database, allowing users to execute SQL queries and manipulate data within the database."
RESPONSE:
# pip install databricks-sql-connector sqlalchemy pandas
# Set environment variables:
#   DATABRICKS_HOST      = e.g. "adb-1234567890123456.7.azuredatabricks.net"
#   DATABRICKS_HTTP_PATH = e.g. "/sql/1.0/warehouses/abcd1234efgh5678"
#   DATABRICKS_TOKEN     = your Databricks personal access token
# Optional:
#   DATABRICKS_CATALOG   = e.g. "hive_metastore" (default)
#   DATABRICKS_SCHEMA    = e.g. "default" (default)
#   DATABRICKS_PORT      = "443" (default)

import os
from sqlalchemy import create_engine, text
from sqlalchemy.engine import URL
import pandas as pd


def make_databricks_engine() -> "sqlalchemy.Engine":
    """Create a SQLAlchemy Engine using the Databricks SQL Connector dialect."""
    host = os.environ["DATABRICKS_HOST"]
    http_path = os.environ["DATABRICKS_HTTP_PATH"]
    token = os.environ["DATABRICKS_TOKEN"]

    port = int(os.getenv("DATABRICKS_PORT", "443"))
    catalog = os.getenv("DATABRICKS_CATALOG", "hive_metastore")
    schema = os.getenv("DATABRICKS_SCHEMA", "default")

    # The Databricks SQL Connector exposes a SQLAlchemy dialect named "databricks+connector"
    # We pass username="token" and the access token as the password.
    url = URL.create(
        "databricks+connector",
        username="token",
        password=token,
        host=host,
        port=port,
        database=None,  # database is not used; catalog/schema are provided via query params
        query={
            "http_path": http_path,
            "catalog": catalog,
            "schema": schema,
        },
    )

    # future=True uses SQLAlchemy 2.0 style behavior
    engine = create_engine(url, future=True)
    return engine


def example_usage():
    engine = make_databricks_engine()

    # Run a simple SELECT
    with engine.connect() as conn:
        result = conn.execute(
            text("SELECT current_catalog() AS catalog, current_schema() AS schema, current_user() AS user")
        )
        print("Session info:", result.mappings().all())

    # Create a table, insert rows, query back with pandas
    with engine.begin() as conn:
        # Create a Delta table in the current catalog/schema
        conn.execute(
            text(
                """
                CREATE TABLE IF NOT EXISTS people (
                    id INT,
                    name STRING
                ) USING delta
                """
            )
        )

        # Parameterized insert
        rows = [{"id": 1, "name": "Alice"}, {"id": 2, "name": "Bob"}]
        conn.execute(text("INSERT INTO people (id, name) VALUES (:id, :name)"), rows)

        # Read data into pandas
        df = pd.read_sql(text("SELECT * FROM people ORDER BY id"), conn)
        print(df)

        # Append via pandas (table must exist)
        df_new = pd.DataFrame([{"id": 3, "name": "Carol"}])
        df_new.to_sql("people", con=conn, if_exists="append", index=False)

        # Verify append
        df_all = pd.read_sql(text("SELECT * FROM people ORDER BY id"), conn)
        print(df_all)

        # Optional cleanup
        # conn.execute(text("DROP TABLE IF EXISTS people"))


if __name__ == "__main__":
    example_usage()
------------------------------------------------------------

PROMPT 254:
"Generate Python code that creates a scalable, fast, ACID-compliant data catalog using Ray's actor model and async workflows, allowing for efficient metadata management and data discovery across distributed datasets."
RESPONSE:
# Requirements:
#   pip install "ray[default]" aiosqlite
#
# This module implements a scalable, fast, ACID-compliant data catalog using:
#   - Ray async actors for concurrency and horizontal sharding
#   - SQLite in WAL mode for ACID guarantees per shard
#   - FTS5 for efficient full-text search across dataset metadata
#   - Async transactions (begin/commit/rollback) scoped to a shard
#
# Notes:
#   - ACID guarantees are per shard. Cross-shard atomic transactions are not supported in SQLite.
#   - The router automatically dispatches operations to shards using a stable hash of namespace or dataset_id.
#   - Search queries fan out across shards and results are merged and ranked.
#   - For production, place each shard DB on fast persistent storage. Tune PRAGMAs as needed.

import asyncio
import dataclasses
import datetime as dt
import hashlib
import json
import os
import pathlib
import typing as t
import uuid

import aiosqlite
import ray


# -----------------------------
# Data models and serialization
# -----------------------------


@dataclasses.dataclass
class Dataset:
    id: str
    name: str
    namespace: str
    description: str
    location: str
    schema: t.Dict[str, t.Any]
    tags: t.List[str]
    properties: t.Dict[str, t.Any]
    size_bytes: int
    partition_count: int
    version: int
    created_at: str
    updated_at: str


@dataclasses.dataclass
class Partition:
    id: str
    dataset_id: str
    partition_key: str
    location: str
    size_bytes: int
    stats: t.Dict[str, t.Any]
    created_at: str


def utcnow_iso() -> str:
    return dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc).isoformat()


def _json_dumps(obj: t.Any) -> str:
    return json.dumps(obj, separators=(",", ":"), sort_keys=True)


def _json_loads(s: t.Optional[str]) -> t.Any:
    return json.loads(s) if s else None


# -----------------------------
# Catalog Shard (Async Ray Actor)
# -----------------------------


@ray.remote(num_cpus=0.25)
class CatalogShard:
    """
    A single-shard, ACID-compliant catalog implemented on SQLite (WAL).
    All operations are async and run in a Ray async actor for concurrency.
    """

    def __init__(self, db_path: str, shard_id: int):
        self.db_path = db_path
        self.shard_id = shard_id
        self._conn: t.Optional[aiosqlite.Connection] = None
        self._txns: dict[str, aiosqlite.Connection] = {}
        self._cache_datasets: dict[str, t.Tuple[int, Dataset]] = {}  # dataset_id -> (version, Dataset)
        self._init_lock = asyncio.Lock()

    async def _ensure_init(self):
        async with self._init_lock:
            if self._conn is not None:
                return
            os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
            self._conn = await aiosqlite.connect(self.db_path, isolation_level=None, timeout=30)
            await self._conn.execute("PRAGMA journal_mode=WAL;")
            await self._conn.execute("PRAGMA synchronous=FULL;")
            await self._conn.execute("PRAGMA foreign_keys=ON;")
            await self._conn.execute("PRAGMA temp_store=MEMORY;")
            await self._conn.execute("PRAGMA cache_size=-200000;")  # ~200MB page cache (negative => KB)
            await self._create_schema()
            await self._conn.commit()

    async def _create_schema(self):
        assert self._conn is not None
        c = self._conn
        await c.execute("""
        CREATE TABLE IF NOT EXISTS datasets(
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            namespace TEXT NOT NULL,
            description TEXT DEFAULT '',
            location TEXT NOT NULL,
            schema_json TEXT NOT NULL,
            properties_json TEXT DEFAULT '{}',
            size_bytes INTEGER DEFAULT 0,
            partition_count INTEGER DEFAULT 0,
            version INTEGER NOT NULL DEFAULT 1,
            created_at TEXT NOT NULL,
            updated_at TEXT NOT NULL
        );
        """)
        await c.execute("CREATE INDEX IF NOT EXISTS idx_datasets_ns_name ON datasets(namespace, name);")
        await c.execute("CREATE INDEX IF NOT EXISTS idx_datasets_updated ON datasets(updated_at);")

        await c.execute("""
        CREATE TABLE IF NOT EXISTS dataset_tags(
            dataset_id TEXT NOT NULL,
            tag TEXT NOT NULL,
            PRIMARY KEY(dataset_id, tag),
            FOREIGN KEY(dataset_id) REFERENCES datasets(id) ON DELETE CASCADE
        );
        """)
        await c.execute("CREATE INDEX IF NOT EXISTS idx_tags_tag ON dataset_tags(tag);")

        await c.execute("""
        CREATE TABLE IF NOT EXISTS partitions(
            id TEXT PRIMARY KEY,
            dataset_id TEXT NOT NULL,
            partition_key TEXT NOT NULL,
            location TEXT NOT NULL,
            size_bytes INTEGER DEFAULT 0,
            stats_json TEXT DEFAULT '{}',
            created_at TEXT NOT NULL,
            FOREIGN KEY(dataset_id) REFERENCES datasets(id) ON DELETE CASCADE
        );
        """)
        await c.execute("CREATE INDEX IF NOT EXISTS idx_partitions_ds_key ON partitions(dataset_id, partition_key);")

        # FTS for discovery (name, description, tags_agg)
        await c.execute("""
        CREATE VIRTUAL TABLE IF NOT EXISTS datasets_fts USING fts5(
            name, description, tags, content='',
            tokenize = 'porter'
        );
        """)
        await c.execute("""
        CREATE TABLE IF NOT EXISTS fts_meta(
            key TEXT PRIMARY KEY,
            value TEXT
        );
        """)

    async def _update_fts_for_dataset(self, conn: aiosqlite.Connection, dataset_id: str):
        # Rebuild the FTS row for a given dataset_id
        async with conn.execute("""
            SELECT name, description, (
                SELECT GROUP_CONCAT(tag, ' ')
                FROM dataset_tags WHERE dataset_id = ?
            )
            FROM datasets WHERE id = ?
        """, (dataset_id, dataset_id)) as cur:
            row = await cur.fetchone()
        if row is None:
            # delete any residual entry
            await conn.execute("DELETE FROM datasets_fts WHERE rowid IN (SELECT rowid FROM datasets_fts WHERE name IS NULL AND description IS NULL AND tags IS NULL);")
            return
        name, description, tags_agg = row
        # Delete any existing row for this dataset by using a shadow key in meta table (map dataset_id->rowid)
        # Since FTS table doesn't store dataset_id, use a simple approach: delete row matching name/description/tags
        await conn.execute("DELETE FROM datasets_fts WHERE name = ? AND description = ?;", (name, description))
        await conn.execute("INSERT INTO datasets_fts(name, description, tags) VALUES (?, ?, ?);", (name, description or "", tags_agg or ""))

    async def _get_conn_for_txn(self, txn_id: t.Optional[str]) -> tuple[aiosqlite.Connection, bool]:
        await self._ensure_init()
        assert self._conn is not None
        if txn_id is None:
            # Autocommit mode: create a write transaction around the operation when needed
            return self._conn, True
        if txn_id not in self._txns:
            raise RuntimeError(f"Unknown transaction id {txn_id} on shard {self.shard_id}")
        return self._txns[txn_id], False

    # --------- Transaction API ----------

    async def begin(self) -> str:
        await self._ensure_init()
        txn_id = str(uuid.uuid4())
        conn = await aiosqlite.connect(self.db_path, isolation_level=None, timeout=30)
        await conn.execute("PRAGMA journal_mode=WAL;")
        await conn.execute("PRAGMA synchronous=FULL;")
        await conn.execute("PRAGMA foreign_keys=ON;")
        await conn.execute("BEGIN IMMEDIATE;")
        self._txns[txn_id] = conn
        return txn_id

    async def commit(self, txn_id: str):
        conn = self._txns.pop(txn_id, None)
        if conn is None:
            raise RuntimeError(f"Unknown transaction id {txn_id}")
        try:
            await conn.commit()
        finally:
            await conn.close()

    async def rollback(self, txn_id: str):
        conn = self._txns.pop(txn_id, None)
        if conn is None:
            return
        try:
            await conn.rollback()
        finally:
            await conn.close()

    # --------- Catalog Ops ----------

    async def create_dataset(
        self,
        name: str,
        namespace: str,
        location: str,
        schema: t.Dict[str, t.Any],
        description: str = "",
        tags: t.Optional[t.List[str]] = None,
        properties: t.Optional[t.Dict[str, t.Any]] = None,
        size_bytes: int = 0,
        txn_id: t.Optional[str] = None,
    ) -> str:
        conn, autocommit = await self._get_conn_for_txn(txn_id)
        ds_id = str(uuid.uuid4())
        now = utcnow_iso()
        try:
            await conn.execute("BEGIN IMMEDIATE;") if autocommit else None
            await conn.execute(
                """
                INSERT INTO datasets(id, name, namespace, description, location,
                   schema_json, properties_json, size_bytes, partition_count,
                   version, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 1, ?, ?);
                """,
                (
                    ds_id, name, namespace, description or "", location,
                    _json_dumps(schema),
                    _json_dumps(properties or {}),
                    int(size_bytes), 0, now, now,
                ),
            )
            if tags:
                await conn.executemany(
                    "INSERT OR IGNORE INTO dataset_tags(dataset_id, tag) VALUES (?, ?);",
                    [(ds_id, t_) for t_ in set(tags)],
                )
            await self._update_fts_for_dataset(conn, ds_id)
            if autocommit:
                await conn.commit()
            # cache
            self._cache_datasets.pop(ds_id, None)
            return ds_id
        except Exception:
            if autocommit:
                await conn.rollback()
            raise

    async def get_dataset(self, dataset_id: str) -> t.Optional[Dataset]:
        await self._ensure_init()
        # check cache
        cached = self._cache_datasets.get(dataset_id)
        if cached:
            return cached[1]

        assert self._conn is not None
        async with self._conn.execute(
            """
            SELECT id, name, namespace, description, location, schema_json,
                   properties_json, size_bytes, partition_count, version,
                   created_at, updated_at
            FROM datasets WHERE id = ?;
            """,
            (dataset_id,),
        ) as cur:
            row = await cur.fetchone()

        if not row:
            return None

        async with self._conn.execute(
            "SELECT tag FROM dataset_tags WHERE dataset_id = ? ORDER BY tag ASC;", (dataset_id,)
        ) as cur:
            tag_rows = await cur.fetchall()
        tags = [tr[0] for tr in tag_rows]

        ds = Dataset(
            id=row[0],
            name=row[1],
            namespace=row[2],
            description=row[3],
            location=row[4],
            schema=_json_loads(row[5]),
            properties=_json_loads(row[6]) or {},
            size_bytes=row[7],
            partition_count=row[8],
            version=row[9],
            created_at=row[10],
            updated_at=row[11],
            tags=tags,
        )
        # cache with version
        self._cache_datasets[dataset_id] = (ds.version, ds)
        return ds

    async def update_dataset(
        self,
        dataset_id: str,
        changes: t.Dict[str, t.Any],
        expected_version: t.Optional[int] = None,
        txn_id: t.Optional[str] = None,
    ) -> int:
        # returns new version
        conn, autocommit = await self._get_conn_for_txn(txn_id)
        fields = []
        params: list[t.Any] = []
        allowed = {"name", "namespace", "description", "location", "schema", "properties", "size_bytes"}
        now = utcnow_iso()

        if "schema" in changes:
            fields.append("schema_json = ?")
            params.append(_json_dumps(changes["schema"]))
        if "properties" in changes:
            fields.append("properties_json = ?")
            params.append(_json_dumps(changes["properties"]))
        for k in ("name", "namespace", "description", "location", "size_bytes"):
            if k in changes:
                if k not in allowed:
                    continue
                fields.append(f"{k} = ?")
                params.append(changes[k])

        if not fields and "tags" not in changes:
            return await self.get_dataset_version(dataset_id)

        try:
            await conn.execute("BEGIN IMMEDIATE;") if autocommit else None

            # versioned update
            where_clause = "id = ?"
            params_where = [dataset_id]
            if expected_version is not None:
                where_clause += " AND version = ?"
                params_where.append(expected_version)
            set_clause = ", ".join(fields + ["version = version + 1", "updated_at = ?"])
            params.append(now)

            if fields:
                res = await conn.execute(
                    f"UPDATE datasets SET {set_clause} WHERE {where_clause};", params + params_where
                )
                if res.rowcount == 0:
                    raise RuntimeError("Concurrent update detected or dataset not found")

            # tags
            if "tags" in changes:
                tags = set(changes["tags"] or [])
                # delete missing, insert new
                await conn.execute(
                    "DELETE FROM dataset_tags WHERE dataset_id = ? AND tag NOT IN ({seq});".format(
                        seq=",".join("?" * len(tags)) if tags else "''"
                    ),
                    (dataset_id, *tags) if tags else (dataset_id,),
                )
                if tags:
                    await conn.executemany(
                        "INSERT OR IGNORE INTO dataset_tags(dataset_id, tag) VALUES (?, ?);",
                        [(dataset_id, t_) for t_ in tags],
                    )

            await self._update_fts_for_dataset(conn, dataset_id)
            if autocommit:
                await conn.commit()

            # invalidate cache
            self._cache_datasets.pop(dataset_id, None)
            # return current version
            return await self.get_dataset_version(dataset_id)
        except Exception:
            if autocommit:
                await conn.rollback()
            raise

    async def get_dataset_version(self, dataset_id: str) -> int:
        await self._ensure_init()
        assert self._conn is not None
        async with self._conn.execute("SELECT version FROM datasets WHERE id = ?;", (dataset_id,)) as cur:
            row = await cur.fetchone()
        if not row:
            raise KeyError(dataset_id)
        return int(row[0])

    async def add_tags(self, dataset_id: str, tags: t.List[str], txn_id: t.Optional[str] = None):
        if not tags:
            return
        conn, autocommit = await self._get_conn_for_txn(txn_id)
        try:
            await conn.execute("BEGIN IMMEDIATE;") if autocommit else None
            await conn.executemany(
                "INSERT OR IGNORE INTO dataset_tags(dataset_id, tag) VALUES (?, ?);",
                [(dataset_id, t_) for t_ in set(tags)],
            )
            await self._update_fts_for_dataset(conn, dataset_id)
            if autocommit:
                await conn.commit()
            self._cache_datasets.pop(dataset_id, None)
        except Exception:
            if autocommit:
                await conn.rollback()
            raise

    async def remove_tags(self, dataset_id: str, tags: t.List[str], txn_id: t.Optional[str] = None):
        if not tags:
            return
        conn, autocommit = await self._get_conn_for_txn(txn_id)
        try:
            await conn.execute("BEGIN IMMEDIATE;") if autocommit else None
            await conn.executemany(
                "DELETE FROM dataset_tags WHERE dataset_id = ? AND tag = ?;",
                [(dataset_id, t_) for t_ in set(tags)],
            )
            await self._update_fts_for_dataset(conn, dataset_id)
            if autocommit:
                await conn.commit()
            self._cache_datasets.pop(dataset_id, None)
        except Exception:
            if autocommit:
                await conn.rollback()
            raise

    async def register_partitions(
        self,
        dataset_id: str,
        partitions: t.List[dict],
        txn_id: t.Optional[str] = None,
    ) -> int:
        """
        Upsert a list of partitions for a dataset. Returns new partition_count.
        Partition dict keys: partition_key, location, size_bytes, stats (dict)
        """
        conn, autocommit = await self._get_conn_for_txn(txn_id)
        now = utcnow_iso()
        try:
            await conn.execute("BEGIN IMMEDIATE;") if autocommit else None
            rows = []
            for p in partitions:
                pid = str(uuid.uuid4())
                rows.append((
                    pid, dataset_id, p["partition_key"], p["location"],
                    int(p.get("size_bytes", 0)), _json_dumps(p.get("stats", {})), now
                ))
            if rows:
                await conn.executemany(
                    """
                    INSERT INTO partitions(id, dataset_id, partition_key, location,
                       size_bytes, stats_json, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?);
                    """,
                    rows,
                )
                # update dataset counters
                await conn.execute(
                    "UPDATE datasets SET partition_count = partition_count + ?, size_bytes = size_bytes + ?, version = version + 1, updated_at = ? WHERE id = ?;",
                    (len(rows), sum(r[4] for r in rows), now, dataset_id),
                )
            await self._update_fts_for_dataset(conn, dataset_id)
            if autocommit:
                await conn.commit()
            self._cache_datasets.pop(dataset_id, None)
            return await self._current_partition_count(dataset_id, conn)
        except Exception:
            if autocommit:
                await conn.rollback()
            raise

    async def _current_partition_count(self, dataset_id: str, conn: aiosqlite.Connection) -> int:
        async with conn.execute("SELECT partition_count FROM datasets WHERE id = ?;", (dataset_id,)) as cur:
            row = await cur.fetchone()
            return int(row[0]) if row else 0

    async def list_datasets(
        self,
        namespace: t.Optional[str] = None,
        tag: t.Optional[str] = None,
        limit: int = 100,
        offset: int = 0,
        order_by: str = "updated_at DESC",
    ) -> list[dict]:
        await self._ensure_init()
        assert self._conn is not None
        params: list[t.Any] = []
        where = []
        if namespace:
            where.append("d.namespace = ?")
            params.append(namespace)
        if tag:
            where.append("EXISTS (SELECT 1 FROM dataset_tags t WHERE t.dataset_id = d.id AND t.tag = ?)")
            params.append(tag)
        where_clause = ("WHERE " + " AND ".join(where)) if where else ""
        q = f"""
        SELECT d.id, d.name, d.namespace, d.description, d.location, d.size_bytes,
               d.partition_count, d.version, d.created_at, d.updated_at
        FROM datasets d
        {where_clause}
        ORDER BY {order_by}
        LIMIT ? OFFSET ?;
        """
        params += [limit, offset]
        async with self._conn.execute(q, tuple(params)) as cur:
            rows = await cur.fetchall()
        res = []
        for r in rows:
            res.append({
                "id": r[0], "name": r[1], "namespace": r[2], "description": r[3],
                "location": r[4], "size_bytes": r[5], "partition_count": r[6],
                "version": r[7], "created_at": r[8], "updated_at": r[9],
            })
        return res

    async def search(
        self,
        query: str,
        namespace: t.Optional[str] = None,
        limit: int = 50,
        offset: int = 0,
    ) -> list[dict]:
        await self._ensure_init()
        assert self._conn is not None
        # Use FTS with bm25 ranking
        # If namespace is specified, filter by namespace by joining to datasets table
        if namespace:
            sql = """
            SELECT d.id, d.name, d.namespace, d.description, d.location,
                   d.size_bytes, d.partition_count, d.version, d.created_at, d.updated_at,
                   bm25(datasets_fts) AS score
            FROM datasets_fts
            JOIN datasets d ON d.name = datasets_fts.name AND d.description = datasets_fts.description
            WHERE datasets_fts MATCH ?
              AND d.namespace = ?
            ORDER BY score LIMIT ? OFFSET ?;
            """
            params = (query, namespace, limit, offset)
        else:
            sql = """
            SELECT d.id, d.name, d.namespace, d.description, d.location,
                   d.size_bytes, d.partition_count, d.version, d.created_at, d.updated_at,
                   bm25(datasets_fts) AS score
            FROM datasets_fts
            JOIN datasets d ON d.name = datasets_fts.name AND d.description = datasets_fts.description
            WHERE datasets_fts MATCH ?
            ORDER BY score LIMIT ? OFFSET ?;
            """
            params = (query, limit, offset)
        async with self._conn.execute(sql, params) as cur:
            rows = await cur.fetchall()
        results = []
        for r in rows:
            results.append({
                "id": r[0], "name": r[1], "namespace": r[2], "description": r[3],
                "location": r[4], "size_bytes": r[5], "partition_count": r[6],
                "version": r[7], "created_at": r[8], "updated_at": r[9],
                "score": float(r[10]),
            })
        return results


# -----------------------------
# Router (Sharded Catalog)
# -----------------------------


def _hash_key(s: str) -> int:
    return int(hashlib.blake2b(s.encode("utf-8"), digest_size=4).hexdigest(), 16)


@ray.remote(num_cpus=0.1)
class CatalogRouter:
    """
    A router that shards catalog operations across N shards using a stable hash of namespace/dataset_id.
    ACID is guaranteed per shard (SQLite transactions). Cross-shard ACID is not supported.
    """

    def __init__(self, base_dir: str, num_shards: int = 4):
        self.base_dir = pathlib.Path(base_dir)
        self.num_shards = max(1, int(num_shards))
        self.shards: list[ray.ActorHandle] = []
        for i in range(self.num_shards):
            db_path = str(self.base_dir / f"shard_{i}.db")
            shard = CatalogShard.options(name=f"catalog_shard_{i}", lifetime="detached").remote(db_path, i)
            self.shards.append(shard)

    # ---- Shard selection helpers ----

    def _shard_for_namespace(self, namespace: str) -> ray.ActorHandle:
        idx = _hash_key(namespace) % self.num_shards
        return self.shards[idx]

    def _shard_for_dataset(self, dataset_id: str) -> ray.ActorHandle:
        idx = _hash_key(dataset_id) % self.num_shards
        return self.shards[idx]

    def _parse_txn_id(self, txn_id: str) -> tuple[int, str]:
        # router-scoped txn id: "{shard_idx}:{uuid}"
        parts = txn_id.split(":", 1)
        if len(parts) != 2:
            raise RuntimeError("Invalid transaction id")
        return int(parts[0]), parts[1]

    # ---- Transactions (per shard) ----

    async def begin(self, namespace_or_dataset: str, is_dataset_id: bool = False) -> str:
        shard = self._shard_for_dataset(namespace_or_dataset) if is_dataset_id else self._shard_for_namespace(namespace_or_dataset)
        inner = await shard.begin.remote()
        # embed shard index to route follow-up operations
        idx = self.shards.index(shard)
        return f"{idx}:{inner}"

    async def commit(self, txn_id: str):
        idx, inner = self._parse_txn_id(txn_id)
        shard = self.shards[idx]
        await shard.commit.remote(inner)

    async def rollback(self, txn_id: str):
        idx, inner = self._parse_txn_id(txn_id)
        shard = self.shards[idx]
        await shard.rollback.remote(inner)

    # ---- Catalog API (delegated) ----

    async def create_dataset(
        self,
        name: str,
        namespace: str,
        location: str,
        schema: dict,
        description: str = "",
        tags: t.Optional[list[str]] = None,
        properties: t.Optional[dict] = None,
        size_bytes: int = 0,
        txn_id: t.Optional[str] = None,
    ) -> str:
        shard = self._shard_for_namespace(namespace)
        inner = None
        if txn_id:
            idx, inner = self._parse_txn_id(txn_id)
            if self.shards[idx] != shard:
                raise RuntimeError("Transaction routed to different shard than namespace")
        ds_id = await shard.create_dataset.remote(
            name, namespace, location, schema, description, tags, properties, size_bytes, inner
        )
        return ds_id

    async def get_dataset(self, dataset_id: str) -> t.Optional[dict]:
        shard = self._shard_for_dataset(dataset_id)
        ds = await shard.get_dataset.remote(dataset_id)
        if ds is None:
            return None
        return dataclasses.asdict(ds)

    async def update_dataset(
        self,
        dataset_id: str,
        changes: dict,
        expected_version: t.Optional[int] = None,
        txn_id: t.Optional[str] = None,
    ) -> int:
        shard = self._shard_for_dataset(dataset_id)
        inner = None
        if txn_id:
            idx, inner = self._parse_txn_id(txn_id)
            if self.shards[idx] != shard:
                raise RuntimeError("Transaction routed to different shard than dataset")
        new_ver = await shard.update_dataset.remote(dataset_id, changes, expected_version, inner)
        return new_ver

    async def add_tags(self, dataset_id: str, tags: list[str], txn_id: t.Optional[str] = None):
        shard = self._shard_for_dataset(dataset_id)
        inner = None
        if txn_id:
            idx, inner = self._parse_txn_id(txn_id)
            if self.shards[idx] != shard:
                raise RuntimeError("Transaction routed to different shard than dataset")
        await shard.add_tags.remote(dataset_id, tags, inner)

    async def remove_tags(self, dataset_id: str, tags: list[str], txn_id: t.Optional[str] = None):
        shard = self._shard_for_dataset(dataset_id)
        inner = None
        if txn_id:
            idx, inner = self._parse_txn_id(txn_id)
            if self.shards[idx] != shard:
                raise RuntimeError("Transaction routed to different shard than dataset")
        await shard.remove_tags.remote(dataset_id, tags, inner)

    async def register_partitions(
        self, dataset_id: str, partitions: list[dict], txn_id: t.Optional[str] = None
    ) -> int:
        shard = self._shard_for_dataset(dataset_id)
        inner = None
        if txn_id:
            idx, inner = self._parse_txn_id(txn_id)
            if self.shards[idx] != shard:
                raise RuntimeError("Transaction routed to different shard than dataset")
        return await shard.register_partitions.remote(dataset_id, partitions, inner)

    async def list_datasets(
        self,
        namespace: t.Optional[str] = None,
        tag: t.Optional[str] = None,
        limit: int = 100,
        offset: int = 0,
        order_by: str = "updated_at DESC",
    ) -> list[dict]:
        # If namespace is given, it maps to a single shard and we can paginate there.
        if namespace:
            shard = self._shard_for_namespace(namespace)
            return await shard.list_datasets.remote(namespace, tag, limit, offset, order_by)

        # Otherwise, fan out and merge results across shards.
        # Since each shard applies its own LIMIT/OFFSET, global pagination is approximate.
        # For a true global pagination, implement keyset pagination with a global merge.
        tasks = [
            shard.list_datasets.remote(None, tag, limit, offset, order_by)
            for shard in self.shards
        ]
        per_shard = await asyncio.gather(*tasks)
        merged = [item for sub in per_shard for item in sub]
        # Re-sort and globally paginate
        reverse = "DESC" in order_by.upper()
        key = order_by.split()[0]
        merged.sort(key=lambda r: r.get(key), reverse=reverse)
        return merged[:limit]

    async def search(
        self,
        query: str,
        namespace: t.Optional[str] = None,
        limit: int = 50,
        offset: int = 0,
    ) -> list[dict]:
        if namespace:
            shard = self._shard_for_namespace(namespace)
            return await shard.search.remote(query, namespace, limit, offset)
        # fan out
        tasks = [shard.search.remote(query, None, limit, offset) for shard in self.shards]
        results = await asyncio.gather(*tasks)
        merged = [x for sub in results for x in sub]
        # smaller score is better with bm25 in SQLite (by default), so sort ascending
        merged.sort(key=lambda r: r.get("score", 0.0))
        return merged[:limit]


# -----------------------------
# Example async workflow utilities
# -----------------------------


@ray.remote(num_cpus=0.5)
def compute_dataset_stats(locations: list[str]) -> dict:
    # Placeholder: compute size and some stats (simulate)
    total = 0
    for _ in locations:
        total += 1024 * 1024  # 1MB each (dummy)
    return {"row_count": len(locations) * 1000, "total_bytes": total}


async def async_register_dataset_with_scan(
    router: ray.ActorHandle,
    name: str,
    namespace: str,
    base_location: str,
    schema: dict,
    description: str,
    tags: list[str],
    partition_keys: list[str],
):
    # Fan-out: compute per-partition stats in parallel, then register transactionally
    partition_locations = [f"{base_location}/{k}" for k in partition_keys]
    stats_ref = compute_dataset_stats.remote(partition_locations)
    stats = await ray.get(stats_ref)

    txn = await router.begin.remote(namespace, False)
    try:
        ds_id = await router.create_dataset.remote(
            name=name,
            namespace=namespace,
            location=base_location,
            schema=schema,
            description=description,
            tags=tags,
            properties={"created_by": "async_workflow", "row_count": stats["row_count"]},
            size_bytes=stats["total_bytes"],
            txn_id=txn,
        )
        parts = []
        for k in partition_keys:
            parts.append({
                "partition_key": k,
                "location": f"{base_location}/{k}",
                "size_bytes": 1024 * 1024,
                "stats": {"files": 1, "compression": "none"},
            })
        await router.register_partitions.remote(ds_id, parts, txn)
        await router.commit.remote(txn)
        return ds_id
    except Exception:
        await router.rollback.remote(txn)
        raise


# -----------------------------
# Demonstration (if run as script)
# -----------------------------


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Ray-based ACID data catalog demo")
    parser.add_argument("--dir", type=str, default="./catalog_data", help="Directory for shard DB files")
    parser.add_argument("--shards", type=int, default=4, help="Number of shards")
    args = parser.parse_args()

    ray.init(ignore_reinit_error=True)

    router = CatalogRouter.options(name="catalog_router", lifetime="detached").remote(args.dir, args.shards)

    async def demo():
        ns = "analytics"
        ds_id = await async_register_dataset_with_scan(
            router=router,
            name="events",
            namespace=ns,
            base_location="s3://bucket/path/events",
            schema={"fields": [{"name": "user_id", "type": "string"}, {"name": "ts", "type": "timestamp"}]},
            description="User events dataset",
            tags=["pii", "raw"],
            partition_keys=[f"dt=2025-10-{d:02d}" for d in range(1, 4)],
        )
        print("Created dataset:", ds_id)

        # Read back
        ds = await router.get_dataset.remote(ds_id)
        print("Dataset metadata:", ds)

        # Update tags and description
        ver = await router.update_dataset.remote(ds_id, {"description": "Updated desc"}, expected_version=ds["version"])
        print("New version:", ver)
        await router.add_tags.remote(ds_id, ["important"])

        # Search
        res = await router.search.remote("user events")
        print("Search results:", res)

        # List by namespace
        lst = await router.list_datasets.remote(namespace=ns, limit=10)
        print("List datasets:", lst)

    asyncio.get_event_loop().run_until_complete(demo())

    ray.shutdown()
------------------------------------------------------------

PROMPT 255:
"Generate Python code that imports the required modules, loads the image data into a suitable format, performs appropriate preprocessing steps, and uses ITK functions to analyze the image data in multiple dimensions."
RESPONSE:
import argparse
import os
import sys
from typing import Optional

import numpy as np
import itk


def read_image_as_float(path: str):
    """
    Read an image using ITK and return a floating-point image with preserved dimension.
    Supports common formats (e.g., NIfTI, NRRD, MHA, MHD, TIFF, PNG).
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"Input path does not exist: {path}")
    # Read directly as float (single component), preserving dimension automatically
    img = itk.imread(path, pixel_type=itk.F)
    return img


def resample_to_isotropic(image, target_spacing: Optional[float] = None):
    """
    Resample an N-D image to isotropic spacing using linear interpolation.
    If target_spacing is None, use the minimum of the original spacings (upsampling).
    """
    dim = image.GetImageDimension()
    spacing = np.array(itk.spacing(image), dtype=float)
    origin = itk.origin(image)
    direction = itk.direction(image)
    size = np.array(itk.size(image), dtype=int)

    if target_spacing is None:
        target_spacing = float(np.min(spacing))

    new_spacing = np.array([target_spacing] * dim, dtype=float)
    new_size = np.maximum(
        1,
        np.round(size * (spacing / new_spacing)).astype(int)
    )

    identity = itk.IdentityTransform[itk.D, dim].New()
    interpolator = itk.LinearInterpolateImageFunction.New(image)

    resampled = itk.resample_image_filter(
        image,
        Transform=identity,
        Interpolator=interpolator,
        Size=tuple(int(x) for x in new_size),
        OutputSpacing=tuple(float(x) for x in new_spacing),
        OutputOrigin=origin,
        OutputDirection=direction,
        DefaultPixelValue=0.0
    )
    return resampled


def normalize_to_unit(image):
    """
    Rescale intensities to [0, 1] for stable downstream processing.
    """
    return itk.rescale_intensity_image_filter(image, OutputMinimum=0.0, OutputMaximum=1.0)


def denoise_anisotropic_diffusion(image, iterations=5, time_step=0.0625, conductance=3.0):
    """
    Curvature Anisotropic Diffusion denoising (N-D).
    """
    return itk.curvature_anisotropic_diffusion_image_filter(
        image,
        NumberOfIterations=int(iterations),
        TimeStep=float(time_step),
        ConductanceParameter=float(conductance),
    )


def compute_gradient_magnitude(image, sigma=1.0):
    """
    N-D gradient magnitude using recursive Gaussian.
    """
    return itk.gradient_magnitude_recursive_gaussian_image_filter(image, Sigma=float(sigma))


def compute_vesselness(image, sigma=1.5, bright_object=True, alpha=0.5, beta=1.0, gamma=5.0):
    """
    N-D Hessian-based objectness measure (often used as a vesselness filter).
    For general blob/tubular/plate-like structures; works for 2D/3D+.
    """
    hessian = itk.hessian_recursive_gaussian_image_filter(image, Sigma=float(sigma))
    objectness = itk.hessian_to_objectness_measure_image_filter(
        hessian,
        BrightObject=bool(bright_object),
        ScaleObjectnessMeasure=False,
        Alpha=float(alpha),
        Beta=float(beta),
        Gamma=float(gamma),
    )
    return objectness


def segment_otsu(image):
    """
    Otsu thresholding to produce a binary mask (0/1).
    """
    # Otsu filter directly emits a binary-like image using Inside/Outside values
    binary = itk.otsu_threshold_image_filter(
        image,
        InsideValue=1,   # Foreground
        OutsideValue=0   # Background
    )
    # Ensure it is an unsigned char image for compact storage and some filters
    dim = image.GetImageDimension()
    out_type = itk.Image[itk.UC, dim]
    binary_uc = itk.cast_image_filter(binary, ttype=(type(binary), out_type))
    return binary_uc


def connected_components_and_relabel(binary_image_uc, min_size=50):
    """
    Connected components labeling and relabel components by size.
    """
    cc = itk.connected_component_image_filter(binary_image_uc)
    relabeled = itk.relabel_component_image_filter(cc, MinimumObjectSize=int(min_size))
    return relabeled


def compute_label_shape_statistics(label_image):
    """
    Compute shape statistics for labeled objects (N-D).
    Returns a dict keyed by label with some representative statistics.
    """
    stats = itk.LabelShapeStatisticsImageFilter.New(Input=label_image)
    stats.Update()

    results = {}
    labels = list(stats.GetLabels())
    for l in labels:
        entry = {}
        # Not all measures are guaranteed in all contexts; guard with try/except.
        try:
            entry["number_of_pixels"] = int(stats.GetNumberOfPixels(l))
        except Exception:
            pass
        try:
            entry["physical_size"] = float(stats.GetPhysicalSize(l))
        except Exception:
            pass
        try:
            entry["centroid"] = tuple(float(x) for x in stats.GetCentroid(l))
        except Exception:
            pass
        try:
            entry["bounding_box"] = tuple(int(x) for x in stats.GetBoundingBox(l))
        except Exception:
            pass
        results[int(l)] = entry
    return results


def save_image(image, out_path: str):
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    itk.imwrite(image, out_path)


def main():
    parser = argparse.ArgumentParser(description="N-D ITK image preprocessing and analysis pipeline.")
    parser.add_argument("input", type=str, help="Path to input image (e.g., .nii, .nrrd, .mha, .tif)")
    parser.add_argument("--outdir", type=str, default="outputs", help="Directory to save results")
    parser.add_argument("--prefix", type=str, default="result", help="Output filename prefix")

    # Preprocessing options
    parser.add_argument("--iso", type=float, default=None, help="Resample to isotropic spacing (e.g., 1.0 mm/pixel). If omitted, uses min original spacing.")
    parser.add_argument("--denoise_iters", type=int, default=5)
    parser.add_argument("--denoise_step", type=float, default=0.0625)
    parser.add_argument("--denoise_cond", type=float, default=3.0)

    # Analysis options
    parser.add_argument("--grad_sigma", type=float, default=1.0)
    parser.add_argument("--hessian_sigma", type=float, default=1.5)
    parser.add_argument("--vessel_bright", action="store_true", help="Assume bright structures on dark background for objectness")
    parser.add_argument("--min_obj", type=int, default=50, help="Min component size to keep during relabeling")

    args = parser.parse_args()

    # Load
    img = read_image_as_float(args.input)
    dim = img.GetImageDimension()
    print(f"Loaded image: dimension={dim}, size={tuple(itk.size(img))}, spacing={tuple(itk.spacing(img))}")

    # Optional isotropic resampling
    if args.iso is not None or (np.ptp(np.array(itk.spacing(img))) > 1e-6):
        img_iso = resample_to_isotropic(img, target_spacing=args.iso)
        print(f"Resampled to isotropic spacing: size={tuple(itk.size(img_iso))}, spacing={tuple(itk.spacing(img_iso))}")
    else:
        img_iso = img

    # Normalize and denoise
    img_norm = normalize_to_unit(img_iso)
    img_denoised = denoise_anisotropic_diffusion(
        img_norm,
        iterations=args.denoise_iters,
        time_step=args.denoise_step,
        conductance=args.denoise_cond,
    )

    # Multi-dimensional analysis
    grad_mag = compute_gradient_magnitude(img_denoised, sigma=args.grad_sigma)
    objectness = compute_vesselness(
        img_denoised,
        sigma=args.hessian_sigma,
        bright_object=args.vessel_bright,
        alpha=0.5,
        beta=1.0,
        gamma=5.0,
    )

    # Segmentation and components
    binary = segment_otsu(img_denoised)
    labels = connected_components_and_relabel(binary, min_size=args.min_obj)
    shape_stats = compute_label_shape_statistics(labels)

    # Save results
    base = os.path.join(args.outdir, args.prefix)
    save_image(img_denoised, f"{base}_denoised.mha")
    save_image(grad_mag, f"{base}_gradmag.mha")
    save_image(objectness, f"{base}_objectness.mha")
    save_image(binary, f"{base}_binary.mha")
    save_image(labels, f"{base}_labels.mha")

    # Print a brief summary of components
    print("Label shape statistics (first 10 labels):")
    for i, (lab, info) in enumerate(sorted(shape_stats.items(), key=lambda x: x[0])):
        print(f"  Label {lab}: {info}")
        if i >= 9:
            break

    print("Done.")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
------------------------------------------------------------

PROMPT 256:
"Generate Python code that imports the requests library and sends a GET request to the Trading Economics API to retrieve economic data, parsing the JSON response into a pandas DataFrame for analysis and visualization."
RESPONSE:
```python
import os
from urllib.parse import quote
import requests
import pandas as pd
import matplotlib.pyplot as plt

def fetch_te_data(country: str, indicator: str, api_key: str) -> pd.DataFrame:
    """
    Fetch historical economic data from Trading Economics and return a pandas DataFrame.
    - country: country name, e.g., 'United States'
    - indicator: indicator name, e.g., 'GDP'
    - api_key: 'user:password' or 'guest:guest' for sample access
    """
    base = "https://api.tradingeconomics.com/historical/country"
    url = f"{base}/{quote(country)}/indicator/{quote(indicator)}"
    params = {"c": api_key, "format": "json"}

    resp = requests.get(url, params=params, timeout=30)
    resp.raise_for_status()
    data = resp.json()

    if not isinstance(data, list) or len(data) == 0:
        raise ValueError(f"Unexpected or empty response: {data}")

    df = pd.DataFrame(data)

    # Normalize common column names
    date_col = next((c for c in ["DateTime", "Date", "date"] if c in df.columns), None)
    value_col = next((c for c in ["Value", "value"] if c in df.columns), None)

    if date_col:
        df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
        df = df.sort_values(date_col).set_index(date_col)

    if value_col:
        df[value_col] = pd.to_numeric(df[value_col], errors="coerce")

    return df

if __name__ == "__main__":
    # Set your API key as 'user:password' in TE_API_KEY, or use 'guest:guest' for sample data
    api_key = os.getenv("TE_API_KEY", "guest:guest")

    country = "United States"
    indicator = "GDP"

    df = fetch_te_data(country, indicator, api_key)

    print("Data sample:")
    print(df.head())

    # Simple visualization
    if "Value" in df.columns:
        ax = df["Value"].plot(title=f"{country} - {indicator} (Trading Economics)", figsize=(10, 4))
        ax.set_xlabel("Date")
        ax.set_ylabel(indicator)
        plt.tight_layout()
        plt.show()
```
------------------------------------------------------------

PROMPT 257:
"Generate Python code that defines a library with support functions for use in Odoo migration scripts, including methods for data cleaning, transformation, and loading, as well as tools for debugging and error handling."
RESPONSE:
# migration_utils.py
# Support library for Odoo data migrations:
# - Data cleaning and transformation helpers
# - Batch loading (create/update) with safety, logging, and dry-run support
# - Debugging, timing, and error handling tools

from __future__ import annotations

import contextlib
import csv
import datetime as dt
import functools
import hashlib
import html
import logging
import re
import time
import unicodedata
from typing import Any, Callable, Dict, Generator, Iterable, Iterator, List, Mapping, MutableMapping, Optional, Sequence, Tuple, Union

try:
    # Odoo imports (assumed available during migrations)
    from odoo import api, tools
    from odoo.modules.registry import Registry
except Exception:  # pragma: no cover - loaded only inside Odoo runtime
    api = None
    tools = None
    Registry = None

try:
    import psycopg2
    from psycopg2 import errors as pg_errors
except Exception:  # pragma: no cover
    psycopg2 = None
    pg_errors = None

try:
    # Odoo ships python-dateutil
    from dateutil import parser as dateutil_parser
    from dateutil import tz as dateutil_tz
except Exception:  # pragma: no cover
    dateutil_parser = None
    dateutil_tz = None


# ------------------------------------------------------------------------------
# Logging setup
# ------------------------------------------------------------------------------

def get_logger(name: str = "migration") -> logging.Logger:
    """
    Get or create a logger preconfigured for migrations.
    """
    logger = logging.getLogger(name)
    if not logger.handlers:
        handler = logging.StreamHandler()
        fmt = "%(asctime)s %(levelname)s [%(name)s] %(message)s"
        handler.setFormatter(logging.Formatter(fmt))
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        logger.propagate = False
    return logger


# ------------------------------------------------------------------------------
# Context managers and decorators
# ------------------------------------------------------------------------------

@contextlib.contextmanager
def time_block(name: str, logger: Optional[logging.Logger] = None) -> Iterator[None]:
    """
    Measure the time spent within the block and log it.
    """
    logger = logger or get_logger()
    start = time.perf_counter()
    try:
        yield
    finally:
        elapsed = time.perf_counter() - start
        logger.info("%s completed in %.3fs", name, elapsed)


@contextlib.contextmanager
def sql_debug(enabled: bool = True, level: int = logging.DEBUG) -> Iterator[None]:
    """
    Temporarily set SQL logger to DEBUG to see queries during the block.
    """
    sql_logger = logging.getLogger("odoo.sql_db")
    old_level = sql_logger.level
    if enabled:
        sql_logger.setLevel(level)
    try:
        yield
    finally:
        sql_logger.setLevel(old_level)


def retry_on_transient(
    attempts: int = 3,
    backoff: float = 0.5,
    max_backoff: float = 5.0,
    logger: Optional[logging.Logger] = None,
) -> Callable:
    """
    Decorator to retry function on transient PostgreSQL errors (deadlocks, serialization failures).
    """
    transient_codes = {
        # Serialization failure, Deadlock detected
        "40001", "40P01",
    }
    logger = logger or get_logger()

    def decorator(fn: Callable) -> Callable:
        @functools.wraps(fn)
        def wrapper(*args, **kwargs):
            delay = backoff
            last_exc = None
            for i in range(1, attempts + 1):
                try:
                    return fn(*args, **kwargs)
                except Exception as e:
                    last_exc = e
                    code = getattr(getattr(e, "pgcode", None), "value", None) if pg_errors else getattr(e, "pgcode", None)
                    if code is None and hasattr(e, "diag"):
                        code = getattr(e.diag, "sqlstate", None)
                    if code in transient_codes:
                        logger.warning("Transient DB error (attempt %s/%s): %s", i, attempts, e)
                        time.sleep(delay)
                        delay = min(delay * 2, max_backoff)
                        continue
                    raise
            raise last_exc
        return wrapper
    return decorator


# ------------------------------------------------------------------------------
# General utilities
# ------------------------------------------------------------------------------

def chunked(seq: Sequence[Any], size: int) -> Iterator[Sequence[Any]]:
    """
    Yield chunks of size 'size' from a sequence/list.
    """
    if size <= 0:
        raise ValueError("size must be > 0")
    for i in range(0, len(seq), size):
        yield seq[i:i + size]


def dedupe(seq: Iterable[Any], key: Optional[Callable[[Any], Any]] = None) -> List[Any]:
    """
    Remove duplicates while preserving order.
    """
    seen = set()
    out = []
    for item in seq:
        k = key(item) if key else item
        if k not in seen:
            seen.add(k)
            out.append(item)
    return out


# ------------------------------------------------------------------------------
# Data cleaning helpers
# ------------------------------------------------------------------------------

_WHITESPACE_RE = re.compile(r"\s+", re.UNICODE)
_EMAIL_RE = re.compile(r"^[^@]+@[^@]+\.[^@]+$")


def clean_text(
    value: Optional[str],
    collapse_whitespace: bool = True,
    strip: bool = True,
    normalize: Optional[str] = "NFKC",
) -> Optional[str]:
    """
    Normalize unicode, strip and collapse whitespace.
    """
    if value is None:
        return None
    text = str(value)
    if normalize:
        text = unicodedata.normalize(normalize, text)
    if strip:
        text = text.strip()
    if collapse_whitespace:
        text = _WHITESPACE_RE.sub(" ", text)
    return text


def null_if_blank(value: Optional[str]) -> Optional[str]:
    """
    Return None if the string is blank after cleaning.
    """
    v = clean_text(value)
    return v if v else None


def clamp_len(value: Optional[str], max_len: int, ellipsis: bool = False) -> Optional[str]:
    """
    Truncate string to max_len. Optionally add ellipsis if truncated.
    """
    if value is None:
        return None
    s = str(value)
    if len(s) <= max_len:
        return s
    return (s[: max_len - 1] + "…") if ellipsis and max_len > 1 else s[:max_len]


def strip_html(value: Optional[str]) -> Optional[str]:
    """
    Remove HTML tags and unescape entities. Minimalist approach for migration.
    """
    if value is None:
        return None
    text = re.sub(r"<[^>]+>", " ", str(value))
    text = html.unescape(text)
    return clean_text(text)


def normalize_email(value: Optional[str]) -> Optional[str]:
    """
    Lowercase and basic validation for emails.
    """
    v = null_if_blank(value)
    if not v:
        return None
    v = v.lower()
    if not _EMAIL_RE.match(v):
        return v  # Keep original; leave strict validation to business logic
    return v


def normalize_phone(value: Optional[str]) -> Optional[str]:
    """
    Basic phone normalization: keep digits plus leading '+'.
    """
    if value is None:
        return None
    s = str(value).strip()
    sign = "+" if s.startswith("+") else ""
    digits = re.sub(r"\D", "", s)
    out = f"{sign}{digits}" if digits else None
    return out


def to_bool(
    value: Any,
    truthy: Iterable[str] = ("1", "true", "yes", "y", "t"),
    falsy: Iterable[str] = ("0", "false", "no", "n", "f"),
    none_if_unknown: bool = True,
) -> Optional[bool]:
    """
    Convert common truthy/falsy strings to bool.
    """
    if value is None:
        return None
    if isinstance(value, bool):
        return value
    if isinstance(value, (int, float)):
        return bool(value)
    s = str(value).strip().lower()
    if s in truthy:
        return True
    if s in falsy:
        return False
    return None if none_if_unknown else bool(s)


def safe_int(value: Any, default: Optional[int] = None) -> Optional[int]:
    try:
        return int(str(value).strip()) if value is not None else default
    except Exception:
        return default


def safe_float(value: Any, default: Optional[float] = None) -> Optional[float]:
    try:
        return float(str(value).strip()) if value is not None else default
    except Exception:
        return default


def parse_date(value: Any, default_tz: str = "UTC") -> Optional[dt.date]:
    """
    Parse a date-like string into a date. Uses dateutil if available.
    """
    if value in (None, ""):
        return None
    if isinstance(value, dt.date) and not isinstance(value, dt.datetime):
        return value
    if isinstance(value, dt.datetime):
        return value.date()
    s = str(value).strip()
    if not s:
        return None
    if dateutil_parser:
        d = dateutil_parser.parse(s)
        if isinstance(d, dt.datetime) and d.tzinfo is None and dateutil_tz:
            d = d.replace(tzinfo=dateutil_tz.gettz(default_tz))
        return d.date()
    # Fallback for ISO
    try:
        return dt.date.fromisoformat(s)
    except Exception:
        return None


def parse_datetime(value: Any, default_tz: str = "UTC") -> Optional[dt.datetime]:
    """
    Parse a datetime-like string into an aware datetime.
    """
    if value in (None, ""):
        return None
    if isinstance(value, dt.datetime):
        if value.tzinfo is None and dateutil_tz:
            return value.replace(tzinfo=dateutil_tz.gettz(default_tz))
        return value
    s = str(value).strip()
    if not s:
        return None
    if dateutil_parser:
        d = dateutil_parser.parse(s)
        if d.tzinfo is None and dateutil_tz:
            d = d.replace(tzinfo=dateutil_tz.gettz(default_tz))
        return d
    # Fallback ISO
    try:
        d = dt.datetime.fromisoformat(s)
        if d.tzinfo is None and dateutil_tz:
            d = d.replace(tzinfo=dateutil_tz.gettz(default_tz))
        return d
    except Exception:
        return None


# ------------------------------------------------------------------------------
# Transformation helpers
# ------------------------------------------------------------------------------

def value_map(
    value: Any,
    mapping: Mapping[Any, Any],
    default: Any = None,
    casefold: bool = True,
    normalize: bool = True,
) -> Any:
    """
    Map value via dict mapping with optional casefold/normalize for string keys.
    """
    v = value
    if isinstance(v, str) and normalize:
        v = clean_text(v) or ""
    if isinstance(v, str) and casefold:
        v = v.casefold()
    # Prepare normalized mapping keys
    norm_map = {}
    for k, val in mapping.items():
        key = k
        if isinstance(key, str) and normalize:
            key = clean_text(key) or ""
        if isinstance(key, str) and casefold:
            key = key.casefold()
        norm_map[key] = val
    return norm_map.get(v, default)


def map_dict_keys(
    row: Mapping[str, Any],
    mapping: Mapping[str, Union[str, Callable[[Mapping[str, Any]], Any]]],
    drop_unmapped: bool = False,
) -> Dict[str, Any]:
    """
    Transform a dict using mapping of destination_field -> source_field or callable(row).
    """
    out: Dict[str, Any] = {}
    for dest, src in mapping.items():
        if callable(src):
            out[dest] = src(row)
        else:
            out[dest] = row.get(src)
    if not drop_unmapped:
        for k, v in row.items():
            if k not in mapping.values():
                out.setdefault(k, v)
    return out


def checksum(value: Union[str, bytes, Mapping[str, Any], Sequence[Any]]) -> str:
    """
    Compute a deterministic checksum for idempotency checks.
    """
    def _serialize(v: Any) -> bytes:
        if v is None:
            return b"null"
        if isinstance(v, bytes):
            return v
        if isinstance(v, str):
            return v.encode("utf-8", "ignore")
        if isinstance(v, (int, float, bool)):
            return str(v).encode("ascii")
        if isinstance(v, dt.date):
            return v.isoformat().encode("ascii")
        if isinstance(v, Mapping):
            items = sorted((str(k), _serialize(v2)) for k, v2 in v.items())
            data = b"{%b}" % b",".join(b"%s:%b" % (k.encode("utf-8"), v2) for k, v2 in items)
            return data
        if isinstance(v, Sequence) and not isinstance(v, (str, bytes)):
            data = b"[%b]" % b",".join(_serialize(x) for x in v)
            return data
        return str(v).encode("utf-8", "ignore")

    data = _serialize(value)
    return hashlib.sha1(data).hexdigest()


def m2m_commands(ids: Sequence[int]) -> List[Tuple[int, int, List[int]]]:
    """
    Build commands to set many2many field: [(6, 0, ids)].
    """
    return [(6, 0, list(ids))]


def o2m_create_commands(vals_list: Sequence[Dict[str, Any]]) -> List[Tuple[int, bool, Dict[str, Any]]]:
    """
    Build commands to create one2many records: [(0, 0, vals), ...]
    """
    return [(0, 0, dict(v)) for v in vals_list]


# ------------------------------------------------------------------------------
# Error handling and debugging
# ------------------------------------------------------------------------------

class RowError(Exception):
    """
    Exception with row context to help locate bad data during migrations.
    """
    def __init__(self, message: str, row: Optional[Mapping[str, Any]] = None, model: Optional[str] = None):
        super().__init__(message)
        self.row = dict(row) if row else None
        self.model = model

    def __str__(self):
        base = super().__str__()
        ctx = []
        if self.model:
            ctx.append(f"model={self.model}")
        if self.row:
            # Keep short
            preview = {k: self.row[k] for k in list(self.row.keys())[:6]}
            ctx.append(f"row={preview}")
        return f"{base} ({', '.join(ctx)})" if ctx else base


def with_row_context(model: Optional[str] = None) -> Callable:
    """
    Decorator to wrap function exceptions with RowError including the 'row' kwarg or first arg.
    """
    def decorator(fn: Callable) -> Callable:
        @functools.wraps(fn)
        def wrapper(*args, **kwargs):
            row = kwargs.get("row")
            if row is None and args:
                # Guess first arg as row for row-processing functions
                row = args[0] if isinstance(args[0], Mapping) else None
            try:
                return fn(*args, **kwargs)
            except RowError:
                raise
            except Exception as e:
                raise RowError(str(e), row=row, model=model) from e
        return wrapper
    return decorator


class ErrorCollector:
    """
    Helper to collect errors and continue processing.
    """
    def __init__(self):
        self.errors: List[Exception] = []

    def wrap(self, fn: Callable) -> Callable:
        @functools.wraps(fn)
        def _wrapped(*args, **kwargs):
            try:
                return fn(*args, **kwargs)
            except Exception as e:
                self.errors.append(e)
                return None
        return _wrapped

    def raise_if_any(self):
        if self.errors:
            raise RuntimeError(f"{len(self.errors)} migration error(s) occurred, inspect .errors for details")


def preview_diff(record, vals: Mapping[str, Any], fields: Optional[Sequence[str]] = None) -> Dict[str, Tuple[Any, Any]]:
    """
    Compute a diff of current vs new values for an Odoo record.
    Returns {field: (old, new)} for changed fields.
    """
    fields = fields or list(vals.keys())
    before = record.read(fields)[0] if record else {}
    diff = {}
    for f in fields:
        old = before.get(f)
        new = vals.get(f)
        if old != new:
            diff[f] = (old, new)
    return diff


# ------------------------------------------------------------------------------
# SQL helpers
# ------------------------------------------------------------------------------

def execute_sql(cr, sql: str, params: Optional[Sequence[Any]] = None, log_sql: bool = False, logger: Optional[logging.Logger] = None):
    """
    Execute raw SQL with optional logging. Returns cursor.
    """
    logger = logger or get_logger()
    if log_sql:
        logger.debug("SQL: %s | params=%s", sql, params)
    cr.execute(sql, params or ())
    return cr


def fetchall_dict(cr) -> List[Dict[str, Any]]:
    """
    Fetch all rows as list of dicts from the current cursor result.
    """
    cols = [d.name if hasattr(d, "name") else d[0] for d in cr.description]
    return [dict(zip(cols, row)) for row in cr.fetchall()]


# ------------------------------------------------------------------------------
# Migration helper class using Odoo ORM
# ------------------------------------------------------------------------------

class MigrationHelper:
    """
    High-level helpers for Odoo migration scripts.

    Usage:
      mh = MigrationHelper(env, dry_run=True)
      mh.batch_create('res.partner', [{'name': 'A'}, {'name': 'B'}])
    """
    def __init__(self, env, logger: Optional[logging.Logger] = None, dry_run: bool = False):
        self.env = env
        self.logger = logger or get_logger()
        self.dry_run = dry_run

    @property
    def cr(self):
        return self.env.cr

    @contextlib.contextmanager
    def savepoint(self, name: Optional[str] = None):
        """
        Use database savepoint for safer batch operations.
        """
        sp = self.cr.savepoint()
        with sp:
            yield

    def ensure_xmlid(self, model: str, record_id: int, module: str, name: str, noupdate: bool = True) -> None:
        """
        Ensure an external ID exists for a record.
        """
        imd = self.env["ir.model.data"]
        existing = imd.search([("module", "=", module), ("name", "=", name)], limit=1)
        if existing:
            return
        if self.dry_run:
            self.logger.info("DRY-RUN ensure xmlid %s.%s for %s(%s)", module, name, model, record_id)
            return
        imd.create({
            "module": module,
            "name": name,
            "model": model,
            "res_id": record_id,
            "noupdate": noupdate,
        })

    def find_by_xmlid(self, xmlid: str):
        """
        Safe env.ref lookup that returns None if not found.
        """
        try:
            return self.env.ref(xmlid)
        except Exception:
            return None

    def get_by_domain(self, model: str, domain: List[Tuple[str, str, Any]], fields: Optional[List[str]] = None):
        """
        Search records and optionally read fields.
        """
        recs = self.env[model].search(domain)
        if fields:
            return recs.read(fields)
        return recs

    def batch_create(self, model: str, vals_list: Sequence[Dict[str, Any]], chunk_size: int = 1000) -> List[int]:
        """
        Create records in chunks with savepoints. Returns created IDs.
        """
        ids: List[int] = []
        if not vals_list:
            return ids
        for chunk in chunked(list(vals_list), chunk_size):
            with self.savepoint("create_chunk"):
                if self.dry_run:
                    self.logger.info("DRY-RUN create %s records in %s: %s", len(chunk), model, chunk[:3])
                    continue
                created = self.env[model].create(chunk)
                ids.extend(created.ids)
        return ids

    def batch_write(self, recordset, vals: Mapping[str, Any], chunk_size: int = 1000) -> None:
        """
        Write to recordset in chunks with savepoints.
        """
        recs = recordset
        if not recs:
            return
        if len(recs) <= chunk_size:
            with self.savepoint("write"):
                if self.dry_run:
                    self.logger.info("DRY-RUN write %s to %s ids=%s", vals, recs._name, recs.ids[:5])
                    return
                recs.write(vals)
            return

        # chunk by ids
        ids = recs.ids
        for chunk_ids in chunked(ids, chunk_size):
            chunk_recs = self.env[recs._name].browse(chunk_ids)
            with self.savepoint("write_chunk"):
                if self.dry_run:
                    self.logger.info("DRY-RUN write %s to %s ids=%s", vals, recs._name, chunk_ids[:5])
                    continue
                chunk_recs.write(vals)

    def upsert_by_unique(
        self,
        model: str,
        unique_fields: Sequence[str],
        rows: Sequence[Dict[str, Any]],
        chunk_size: int = 500,
        update_fields: Optional[Sequence[str]] = None,
    ) -> Dict[str, int]:
        """
        Upsert by unique_fields. For each row, search by unique tuple, create if missing, otherwise write.
        Returns dict counters: {'created': x, 'updated': y, 'skipped': z}
        Note: For performance, prefer batching and pre-indexing when possible.
        """
        counters = {"created": 0, "updated": 0, "skipped": 0}
        if not rows:
            return counters

        # Preload an index for unique keys when unique fields are simple and small dataset
        # Otherwise fallback to per-row search.
        Model = self.env[model]

        def key_from_vals(vals: Mapping[str, Any]) -> Tuple:
            return tuple(vals.get(f) for f in unique_fields)

        # Build an index of existing records if key cardinality is manageable
        keys = [key_from_vals(r) for r in rows]
        unique_keys = dedupe(keys)
        existing_map: Dict[Tuple, int] = {}

        # Heuristic: if keys are not too many, build a single domain
        if len(unique_keys) <= 2000 and all(all(k is not None for k in key) for key in unique_keys):
            # Build domain: OR of ANDs
            domain: List = []
            for key in unique_keys:
                clause = []
                for f, v in zip(unique_fields, key):
                    clause.append((f, "=", v))
                if len(clause) == 1:
                    domain.append(clause[0])
                else:
                    domain.append(["&"] * (len(clause) - 1) + clause)
            # Normalize domain of ORs
            or_domain: List = []
            for d in domain:
                or_domain = d if not or_domain else ["|"] + or_domain + (d if isinstance(d, list) else [d])
            recs = Model.search(or_domain) if or_domain else Model.browse()
            for rec in recs:
                existing_map[key_from_vals(rec.read(unique_fields)[0])] = rec.id

        # Process rows
        for batch in chunked(list(rows), chunk_size):
            with self.savepoint("upsert_batch"):
                for vals in batch:
                    key = key_from_vals(vals)
                    record = None

                    if all(k is not None for k in key) and key in existing_map:
                        record = Model.browse(existing_map[key])
                    else:
                        # Fallback to search
                        domain = [(f, "=", v) for f, v in zip(unique_fields, key)]
                        record = Model.search(domain, limit=1)

                    if not record:
                        if self.dry_run:
                            self.logger.info("DRY-RUN create in %s: %s", model, vals)
                            counters["created"] += 1
                            continue
                        created = Model.create(vals)
                        if all(k is not None for k in key):
                            existing_map[key] = created.id
                        counters["created"] += 1
                    else:
                        if update_fields:
                            write_vals = {f: vals.get(f) for f in update_fields if f in vals}
                        else:
                            write_vals = dict(vals)
                        # Prevent attempt to modify unique fields if not changed
                        if not write_vals:
                            counters["skipped"] += 1
                            continue
                        if self.dry_run:
                            self.logger.info("DRY-RUN update %s(%s): %s", model, record.id, write_vals)
                            counters["updated"] += 1
                            continue
                        record.write(write_vals)
                        counters["updated"] += 1
        return counters

    def find_or_create(
        self,
        model: str,
        search_domain: List[Tuple[str, str, Any]],
        create_vals: Dict[str, Any],
    ):
        """
        Find record by domain or create it. Returns the record.
        """
        rec = self.env[model].search(search_domain, limit=1)
        if rec:
            return rec
        if self.dry_run:
            self.logger.info("DRY-RUN create %s: %s", model, create_vals)
            return self.env[model].browse()  # empty
        return self.env[model].create(create_vals)

    def set_m2m(self, record, field_name: str, ids: Sequence[int]) -> None:
        """
        Set many2many field with full replace semantics.
        """
        cmds = m2m_commands(ids)
        if self.dry_run:
            self.logger.info("DRY-RUN set %s.%s to ids=%s", record._name, field_name, list(ids)[:10])
            return
        record.write({field_name: cmds})

    def load_csv(
        self,
        path: str,
        model: str,
        row_to_vals: Callable[[Dict[str, str]], Dict[str, Any]],
        encoding: str = "utf-8",
        delimiter: str = ",",
        skip_header: bool = False,
        limit: Optional[int] = None,
        chunk_size: int = 500,
    ) -> Dict[str, int]:
        """
        Load a CSV file into a model using a row transform function.
        Returns counters: created/updated/skipped as applicable.
        """
        counters = {"created": 0, "updated": 0, "skipped": 0}
        with open(path, "r", encoding=encoding, newline="") as f:
            reader = csv.DictReader(f, delimiter=delimiter)
            if skip_header:
                next(reader, None)
            batch_vals: List[Dict[str, Any]] = []
            for i, row in enumerate(reader, 1):
                vals = row_to_vals(row)
                batch_vals.append(vals)
                if limit and i >= limit:
                    break
                if len(batch_vals) >= chunk_size:
                    ids = self.batch_create(model, batch_vals, chunk_size=chunk_size)
                    counters["created"] += len(ids)
                    batch_vals = []
            if batch_vals:
                ids = self.batch_create(model, batch_vals, chunk_size=chunk_size)
                counters["created"] += len(ids)
        return counters


# ------------------------------------------------------------------------------
# Example data validators
# ------------------------------------------------------------------------------

def assert_required(vals: Mapping[str, Any], fields: Sequence[str], row: Optional[Mapping[str, Any]] = None, model: Optional[str] = None):
    """
    Raise RowError if required fields are missing or blank.
    """
    missing = [f for f in fields if vals.get(f) in (None, "", [], {})]
    if missing:
        raise RowError(f"Missing required fields: {missing}", row=row, model=model)


def assert_max_length(vals: Mapping[str, Any], spec: Mapping[str, int], row: Optional[Mapping[str, Any]] = None, truncate: bool = True):
    """
    Ensure textual fields do not exceed max length. Optionally truncate.
    """
    for f, max_len in spec.items():
        v = vals.get(f)
        if v is None:
            continue
        s = str(v)
        if len(s) > max_len:
            if truncate:
                vals[f] = s[:max_len]
            else:
                raise RowError(f"Value too long for {f}: {len(s)} > {max_len}", row=row)


# ------------------------------------------------------------------------------
# Convenience helpers for common Odoo relations
# ------------------------------------------------------------------------------

def get_ids_by_xmlid(env, xmlids: Sequence[str]) -> Dict[str, int]:
    """
    Resolve a list of xmlids to record IDs. Missing ones are omitted.
    """
    result: Dict[str, int] = {}
    for xid in xmlids:
        try:
            rec = env.ref(xid)
            if rec:
                result[xid] = rec.id
        except Exception:
            continue
    return result


def find_partner_by_name_email(env, name: Optional[str], email: Optional[str]):
    """
    Simple heuristic to find a partner by name/email.
    """
    domain = []
    if email:
        domain = ["|", ("email", "=", email), ("email_formatted", "ilike", email)]
    if name:
        domain = domain + [("name", "ilike", name)] if domain else [("name", "ilike", name)]
    return env["res.partner"].search(domain, limit=1)


# ------------------------------------------------------------------------------
# End of library
# ------------------------------------------------------------------------------

# If you want to run quick self-test timings in Odoo shell:
# from migration_utils import MigrationHelper, time_block, sql_debug, clean_text, ...
# mh = MigrationHelper(env, dry_run=True)
# with time_block("example"):
#     mh.batch_create('res.partner', [{'name': 'ACME'}, {'name': 'Globex'}])
------------------------------------------------------------

PROMPT 258:
"Generate Python code that uses the PyPDF2 library to read and write PDF files, allowing you to create and modify PDF documents programmatically, similar to the functionality provided by the PDF library of RPA Framework."
RESPONSE:
# pdf_tools.py
# Utilities for reading, writing, merging, splitting, rotating, watermarking,
# encrypting, and otherwise manipulating PDFs using PyPDF2.
#
# Install:
#   pip install PyPDF2
#
# Notes:
# - This focuses on page-level operations (merge/split/rotate/crop/metadata/encrypt).
# - Writing arbitrary text or images onto pages is not supported by PyPDF2 alone; use
#   reportlab or similar to generate pages you can insert or overlay.

from __future__ import annotations

import os
from typing import Iterable, List, Optional, Sequence, Tuple, Union

try:
    # Newer API names (PyPDF2 >= 2.x and pypdf-style)
    from PyPDF2 import PdfReader, PdfWriter, PdfMerger
except ImportError:
    # Fallback older names (rare today, but keeps compatibility hints)
    from PyPDF2 import PdfFileReader as PdfReader
    from PyPDF2 import PdfFileWriter as PdfWriter
    from PyPDF2 import PdfFileMerger as PdfMerger


def _open_reader(pdf_path: str, password: Optional[str] = None) -> PdfReader:
    f = open(pdf_path, "rb")
    reader = PdfReader(f)
    # Some versions hold the file handle; keep reference to close later on caller
    # If encrypted, try to decrypt
    try:
        encrypted = getattr(reader, "is_encrypted", False)
    except Exception:
        encrypted = False

    if encrypted:
        if password:
            try:
                ok = reader.decrypt(password)
                # Some versions return 0/1, some return str
                if ok == 0:
                    raise ValueError("Incorrect password for encrypted PDF.")
            except Exception as e:
                f.close()
                raise
        else:
            f.close()
            raise ValueError("PDF is encrypted; provide a password.")
    return reader


def read_info(pdf_path: str, password: Optional[str] = None) -> dict:
    # Returns basic information: number of pages and metadata dictionary.
    reader = _open_reader(pdf_path, password)
    try:
        num_pages = len(reader.pages)
        raw_meta = getattr(reader, "metadata", None) or getattr(reader, "documentInfo", {}) or {}
        # Normalize metadata keys to strings without leading slashes if present
        meta = {}
        for k, v in dict(raw_meta).items():
            if isinstance(k, str) and k.startswith("/"):
                meta[k[1:]] = v
            else:
                meta[str(k)] = v
        return {"pages": num_pages, "metadata": meta}
    finally:
        # Close underlying file handle if present
        try:
            reader.stream.close()  # pypdf
        except Exception:
            try:
                reader._file.close()  # older PyPDF2
            except Exception:
                pass


def extract_text(pdf_path: str, password: Optional[str] = None) -> List[str]:
    # Extract text per page. Returns a list of strings, one per page.
    reader = _open_reader(pdf_path, password)
    try:
        texts = []
        for page in reader.pages:
            try:
                texts.append(page.extract_text() or "")
            except Exception:
                texts.append("")
        return texts
    finally:
        try:
            reader.stream.close()
        except Exception:
            try:
                reader._file.close()
            except Exception:
                pass


def extract_text_to_file(pdf_path: str, txt_output_path: str, password: Optional[str] = None) -> None:
    texts = extract_text(pdf_path, password)
    with open(txt_output_path, "w", encoding="utf-8") as f:
        for i, t in enumerate(texts, 1):
            f.write(f"----- Page {i} -----\n")
            f.write(t)
            f.write("\n\n")


def merge_pdfs(input_paths: Sequence[str], output_path: str) -> None:
    # Merge multiple PDFs in order.
    merger = PdfMerger()
    try:
        for path in input_paths:
            merger.append(path)
        with open(output_path, "wb") as out:
            merger.write(out)
    finally:
        try:
            merger.close()
        except Exception:
            pass


def split_pdf(
    input_path: str,
    output_dir: str,
    ranges: Optional[str] = None,
    password: Optional[str] = None,
    base_name: Optional[str] = None,
) -> List[str]:
    # Split a PDF into multiple files.
    # - ranges syntax (optional): "1-3,5,7-" (1-based, inclusive; open-ended to last page)
    #   If None, splits into one file per page.
    # Returns list of created file paths.
    reader = _open_reader(input_path, password)
    created: List[str] = []
    try:
        n = len(reader.pages)
        os.makedirs(output_dir, exist_ok=True)
        base = base_name or os.path.splitext(os.path.basename(input_path))[0]

        def parse_ranges(expr: str) -> List[Tuple[int, int]]:
            spans: List[Tuple[int, int]] = []
            parts = [p.strip() for p in expr.split(",") if p.strip()]
            for p in parts:
                if "-" in p:
                    a, b = p.split("-", 1)
                    start = int(a) if a else 1
                    end = int(b) if b else n
                else:
                    start = end = int(p)
                start = max(1, start)
                end = min(n, end)
                if start <= end:
                    spans.append((start, end))
            return spans

        spans = parse_ranges(ranges) if ranges else [(i, i) for i in range(1, n + 1)]

        for idx, (start, end) in enumerate(spans, 1):
            writer = PdfWriter()
            for p in range(start - 1, end):
                writer.add_page(reader.pages[p])
            out_path = os.path.join(output_dir, f"{base}_part_{idx}_{start}-{end}.pdf")
            with open(out_path, "wb") as f:
                writer.write(f)
            created.append(out_path)
        return created
    finally:
        try:
            reader.stream.close()
        except Exception:
            try:
                reader._file.close()
            except Exception:
                pass


def select_pages(
    input_path: str,
    output_path: str,
    pages: Iterable[int],
    password: Optional[str] = None,
) -> None:
    # Write a new PDF containing only specified 0-based page indices.
    reader = _open_reader(input_path, password)
    try:
        writer = PdfWriter()
        for i in pages:
            writer.add_page(reader.pages[i])
        with open(output_path, "wb") as out:
            writer.write(out)
    finally:
        try:
            reader.stream.close()
        except Exception:
            try:
                reader._file.close()
            except Exception:
                pass


def rotate_pages(
    input_path: str,
    output_path: str,
    degrees: int = 90,
    page_indexes: Optional[Iterable[int]] = None,
    password: Optional[str] = None,
) -> None:
    # Rotate specified pages (0-based indices). If page_indexes is None, rotate all pages.
    reader = _open_reader(input_path, password)
    try:
        writer = PdfWriter()
        targets = set(page_indexes) if page_indexes is not None else None
        for idx, page in enumerate(reader.pages):
            if targets is None or idx in targets:
                try:
                    # New API
                    page.rotate(degrees)
                except Exception:
                    # Older API
                    if degrees % 360 != 0:
                        if degrees % 360 == 90:
                            page.rotateClockwise(90)
                        elif degrees % 360 == 180:
                            page.rotateClockwise(180)
                        elif degrees % 360 == 270:
                            page.rotateCounterClockwise(90)
            writer.add_page(page)
        with open(output_path, "wb") as out:
            writer.write(out)
    finally:
        try:
            reader.stream.close()
        except Exception:
            try:
                reader._file.close()
            except Exception:
                pass


def add_watermark(
    input_path: str,
    watermark_pdf_path: str,
    output_path: str,
    pages: Optional[Iterable[int]] = None,
    password: Optional[str] = None,
) -> None:
    # Overlay a watermark PDF (assumed single-page) onto specified pages.
    # If pages is None, watermark all pages.
    reader = _open_reader(input_path, password)
    wm_reader = _open_reader(watermark_pdf_path)
    try:
        wm_page = wm_reader.pages[0]
        writer = PdfWriter()
        targets = set(pages) if pages is not None else None
        for idx, page in enumerate(reader.pages):
            if targets is None or idx in targets:
                try:
                    page.merge_page(wm_page)  # new API
                except Exception:
                    page.mergePage(wm_page)  # old API
            writer.add_page(page)
        with open(output_path, "wb") as out:
            writer.write(out)
    finally:
        for r in (reader, wm_reader):
            try:
                r.stream.close()
            except Exception:
                try:
                    r._file.close()
                except Exception:
                    pass


def add_metadata(
    input_path: str,
    output_path: str,
    title: Optional[str] = None,
    author: Optional[str] = None,
    subject: Optional[str] = None,
    keywords: Optional[Union[str, Sequence[str]]] = None,
    password: Optional[str] = None,
) -> None:
    # Add or update document metadata.
    reader = _open_reader(input_path, password)
    try:
        writer = PdfWriter()
        for page in reader.pages:
            writer.add_page(page)

        md = {}
        if title:
            md["/Title"] = title
        if author:
            md["/Author"] = author
        if subject:
            md["/Subject"] = subject
        if keywords:
            if isinstance(keywords, (list, tuple, set)):
                md["/Keywords"] = ", ".join(keywords)
            else:
                md["/Keywords"] = str(keywords)

        # Preserve existing metadata where possible
        try:
            existing = reader.metadata or {}
        except Exception:
            existing = getattr(reader, "documentInfo", {}) or {}

        merged = dict(existing)
        merged.update(md)
        try:
            writer.add_metadata(merged)  # new API
        except Exception:
            try:
                writer.addMetadata(merged)  # old API
            except Exception:
                pass

        with open(output_path, "wb") as out:
            writer.write(out)
    finally:
        try:
            reader.stream.close()
        except Exception:
            try:
                reader._file.close()
            except Exception:
                pass


def crop_pages(
    input_path: str,
    output_path: str,
    crop_box: Tuple[float, float, float, float],
    pages: Optional[Iterable[int]] = None,
    password: Optional[str] = None,
) -> None:
    # Crop specified pages to the rectangle (x0, y0, x1, y1) in points.
    reader = _open_reader(input_path, password)
    try:
        x0, y0, x1, y1 = crop_box
        writer = PdfWriter()
        targets = set(pages) if pages is not None else None
        for idx, page in enumerate(reader.pages):
            if targets is None or idx in targets:
                try:
                    # pypdf-style box objects
                    page.cropbox.lower_left = (x0, y0)
                    page.cropbox.upper_right = (x1, y1)
                except Exception:
                    # Older style
                    page.cropBox.lowerLeft = (x0, y0)
                    page.cropBox.upperRight = (x1, y1)
            writer.add_page(page)
        with open(output_path, "wb") as out:
            writer.write(out)
    finally:
        try:
            reader.stream.close()
        except Exception:
            try:
                reader._file.close()
            except Exception:
                pass


def insert_pdf(
    base_pdf_path: str,
    insert_pdf_path: str,
    output_path: str,
    position: int = 0,
    base_password: Optional[str] = None,
    insert_password: Optional[str] = None,
) -> None:
    # Insert all pages from insert_pdf_path into base_pdf_path at 0-based position.
    base_reader = _open_reader(base_pdf_path, base_password)
    insert_reader = _open_reader(insert_pdf_path, insert_password)
    try:
        writer = PdfWriter()
        # Copy pages before insertion point
        for i in range(min(position, len(base_reader.pages))):
            writer.add_page(base_reader.pages[i])
        # Inserted pages
        for p in insert_reader.pages:
            writer.add_page(p)
        # Remaining base pages
        for i in range(position, len(base_reader.pages)):
            writer.add_page(base_reader.pages[i])
        with open(output_path, "wb") as out:
            writer.write(out)
    finally:
        for r in (base_reader, insert_reader):
            try:
                r.stream.close()
            except Exception:
                try:
                    r._file.close()
                except Exception:
                    pass


def create_blank_pdf(
    output_path: str,
    page_sizes: Sequence[Tuple[float, float]],
    metadata: Optional[dict] = None,
) -> None:
    # Create a new PDF with given blank page sizes [(width, height), ...]
    writer = PdfWriter()
    for w, h in page_sizes:
        try:
            writer.add_blank_page(width=w, height=h)  # new API
        except Exception:
            writer.addBlankPage(width=w, height=h)  # old API
    if metadata:
        try:
            writer.add_metadata(metadata)
        except Exception:
            try:
                writer.addMetadata(metadata)
            except Exception:
                pass
    with open(output_path, "wb") as out:
        writer.write(out)


def encrypt_pdf(
    input_path: str,
    output_path: str,
    user_password: Optional[str] = None,
    owner_password: Optional[str] = None,
    password: Optional[str] = None,
) -> None:
    # Encrypt a PDF. If only user_password is given, owner defaults to same.
    # Note: Fine-grained permission control differs by version and is omitted here.
    reader = _open_reader(input_path, password)
    try:
        writer = PdfWriter()
        for page in reader.pages:
            writer.add_page(page)
        u = user_password or ""
        o = owner_password if owner_password is not None else (user_password or "")
        # Try new signatures first
        try:
            writer.encrypt(user_password=u, owner_password=o)
        except TypeError:
            try:
                writer.encrypt(u, o, use_128bit=True)
            except Exception:
                writer.encrypt(u)
        with open(output_path, "wb") as out:
            writer.write(out)
    finally:
        try:
            reader.stream.close()
        except Exception:
            try:
                reader._file.close()
            except Exception:
                pass


def decrypt_pdf(
    input_path: str,
    output_path: str,
    password: str,
) -> None:
    # Remove encryption and write a decrypted copy (assuming you provide a valid password).
    reader = _open_reader(input_path, password=password)
    try:
        writer = PdfWriter()
        for page in reader.pages:
            writer.add_page(page)
        # Writing without calling encrypt() produces an unencrypted PDF
        with open(output_path, "wb") as out:
            writer.write(out)
    finally:
        try:
            reader.stream.close()
        except Exception:
            try:
                reader._file.close()
            except Exception:
                pass


# Example usage
if __name__ == "__main__":
    # Adjust paths to your environment before running this block.
    sample_dir = "examples"
    os.makedirs(sample_dir, exist_ok=True)

    # 1) Create a blank PDF with two pages (A4 and Letter)
    create_blank_pdf(
        os.path.join(sample_dir, "blank.pdf"),
        page_sizes=[(595.28, 841.89), (612, 792)],  # A4, Letter in points
        metadata={"/Title": "Blank Sample", "/Author": "PyPDF2 tools"},
    )

    # 2) Merge PDFs
    # merge_pdfs(["file1.pdf", "file2.pdf"], os.path.join(sample_dir, "merged.pdf"))

    # 3) Split a PDF into page ranges
    # split_pdf("big.pdf", sample_dir, ranges="1-3,5,7-")

    # 4) Rotate all pages 90 degrees clockwise
    # rotate_pages("input.pdf", os.path.join(sample_dir, "rotated.pdf"), degrees=90)

    # 5) Add a watermark (create or supply a single-page watermark PDF first)
    # add_watermark("input.pdf", "watermark.pdf", os.path.join(sample_dir, "watermarked.pdf"))

    # 6) Update metadata
    # add_metadata("input.pdf", os.path.join(sample_dir, "with_meta.pdf"), title="New Title", author="Me")

    # 7) Crop pages
    # crop_pages("input.pdf", os.path.join(sample_dir, "cropped.pdf"), crop_box=(50, 50, 500, 700))

    # 8) Encrypt and decrypt
    # encrypt_pdf("input.pdf", os.path.join(sample_dir, "locked.pdf"), user_password="1234")
    # decrypt_pdf(os.path.join(sample_dir, "locked.pdf"), os.path.join(sample_dir, "unlocked.pdf"), password="1234")

    # 9) Extract text
    # extract_text_to_file("input.pdf", os.path.join(sample_dir, "input.txt"))

    print("pdf_tools.py demo setup complete. Uncomment example calls in __main__ to try features.")
------------------------------------------------------------

PROMPT 259:
"Generate Python code that imports an SSH client library and creates a simple configuration class to store host information, then uses the SSH client to connect to hosts and perform actions based on their configurations."
RESPONSE:
# pip install paramiko

from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
import paramiko
import socket
import os


@dataclass
class HostConfig:
    hostname: str
    username: str
    port: int = 22
    # Either password or key_filename can be used for auth
    password: Optional[str] = None
    key_filename: Optional[str] = None
    # Actions to perform on the host
    # Supported actions:
    #   {"type": "command", "cmd": "uname -a", "sudo": False}
    #   {"type": "upload", "local_path": "local.txt", "remote_path": "/tmp/remote.txt"}
    #   {"type": "download", "remote_path": "/etc/hosts", "local_path": "hosts_copy"}
    actions: List[Dict[str, Any]] = field(default_factory=list)

    connect_timeout: float = 10.0
    command_timeout: Optional[float] = 60.0
    keepalive_interval: int = 30


def connect_ssh(cfg: HostConfig) -> paramiko.SSHClient:
    client = paramiko.SSHClient()
    # In production, consider loading and enforcing known_hosts instead
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

    connect_params = {
        "hostname": cfg.hostname,
        "port": cfg.port,
        "username": cfg.username,
        "password": cfg.password,
        "key_filename": cfg.key_filename,
        "look_for_keys": True,     # also try keys from ~/.ssh
        "allow_agent": True,       # and from ssh-agent
        "timeout": cfg.connect_timeout,
    }

    client.connect(**{k: v for k, v in connect_params.items() if v is not None})

    transport = client.get_transport()
    if transport is not None and cfg.keepalive_interval > 0:
        transport.set_keepalive(cfg.keepalive_interval)

    return client


def run_command(
    client: paramiko.SSHClient,
    cmd: str,
    sudo: bool = False,
    password: Optional[str] = None,
    timeout: Optional[float] = None,
) -> Dict[str, Any]:
    full_cmd = cmd
    get_pty = False
    if sudo:
        # -S reads password from stdin, -p '' avoids sudo asking with a prompt
        full_cmd = f"sudo -S -p '' {cmd}"
        get_pty = True  # many sudo configs require a TTY

    stdin, stdout, stderr = client.exec_command(full_cmd, get_pty=get_pty, timeout=timeout)

    if sudo:
        if not password:
            raise ValueError("sudo requested but no password provided")
        # Send password followed by newline
        stdin.write(password + "\n")
        stdin.flush()

    # Read outputs
    out = stdout.read().decode("utf-8", errors="replace")
    err = stderr.read().decode("utf-8", errors="replace")
    exit_status = stdout.channel.recv_exit_status()

    return {"exit_status": exit_status, "stdout": out, "stderr": err}


def perform_actions(client: paramiko.SSHClient, cfg: HostConfig) -> None:
    for i, action in enumerate(cfg.actions, start=1):
        a_type = action.get("type")
        print(f"[{cfg.hostname}] Action {i}: {a_type}")

        try:
            if a_type == "command":
                cmd = action["cmd"]
                sudo = bool(action.get("sudo", False))
                result = run_command(
                    client,
                    cmd=cmd,
                    sudo=sudo,
                    password=cfg.password,
                    timeout=cfg.command_timeout,
                )
                print(f"[{cfg.hostname}] Command: {cmd}")
                print(f"[{cfg.hostname}] Exit: {result['exit_status']}")
                if result["stdout"]:
                    print(f"[{cfg.hostname}] STDOUT:\n{result['stdout']}")
                if result["stderr"]:
                    print(f"[{cfg.hostname}] STDERR:\n{result['stderr']}")

            elif a_type == "upload":
                local_path = action["local_path"]
                remote_path = action["remote_path"]
                if not os.path.exists(local_path):
                    raise FileNotFoundError(f"Local file not found: {local_path}")
                sftp = client.open_sftp()
                try:
                    sftp.put(local_path, remote_path)
                finally:
                    sftp.close()
                print(f"[{cfg.hostname}] Uploaded {local_path} -> {remote_path}")

            elif a_type == "download":
                remote_path = action["remote_path"]
                local_path = action["local_path"]
                sftp = client.open_sftp()
                try:
                    sftp.get(remote_path, local_path)
                finally:
                    sftp.close()
                print(f"[{cfg.hostname}] Downloaded {remote_path} -> {local_path}")

            else:
                print(f"[{cfg.hostname}] Unknown action type: {a_type}")

        except Exception as e:
            print(f"[{cfg.hostname}] Action failed: {e}")


def manage_hosts(hosts: List[HostConfig]) -> None:
    for cfg in hosts:
        print(f"Connecting to {cfg.username}@{cfg.hostname}:{cfg.port} ...")
        client: Optional[paramiko.SSHClient] = None
        try:
            client = connect_ssh(cfg)
            print(f"Connected to {cfg.hostname}")
            perform_actions(client, cfg)
        except (paramiko.SSHException, socket.error, TimeoutError) as e:
            print(f"Failed to connect to {cfg.hostname}: {e}")
        finally:
            if client:
                client.close()
                print(f"Disconnected from {cfg.hostname}")


if __name__ == "__main__":
    # Example configurations
    hosts_to_manage = [
        HostConfig(
            hostname="192.168.1.10",
            username="ubuntu",
            key_filename="~/.ssh/id_rsa",
            actions=[
                {"type": "command", "cmd": "uname -a"},
                {"type": "command", "cmd": "id"},
            ],
        ),
        HostConfig(
            hostname="server.example.com",
            username="admin",
            password="your_password_here",
            actions=[
                {"type": "command", "cmd": "apt-get update -y", "sudo": True},
                {"type": "upload", "local_path": "local.txt", "remote_path": "/tmp/remote.txt"},
                {"type": "download", "remote_path": "/etc/hosts", "local_path": "hosts_copy"},
            ],
        ),
    ]

    manage_hosts(hosts_to_manage)
------------------------------------------------------------

PROMPT 260:
"Generate Python code that implements a dictionary-based system for managing binary data, complete with functions for encoding and decoding data, storing and retrieving data, and iterating over the data in the dictionary."
RESPONSE:
```python
from __future__ import annotations

import base64
from collections.abc import Iterable, Iterator, MutableMapping
from typing import Dict, Tuple, Union

BytesLike = Union[bytes, bytearray, memoryview]


# Encoding/decoding helpers

def encode_base64(data: BytesLike) -> str:
    """
    Encode arbitrary binary data as a base64 string (URL-safe, no padding stripped).
    """
    if not isinstance(data, (bytes, bytearray, memoryview)):
        raise TypeError("encode_base64: 'data' must be bytes-like")
    return base64.urlsafe_b64encode(bytes(data)).decode("ascii")


def decode_base64(b64: str) -> bytes:
    """
    Decode a base64 string (URL-safe) back into bytes.
    """
    if not isinstance(b64, str):
        raise TypeError("decode_base64: 'b64' must be a string")
    # Add padding if missing
    padding = '=' * (-len(b64) % 4)
    return base64.urlsafe_b64decode(b64 + padding)


def encode_text(text: str, encoding: str = "utf-8", errors: str = "strict") -> bytes:
    """
    Encode a text string into bytes using the specified encoding.
    """
    if not isinstance(text, str):
        raise TypeError("encode_text: 'text' must be a string")
    return text.encode(encoding, errors)


def decode_text(data: BytesLike, encoding: str = "utf-8", errors: str = "strict") -> str:
    """
    Decode bytes into a text string using the specified encoding.
    """
    if not isinstance(data, (bytes, bytearray, memoryview)):
        raise TypeError("decode_text: 'data' must be bytes-like")
    return bytes(data).decode(encoding, errors)


def encode_hex(data: BytesLike) -> str:
    """
    Encode bytes into a hexadecimal string (lowercase).
    """
    if not isinstance(data, (bytes, bytearray, memoryview)):
        raise TypeError("encode_hex: 'data' must be bytes-like")
    return bytes(data).hex()


def decode_hex(hexstr: str) -> bytes:
    """
    Decode a hexadecimal string back into bytes.
    """
    if not isinstance(hexstr, str):
        raise TypeError("decode_hex: 'hexstr' must be a string")
    return bytes.fromhex(hexstr)


class BinaryDataStore(MutableMapping[str, bytes]):
    """
    A dictionary-like store for managing binary data with convenience methods
    for encoding/decoding and iteration.
    """

    def __init__(self, initial: Dict[str, BytesLike] | None = None) -> None:
        self._store: Dict[str, bytes] = {}
        if initial:
            for k, v in initial.items():
                self[k] = v  # uses __setitem__ to normalize to bytes

    # MutableMapping required methods

    def __getitem__(self, key: str) -> bytes:
        return self._store[key]

    def __setitem__(self, key: str, value: BytesLike) -> None:
        if not isinstance(key, str):
            raise TypeError("BinaryDataStore keys must be strings")
        if not isinstance(value, (bytes, bytearray, memoryview)):
            raise TypeError("BinaryDataStore values must be bytes-like")
        self._store[key] = bytes(value)  # store immutable copy

    def __delitem__(self, key: str) -> None:
        del self._store[key]

    def __iter__(self) -> Iterator[str]:
        return iter(self._store)

    def __len__(self) -> int:
        return len(self._store)

    # Convenience methods

    def put(self, key: str, data: BytesLike) -> None:
        """
        Store data under 'key'.
        """
        self[key] = data

    def put_from_text(self, key: str, text: str, encoding: str = "utf-8", errors: str = "strict") -> None:
        """
        Encode text and store it as bytes under 'key'.
        """
        self[key] = encode_text(text, encoding=encoding, errors=errors)

    def put_from_base64(self, key: str, b64: str) -> None:
        """
        Decode base64 string and store as bytes under 'key'.
        """
        self[key] = decode_base64(b64)

    def put_from_hex(self, key: str, hexstr: str) -> None:
        """
        Decode hex string and store as bytes under 'key'.
        """
        self[key] = decode_hex(hexstr)

    def get_bytes(self, key: str) -> bytes:
        """
        Retrieve raw bytes stored under 'key'.
        """
        return self[key]

    def get_as_text(self, key: str, encoding: str = "utf-8", errors: str = "strict") -> str:
        """
        Retrieve data under 'key' and decode as text.
        """
        return decode_text(self[key], encoding=encoding, errors=errors)

    def get_as_base64(self, key: str) -> str:
        """
        Retrieve data under 'key' and encode as base64.
        """
        return encode_base64(self[key])

    def get_as_hex(self, key: str) -> str:
        """
        Retrieve data under 'key' and encode as hex.
        """
        return encode_hex(self[key])

    def pop_bytes(self, key: str) -> bytes:
        """
        Remove and return raw bytes stored under 'key'.
        """
        return self._store.pop(key)

    def exists(self, key: str) -> bool:
        """
        Check if 'key' exists in the store.
        """
        return key in self._store

    def clear_all(self) -> None:
        """
        Remove all items from the store.
        """
        self._store.clear()

    def total_size(self) -> int:
        """
        Total number of bytes across all values.
        """
        return sum(len(v) for v in self._store.values())

    # Iteration helpers

    def iter_keys(self) -> Iterator[str]:
        """
        Iterate over keys (same as iter(store)).
        """
        return iter(self._store.keys())

    def iter_values(self) -> Iterator[bytes]:
        """
        Iterate over values (bytes).
        """
        return iter(self._store.values())

    def iter_items(self) -> Iterator[Tuple[str, bytes]]:
        """
        Iterate over (key, bytes) pairs.
        """
        return iter(self._store.items())

    def iter_chunks(self, key: str, chunk_size: int) -> Iterator[bytes]:
        """
        Iterate over the data for 'key' in fixed-size chunks.
        """
        if chunk_size <= 0:
            raise ValueError("chunk_size must be > 0")
        data = self[key]
        for i in range(0, len(data), chunk_size):
            yield data[i:i + chunk_size]


# Example usage
if __name__ == "__main__":
    store = BinaryDataStore()

    # Store raw bytes
    store.put("img", b"\x89PNG\r\n\x1a\n...")

    # Store from text
    store.put_from_text("greeting", "Hello, world!")

    # Store from base64 and hex
    store.put_from_base64("b64blob", encode_base64(b"\x00\x01\x02"))
    store.put_from_hex("hexblob", "deadbeef")

    # Retrieve
    print("greeting as text:", store.get_as_text("greeting"))
    print("img as base64:", store.get_as_base64("img"))
    print("hexblob bytes:", store.get_bytes("hexblob"))

    # Iterate
    print("All keys:", list(store))
    print("Total size:", store.total_size(), "bytes")

    # Iterate over chunks
    store.put("big", b"A" * 25)
    print("Chunks of 'big' (size 8):")
    for chunk in store.iter_chunks("big", 8):
        print(repr(chunk))
```
------------------------------------------------------------

PROMPT 261:
"Generate Python code that allows you to create and manage Firebase Cloud Functions using the Firebase Functions Python SDK, including creating functions, deploying them to Firebase, and triggering them via HTTP requests."
RESPONSE:
Short answer up front: there isn’t an official “Firebase Functions Python SDK.” Firebase Cloud Functions are powered by Google Cloud Functions. You can write Python HTTP functions with the Functions Framework for Python and deploy them to your Firebase project (it’s just a Google Cloud project) using gcloud. You can then route Firebase Hosting traffic to them if desired.

Below is a minimal, working setup that:
- Defines multiple HTTP-triggered Python functions
- Deploys them to your Firebase project
- Retrieves their URLs
- Triggers them via HTTP from Python

Files:
1) requirements.txt
2) main.py (your Cloud Functions)
3) deploy.py (Python-based deploy/list/delete/URL helper using gcloud)
4) trigger.py (Python client to call the HTTP functions)

requirements.txt
```
functions-framework==3.*
Flask>=2.2,<3
google-cloud-logging>=3.8.0
requests>=2.31.0
```

main.py
```
import json
import logging
from typing import Tuple, Dict, Any

# Optional: Structured logging to Cloud Logging
try:
    import google.cloud.logging  # type: ignore
    google.cloud.logging.Client().setup_logging()
except Exception:
    pass

# Each function below is a valid HTTP Cloud Function entry point.
# They accept a Flask-like request and return a JSON response.

def _cors_response(payload: Dict[str, Any], status: int = 200) -> Tuple[str, int, Dict[str, str]]:
    headers = {
        "Content-Type": "application/json",
        "Access-Control-Allow-Origin": "*",
        "Access-Control-Allow-Methods": "GET,POST,OPTIONS",
        "Access-Control-Allow-Headers": "Content-Type,Authorization",
    }
    return json.dumps(payload), status, headers

def _parse_json(request):
    try:
        if request.is_json:
            return request.get_json(silent=True) or {}
        if request.data:
            return json.loads(request.data.decode("utf-8"))
    except Exception:
        pass
    return {}

def hello(request):
    # Handle CORS preflight
    if request.method == "OPTIONS":
        return _cors_response({"ok": True})

    args = request.args or {}
    body = _parse_json(request)
    name = args.get("name") or body.get("name") or "world"
    logging.info("hello invoked", extra={"name": name})
    return _cors_response({"message": f"Hello, {name}!"})

def echo(request):
    if request.method == "OPTIONS":
        return _cors_response({"ok": True})

    args = request.args or {}
    body = _parse_json(request)
    headers = {k: v for k, v in request.headers.items()}
    return _cors_response({
        "method": request.method,
        "path": request.path,
        "args": dict(args),
        "json": body,
        "headers": headers,
    })

def add(request):
    if request.method == "OPTIONS":
        return _cors_response({"ok": True})

    body = _parse_json(request)
    try:
        a = float(body.get("a", 0))
        b = float(body.get("b", 0))
        result = a + b
        return _cors_response({"a": a, "b": b, "sum": result})
    except Exception as e:
        return _cors_response({"error": str(e)}, status=400)

def health(request):
    if request.method == "OPTIONS":
        return _cors_response({"ok": True})

    return _cors_response({"status": "ok"})
```

deploy.py
```
import argparse
import json
import subprocess
import sys
from typing import List, Dict

# Define the functions you want to manage from this directory.
# name: the deployed function name
# entry: the Python function in main.py
FUNCTIONS: List[Dict[str, str]] = [
    {"name": "hello", "entry": "hello"},
    {"name": "echo", "entry": "echo"},
    {"name": "add", "entry": "add"},
    {"name": "health", "entry": "health"},
]

DEFAULT_REGION = "us-central1"
DEFAULT_RUNTIME = "python311"

def run(cmd: List[str]) -> subprocess.CompletedProcess:
    print("Running:", " ".join(cmd))
    res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
    if res.returncode != 0:
        print(res.stdout)
        raise RuntimeError(f"Command failed: {' '.join(cmd)}")
    return res

def deploy(project: str, region: str, runtime: str, allow_unauth: bool = True):
    for f in FUNCTIONS:
        cmd = [
            "gcloud", "functions", "deploy", f["name"],
            "--project", project,
            "--gen2",
            "--region", region,
            "--runtime", runtime,
            "--source", ".",
            "--entry-point", f["entry"],
            "--trigger-http",
            "--memory", "256Mi",
        ]
        if allow_unauth:
            cmd.append("--allow-unauthenticated")
        run(cmd)

def delete(project: str, region: str, names: List[str]):
    for name in names:
        cmd = [
            "gcloud", "functions", "delete", name,
            "--project", project,
            "--gen2",
            "--region", region,
            "--quiet",
        ]
        run(cmd)

def urls(project: str, region: str):
    out = {}
    for f in FUNCTIONS:
        cmd = [
            "gcloud", "functions", "describe", f["name"],
            "--project", project,
            "--gen2",
            "--region", region,
            "--format", "value(serviceConfig.uri)",
        ]
        res = run(cmd)
        url = res.stdout.strip()
        out[f["name"]] = url
    print(json.dumps(out, indent=2))
    return out

def list_functions(project: str, region: str):
    cmd = [
        "gcloud", "functions", "list",
        "--project", project,
        "--gen2",
        "--region", region,
        "--format", "table(name,serviceConfig.runtime,serviceConfig.uri,status)",
    ]
    res = run(cmd)
    print(res.stdout)

def main():
    parser = argparse.ArgumentParser(description="Manage Python HTTP functions on Cloud Functions (Gen2) in a Firebase project")
    parser.add_argument("--project", required=True, help="Firebase/Google Cloud project ID")
    parser.add_argument("--region", default=DEFAULT_REGION)
    parser.add_argument("--runtime", default=DEFAULT_RUNTIME)
    sub = parser.add_subparsers(dest="cmd", required=True)

    sub.add_parser("deploy")
    sub.add_parser("list")
    sub.add_parser("urls")

    delete_p = sub.add_parser("delete")
    delete_p.add_argument("--names", nargs="+", default=[f["name"] for f in FUNCTIONS], help="Function names to delete")

    args = parser.parse_args()

    if args.cmd == "deploy":
        deploy(args.project, args.region, args.runtime)
        urls(args.project, args.region)
    elif args.cmd == "list":
        list_functions(args.project, args.region)
    elif args.cmd == "urls":
        urls(args.project, args.region)
    elif args.cmd == "delete":
        delete(args.project, args.region, args.names)
    else:
        parser.print_help()
        sys.exit(2)

if __name__ == "__main__":
    main()
```

trigger.py
```
import argparse
import json
import requests

def call(url: str, method: str = "GET", payload: dict | None = None):
    method = method.upper()
    if method in ("POST", "PUT", "PATCH"):
        r = requests.request(method, url, json=payload, timeout=30)
    else:
        r = requests.request(method, url, params=(payload or {}), timeout=30)
    r.raise_for_status()
    try:
        print(json.dumps(r.json(), indent=2))
    except Exception:
        print(r.text)

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--url", required=True)
    p.add_argument("--method", default="GET")
    p.add_argument("--data", help="JSON payload for POST/PUT/PATCH")
    args = p.parse_args()
    payload = json.loads(args.data) if args.data else None
    call(args.url, args.method, payload)
```

How to use
- Prereqs:
  - Python 3.11+ recommended
  - gcloud CLI installed and authenticated
  - A Firebase project (PROJECT_ID). You can create one in the Firebase console; it maps to a Google Cloud project.

- Install deps:
  - pip install -r requirements.txt

- Local test:
  - python -m functions_framework --target hello --port 8080
  - curl "http://localhost:8080?name=Firebase"

- Deploy to your Firebase project (Cloud Functions Gen 2):
  - python deploy.py --project PROJECT_ID deploy

- Get function URLs:
  - python deploy.py --project PROJECT_ID urls
  - Example output:
    {"hello": "https://REGION-PROJECT_ID.cloudfunctions.net/hello", ...}

- Trigger via HTTP:
  - curl "$(python deploy.py --project PROJECT_ID urls | jq -r .hello)?name=Alice"
  - or:
    python trigger.py --url https://REGION-PROJECT_ID.cloudfunctions.net/add --method POST --data '{"a": 2, "b": 3}'

Optional: route Firebase Hosting to your functions
- If you use Firebase Hosting, you can rewrite paths to these HTTP functions (they just need to be in the same project/region). In firebase.json:
  {
    "hosting": {
      "rewrites": [
        { "source": "/api/hello", "function": { "functionId": "hello", "region": "us-central1" } }
      ]
    }
  }
- Then: firebase deploy --only hosting

Notes
- This uses Google Cloud Functions Gen 2 under the hood, which is what Firebase Cloud Functions run on. There is no separate Python-specific Firebase Functions SDK.
- For non-HTTP triggers (Firestore, Storage, Auth), Python support relies on Eventarc/CloudEvents on Gen 2 and is more advanced to configure; HTTP triggers are the simplest path in Python.
------------------------------------------------------------

PROMPT 262:
"Generate Python code that reads a JSON file containing an OAuth2 client ID and creates a new file containing the client secret, using the `json` module to parse the input file and the `os` module to write the output file."
RESPONSE:
import json
import os
import sys

def find_first_key(obj, key):
    if isinstance(obj, dict):
        if key in obj:
            return obj[key]
        for v in obj.values():
            found = find_first_key(v, key)
            if found is not None:
                return found
    elif isinstance(obj, list):
        for item in obj:
            found = find_first_key(item, key)
            if found is not None:
                return found
    return None

def main():
    if len(sys.argv) != 3:
        print(f"Usage: {os.path.basename(sys.argv[0])} <input.json> <output_secret_file>", file=sys.stderr)
        sys.exit(1)

    input_path = sys.argv[1]
    output_path = sys.argv[2]

    try:
        with open(input_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"Error: Input file not found: {input_path}", file=sys.stderr)
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"Error: Failed to parse JSON: {e}", file=sys.stderr)
        sys.exit(1)

    client_secret = find_first_key(data, "client_secret")
    if client_secret is None:
        print("Error: 'client_secret' not found in the input JSON.", file=sys.stderr)
        sys.exit(1)

    # Ensure output directory exists
    out_dir = os.path.dirname(os.path.abspath(output_path))
    if out_dir:
        os.makedirs(out_dir, exist_ok=True)

    # Write the secret using os module with restrictive permissions (0600)
    flags = os.O_WRONLY | os.O_CREAT | os.O_TRUNC
    mode = 0o600
    try:
        fd = os.open(output_path, flags, mode)
        with os.fdopen(fd, "w", encoding="utf-8") as out_f:
            out_f.write(str(client_secret))
            out_f.write("\n")
    except OSError as e:
        print(f"Error: Failed to write secret to {output_path}: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 263:
"Generate Python code that imports the necessary modules and creates wrappers for the ExtensionKit framework on macOS, allowing developers to easily interact with the framework's functionality in their Python applications."
RESPONSE:
# -*- coding: utf-8 -*-
"""
Lightweight Python wrappers for Apple's ExtensionKit framework (macOS 13+)
using PyObjC. This module helps you list and inspect app extensions from
Python. It is designed to be safe to import on non-macOS platforms and to
fail with actionable errors when prerequisites are missing.

Requirements:
- macOS 13 Ventura or newer
- Python 3.8+
- PyObjC 9.0+ (strongly recommended)
  pip install pyobjc-core pyobjc-framework-Cocoa pyobjc-framework-ExtensionKit

Notes:
- This focuses on discovery (browsing) of extensions through EXAppExtensionBrowser.
- Launch/control APIs often involve blocks and XPC; reliable usage requires
  the dedicated pyobjc-framework-ExtensionKit, which provides metadata for block
  signatures and method mappings.
- These wrappers aim to be future-friendly and will try multiple selector names
  for APIs that have changed across macOS releases.
"""

from __future__ import annotations

import os
import sys
import time
import threading
import platform
from typing import Any, Dict, List, Optional, Tuple

try:
    import objc
except Exception as exc:
    raise RuntimeError(
        "PyObjC is required. Install with: pip install pyobjc-core pyobjc-framework-Cocoa"
    ) from exc

# Cocoa/Foundation are needed to run the run loop and work with ObjC collections.
try:
    from Foundation import (
        NSObject,
        NSBundle,
        NSDictionary,
        NSMutableDictionary,
        NSArray,
        NSDate,
        NSRunLoop,
        NSDefaultRunLoopMode,
        NSOperationQueue,
    )
except Exception as exc:
    raise RuntimeError(
        "Cannot import Foundation. Install with: pip install pyobjc-framework-Cocoa"
    ) from exc


# Try to import the dedicated ExtensionKit PyObjC wrapper (recommended).
_HAVE_PYOBJC_EXTENSIONKIT = False
try:
    # If installed, these names will exist. We only import what we need.
    from ExtensionKit import EXAppExtensionBrowser  # type: ignore
    _HAVE_PYOBJC_EXTENSIONKIT = True
except Exception:
    _HAVE_PYOBJC_EXTENSIONKIT = False


class ExtensionKitError(RuntimeError):
    pass


def _is_macos_ventura_or_newer() -> bool:
    if platform.system() != "Darwin":
        return False
    try:
        ver = platform.mac_ver()[0]  # '14.5' etc
        if not ver:
            return False
        major = int(ver.split(".")[0])
        # macOS 13 Ventura has Darwin version 22.x; but platform.mac_ver() returns marketing version.
        return major >= 13
    except Exception:
        return False


def _assert_environment() -> None:
    if platform.system() != "Darwin":
        raise ExtensionKitError("ExtensionKit is only available on macOS.")
    if not _is_macos_ventura_or_newer():
        raise ExtensionKitError("ExtensionKit requires macOS 13 Ventura or newer.")


def _load_extensionkit_bundle_if_needed() -> None:
    """
    If the dedicated PyObjC wrapper isn't installed, we try to load the framework bundle
    so that ObjC classes are visible. Note: without the PyObjC metadata for ExtensionKit,
    calling methods that use blocks may not work reliably.
    """
    if _HAVE_PYOBJC_EXTENSIONKIT:
        return
    # Try to load the public framework bundle to make classes discoverable.
    # Public framework path (Ventura+):
    bundle_path = "/System/Library/Frameworks/ExtensionKit.framework"
    if not os.path.exists(bundle_path):
        # Some betas/previews might place it differently; try best-effort lookup by identifier.
        bundle = NSBundle.bundleWithIdentifier_("com.apple.ExtensionKit")
        if bundle is None:
            # If we cannot load the bundle, we still keep going; but APIs that need it will fail with clear errors.
            return
    else:
        try:
            objc.loadBundle("ExtensionKit", bundle_path=bundle_path, module_globals=globals())
        except Exception:
            # Keep going; callers will see a clear error when they actually try to use functionality
            pass


def _lookup_class(name: str):
    try:
        return objc.lookUpClass(name)
    except Exception:
        return None


def _nsdate_in(seconds: float):
    return NSDate.dateWithTimeIntervalSinceNow_(seconds)


def _spin_runloop_until(event: threading.Event, timeout: float) -> bool:
    """
    Keep the current runloop alive until 'event' is set or timeout expires.
    Returns True if the event was set, False if timed out.
    """
    deadline = time.time() + max(0.0, timeout)
    while not event.is_set():
        remaining = deadline - time.time()
        if remaining <= 0.0:
            break
        # Tick the runloop briefly; this lets completion handlers run.
        NSRunLoop.currentRunLoop().runMode_beforeDate_(NSDefaultRunLoopMode, _nsdate_in(min(0.05, remaining)))
    return event.is_set()


def _to_py(obj: Any) -> Any:
    """
    Convert common Foundation collections into Python types recursively.
    Leaves unknown objects as-is (you can still interact with them as ObjC proxies).
    """
    if obj is None:
        return None
    if isinstance(obj, (str, bytes, int, float, bool)):
        return obj
    if isinstance(obj, NSDictionary):
        return { _to_py(k): _to_py(v) for k, v in obj.items() }
    if isinstance(obj, NSArray):
        return [ _to_py(x) for x in obj ]
    return obj


def _try_kvc(obj: Any, keys: List[str]) -> Optional[Any]:
    """
    Try valueForKey_ for any of the keys; returns first non-None value.
    """
    try:
        for k in keys:
            try:
                val = obj.valueForKey_(k)
                if val is not None:
                    return val
            except Exception:
                continue
    except Exception:
        pass
    return None


def _extension_obj_to_info(ext_obj: Any) -> Dict[str, Any]:
    """
    Best-effort conversion of an EXAppExtension (or similar) object into a Python dict.
    Because APIs evolve, we try several KVC keys defensively.
    """
    info: Dict[str, Any] = {}
    # Common keys we'd like to surface if present:
    name = _try_kvc(ext_obj, ["displayName", "localizedName", "name"])
    if name is not None:
        info["name"] = str(name)

    identifier = _try_kvc(ext_obj, ["bundleIdentifier", "identifier", "extensionIdentifier"])
    if identifier is not None:
        info["identifier"] = str(identifier)

    point_id = _try_kvc(ext_obj, ["extensionPointIdentifier", "EXExtensionPointIdentifier", "NSExtensionPointIdentifier"])
    if point_id is not None:
        info["extensionPointIdentifier"] = str(point_id)

    version = _try_kvc(ext_obj, ["version", "shortVersionString", "bundleShortVersionString"])
    if version is not None:
        info["version"] = str(version)

    attrs = _try_kvc(ext_obj, ["attributes", "extensionAttributes", "infoDictionary"])
    if attrs is not None:
        info["attributes"] = _to_py(attrs)

    # Identity details if present
    identity = _try_kvc(ext_obj, ["identity", "extensionIdentity"])
    if identity is not None:
        ident_dict: Dict[str, Any] = {}
        ident_bid = _try_kvc(identity, ["bundleIdentifier", "identifier"])
        if ident_bid is not None:
            ident_dict["bundleIdentifier"] = str(ident_bid)
        ident_point = _try_kvc(identity, ["extensionPointIdentifier"])
        if ident_point is not None:
            ident_dict["extensionPointIdentifier"] = str(ident_point)
        if ident_dict:
            info["identity"] = ident_dict

    return info


class AppExtensionBrowser:
    """
    Wrapper around EXAppExtensionBrowser for listing extensions.

    Methods:
    - list_extensions(extension_point_identifier: Optional[str], attributes: Optional[Dict], timeout: float) -> List[Dict]
      Returns a list of Python dicts with best-effort information about each extension.
    """

    def __init__(self) -> None:
        _assert_environment()
        _load_extensionkit_bundle_if_needed()

        # Get the browser class
        browser_cls = _lookup_class("EXAppExtensionBrowser")
        if browser_cls is None:
            raise ExtensionKitError(
                "EXAppExtensionBrowser not found. "
                "Make sure you're on macOS 13+ and install: pip install pyobjc-framework-ExtensionKit"
            )

        try:
            self._browser = browser_cls.alloc().init()
        except Exception as exc:
            raise ExtensionKitError(f"Failed to instantiate EXAppExtensionBrowser: {exc}") from exc

        # Cache available method names to adapt across OS/PyObjC variants
        self._method_names = dir(self._browser)

    def _call_extensions_matching(self, attrs_ns: Optional[NSDictionary], timeout: float) -> List[Any]:
        """
        Calls one of the known 'extensionsMatching...' APIs, depending on availability.
        Relies on the ExtensionKit PyObjC metadata for block signatures; without it,
        this will raise a helpful error.
        """
        # Candidate method names observed across SDK/API variants:
        candidates = [
            # Most likely ObjC selector mapping in PyObjC:
            "extensionsMatchingAttributes_completion_",
            # Alternative naming with 'Handler':
            "extensionsMatchingAttributes_completionHandler_",
            # Some SDKs might use a generic 'extensionsMatching' name:
            "extensionsMatching_completion_",
            "extensionsMatching_completionHandler_",
        ]
        method_name = next((m for m in candidates if m in self._method_names), None)
        if method_name is None:
            raise ExtensionKitError(
                "No compatible 'extensionsMatching...' method found on EXAppExtensionBrowser. "
                "You likely need a newer PyObjC: pip install -U pyobjc-framework-ExtensionKit"
            )

        if not _HAVE_PYOBJC_EXTENSIONKIT:
            raise ExtensionKitError(
                "This call requires block metadata from pyobjc-framework-ExtensionKit. "
                "Install/upgrade it and try again: pip install -U pyobjc-framework-ExtensionKit"
            )

        results_box: Dict[str, Any] = {"extensions": None, "error": None}
        done = threading.Event()

        def _completion_handler(extensions, error=None):
            # PyObjC should coerce NSArray -> NSArray proxy; convert later.
            results_box["extensions"] = extensions
            results_box["error"] = error
            done.set()

        # Some methods accept (attrs, completion), others might accept (attrs, completionHandler)
        try:
            fn = getattr(self._browser, method_name)
            # Execute on main queue to match Cocoa expectations
            def _invoke():
                try:
                    if attrs_ns is None:
                        fn(None, _completion_handler)
                    else:
                        fn(attrs_ns, _completion_handler)
                except Exception as exc:
                    results_box["error"] = exc
                    done.set()

            NSOperationQueue.mainQueue().addOperationWithBlock_(_invoke)
        except Exception as exc:
            raise ExtensionKitError(f"Failed to invoke EXAppExtensionBrowser.{method_name}: {exc}") from exc

        ok = _spin_runloop_until(done, timeout)
        if not ok:
            raise ExtensionKitError(f"Timed out waiting for extension query after {timeout:.1f}s")

        err = results_box.get("error")
        if err is not None:
            raise ExtensionKitError(f"Extension query error: {err}")

        nsarray = results_box.get("extensions")
        if nsarray is None:
            return []
        if isinstance(nsarray, NSArray):
            return list(nsarray)
        # In case the API returns something else unexpected
        return [nsarray]

    @staticmethod
    def _make_attrs_dict(extension_point_identifier: Optional[str], attributes: Optional[Dict[str, Any]]) -> Optional[NSDictionary]:
        if extension_point_identifier is None and not attributes:
            return None
        md = NSMutableDictionary.dictionary()
        if attributes:
            for k, v in attributes.items():
                if k is None:
                    continue
                md[k] = v
        # Try both common keys to filter by extension point:
        if extension_point_identifier:
            md["EXExtensionPointIdentifier"] = extension_point_identifier
            md["NSExtensionPointIdentifier"] = extension_point_identifier
        return md

    def list_extensions(
        self,
        extension_point_identifier: Optional[str] = None,
        attributes: Optional[Dict[str, Any]] = None,
        timeout: float = 5.0,
    ) -> List[Dict[str, Any]]:
        """
        Discover available extensions. Optionally filter by an extension point identifier
        and/or provide matching attributes.

        Returns a list of dictionaries with best-effort metadata for each extension.
        """
        attrs_ns = self._make_attrs_dict(extension_point_identifier, attributes)
        objs = self._call_extensions_matching(attrs_ns, timeout=timeout)
        return [_extension_obj_to_info(o) for o in objs]


class ExtensionKit:
    """
    High-level convenience wrapper and entry point.
    """

    @staticmethod
    def is_available() -> bool:
        if platform.system() != "Darwin":
            return False
        if not _is_macos_ventura_or_newer():
            return False
        _load_extensionkit_bundle_if_needed()
        return _lookup_class("EXAppExtensionBrowser") is not None

    @staticmethod
    def browser() -> AppExtensionBrowser:
        return AppExtensionBrowser()


# Example usage
if __name__ == "__main__":
    if not ExtensionKit.is_available():
        print("ExtensionKit not available. Ensure macOS 13+ and install pyobjc-framework-ExtensionKit.")
        sys.exit(1)

    browser = ExtensionKit.browser()

    # Example 1: list all visible extensions (best-effort)
    try:
        all_exts = browser.list_extensions(timeout=8.0)
        print(f"Found {len(all_exts)} extensions")
        for i, info in enumerate(all_exts, 1):
            print(f"{i:3d}. {info.get('name') or '(no name)'} "
                  f"- id={info.get('identifier')}, point={info.get('extensionPointIdentifier')}")
    except ExtensionKitError as e:
        print(f"Error listing extensions: {e}")

    # Example 2: filter by known extension point identifier (replace with your target)
    # For example, Finder Sync extensions historically use 'com.apple.FinderSync'
    # Note: ExtensionKit-based points may differ; consult your extension's point identifier.
    try:
        finder_exts = browser.list_extensions(extension_point_identifier="com.apple.FinderSync", timeout=8.0)
        print(f"\nFinder Sync-like extensions discovered: {len(finder_exts)}")
        for info in finder_exts:
            print(f"- {info.get('name')} ({info.get('identifier')})")
    except ExtensionKitError as e:
        print(f"Error filtering by extension point: {e}")
------------------------------------------------------------

PROMPT 264:
"Generate Python code that uses the `pydantic` library to generate PEP 561 type stubs for pybind11 modules, allowing for automatic generation of type hints and documentation for functions and classes defined in the module."
RESPONSE:
Below is a single-file script that uses pydantic models to introspect a pybind11-backed extension module, model its API, and emit PEP 561-compliant .pyi stub files with docstrings. It attempts to get signatures via inspect.signature, and falls back to parsing pybind11-style docstring signatures (including multiple overloads). It renders classes, functions, methods, properties, and variables, and writes a single .pyi file per module.

Save as generate_pybind11_stubs.py and run:
  python generate_pybind11_stubs.py your_module_name -o ./stubs

If your module isn’t on sys.path, add paths with --sys-path DIR.

Code:
import argparse
import importlib
import inspect
import os
import pkgutil
import re
import sys
import textwrap
from dataclasses import dataclass
from pathlib import Path
from types import ModuleType
from typing import Any, Iterable, Iterator, Optional, Sequence

from pydantic import BaseModel, Field


# -------------------------------
# Pydantic Models for API schema
# -------------------------------
class ParameterSpec(BaseModel):
    name: str
    annotation: str = "Any"
    default: Optional[str] = None
    kind: str = "positional_or_keyword"  # positional_only, positional_or_keyword, var_positional, keyword_only, var_keyword, marker('/', '*')
    doc: Optional[str] = None

    def is_marker(self) -> bool:
        return self.kind in {"/", "*"}


class SimpleSignature(BaseModel):
    parameters: list[ParameterSpec] = Field(default_factory=list)
    returns: str = "Any"


class FunctionSpec(BaseModel):
    name: str
    signatures: list[SimpleSignature] = Field(default_factory=list)
    doc: Optional[str] = None
    decorators: list[str] = Field(default_factory=list)
    is_method: bool = False
    is_classmethod: bool = False
    is_staticmethod: bool = False
    is_property: bool = False


class AttributeSpec(BaseModel):
    name: str
    annotation: str = "Any"
    doc: Optional[str] = None
    value_repr: Optional[str] = None
    is_property: bool = False


class ClassSpec(BaseModel):
    name: str
    bases: list[str] = Field(default_factory=lambda: ["object"])
    methods: list[FunctionSpec] = Field(default_factory=list)
    attributes: list[AttributeSpec] = Field(default_factory=list)
    doc: Optional[str] = None


class ModuleSpec(BaseModel):
    name: str
    doc: Optional[str] = None
    functions: list[FunctionSpec] = Field(default_factory=list)
    classes: list[ClassSpec] = Field(default_factory=list)
    variables: list[AttributeSpec] = Field(default_factory=list)
    imports: set[str] = Field(default_factory=set)


# -------------------------------
# Utilities
# -------------------------------
def format_annotation(ann: Any) -> str:
    if ann is inspect._empty:
        return "Any"
    try:
        # Handle typing objects and builtins gracefully
        s = getattr(ann, "__name__", None)
        if s:
            if getattr(ann, "__module__", "builtins") in ("builtins", "typing"):
                return s
            return f"{ann.__module__}.{s}"
        # For typing constructs, forward refs, strings, etc.
        return str(ann).replace("NoneType", "None")
    except Exception:
        return "Any"


def format_default(val: Any) -> Optional[str]:
    if val is inspect._empty:
        return None
    try:
        if val is None:
            return "None"
        if isinstance(val, (int, float, bool, str)):
            return repr(val)
        # Complex defaults: just use ellipsis to indicate presence of a default
        return "..."
    except Exception:
        return "..."


def split_params(param_str: str) -> list[str]:
    # Split by commas not nested within (), [], {}
    parts = []
    buf = []
    depth = 0
    for ch in param_str:
        if ch in "([{":
            depth += 1
            buf.append(ch)
        elif ch in ")]}":
            depth -= 1
            buf.append(ch)
        elif ch == "," and depth == 0:
            parts.append("".join(buf).strip())
            buf = []
        else:
            buf.append(ch)
    if buf:
        parts.append("".join(buf).strip())
    return parts


_sig_line_re = re.compile(r"^\s*([A-Za-z_][\w\.]*)\s*\((.*)\)\s*->\s*([^\n]+)\s*$")


def parse_docstring_signatures(name: str, doc: Optional[str]) -> list[SimpleSignature]:
    if not doc:
        return []
    lines = doc.splitlines()
    sigs: list[SimpleSignature] = []
    for line in lines[:32]:  # Scan first 32 lines for signatures
        m = _sig_line_re.match(line)
        if not m:
            # pybind11 often shows multiple overloads; stop at separator
            if line.strip().startswith("--"):
                break
            continue
        func_name, param_str, ret_str = m.groups()
        short_name = func_name.split(".")[-1]
        if short_name != name:
            continue
        parameters: list[ParameterSpec] = []
        if param_str.strip():
            for p in split_params(param_str):
                if p == "/":
                    parameters.append(ParameterSpec(name="/", kind="/"))
                    continue
                if p == "*":
                    parameters.append(ParameterSpec(name="*", kind="*"))
                    continue
                # Handle **kwargs and *args
                kind = "positional_or_keyword"
                p_work = p
                if p_work.startswith("**"):
                    name_part = p_work[2:]
                    ann = "Any"
                    default = None
                    if ":" in name_part:
                        nm, ann_str = [s.strip() for s in name_part.split(":", 1)]
                        ann = ann_str or "Any"
                        name_part = nm
                    parameters.append(
                        ParameterSpec(
                            name=name_part, annotation=ann, default=None, kind="var_keyword"
                        )
                    )
                    continue
                if p_work.startswith("*"):
                    name_part = p_work[1:]
                    ann = "Any"
                    if ":" in name_part:
                        nm, ann_str = [s.strip() for s in name_part.split(":", 1)]
                        ann = ann_str or "Any"
                        name_part = nm
                    parameters.append(
                        ParameterSpec(
                            name=name_part, annotation=ann, default=None, kind="var_positional"
                        )
                    )
                    continue
                # Normal param possibly with annotation and default
                default = None
                ann = "Any"
                if ":" in p_work:
                    left, ann_str = [s.strip() for s in p_work.split(":", 1)]
                    p_work = left
                    if "=" in ann_str:
                        ann_str, def_str = [s.strip() for s in ann_str.split("=", 1)]
                        default = def_str or "..."
                    ann = ann_str or "Any"
                if "=" in p_work and default is None:
                    nm, def_str = [s.strip() for s in p_work.split("=", 1)]
                    p_work = nm
                    default = def_str or "..."
                parameters.append(ParameterSpec(name=p_work, annotation=ann, default=default, kind=kind))
        sigs.append(SimpleSignature(parameters=parameters, returns=ret_str.strip() or "Any"))
    return sigs


def get_signature_from_inspect(obj: Any) -> Optional[SimpleSignature]:
    try:
        sig = inspect.signature(obj)
    except (TypeError, ValueError):
        return None
    params: list[ParameterSpec] = []
    saw_positional_only = False
    saw_kwonly = False
    for p in sig.parameters.values():
        if p.kind == inspect.Parameter.POSITIONAL_ONLY:
            saw_positional_only = True
            params.append(
                ParameterSpec(
                    name=p.name,
                    annotation=format_annotation(p.annotation),
                    default=format_default(p.default),
                    kind="positional_only",
                )
            )
        elif p.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
            params.append(
                ParameterSpec(
                    name=p.name,
                    annotation=format_annotation(p.annotation),
                    default=format_default(p.default),
                    kind="positional_or_keyword",
                )
            )
        elif p.kind == inspect.Parameter.VAR_POSITIONAL:
            params.append(
                ParameterSpec(
                    name=p.name, annotation=format_annotation(p.annotation), kind="var_positional"
                )
            )
        elif p.kind == inspect.Parameter.KEYWORD_ONLY:
            saw_kwonly = True
            params.append(
                ParameterSpec(
                    name=p.name,
                    annotation=format_annotation(p.annotation),
                    default=format_default(p.default),
                    kind="keyword_only",
                )
            )
        elif p.kind == inspect.Parameter.VAR_KEYWORD:
            params.append(
                ParameterSpec(
                    name=p.name, annotation=format_annotation(p.annotation), kind="var_keyword"
                )
            )
    # Insert markers / and * when needed
    if saw_positional_only:
        # find first non-positional-only index
        idx = 0
        for q in sig.parameters.values():
            if q.kind != inspect.Parameter.POSITIONAL_ONLY:
                break
            idx += 1
        params.insert(idx, ParameterSpec(name="/", kind="/"))
    if saw_kwonly and not any(p.kind == "var_positional" for p in params):
        # Add '*' before first keyword-only param
        first_kwonly = next(i for i, p in enumerate(params) if p.kind == "keyword_only")
        params.insert(first_kwonly, ParameterSpec(name="*", kind="*"))

    return SimpleSignature(parameters=params, returns=format_annotation(sig.return_annotation))


def get_public_members(module: ModuleType):
    for name, value in inspect.getmembers(module):
        if name.startswith("_"):
            continue
        yield name, value


def qualname_or_name(tp: type) -> str:
    try:
        if tp is object:
            return "object"
        mod = getattr(tp, "__module__", "builtins")
        name = getattr(tp, "__qualname__", getattr(tp, "__name__", "object"))
        if mod in ("builtins",):
            return name
        return f"{mod}.{name}"
    except Exception:
        return "object"


def collect_module_spec(module: ModuleType) -> ModuleSpec:
    mod_spec = ModuleSpec(name=module.__name__, doc=inspect.getdoc(module) or "")
    mod_name = module.__name__

    # Collect classes first
    classes: dict[str, ClassSpec] = {}
    for name, obj in get_public_members(module):
        if inspect.isclass(obj) and getattr(obj, "__module__", None) == mod_name:
            cls = ClassSpec(
                name=name,
                bases=[qualname_or_name(b) for b in getattr(obj, "__bases__", (object,)) if b is not object] or ["object"],
                doc=inspect.getdoc(obj) or "",
            )
            # Attributes and methods
            for member_name, member in inspect.getmembers(obj):
                if member_name.startswith("_"):
                    continue
                # Methods and function descriptors
                is_prop = isinstance(getattr(obj, member_name, None), property)
                if inspect.isfunction(member) or inspect.ismethoddescriptor(member) or inspect.isbuiltin(member):
                    fs = FunctionSpec(
                        name=member_name,
                        doc=(inspect.getdoc(member) or ""),
                        is_method=True,
                    )
                    # Determine decorators from class dict when possible
                    raw = getattr(obj, member_name, None)
                    if isinstance(raw, classmethod):
                        fs.is_classmethod = True
                        fs.decorators.append("classmethod")
                    elif isinstance(raw, staticmethod):
                        fs.is_staticmethod = True
                        fs.decorators.append("staticmethod")

                    # Signature
                    s = get_signature_from_inspect(member)
                    if s:
                        fs.signatures.append(s)
                    else:
                        fs.signatures.extend(parse_docstring_signatures(member_name, inspect.getdoc(member)))
                        if not fs.signatures:
                            # best-effort: unknown signature
                            fs.signatures.append(SimpleSignature(parameters=[], returns="Any"))
                    cls.methods.append(fs)
                elif is_prop:
                    # Render as property in stub
                    fs = FunctionSpec(
                        name=member_name,
                        doc=(inspect.getdoc(getattr(obj, member_name)) or ""),
                        is_method=True,
                        is_property=True,
                        decorators=["property"],
                    )
                    # We don't know the type; attempt from fget annotations if any
                    ann = "Any"
                    try:
                        fget = getattr(obj, member_name).fget
                        if fget is not None:
                            sig = inspect.signature(fget)
                            ann = format_annotation(sig.return_annotation)
                    except Exception:
                        pass
                    fs.signatures.append(SimpleSignature(parameters=[ParameterSpec(name="self", annotation=cls.name)], returns=ann))
                    cls.methods.append(fs)
                else:
                    # Attribute
                    cls.attributes.append(AttributeSpec(name=member_name, annotation="Any", doc=inspect.getdoc(member)))
            classes[name] = cls

    # Functions and variables
    for name, obj in get_public_members(module):
        if inspect.isclass(obj) and name in classes:
            continue
        if inspect.isfunction(obj) or inspect.isbuiltin(obj):
            fs = FunctionSpec(name=name, doc=(inspect.getdoc(obj) or ""))
            s = get_signature_from_inspect(obj)
            if s:
                fs.signatures.append(s)
            else:
                fs.signatures.extend(parse_docstring_signatures(name, inspect.getdoc(obj)))
                if not fs.signatures:
                    fs.signatures.append(SimpleSignature(parameters=[], returns="Any"))
            mod_spec.functions.append(fs)
        else:
            # Constants or module-level attributes
            mod_spec.variables.append(AttributeSpec(name=name, annotation="Any", doc=inspect.getdoc(obj)))

    mod_spec.classes = list(classes.values())
    return mod_spec


# -------------------------------
# Rendering .pyi
# -------------------------------
def render_parameter(p: ParameterSpec) -> str:
    if p.is_marker():
        return p.kind  # '/' or '*'
    if p.kind == "var_positional":
        base = f"*{p.name}: {p.annotation}"
    elif p.kind == "var_keyword":
        base = f"**{p.name}: {p.annotation}"
    else:
        base = f"{p.name}: {p.annotation}"
    if p.default is not None:
        return f"{base} = {p.default}"
    return base


def render_signature(sig: SimpleSignature) -> str:
    params = ", ".join(render_parameter(p) for p in sig.parameters)
    return f"({params}) -> {sig.returns}"


def indent(text: str, level: int = 1, spaces: int = 4) -> str:
    pad = " " * (level * spaces)
    return "\n".join(pad + line if line else "" for line in text.splitlines())


def sanitize_doc(doc: Optional[str]) -> str:
    if not doc:
        return ""
    doc = doc.strip("\n")
    # Avoid triple-quote collisions
    doc = doc.replace('"""', '\"\"\"')
    return doc


def render_function(fn: FunctionSpec, level: int = 0) -> str:
    lines: list[str] = []
    deco_prefix = "@" if level == 0 else indent("@", level)[:-1]  # not actually used directly
    for dec in fn.decorators:
        lines.append(indent(f"@{dec}", level))
    if len(fn.signatures) <= 1:
        sig = render_signature(fn.signatures[0] if fn.signatures else SimpleSignature())
        lines.append(indent(f"def {fn.name}{sig}: ...", level))
    else:
        # Overloads
        for sig in fn.signatures:
            lines.append(indent("@overload", level))
            lines.append(indent(f"def {fn.name}{render_signature(sig)}: ...", level))
    if fn.doc:
        doc = sanitize_doc(fn.doc)
        if doc:
            lines.append(indent(f'"""', level))
            lines.append(indent(doc, level))
            lines.append(indent(f'"""', level))
    return "\n".join(lines)


def render_class(cls: ClassSpec, level: int = 0) -> str:
    bases = ", ".join(cls.bases) if cls.bases else "object"
    lines: list[str] = [indent(f"class {cls.name}({bases}):", level)]
    body: list[str] = []
    if cls.doc:
        doc = sanitize_doc(cls.doc)
        if doc:
            body.append(indent('"""', level + 1))
            body.append(indent(doc, level + 1))
            body.append(indent('"""', level + 1))
    # Attributes
    for attr in sorted(cls.attributes, key=lambda a: a.name):
        body.append(indent(f"{attr.name}: {attr.annotation}", level + 1))
    # Methods and properties
    for fn in sorted(cls.methods, key=lambda f: (not f.is_property, f.name)):
        if fn.is_property and "property" not in fn.decorators:
            fn.decorators.insert(0, "property")
        body.append(render_function(fn, level + 1))
    if not body:
        body.append(indent("...", level + 1))
    return "\n".join(lines + body)


def render_module(mod: ModuleSpec) -> str:
    lines: list[str] = []
    lines.append("# This file was autogenerated. Edit with care.")
    lines.append("from __future__ import annotations")
    lines.append("from typing import Any, Optional, Iterable, Sequence, Mapping, overload")
    if mod.doc:
        doc = sanitize_doc(mod.doc)
        if doc:
            lines.append('"""')
            lines.append(doc)
            lines.append('"""')
    # Variables
    for var in sorted(mod.variables, key=lambda v: v.name):
        lines.append(f"{var.name}: {var.annotation}")
    # Functions
    for fn in sorted(mod.functions, key=lambda f: f.name):
        lines.append(render_function(fn, 0))
    # Classes
    for cls in sorted(mod.classes, key=lambda c: c.name):
        lines.append(render_class(cls, 0))
    return "\n\n".join(lines) + "\n"


# -------------------------------
# Module discovery (handle packages)
# -------------------------------
def iter_submodules(root_name: str) -> Iterator[str]:
    try:
        root_mod = importlib.import_module(root_name)
    except Exception:
        return
    if not hasattr(root_mod, "__path__"):
        yield root_name
        return
    # Package: yield package and submodules
    yield root_name
    for modinfo in pkgutil.walk_packages(root_mod.__path__, root_mod.__name__ + "."):
        yield modinfo.name


# -------------------------------
# CLI
# -------------------------------
def generate_stub_for_module(module_name: str, out_dir: Path) -> Optional[Path]:
    try:
        mod = importlib.import_module(module_name)
    except Exception as e:
        print(f"[WARN] Failed to import {module_name}: {e}", file=sys.stderr)
        return None
    spec = collect_module_spec(mod)
    text = render_module(spec)

    # Write to file path reflecting package structure
    parts = module_name.split(".")
    target_dir = out_dir.joinpath(*parts[:-1])
    target_dir.mkdir(parents=True, exist_ok=True)
    pyi_path = target_dir / f"{parts[-1]}.pyi"
    pyi_path.write_text(text, encoding="utf-8")
    return pyi_path


def main(argv: Optional[Sequence[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="Generate PEP 561 .pyi stubs for pybind11 modules using pydantic models.")
    parser.add_argument("module", help="Root module or package name (e.g., myext or mypkg)")
    parser.add_argument("-o", "--out-dir", default="stubs", help="Output directory for .pyi files")
    parser.add_argument("--recursive", action="store_true", help="Recurse into submodules if the target is a package")
    parser.add_argument("--sys-path", action="append", default=[], help="Additional entries to prepend to sys.path")
    args = parser.parse_args(argv)

    for p in reversed(args.sys_path):
        sys.path.insert(0, p)

    out_dir = Path(args.out_dir).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    targets: Iterable[str]
    if args.recursive:
        targets = list(iter_submodules(args.module)) or [args.module]
    else:
        targets = [args.module]

    wrote = 0
    for modname in targets:
        p = generate_stub_for_module(modname, out_dir)
        if p:
            wrote += 1
            print(f"Wrote {p}")
    if wrote == 0:
        print("No stubs generated.", file=sys.stderr)
        return 1
    else:
        # PEP 561 note: For stub-only packages, distribute as <name>-stubs with .pyi files.
        # If you plan to ship alongside the runtime package, you can place these .pyi files
        # in the corresponding package directories. You do NOT need py.typed for stub-only content.
        return 0


if __name__ == "__main__":
    raise SystemExit(main())

Notes and tips:
- This favors best-effort types. Where annotations are missing, it defaults to Any.
- For pybind11 extensions, many signatures are available in __doc__. The parser extracts multiple overloads and emits @overload for each.
- To publish stubs, the recommended PEP 561 approach is a separate -stubs package containing only .pyi files. Place the generated directory into a distribution named yourpkg-stubs and do not include py.typed. If you co-locate .pyi with Python sources, then include a py.typed marker in the runtime package.
------------------------------------------------------------

PROMPT 265:
"Generate Python code that implements a scalable data loading library that can handle large datasets without requiring a backend, utilizing efficient file reading and writing techniques, and supporting various data formats."
RESPONSE:
# scaldata.py
# A lightweight, scalable data loading library for large datasets.
# - No server/backend required: pure file/URL streaming.
# - Efficient reading with chunked/streamed I/O and optional parallelism.
# - Supports CSV, JSON Lines, and Parquet (optional pyarrow).
# - Supports gzip/bz2/xz compression.
# - Stream-safe iterators for rows or batches; write utilities.

from __future__ import annotations

import csv
import io
import os
import json
import bz2
import gzip
import lzma
import math
import mmap
import errno
import typing as _t
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, date

# Optional dependencies: pyarrow for parquet; requests for HTTP
try:
    import pyarrow as _pa  # type: ignore
    import pyarrow.parquet as _pq  # type: ignore
except Exception:
    _pa = None
    _pq = None

try:
    import requests as _requests  # type: ignore
except Exception:
    _requests = None


# -------------------------
# Utility helpers
# -------------------------

_EXT_TO_FORMAT = {
    ".csv": "csv",
    ".tsv": "csv",
    ".txt": "csv",
    ".jsonl": "jsonl",
    ".ndjson": "jsonl",
    ".parquet": "parquet",
}

_COMP_EXTS = {".gz", ".bz2", ".xz"}

TextIter = _t.Iterable[str]
Record = _t.Dict[str, _t.Any]
RecordIter = _t.Iterable[Record]

NA_DEFAULTS = {"", "na", "n/a", "null", "none", "nan"}


def _is_url(path: _t.Union[str, os.PathLike]) -> bool:
    p = str(path)
    return p.startswith("http://") or p.startswith("https://")


def _infer_format(path: _t.Union[str, os.PathLike], fmt: str | None) -> str:
    if fmt:
        return fmt.lower()
    suf = "".join(Path(str(path)).suffixes).lower()
    # handle .csv.gz, .jsonl.gz, etc. by stripping compression suffixes
    if suf:
        parts = []
        for s in Path(str(path)).suffixes:
            parts.append(s.lower())
        # Remove trailing compression extension if present
        if parts and parts[-1] in _COMP_EXTS:
            parts = parts[:-1]
        if parts:
            # examine last non-compression suffix
            last = parts[-1]
            return _EXT_TO_FORMAT.get(last, _EXT_TO_FORMAT.get(parts[-1], "csv"))
    # Fallback
    ext = Path(str(path)).suffix.lower()
    return _EXT_TO_FORMAT.get(ext, "csv")


def _infer_compression(path: _t.Union[str, os.PathLike], compression: str | None) -> str | None:
    if compression and compression != "infer":
        return compression
    ext = Path(str(path)).suffix.lower()
    if ext in {".gz"}:
        return "gzip"
    if ext in {".bz2"}:
        return "bz2"
    if ext in {".xz"}:
        return "xz"
    return None


def _open_text_local(
    path: _t.Union[str, os.PathLike],
    mode: str = "rt",
    encoding: str = "utf-8",
    compression: str | None = None,
    buffering: int = 1024 * 1024,
) -> io.TextIOBase:
    if "b" in mode:
        raise ValueError("Use text mode for _open_text_local")
    comp = _infer_compression(path, compression)
    if comp == "gzip":
        return gzip.open(path, mode=mode, encoding=encoding)
    elif comp == "bz2":
        return bz2.open(path, mode=mode, encoding=encoding)
    elif comp == "xz":
        return lzma.open(path, mode=mode, encoding=encoding)
    else:
        return open(path, mode=mode, encoding=encoding, buffering=buffering, newline="")


def _open_binary_local(
    path: _t.Union[str, os.PathLike],
    mode: str = "rb",
    compression: str | None = None,
    buffering: int = 1024 * 1024,
) -> io.BufferedReader | io.BufferedWriter:
    comp = _infer_compression(path, compression)
    if comp == "gzip":
        return gzip.open(path, mode=mode)
    elif comp == "bz2":
        return bz2.open(path, mode=mode)
    elif comp == "xz":
        return lzma.open(path, mode=mode)
    else:
        return open(path, mode=mode, buffering=buffering)


def _http_iter_lines(
    url: str,
    encoding: str = "utf-8",
    chunk_size: int = 1024 * 128,
) -> TextIter:
    if _requests is None:
        raise ImportError("requests is required for HTTP/HTTPS URLs")
    resp = _requests.get(url, stream=True)
    resp.raise_for_status()
    # decode_unicode yields str lines
    return resp.iter_lines(decode_unicode=True, chunk_size=chunk_size)


# -------------------------
# Converters and casting
# -------------------------

def _to_bool(v: str) -> bool | None:
    if v is None:
        return None
    t = v.strip().lower()
    if t in {"true", "t", "1", "yes", "y"}:
        return True
    if t in {"false", "f", "0", "no", "n"}:
        return False
    return None


def _to_int(v: str) -> int | None:
    try:
        return int(v)
    except Exception:
        return None


def _to_float(v: str) -> float | None:
    try:
        return float(v)
    except Exception:
        return None


def _to_date(v: str) -> date | None:
    try:
        # Accepts ISO 8601 dates
        return date.fromisoformat(v)
    except Exception:
        return None


def _to_datetime(v: str) -> datetime | None:
    try:
        # Accepts ISO 8601 timestamps
        return datetime.fromisoformat(v.replace("Z", "+00:00"))
    except Exception:
        return None


_DTYPE_MAP: dict[str, _t.Callable[[str], _t.Any]] = {
    "str": lambda x: x,
    "string": lambda x: x,
    "int": _to_int,
    "float": _to_float,
    "bool": _to_bool,
    "date": _to_date,
    "datetime": _to_datetime,
}


def _build_converters(
    dtypes: dict[str, _t.Union[str, _t.Callable[[str], _t.Any]]] | None,
) -> dict[str, _t.Callable[[str], _t.Any]]:
    if not dtypes:
        return {}
    conv: dict[str, _t.Callable[[str], _t.Any]] = {}
    for k, v in dtypes.items():
        if callable(v):
            conv[k] = v  # type: ignore
        elif isinstance(v, str):
            fn = _DTYPE_MAP.get(v.lower())
            if fn is None:
                raise ValueError(f"Unknown dtype for column {k}: {v}")
            conv[k] = fn
        else:
            raise ValueError(f"Invalid dtype for column {k}: {type(v)}")
    return conv


def _apply_converters(
    row: Record, converters: dict[str, _t.Callable[[str], _t.Any]], na_values: set[str], coerce_errors: bool
) -> Record:
    if not converters and not na_values:
        return row
    out = {}
    for k, v in row.items():
        if isinstance(v, str):
            # normalize NA
            if v.strip().lower() in na_values:
                out[k] = None
                continue
        if k in converters:
            try:
                out[k] = converters[k](v) if isinstance(v, str) else v
            except Exception:
                out[k] = None if coerce_errors else (_raise_cast_error(k, v))
        else:
            out[k] = v
    return out


def _raise_cast_error(col: str, v: _t.Any):
    raise ValueError(f"Failed to cast column {col} value={v!r}")


# -------------------------
# Byte range partitioning (for parallel reading)
# -------------------------

def _file_size(path: _t.Union[str, os.PathLike]) -> int:
    return os.path.getsize(path)


def _aligned_ranges_by_newline(
    path: _t.Union[str, os.PathLike],
    num_workers: int,
    min_chunk_size: int = 1 << 20,
) -> list[tuple[int, int]]:
    size = _file_size(path)
    if num_workers <= 1 or size <= 0 or size < min_chunk_size:
        return [(0, size)]
    step = size // num_workers
    ranges: list[tuple[int, int]] = []
    start = 0
    with open(path, "rb") as f:
        for i in range(num_workers):
            if i == num_workers - 1:
                end = size
            else:
                end = (i + 1) * step
                f.seek(end)
                # scan forward to next newline to avoid splitting a record
                buf = f.read(1024 * 1024)
                pos = buf.find(b"\n")
                if pos == -1:
                    # try to find newline in the next chunks
                    total = len(buf)
                    while True:
                        more = f.read(1024 * 1024)
                        if not more:
                            end = size
                            break
                        p2 = more.find(b"\n")
                        total += len(more)
                        if p2 != -1:
                            end = end + total - len(more) + p2 + 1
                            break
                else:
                    end = end + pos + 1
            ranges.append((start, end))
            start = end
    # Ensure coverage
    if ranges and ranges[-1][1] != size:
        ranges[-1] = (ranges[-1][0], size)
    return ranges


# -------------------------
# CSV loaders
# -------------------------

def _csv_reader_serial(
    path_or_url: _t.Union[str, os.PathLike],
    delimiter: str,
    has_header: bool,
    encoding: str,
    compression: str | None,
    na_values: set[str],
    dtypes: dict[str, _t.Union[str, _t.Callable[[str], _t.Any]]] | None,
    columns: list[str] | None,
) -> _t.Iterator[Record]:
    converters = _build_converters(dtypes)
    coerce_errors = True
    if _is_url(path_or_url):
        for line in _http_iter_lines(str(path_or_url), encoding=encoding):
            yield from _csv_parse_from_lines([line], delimiter, has_header, converters, na_values, columns, collect_stream=True)
        return

    with _open_text_local(path_or_url, mode="rt", encoding=encoding, compression=compression) as f:
        if has_header:
            reader = csv.DictReader(f, delimiter=delimiter)
        else:
            if not columns:
                raise ValueError("columns must be provided when has_header=False")
            reader = csv.DictReader(f, delimiter=delimiter, fieldnames=columns)
            # But DictReader requires header row; trick: set fieldnames and iterate raw
        for row in reader:
            if columns:
                row = {k: row.get(k) for k in columns}
            row = _apply_converters(row, converters, na_values, coerce_errors)
            yield row


def _csv_parse_from_lines(
    lines: _t.Iterable[str],
    delimiter: str,
    has_header: bool,
    converters: dict[str, _t.Callable[[str], _t.Any]],
    na_values: set[str],
    columns: list[str] | None,
    collect_stream: bool = False,
) -> _t.Iterator[Record]:
    # Internal helper for HTTP streaming; note this collects only current chunk passed in.
    coerce_errors = True
    reader = csv.reader(lines, delimiter=delimiter)
    header = None
    if has_header:
        try:
            header = next(reader)
        except StopIteration:
            return
    else:
        if not columns:
            raise ValueError("columns must be provided when has_header=False")
        header = columns
    for parts in reader:
        row = {header[i]: parts[i] if i < len(parts) else None for i in range(len(header))}
        if columns:
            row = {k: row.get(k) for k in columns}
        yield _apply_converters(row, converters, na_values, coerce_errors)


def _csv_worker_chunk(
    path: str,
    start: int,
    end: int,
    delimiter: str,
    has_header: bool,
    encoding: str,
    header: list[str] | None,
    dtypes: dict[str, _t.Union[str, _t.Callable[[str], _t.Any]]] | None,
    na_values: set[str],
    columns: list[str] | None,
) -> list[Record]:
    # WARNING: Parallel CSV parsing assumes no embedded newlines inside quoted fields.
    # If that assumption is violated, results may be incorrect.
    converters = _build_converters(dtypes)
    out: list[Record] = []
    with open(path, "rb") as f:
        f.seek(start)
        data = f.read(end - start)
    text = io.TextIOWrapper(io.BytesIO(data), encoding=encoding, newline="")
    reader = csv.reader(text, delimiter=delimiter)
    cols = header
    if has_header and start == 0:
        # Read header from first chunk
        try:
            cols = next(reader)
        except StopIteration:
            return out
    if not cols:
        raise ValueError("Header/columns could not be determined.")
    for parts in reader:
        row = {cols[i]: parts[i] if i < len(parts) else None for i in range(len(cols))}
        if columns:
            row = {k: row.get(k) for k in columns}
        row = _apply_converters(row, converters, na_values, True)
        out.append(row)
    return out


def iter_csv_rows(
    path_or_url: _t.Union[str, os.PathLike],
    delimiter: str = ",",
    has_header: bool = True,
    encoding: str = "utf-8",
    compression: str | None = "infer",
    dtypes: dict[str, _t.Union[str, _t.Callable[[str], _t.Any]]] | None = None,
    columns: list[str] | None = None,
    na_values: set[str] | None = None,
    parallel: bool = False,
    num_workers: int | None = None,
) -> _t.Iterator[Record]:
    """
    Stream rows from a CSV/TSV file. If parallel=True, assumes no embedded newlines in fields.
    - path_or_url: local path or http(s) URL (URL forces serial streaming).
    - returns generator of dict rows.
    """
    if na_values is None:
        na_values = set(NA_DEFAULTS)
    path_str = str(path_or_url)
    if _is_url(path_or_url):
        # Stream via HTTP; no parallel
        converters = _build_converters(dtypes)
        col_header: list[str] | None = None
        first_line = True
        buf: list[str] = []
        for line in _http_iter_lines(path_str, encoding=encoding):
            if first_line and has_header:
                col_header = next(csv.reader([line], delimiter=delimiter), None)
                first_line = False
                continue
            if not has_header and first_line:
                if not columns:
                    raise ValueError("columns must be provided when has_header=False (HTTP)")
                col_header = columns
                first_line = False
            parts = next(csv.reader([line], delimiter=delimiter), [])
            row = {col_header[i]: parts[i] if i < len(parts) else None for i in range(len(col_header))}
            if columns:
                row = {k: row.get(k) for k in columns}
            yield _apply_converters(row, converters, na_values, True)
        return

    # Local file
    if not parallel or compression not in (None, "infer") or _infer_compression(path_str, compression) is not None:
        yield from _csv_reader_serial(path_or_url, delimiter, has_header, encoding, compression, na_values, dtypes, columns)
        return

    # Parallel path (uncompressed local file only)
    ranges = _aligned_ranges_by_newline(path_str, num_workers or os.cpu_count() or 2)
    header_row: list[str] | None = None
    if has_header:
        # Read header from start of file
        with _open_text_local(path_or_url, "rt", encoding=encoding, compression=None) as f:
            r = csv.reader(f, delimiter=delimiter)
            try:
                header_row = next(r)
            except StopIteration:
                return

    with ProcessPoolExecutor(max_workers=len(ranges)) as ex:
        futures = []
        for (start, end) in ranges:
            futures.append(
                ex.submit(
                    _csv_worker_chunk,
                    path_str,
                    start,
                    end,
                    delimiter,
                    has_header,
                    encoding,
                    header_row,
                    dtypes,
                    na_values,
                    columns,
                )
            )
        # Yield in order of file position (submit order matches ranges order)
        for fut in futures:
            chunk_rows = fut.result()
            for row in chunk_rows:
                yield row


def iter_csv_batches(
    path_or_url: _t.Union[str, os.PathLike],
    batch_size: int = 10000,
    **kwargs,
) -> _t.Iterator[list[Record]]:
    batch: list[Record] = []
    for row in iter_csv_rows(path_or_url, **kwargs):
        batch.append(row)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    if batch:
        yield batch


# -------------------------
# JSONL loaders
# -------------------------

def _jsonl_worker_chunk(
    path: str,
    start: int,
    end: int,
    encoding: str,
    dtypes: dict[str, _t.Union[str, _t.Callable[[str], _t.Any]]] | None,
    na_values: set[str],
) -> list[Record]:
    converters = _build_converters(dtypes)
    out: list[Record] = []
    with open(path, "rb") as f:
        f.seek(start)
        pos = start
        while pos < end:
            line = f.readline()
            if not line:
                break
            pos += len(line)
            if not line.strip():
                continue
            try:
                obj = json.loads(line.decode(encoding))
            except Exception:
                continue
            # cast only string values; nested dicts/lists preserved
            casted = {
                k: _apply_converters({k: v}, converters, na_values, True)[k]
                if isinstance(v, str) and k in converters
                else (None if isinstance(v, str) and v.strip().lower() in na_values else v)
                for k, v in obj.items()
            }
            out.append(casted)
    return out


def iter_jsonl_rows(
    path_or_url: _t.Union[str, os.PathLike],
    encoding: str = "utf-8",
    compression: str | None = "infer",
    dtypes: dict[str, _t.Union[str, _t.Callable[[str], _t.Any]]] | None = None,
    na_values: set[str] | None = None,
    parallel: bool = False,
    num_workers: int | None = None,
) -> _t.Iterator[Record]:
    """
    Stream records from a JSON Lines (one JSON object per line) file.
    Safe for parallelization because JSONL requires one object per line.
    """
    if na_values is None:
        na_values = set(NA_DEFAULTS)

    if _is_url(path_or_url):
        for line in _http_iter_lines(str(path_or_url), encoding=encoding):
            if not line.strip():
                continue
            obj = json.loads(line)
            if dtypes:
                conv = _build_converters(dtypes)
                obj = {
                    k: (conv[k](v) if isinstance(v, str) and k in conv else (None if isinstance(v, str) and v.strip().lower() in na_values else v))
                    for k, v in obj.items()
                }
            yield obj
        return

    comp = _infer_compression(path_or_url, compression)
    if parallel and comp is None:
        ranges = _aligned_ranges_by_newline(str(path_or_url), num_workers or os.cpu_count() or 2)
        with ProcessPoolExecutor(max_workers=len(ranges)) as ex:
            futures = [
                ex.submit(_jsonl_worker_chunk, str(path_or_url), start, end, encoding, dtypes, na_values) for (start, end) in ranges
            ]
            for fut in futures:
                for row in fut.result():
                    yield row
        return

    # Serial streaming (local or compressed)
    with _open_text_local(path_or_url, "rt", encoding=encoding, compression=comp) as f:
        conv = _build_converters(dtypes)
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            if conv or na_values:
                obj = {
                    k: (conv[k](v) if isinstance(v, str) and k in conv else (None if isinstance(v, str) and v.strip().lower() in na_values else v))
                    for k, v in obj.items()
                }
            yield obj


def iter_jsonl_batches(
    path_or_url: _t.Union[str, os.PathLike],
    batch_size: int = 10000,
    **kwargs,
) -> _t.Iterator[list[Record]]:
    batch: list[Record] = []
    for row in iter_jsonl_rows(path_or_url, **kwargs):
        batch.append(row)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    if batch:
        yield batch


# -------------------------
# Parquet loaders (optional pyarrow)
# -------------------------

def iter_parquet_batches(
    path: _t.Union[str, os.PathLike],
    columns: list[str] | None = None,
    batch_size: int = 65536,
    memory_map: bool = True,
) -> _t.Iterator[list[Record]]:
    """
    Stream batches from a Parquet file using pyarrow, row-group aware and memory-mapped.
    """
    if _pq is None:
        raise ImportError("pyarrow is required to read parquet")
    pf = _pq.ParquetFile(str(path))
    for rg in range(pf.num_row_groups):
        # Efficient: load one row-group in batches
        for rec_batch in pf.iter_batches(
            row_groups=[rg], columns=columns, batch_size=batch_size, use_threads=True
        ):
            # Convert to list of dicts without loading entire table
            cols = [f.name for f in rec_batch.schema]
            cols_sel = columns or cols
            arrays = [rec_batch.column(i) for i in range(rec_batch.num_columns)]
            # pyarrow RecordBatch supports to_pylist, but that materializes batch (which is fine)
            for row in rec_batch.to_pylist():
                if columns:
                    yield [{k: row.get(k) for k in columns}]
                else:
                    yield [row]


def iter_parquet_rows(
    path: _t.Union[str, os.PathLike],
    columns: list[str] | None = None,
    batch_size: int = 65536,
) -> _t.Iterator[Record]:
    if _pq is None:
        raise ImportError("pyarrow is required to read parquet")
    pf = _pq.ParquetFile(str(path))
    for rec_batch in pf.iter_batches(columns=columns, batch_size=batch_size, use_threads=True):
        for row in rec_batch.to_pylist():
            yield row


# -------------------------
# Writers
# -------------------------

def write_csv(
    records: RecordIter,
    path: _t.Union[str, os.PathLike],
    columns: list[str] | None = None,
    delimiter: str = ",",
    encoding: str = "utf-8",
    compression: str | None = "infer",
    write_header: bool = True,
    append: bool = False,
) -> None:
    mode = "at" if append else "wt"
    first_row: Record | None = None
    it = iter(records)
    try:
        first_row = next(it)
    except StopIteration:
        # Nothing to write
        # Create empty file if needed
        if not append:
            _open_text_local(path, mode="wt", encoding=encoding, compression=compression).close()
        return
    cols = columns or list(first_row.keys())
    with _open_text_local(path, mode=mode, encoding=encoding, compression=compression) as f:
        w = csv.DictWriter(f, fieldnames=cols, delimiter=delimiter, lineterminator="\n")
        if write_header and not append:
            w.writeheader()
        w.writerow({k: first_row.get(k) for k in cols})
        for row in it:
            w.writerow({k: row.get(k) for k in cols})


def write_jsonl(
    records: RecordIter,
    path: _t.Union[str, os.PathLike],
    encoding: str = "utf-8",
    compression: str | None = "infer",
    append: bool = False,
    sort_keys: bool = False,
    separators: tuple[str, str] = (",", ":"),
) -> None:
    mode = "at" if append else "wt"
    with _open_text_local(path, mode=mode, encoding=encoding, compression=compression) as f:
        for rec in records:
            f.write(json.dumps(rec, ensure_ascii=False, sort_keys=sort_keys, separators=separators))
            f.write("\n")


def write_parquet(
    records: RecordIter,
    path: _t.Union[str, os.PathLike],
    schema_fields: dict[str, str] | None = None,
    batch_size: int = 65536,
) -> None:
    if _pa is None or _pq is None:
        raise ImportError("pyarrow is required to write parquet")
    # Buffer rows and write in chunks
    buf: list[Record] = []
    writer = None
    try:
        for rec in records:
            buf.append(rec)
            if len(buf) >= batch_size:
                table = _pa.Table.from_pylist(buf)
                if writer is None:
                    writer = _pq.ParquetWriter(str(path), table.schema, compression="zstd")
                writer.write_table(table)
                buf.clear()
        if buf:
            table = _pa.Table.from_pylist(buf)
            if writer is None:
                writer = _pq.ParquetWriter(str(path), table.schema, compression="zstd")
            writer.write_table(table)
    finally:
        if writer is not None:
            writer.close()


# -------------------------
# Unified public API
# -------------------------

@dataclass
class LoadOptions:
    fmt: str | None = None
    # CSV options
    delimiter: str = ","
    has_header: bool = True
    columns: list[str] | None = None
    # Shared
    encoding: str = "utf-8"
    compression: str | None = "infer"
    dtypes: dict[str, _t.Union[str, _t.Callable[[str], _t.Any]]] | None = None
    na_values: set[str] | None = None
    # Execution
    parallel: bool = False
    num_workers: int | None = None
    # Parquet
    parquet_columns: list[str] | None = None
    parquet_batch_size: int = 65536


def load_rows(
    path_or_url: _t.Union[str, os.PathLike],
    options: LoadOptions | None = None,
) -> _t.Iterator[Record]:
    opts = options or LoadOptions()
    fmt = _infer_format(path_or_url, opts.fmt)
    if fmt == "csv":
        return iter_csv_rows(
            path_or_url=path_or_url,
            delimiter=opts.delimiter,
            has_header=opts.has_header,
            encoding=opts.encoding,
            compression=opts.compression,
            dtypes=opts.dtypes,
            columns=opts.columns,
            na_values=opts.na_values,
            parallel=opts.parallel,
            num_workers=opts.num_workers,
        )
    elif fmt == "jsonl":
        return iter_jsonl_rows(
            path_or_url=path_or_url,
            encoding=opts.encoding,
            compression=opts.compression,
            dtypes=opts.dtypes,
            na_values=opts.na_values,
            parallel=opts.parallel,
            num_workers=opts.num_workers,
        )
    elif fmt == "parquet":
        return iter_parquet_rows(path_or_url, columns=opts.parquet_columns, batch_size=opts.parquet_batch_size)
    else:
        raise ValueError(f"Unsupported format: {fmt}")


def load_batches(
    path_or_url: _t.Union[str, os.PathLike],
    batch_size: int = 10000,
    options: LoadOptions | None = None,
) -> _t.Iterator[list[Record]]:
    opts = options or LoadOptions()
    fmt = _infer_format(path_or_url, opts.fmt)
    if fmt == "csv":
        return iter_csv_batches(
            path_or_url=path_or_url,
            batch_size=batch_size,
            delimiter=opts.delimiter,
            has_header=opts.has_header,
            encoding=opts.encoding,
            compression=opts.compression,
            dtypes=opts.dtypes,
            columns=opts.columns,
            na_values=opts.na_values,
            parallel=opts.parallel,
            num_workers=opts.num_workers,
        )
    elif fmt == "jsonl":
        return iter_jsonl_batches(
            path_or_url=path_or_url,
            batch_size=batch_size,
            encoding=opts.encoding,
            compression=opts.compression,
            dtypes=opts.dtypes,
            na_values=opts.na_values,
            parallel=opts.parallel,
            num_workers=opts.num_workers,
        )
    elif fmt == "parquet":
        # Materialize parquet batches into list-of-dict batches
        batch: list[Record] = []
        for row in iter_parquet_rows(path_or_url, columns=opts.parquet_columns, batch_size=opts.parquet_batch_size):
            batch.append(row)
            if len(batch) >= batch_size:
                yield batch
                batch = []
        if batch:
            yield batch
        return
    else:
        raise ValueError(f"Unsupported format: {fmt}")


def save_records(
    records: RecordIter,
    path: _t.Union[str, os.PathLike],
    fmt: str | None = None,
    encoding: str = "utf-8",
    compression: str | None = "infer",
    columns: list[str] | None = None,
) -> None:
    target_fmt = _infer_format(path, fmt)
    if target_fmt == "csv":
        write_csv(records, path, columns=columns, encoding=encoding, compression=compression)
    elif target_fmt == "jsonl":
        write_jsonl(records, path, encoding=encoding, compression=compression)
    elif target_fmt == "parquet":
        write_parquet(records, path)
    else:
        raise ValueError(f"Unsupported save format: {target_fmt}")


# -------------------------
# Example usage (commented)
# -------------------------
# opts = LoadOptions(dtypes={"age": "int", "price": "float"}, parallel=True)
# for batch in load_batches("data.csv.gz", batch_size=50000, options=opts):
#     process(batch)
#
# for row in load_rows("https://example.com/data.jsonl", options=LoadOptions(fmt="jsonl")):
#     handle(row)
#
# # Save back
# save_records(load_rows("data.jsonl"), "out.parquet")  # requires pyarrow
#
# Notes:
# - Parallel CSV reading assumes no embedded newlines inside fields.
# - Parallel reading is disabled for compressed files and HTTP URLs.
# - JSON Lines is safest for parallelism and streaming.
# - Parquet requires pyarrow and reads in efficient columnar batches.
------------------------------------------------------------

PROMPT 266:
"Generate Python code that uses the psycopg2 library to connect to a PostgreSQL database and retrieve information about the schema, such as table names, column names, and data types, and print it in a human-readable format."
RESPONSE:
import os
import sys
import argparse
import psycopg2
import psycopg2.extras


def get_schema_info(conn, schemas=None, include_system=False):
    """
    Retrieve schema information (tables and their columns with data types) from PostgreSQL.
    Returns a nested dict: {schema: {table_name: {"type": table_type, "columns": [ ... ]}}}
    """
    conditions = ["c.relkind IN ('r','p','v','m','f')"]  # tables, partitioned tables, views, matviews, foreign tables
    params = []

    if not include_system:
        conditions.append("n.nspname NOT IN ('pg_catalog', 'information_schema')")

    if schemas:
        conditions.append("n.nspname::text = ANY(%s)")
        params.append(schemas)

    where_clause = "WHERE " + " AND ".join(conditions)

    sql = f"""
        SELECT
            n.nspname AS schema_name,
            c.relname AS table_name,
            CASE c.relkind
                WHEN 'r' THEN 'BASE TABLE'
                WHEN 'p' THEN 'PARTITIONED TABLE'
                WHEN 'v' THEN 'VIEW'
                WHEN 'm' THEN 'MATERIALIZED VIEW'
                WHEN 'f' THEN 'FOREIGN TABLE'
                ELSE c.relkind::text
            END AS table_type,
            a.attnum,
            a.attname AS column_name,
            pg_catalog.format_type(a.atttypid, a.atttypmod) AS data_type,
            NOT a.attnotnull AS is_nullable,
            pg_catalog.pg_get_expr(ad.adbin, ad.adrelid) AS column_default
        FROM pg_catalog.pg_class c
        JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
        JOIN pg_catalog.pg_attribute a
          ON a.attrelid = c.oid
         AND a.attnum > 0
         AND NOT a.attisdropped
        LEFT JOIN pg_catalog.pg_attrdef ad
          ON ad.adrelid = c.oid
         AND ad.adnum = a.attnum
        {where_clause}
        ORDER BY n.nspname, c.relname, a.attnum;
    """

    schema_info = {}
    with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:
        cur.execute(sql, params)
        rows = cur.fetchall()

    for row in rows:
        schema = row["schema_name"]
        table = row["table_name"]
        table_type = row["table_type"]
        column = {
            "name": row["column_name"],
            "data_type": row["data_type"],
            "is_nullable": row["is_nullable"],
            "default": row["column_default"],
        }

        if schema not in schema_info:
            schema_info[schema] = {}
        if table not in schema_info[schema]:
            schema_info[schema][table] = {"type": table_type, "columns": []}
        schema_info[schema][table]["columns"].append(column)

    return schema_info


def print_schema_info(schema_info):
    if not schema_info:
        print("No tables or views found for the given filters.")
        return

    for schema in sorted(schema_info.keys()):
        print(f"Schema: {schema}")
        tables = schema_info[schema]
        for table_name in sorted(tables.keys()):
            t = tables[table_name]
            print(f"  Table: {table_name} ({t['type']})")
            for col in t["columns"]:
                nullability = "" if not col["is_nullable"] else " NULL"
                # If NOT NULL, show explicitly for clarity
                if not col["is_nullable"]:
                    nullability = " NOT NULL"
                default = f" DEFAULT {col['default']}" if col["default"] is not None else ""
                print(f"    - {col['name']}: {col['data_type']}{nullability}{default}")
        print()  # blank line between schemas


def main():
    parser = argparse.ArgumentParser(
        description="Inspect PostgreSQL schema and print tables/columns with data types using psycopg2."
    )
    parser.add_argument("--host", default=os.getenv("PGHOST", "localhost"))
    parser.add_argument("--port", type=int, default=int(os.getenv("PGPORT", "5432")))
    parser.add_argument("--dbname", default=os.getenv("PGDATABASE"))
    parser.add_argument("--user", default=os.getenv("PGUSER"))
    parser.add_argument("--password", default=os.getenv("PGPASSWORD"))
    parser.add_argument(
        "--schemas",
        nargs="*",
        help="Space-separated schema names to include (e.g., public sales). Defaults to all non-system schemas.",
    )
    parser.add_argument(
        "--include-system",
        action="store_true",
        help="Include system schemas (pg_catalog, information_schema).",
    )
    args = parser.parse_args()

    if not args.dbname:
        print("Error: database name is required. Provide --dbname or set PGDATABASE.", file=sys.stderr)
        sys.exit(2)

    conn = None
    try:
        conn = psycopg2.connect(
            host=args.host,
            port=args.port,
            dbname=args.dbname,
            user=args.user,
            password=args.password,
        )
        conn.autocommit = True  # read-only usage; no transaction needed

        schema_info = get_schema_info(
            conn,
            schemas=args.schemas,
            include_system=args.include_system,
        )
        print_schema_info(schema_info)

    except psycopg2.Error as e:
        print(f"Database error: {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"Unexpected error: {e}", file=sys.stderr)
        sys.exit(1)
    finally:
        if conn is not None:
            conn.close()


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 267:
"Generate Python code that imports the required modules, defines a function with the appropriate signature and docstring, and uses the `invoke` function from the `typing` module to create type-safe command line invocations."
RESPONSE:
It looks like there’s a mismatch: Python’s standard typing module does not provide an invoke function. Did you mean:

- The invoke package (pyinvoke), i.e., from invoke import run, Result, or
- A typed helper you’d like to name invoke that wraps subprocess?

Below are both approaches so you can pick the one you intended.

Option A: Use the “invoke” package with type hints (pip install invoke)
```python
from __future__ import annotations

import shlex
from typing import Literal, Mapping, Optional, Sequence

from invoke import Result, run

# Restrict commands to a known set for type safety. Extend as needed.
Command = Literal["echo", "ls", "cat"]

def run_cli(cmd: Command, args: Sequence[str] = (), env: Optional[Mapping[str, str]] = None) -> Result:
    """
    Run a shell command using the Invoke task runner with type-safe command names.

    Args:
        cmd: The base command to execute (restricted to known literals for type safety).
        args: Additional arguments to pass to the command.
        env: Optional environment variables to merge with the current environment.

    Returns:
        invoke.runners.Result describing the completed process.

    Raises:
        invoke.exceptions.Failure: If the command exits with a non-zero status when warn=False.
    """
    command_line = " ".join([shlex.quote(cmd), *(shlex.quote(a) for a in args)])
    # hide=False shows output; set warn=True if you don’t want exceptions on non-zero exit codes.
    return run(command_line, env=env, hide=False, warn=False)
```

Option B: Pure stdlib, typed “invoke” helper via subprocess
```python
from __future__ import annotations

import subprocess
from typing import Literal

# Restrict commands to a known set for type safety. Extend as needed.
Command = Literal["echo", "ls", "cat"]

def invoke(cmd: Command, *args: str, check: bool = True) -> subprocess.CompletedProcess[str]:
    """
    Run a typed shell command via subprocess.run.

    Args:
        cmd: The base command (restricted by Literal for type safety).
        *args: Additional arguments to pass to the command.
        check: If True, raises CalledProcessError on non-zero exit status.

    Returns:
        subprocess.CompletedProcess with stdout/stderr captured as text.
    """
    return subprocess.run([cmd, *args], check=check, text=True, capture_output=True)
```

Tell me which approach you want (pyinvoke vs. stdlib), and what commands you’d like to allow, and I can tailor the function and types accordingly.
------------------------------------------------------------

PROMPT 268:
"Generate Python code that imports the requests library and uses it to send a GET request to the quickchart.io API with the necessary parameters to generate a static chart image, then returns the image data as a bytes object."
RESPONSE:
import json
import requests


def get_quickchart_image(chart_config, width=600, height=300, image_format="png", background_color="white", timeout=30) -> bytes:
    """
    Generate a static chart image from QuickChart and return the image data as bytes.

    Args:
        chart_config (dict): Chart.js configuration dictionary.
        width (int): Image width in pixels.
        height (int): Image height in pixels.
        image_format (str): Image format, e.g., 'png', 'svg', 'webp'.
        background_color (str): Background color.
        timeout (int or float): Request timeout in seconds.

    Returns:
        bytes: The image data.
    """
    url = "https://quickchart.io/chart"
    params = {
        "c": json.dumps(chart_config, separators=(",", ":")),
        "width": width,
        "height": height,
        "format": image_format,
        "backgroundColor": background_color,
    }

    response = requests.get(url, params=params, timeout=timeout)
    response.raise_for_status()
    return response.content


# Example usage:
if __name__ == "__main__":
    chart_cfg = {
        "type": "line",
        "data": {
            "labels": ["Mon", "Tue", "Wed", "Thu", "Fri"],
            "datasets": [
                {"label": "Visitors", "data": [120, 90, 140, 110, 160], "fill": False, "borderColor": "blue"}
            ],
        },
        "options": {"plugins": {"legend": {"display": True}}},
    }

    image_bytes = get_quickchart_image(chart_cfg, width=800, height=400, image_format="png")
    with open("chart.png", "wb") as f:
        f.write(image_bytes)
------------------------------------------------------------

PROMPT 269:
"Generate Python code that imports the necessary modules, sets up a PostgreSQL database connection using Tortoise ORM, creates a table with the appropriate columns, inserts data into the table, and queries the data using SQL."
RESPONSE:
# pip install tortoise-orm asyncpg
import asyncio
import os
from decimal import Decimal

from tortoise import Tortoise, fields, models


# Define a model; Tortoise will create the table from this
class Product(models.Model):
    id = fields.IntField(pk=True)
    name = fields.CharField(max_length=100, unique=True)
    price = fields.DecimalField(max_digits=10, decimal_places=2)
    created_at = fields.DatetimeField(auto_now_add=True)

    class Meta:
        table = "product"  # optional; Tortoise would default to "product"


async def main():
    # Update DATABASE_URL or set the environment variable before running
    DATABASE_URL = os.getenv(
        "DATABASE_URL",
        "postgres://postgres:postgres@localhost:5432/exampledb",
    )

    # Initialize Tortoise and register models
    await Tortoise.init(
        db_url=DATABASE_URL,
        modules={"models": ["__main__"]},
    )

    # Create tables if they don't exist
    await Tortoise.generate_schemas(safe=True)

    # Insert sample data via the ORM
    await Product.all().delete()  # start clean for demo purposes
    await Product.create(name="Widget", price=Decimal("9.99"))
    await Product.create(name="Gadget", price=Decimal("19.50"))
    await Product.create(name="Doohickey", price=Decimal("5.25"))

    # Query the data using raw SQL
    conn = Tortoise.get_connection("default")
    table = Product._meta.db_table

    # 1) Get all rows
    all_rows = await conn.execute_query_dict(
        f'SELECT id, name, price, created_at FROM "{table}" ORDER BY id;'
    )
    print("All products:", all_rows)

    # 2) Parameterized query (asyncpg uses $1, $2 placeholders)
    min_price = Decimal("10.00")
    filtered_rows = await conn.execute_query_dict(
        f'SELECT id, name, price FROM "{table}" WHERE price > $1 ORDER BY price;',
        [min_price],
    )
    print(f"Products with price > {min_price}:", filtered_rows)

    # Close connections cleanly
    await Tortoise.close_connections()


if __name__ == "__main__":
    asyncio.run(main())
------------------------------------------------------------

PROMPT 270:
"Generate Python code that imports the necessary libraries and creates a SiftScience client object to interact with the Sift Science API, including functions to create and manage models, predict fraud risk, and retrieve data."
RESPONSE:
# Requirements:
#   pip install requests
#   Set environment variables SIFT_API_KEY and SIFT_ACCOUNT_ID (or pass directly to SiftAPI)

import os
import time
from typing import Any, Dict, Iterable, Optional
import requests


class SiftAPI:
    """
    Minimal Python client for interacting with Sift (Sift Science) REST APIs via requests.
    Uses Basic Auth with API key as the username and an empty password.

    Note on "models":
      Sift does not provide a public REST API to create or manage ML models. Those are typically
      configured in the Sift Console or via your Sift account team. The create/manage model
      methods below are placeholders that raise NotImplementedError with guidance.
    """

    BASE_V205 = "https://api.sift.com/v205"
    BASE_V3 = "https://api.sift.com/v3"

    def __init__(self, api_key: str, account_id: str, timeout: float = 10.0):
        self.api_key = api_key
        self.account_id = account_id
        self.timeout = timeout

        self.session = requests.Session()
        self.session.auth = (self.api_key, "")  # Basic auth: username=API key, password empty
        self.session.headers.update({"Content-Type": "application/json"})

    # ----------------------------
    # Event ingestion (track)
    # ----------------------------
    def track_event(
        self,
        event_type: str,
        properties: Dict[str, Any],
        return_action: bool = False,
    ) -> Dict[str, Any]:
        """
        Send an event to Sift for ingestion and optional real-time workflow action results.

        event_type: e.g., "$create_order", "$transaction", "$login", etc.
        properties: Must include "$user_id" or "$session_id" as required by Sift.
                    Do NOT include "$type" or "$api_key"; they are added here.
        return_action: If True, Sift returns workflow action results (if configured).

        Returns the JSON response from Sift.
        """
        url = f"{self.BASE_V205}/events"
        payload = dict(properties)
        payload["$type"] = event_type
        payload["$api_key"] = self.api_key

        params = {"return_action": "true"} if return_action else None

        resp = self.session.post(url, json=payload, params=params, timeout=self.timeout)
        resp.raise_for_status()
        return resp.json()

    # ----------------------------
    # Risk scoring (predict fraud risk)
    # ----------------------------
    def score_user(
        self,
        user_id: str,
        abuse_types: Optional[Iterable[str]] = None,
        return_action: bool = True,
    ) -> Dict[str, Any]:
        """
        Retrieve a user's fraud risk score for one or more abuse types.

        abuse_types: e.g., ["payment_abuse", "account_takeover", "content_abuse"].
                      If None, Sift defaults apply.
        return_action: If True, include workflow action recommendation(s).

        Returns the JSON response containing scores and (optionally) actions.
        """
        url = f"{self.BASE_V205}/score/{user_id}"
        params: Dict[str, str] = {}
        if abuse_types:
            params["abuse_types"] = ",".join(abuse_types)
        if return_action:
            params["return_action"] = "true"

        resp = self.session.get(url, params=params, timeout=self.timeout)
        resp.raise_for_status()
        return resp.json()

    # ----------------------------
    # Decisions (apply/retrieve)
    # ----------------------------
    def apply_user_decision(
        self,
        user_id: str,
        decision_id: str,
        source: str = "MANUAL_REVIEW",
        description: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Apply a decision to a user (e.g., block, watch, approve).
        decision_id: The ID of a decision configured in Sift Console.
        source: Typically "MANUAL_REVIEW" or "AUTOMATION".
        description: Optional free-text context.

        Returns Sift's JSON response for the decision application.
        """
        url = f"{self.BASE_V3}/accounts/{self.account_id}/users/{user_id}/decisions"
        body: Dict[str, Any] = {"decision_id": decision_id, "source": source}
        if description:
            body["description"] = description

        resp = self.session.post(url, json=body, timeout=self.timeout)
        resp.raise_for_status()
        return resp.json()

    def get_user_decisions(
        self,
        user_id: str,
        abuse_types: Optional[Iterable[str]] = None,
    ) -> Dict[str, Any]:
        """
        Retrieve decisions that currently apply to a user.
        """
        url = f"{self.BASE_V3}/accounts/{self.account_id}/users/{user_id}/decisions"
        params: Dict[str, str] = {}
        if abuse_types:
            params["abuse_types"] = ",".join(abuse_types)

        resp = self.session.get(url, params=params, timeout=self.timeout)
        resp.raise_for_status()
        return resp.json()

    # ----------------------------
    # Labels (legacy manual feedback)
    # ----------------------------
    def label_user(
        self,
        user_id: str,
        is_bad: bool,
        abuse_type: str,
        description: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Add a label (feedback) for a user. Labels are legacy; Decisions are preferred.

        abuse_type: e.g., "payment_abuse", "account_takeover", "content_abuse".
        """
        url = f"{self.BASE_V205}/users/{user_id}/labels"
        body: Dict[str, Any] = {"is_bad": is_bad, "abuse_type": abuse_type}
        if description:
            body["description"] = description

        resp = self.session.post(url, json=body, timeout=self.timeout)
        resp.raise_for_status()
        return resp.json()

    def unlabel_user(self, user_id: str, abuse_type: Optional[str] = None) -> None:
        """
        Remove a label from a user.
        If abuse_type is provided, remove for that type; otherwise remove all labels.
        """
        url = f"{self.BASE_V205}/users/{user_id}/labels"
        params = {"abuse_type": abuse_type} if abuse_type else None
        resp = self.session.delete(url, params=params, timeout=self.timeout)
        resp.raise_for_status()

    # ----------------------------
    # Workflow runs (retrieve data)
    # ----------------------------
    def get_workflow_run(self, run_id: str) -> Dict[str, Any]:
        """
        Retrieve details of a specific workflow run by its run_id (returned by track with return_action).
        """
        url = f"{self.BASE_V3}/accounts/{self.account_id}/workflows/runs/{run_id}"
        resp = self.session.get(url, timeout=self.timeout)
        resp.raise_for_status()
        return resp.json()

    # ----------------------------
    # Models (placeholders; not publicly exposed by Sift APIs)
    # ----------------------------
    def create_model(self, *args, **kwargs) -> None:
        """
        Placeholder. Sift does not expose a public API for model creation.
        Contact Sift Support or use the Sift Console for Custom Models.
        """
        raise NotImplementedError(
            "Model creation/management is not available via Sift's public API. "
            "Use the Sift Console or consult Sift Support."
        )

    def list_models(self) -> None:
        """
        Placeholder. Sift does not expose a public API for listing models.
        """
        raise NotImplementedError(
            "Model listing is not available via Sift's public API."
        )

    def update_model(self, *args, **kwargs) -> None:
        """
        Placeholder. Sift does not expose a public API for updating models.
        """
        raise NotImplementedError(
            "Model updates are not available via Sift's public API."
        )

    def delete_model(self, *args, **kwargs) -> None:
        """
        Placeholder. Sift does not expose a public API for deleting models.
        """
        raise NotImplementedError(
            "Model deletion is not available via Sift's public API."
        )


# ----------------------------
# Example usage
# ----------------------------
if __name__ == "__main__":
    API_KEY = os.getenv("SIFT_API_KEY", "YOUR_SIFT_API_KEY")
    ACCOUNT_ID = os.getenv("SIFT_ACCOUNT_ID", "YOUR_SIFT_ACCOUNT_ID")

    sift_client = SiftAPI(api_key=API_KEY, account_id=ACCOUNT_ID, timeout=10.0)

    # 1) Track an event (e.g., $create_order) and request workflow action
    try:
        track_resp = sift_client.track_event(
            event_type="$create_order",
            properties={
                "$user_id": "user-123",
                "$order_id": "order-789",
                "$amount": 4999,  # amount in cents
                "$currency_code": "USD",
            },
            return_action=True,
        )
        print("Track response:", track_resp)
        workflow_run_id = track_resp.get("workflow_run_id")
    except requests.HTTPError as e:
        print("Track error:", e, getattr(e.response, "text", ""))

    # 2) Get risk scores for a user
    try:
        score = sift_client.score_user(
            user_id="user-123",
            abuse_types=["payment_abuse", "account_takeover"],
            return_action=True,
        )
        print("Score response:", score)
    except requests.HTTPError as e:
        print("Score error:", e, getattr(e.response, "text", ""))

    # 3) Apply a decision to a user (requires a valid decision_id configured in Sift)
    try:
        decision_resp = sift_client.apply_user_decision(
            user_id="user-123",
            decision_id="block_user",  # example; replace with your actual decision_id
            source="MANUAL_REVIEW",
            description="High-risk behavior observed.",
        )
        print("Decision response:", decision_resp)
    except requests.HTTPError as e:
        print("Decision error:", e, getattr(e.response, "text", ""))

    # 4) Retrieve current decisions for the user
    try:
        decisions = sift_client.get_user_decisions(
            user_id="user-123", abuse_types=["payment_abuse"]
        )
        print("User decisions:", decisions)
    except requests.HTTPError as e:
        print("Get decisions error:", e, getattr(e.response, "text", ""))

    # 5) Optional: get workflow run details (if you captured a workflow_run_id)
    # if workflow_run_id:
    #     try:
    #         run = sift_client.get_workflow_run(workflow_run_id)
    #         print("Workflow run:", run)
    #     except requests.HTTPError as e:
    #         print("Workflow run error:", e, getattr(e.response, "text", ""))

    # 6) Labels (legacy)
    # try:
    #     sift_client.label_user("user-123", is_bad=True, abuse_type="payment_abuse", description="Chargeback")
    #     print("User labeled.")
    #     sift_client.unlabel_user("user-123", abuse_type="payment_abuse")
    #     print("User unlabelled.")
    # except requests.HTTPError as e:
    #     print("Label error:", e, getattr(e.response, "text", ""))

    # 7) Models (not available via public API)
    try:
        sift_client.create_model()
    except NotImplementedError as e:
        print("Models:", e)
------------------------------------------------------------

PROMPT 271:
"Generate Python code that uses the argparse library to create command-line interfaces for functions by defining arguments and parsing them using the parse_args() function, then calling the function with the parsed arguments."
RESPONSE:
import argparse
import inspect
import typing as t
import pathlib
import enum


def str2bool(v: str) -> bool:
    if isinstance(v, bool):
        return v
    val = v.strip().lower()
    if val in ("y", "yes", "t", "true", "on", "1"):
        return True
    if val in ("n", "no", "f", "false", "off", "0"):
        return False
    raise argparse.ArgumentTypeError(f"Expected a boolean value, got: {v}")


def unwrap_optional(ann):
    origin = t.get_origin(ann)
    if origin is t.Union:
        args = tuple(a for a in t.get_args(ann) if a is not type(None))  # noqa: E721
        if len(args) == 1:
            return args[0], True
    return ann, False


def enum_type(enum_cls):
    def parse_enum(s: str):
        # Allow both name (case-insensitive) and value
        try:
            return enum_cls[s.upper()]
        except KeyError:
            # Try by value
            for member in enum_cls:
                if str(member.value).lower() == s.lower():
                    return member
            raise argparse.ArgumentTypeError(
                f"Invalid value '{s}'. Expected one of: "
                f"{', '.join(m.name for m in enum_cls)}"
            )
    return parse_enum


def derive_arg_type(ann):
    # Returns (type_func, choices, is_list)
    if ann is inspect._empty:
        return str, None, False

    ann, _ = unwrap_optional(ann)

    origin = t.get_origin(ann)
    args = t.get_args(ann)

    # List[T]
    if origin in (list, t.List):
        inner = args[0] if args else str
        inner_type, choices, _ = derive_arg_type(inner)
        return inner_type, choices, True

    # Enums
    if inspect.isclass(ann) and issubclass(ann, enum.Enum):
        return enum_type(ann), [m.name for m in ann], False

    # Common primitives and paths
    if ann in (str, int, float, bool, pathlib.Path):
        return (str2bool if ann is bool else ann), None, False

    # Fallback
    return str, None, False


def add_function_subparser(
    func,
    subparsers,
    name: str | None = None,
    help_text: str | None = None,
    description: str | None = None,
):
    sig = inspect.signature(func)
    func_name = name or func.__name__
    doc = inspect.getdoc(func) or ""
    first_line = doc.splitlines()[0] if doc else None

    parser = subparsers.add_parser(
        func_name,
        help=help_text or first_line,
        description=description or (doc if doc else help_text or first_line),
    )

    for param in sig.parameters.values():
        if param.kind in (param.VAR_POSITIONAL, param.VAR_KEYWORD):
            continue

        ann = param.annotation
        ptype, choices, is_list = derive_arg_type(ann)
        has_default = param.default is not inspect._empty
        default = None if not has_default else param.default

        # Build argument flags/names
        flag = f"--{param.name.replace('_', '-')}"
        dest = param.name

        # Positional or option?
        make_positional = not has_default and ptype is not str2bool and not is_list

        kwargs = {}
        if choices:
            kwargs["choices"] = choices

        if is_list:
            kwargs["nargs"] = "+"  # at least one
        if ptype is not str2bool:
            kwargs["type"] = ptype

        if make_positional:
            argname = dest
        else:
            argname = flag
            kwargs["dest"] = dest

        # Boolean handling
        if ptype is str2bool:
            if has_default:
                # Add a toggling flag based on the default
                if default is False:
                    kwargs["action"] = "store_true"
                    kwargs["default"] = False
                else:
                    kwargs["action"] = "store_false"
                    kwargs["default"] = True
                    # For readability, invert flag if default True (add --no-flag)
                    argname = f"--no-{param.name.replace('_', '-')}"
            else:
                # Require explicit boolean value
                kwargs["type"] = str2bool
                kwargs["required"] = True

        if has_default and ptype is not str2bool:
            kwargs["default"] = default
            kwargs["required"] = False

        parser.add_argument(argname, **kwargs)

    parser.set_defaults(_func=func)
    return parser


# Example enums and functions to expose as CLIs
class Flavor(enum.Enum):
    VANILLA = "vanilla"
    CHOCOLATE = "chocolate"
    STRAWBERRY = "strawberry"


def add(a: int, b: int, verbose: bool = False) -> int:
    """Add two integers and optionally print details."""
    total = a + b
    if verbose:
        print(f"{a} + {b} = {total}")
    return total


def greet(name: str, times: int = 1, excited: bool = False) -> None:
    """Greet a person multiple times."""
    punctuation = "!" if excited else "."
    for _ in range(times):
        print(f"Hello, {name}{punctuation}")


def make_sundae(scoops: int, flavor: Flavor = Flavor.VANILLA, toppings: list[str] = None) -> str:
    """Describe a sundae order."""
    toppings = toppings or []
    return f"{scoops} scoop(s) of {flavor.value} with " + (", ".join(toppings) if toppings else "no toppings")


def concat(items: list[str], sep: str = ",") -> str:
    """Concatenate a list of strings with a separator."""
    return sep.join(items)


def main():
    parser = argparse.ArgumentParser(prog="cli", description="CLI wrapper for functions")
    subparsers = parser.add_subparsers(dest="command", required=True)

    add_function_subparser(add, subparsers, help_text="Add two integers")
    add_function_subparser(greet, subparsers, help_text="Greet someone")
    add_function_subparser(make_sundae, subparsers, help_text="Build a sundae description")
    add_function_subparser(concat, subparsers, help_text="Concatenate strings")

    args = parser.parse_args()

    func = getattr(args, "_func")
    sig = inspect.signature(func)

    # Build kwargs from parsed args based on the function signature
    kwargs = {}
    for param in sig.parameters.values():
        if param.kind in (param.VAR_POSITIONAL, param.VAR_KEYWORD):
            continue
        kwargs[param.name] = getattr(args, param.name)

    result = func(**kwargs)

    # If the function returns something, print it
    if result is not None:
        print(result)


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 272:
"Generate Python code that implements a WSGI proxy with various HTTP client backends, using the `wsgiref` module for WSGI support and choosing an appropriate HTTP client backend such as `requests`, `urllib3`, or `http.client`."
RESPONSE:
#!/usr/bin/env python3
"""
A simple WSGI reverse proxy using wsgiref, with pluggable HTTP client backends:
- requests
- urllib3
- http.client (stdlib)

Usage:
  python wsgi_proxy.py --upstream http://example.com --host 127.0.0.1 --port 8080 --backend auto

Notes:
- This is a basic educational proxy; do not use as-is for production.
- Removes hop-by-hop headers and adds X-Forwarded-* headers.
- Streams responses where supported by the backend and server.
"""

import argparse
import os
import sys
import socket
import logging
import urllib.parse
from wsgiref.simple_server import make_server, WSGIServer, WSGIRequestHandler

# Optional imports
_HAS_REQUESTS = False
_HAS_URLLIB3 = False
try:
    import requests  # type: ignore
    _HAS_REQUESTS = True
except Exception:
    pass

try:
    import urllib3  # type: ignore
    _HAS_URLLIB3 = True
except Exception:
    pass

import http.client


LOG = logging.getLogger("wsgi_proxy")

HOP_BY_HOP_HEADERS = {
    "connection",
    "keep-alive",
    "proxy-authenticate",
    "proxy-authorization",
    "te",
    "trailer",
    "transfer-encoding",
    "upgrade",
}


def normalize_header_name(name: str) -> str:
    # Produce canonical header casing (e.g., content-type -> Content-Type)
    return "-".join(s.capitalize() for s in name.split("-"))


def build_upstream_url(base: str, path_info: str, query_string: str) -> str:
    # Ensure correct joining of base and path
    # base can be like http://host:port or http://host:port/base
    # path_info starts with "/", query_string without '?'
    base_parsed = urllib.parse.urlsplit(base)
    base_path = base_parsed.path or ""
    if base_path.endswith("/") and path_info.startswith("/"):
        joined_path = base_path[:-1] + path_info
    elif not base_path.endswith("/") and not path_info.startswith("/"):
        joined_path = base_path + "/" + path_info
    else:
        joined_path = base_path + path_info

    new_parts = (
        base_parsed.scheme,
        base_parsed.netloc,
        joined_path,
        query_string,
        "",  # fragment
    )
    return urllib.parse.urlunsplit(new_parts)


def extract_incoming_headers(environ) -> dict:
    headers = {}
    for key, value in environ.items():
        if not value:
            continue
        if key.startswith("HTTP_"):
            name = key[5:].replace("_", "-")
            headers[name] = value
        elif key in ("CONTENT_TYPE", "CONTENT_LENGTH"):
            name = key.replace("_", "-")
            headers[name] = value
    return headers


def remove_hop_by_hop(headers: dict) -> dict:
    cleaned = {}
    connection_tokens = set()
    # If "Connection" is present, its value may contain a list of header names to remove as hop-by-hop
    conn_val = None
    for k, v in headers.items():
        if k.lower() == "connection":
            conn_val = v
            break
    if conn_val:
        for token in conn_val.split(","):
            token = token.strip()
            if token:
                connection_tokens.add(token.lower())

    for k, v in headers.items():
        kl = k.lower()
        if kl in HOP_BY_HOP_HEADERS or kl in connection_tokens:
            continue
        cleaned[k] = v
    return cleaned


def add_forwarded_headers(headers: dict, environ) -> dict:
    # Add/append standard X-Forwarded-* headers
    hdrs = dict(headers)  # shallow copy
    remote_addr = environ.get("REMOTE_ADDR")
    if remote_addr:
        prior = hdrs.get("X-Forwarded-For")
        hdrs["X-Forwarded-For"] = (prior + ", " + remote_addr) if prior else remote_addr

    proto = environ.get("wsgi.url_scheme")
    if proto:
        hdrs.setdefault("X-Forwarded-Proto", proto)

    host = environ.get("HTTP_HOST") or environ.get("SERVER_NAME")
    if host:
        hdrs.setdefault("X-Forwarded-Host", host)

    # Remove incorrect Host; upstream Host should be set by the backend to the target
    hdrs.pop("Host", None)
    return hdrs


class ProxyResponse:
    def __init__(self, status_code: int, reason: str, headers: list, iterator, closer=None):
        self.status_code = status_code
        self.reason = reason
        self.headers = headers  # list of (name, value)
        self.iterator = iterator  # generator yielding bytes
        self.closer = closer

    def close(self):
        if self.closer:
            try:
                self.closer()
            except Exception:
                LOG.debug("Error closing response", exc_info=True)


class BackendBase:
    def request(self, method: str, url: str, headers: dict, body_source, timeout: float, verify: bool) -> ProxyResponse:
        raise NotImplementedError


class RequestsBackend(BackendBase):
    def __init__(self):
        self.session = requests.Session()

    def request(self, method: str, url: str, headers: dict, body_source, timeout: float, verify: bool) -> ProxyResponse:
        # body_source can be bytes or None
        resp = self.session.request(
            method=method,
            url=url,
            headers=headers,
            data=body_source,
            stream=True,
            timeout=timeout,
            verify=verify,
        )

        def iterator():
            try:
                for chunk in resp.iter_content(chunk_size=64 * 1024):
                    if chunk:
                        yield chunk
            finally:
                resp.close()

        status = resp.status_code
        reason = resp.reason or ""
        out_headers = []
        for k, v in resp.headers.items():
            if k.lower() in HOP_BY_HOP_HEADERS:
                continue
            out_headers.append((str(k), str(v)))
        return ProxyResponse(status, reason, out_headers, iterator())


class Urllib3Backend(BackendBase):
    def __init__(self):
        # Disable urllib3 warnings if desired; leave defaults here
        self.pool = urllib3.PoolManager(retries=False)

    def request(self, method: str, url: str, headers: dict, body_source, timeout: float, verify: bool) -> ProxyResponse:
        # urllib3 verify is handled via cert_reqs/ca_certs; for simplicity, override per-request via kw
        kw = {
            "method": method,
            "url": url,
            "headers": headers,
            "body": body_source,
            "preload_content": False,
            "redirect": False,
            "timeout": urllib3.Timeout.connect_timeout if isinstance(timeout, (int, float)) else timeout,
        }

        if not verify:
            # Insecure: do not verify
            kw["cert_reqs"] = "CERT_NONE"
            kw["assert_hostname"] = False

        resp = self.pool.request(**kw)

        def iterator():
            try:
                while True:
                    chunk = resp.read(64 * 1024)
                    if not chunk:
                        break
                    yield chunk
            finally:
                try:
                    resp.release_conn()
                except Exception:
                    pass
                try:
                    resp.close()
                except Exception:
                    pass

        status = int(resp.status)
        reason = getattr(resp, "reason", "") or ""
        out_headers = []
        for k, v in resp.headers.items():
            if k.lower() in HOP_BY_HOP_HEADERS:
                continue
            out_headers.append((str(k), str(v)))
        return ProxyResponse(status, reason, out_headers, iterator())


class HttpClientBackend(BackendBase):
    def request(self, method: str, url: str, headers: dict, body_source, timeout: float, verify: bool) -> ProxyResponse:
        # Parse URL
        parsed = urllib.parse.urlsplit(url)
        is_https = parsed.scheme == "https"
        port = parsed.port
        host = parsed.hostname
        if port is None:
            port = 443 if is_https else 80
        path_and_qs = urllib.parse.urlunsplit(("", "", parsed.path or "/", parsed.query, ""))

        # Create connection
        if is_https:
            conn = http.client.HTTPSConnection(host, port=port, timeout=timeout, context=None)
            # Note: stdlib http.client lacks per-request verify toggle without custom SSLContext
            if not verify:
                import ssl
                ctx = ssl.create_default_context()
                ctx.check_hostname = False
                ctx.verify_mode = ssl.CERT_NONE
                conn = http.client.HTTPSConnection(host, port=port, timeout=timeout, context=ctx)
        else:
            conn = http.client.HTTPConnection(host, port=port, timeout=timeout)

        # Set Host header for upstream
        headers = dict(headers)
        headers["Host"] = host if parsed.port is None else f"{host}:{port}"

        body = body_source if (body_source is None or isinstance(body_source, (bytes, bytearray))) else None
        conn.request(method, path_and_qs, body=body, headers=headers)
        resp = conn.getresponse()

        def iterator():
            try:
                while True:
                    chunk = resp.read(64 * 1024)
                    if not chunk:
                        break
                    yield chunk
            finally:
                try:
                    resp.close()
                except Exception:
                    pass
                try:
                    conn.close()
                except Exception:
                    pass

        status = int(resp.status)
        reason = resp.reason or ""
        out_headers = []
        for k, v in resp.getheaders():
            if k.lower() in HOP_BY_HOP_HEADERS:
                continue
            out_headers.append((str(k), str(v)))
        return ProxyResponse(status, reason, out_headers, iterator())


def choose_backend(name: str):
    name = name.lower().strip()
    if name == "auto":
        if _HAS_REQUESTS:
            LOG.info("Using requests backend")
            return RequestsBackend()
        elif _HAS_URLLIB3:
            LOG.info("Using urllib3 backend")
            return Urllib3Backend()
        else:
            LOG.info("Using http.client backend")
            return HttpClientBackend()
    elif name == "requests":
        if not _HAS_REQUESTS:
            raise RuntimeError("requests not available")
        return RequestsBackend()
    elif name == "urllib3":
        if not _HAS_URLLIB3:
            raise RuntimeError("urllib3 not available")
        return Urllib3Backend()
    elif name == "httpclient":
        return HttpClientBackend()
    else:
        raise ValueError(f"Unknown backend: {name}")


class WSGIReverseProxy:
    def __init__(self, upstream_base: str, backend: BackendBase, timeout: float = 30.0, verify_ssl: bool = True):
        self.upstream_base = upstream_base.rstrip("/")
        self.backend = backend
        self.timeout = timeout
        self.verify_ssl = verify_ssl

    def __call__(self, environ, start_response):
        try:
            method = environ.get("REQUEST_METHOD", "GET").upper()
            path_info = environ.get("PATH_INFO", "") or "/"
            query_string = environ.get("QUERY_STRING", "") or ""

            upstream_url = build_upstream_url(self.upstream_base, path_info, query_string)

            # Extract incoming headers and sanitize
            incoming_headers = extract_incoming_headers(environ)
            incoming_headers = remove_hop_by_hop(incoming_headers)
            outgoing_headers = add_forwarded_headers(incoming_headers, environ)

            # Read request body (WSGI may not support unknown length; wsgiref expects Content-Length)
            content_length = environ.get("CONTENT_LENGTH")
            body = None
            if content_length:
                try:
                    to_read = int(content_length)
                    if to_read > 0:
                        body = environ["wsgi.input"].read(to_read)
                    else:
                        body = b""
                except (ValueError, KeyError):
                    body = environ["wsgi.input"].read()
            else:
                # No content-length; safest is read nothing (for GET/HEAD) or read to EOF if the server provided it.
                if method not in ("GET", "HEAD", "DELETE", "OPTIONS"):
                    # Try to read whatever is available
                    try:
                        body = environ["wsgi.input"].read()
                    except Exception:
                        body = None

            # Send upstream
            resp = self.backend.request(
                method=method,
                url=upstream_url,
                headers=outgoing_headers,
                body_source=body,
                timeout=self.timeout,
                verify=self.verify_ssl,
            )

            status_line = f"{resp.status_code} {resp.reason or http.client.responses.get(resp.status_code, '')}".strip()

            # Sanitize response headers for WSGI
            response_headers = []
            for k, v in resp.headers:
                # Ensure header values are str and without newlines per PEP 333
                k = str(k)
                v = str(v).replace("\r", "").replace("\n", "")
                if k.lower() in HOP_BY_HOP_HEADERS:
                    continue
                response_headers.append((k, v))

            start_response(status_line, response_headers)

            def app_iter():
                try:
                    for chunk in resp.iterator:
                        # Ensure bytes
                        if not isinstance(chunk, (bytes, bytearray)):
                            chunk = bytes(chunk)
                        yield chunk
                finally:
                    resp.close()

            return app_iter()

        except Exception as e:
            LOG.error("Proxy error: %s", e, exc_info=True)
            status = "502 Bad Gateway"
            headers = [("Content-Type", "text/plain; charset=utf-8")]
            start_response(status, headers)
            return [b"Bad Gateway"]


class QuietHandler(WSGIRequestHandler):
    def log_message(self, format, *args):
        LOG.info("%s - - [%s] %s", self.address_string(), self.log_date_time_string(), format % args)


def parse_args():
    p = argparse.ArgumentParser(description="WSGI reverse proxy with pluggable HTTP backends")
    p.add_argument("--upstream", required=True, help="Upstream base URL (e.g., http://localhost:9000 or https://api.example.com)")
    p.add_argument("--host", default=os.environ.get("PROXY_HOST", "127.0.0.1"), help="Bind host (default 127.0.0.1)")
    p.add_argument("--port", type=int, default=int(os.environ.get("PROXY_PORT", "8080")), help="Bind port (default 8080)")
    p.add_argument("--backend", default=os.environ.get("PROXY_BACKEND", "auto"), choices=["auto", "requests", "urllib3", "httpclient"], help="HTTP client backend")
    p.add_argument("--timeout", type=float, default=float(os.environ.get("PROXY_TIMEOUT", "30")), help="Upstream request timeout (seconds)")
    p.add_argument("--insecure", action="store_true", help="Disable SSL verification for upstream (NOT recommended)")
    p.add_argument("--log-level", default=os.environ.get("PROXY_LOG_LEVEL", "INFO"), help="Logging level (DEBUG, INFO, WARNING, ERROR)")
    return p.parse_args()


def main():
    args = parse_args()
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper(), logging.INFO),
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    )

    # Validate upstream URL
    parsed = urllib.parse.urlsplit(args.upstream)
    if parsed.scheme not in ("http", "https"):
        LOG.error("Unsupported upstream scheme: %s", parsed.scheme)
        sys.exit(2)
    if not parsed.netloc:
        LOG.error("Invalid upstream URL (missing host): %s", args.upstream)
        sys.exit(2)

    backend = choose_backend(args.backend)
    app = WSGIReverseProxy(upstream_base=args.upstream, backend=backend, timeout=args.timeout, verify_ssl=not args.insecure)

    with make_server(args.host, args.port, app, handler_class=QuietHandler) as httpd:
        sa = httpd.socket.getsockname()
        LOG.info("Serving proxy on http://%s:%s -> %s (backend=%s, insecure=%s)", sa[0], sa[1], args.upstream, args.backend, args.insecure)
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            LOG.info("Shutting down...")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 273:
"Generate Python code that uses the AWS CDK v2 library to automatically instrument Python and Node Lambda functions with Datadog, by creating a construct that wraps around the Lambda function and configures Datadog monitoring."
RESPONSE:
Below is a self-contained example that defines a reusable CDK v2 construct which wraps an AWS Lambda function and automatically instruments it with Datadog for Python and Node runtimes. It adds the Datadog Lambda and Extension layers, swaps the handler to the Datadog wrapper, and configures common environment variables for logs, traces, tags, and site. It supports reading the Datadog API key from AWS Secrets Manager.

Update the Datadog layer version numbers with the latest values from the Datadog docs before deploying.

from typing import Optional, Dict
from constructs import Construct
from aws_cdk import (
    Stack,
    Duration,
    aws_lambda as _lambda,
    aws_secretsmanager as secretsmanager,
    RemovalPolicy,
)


class DatadogInstrumentationProps:
    def __init__(
        self,
        *,
        # Provide one of these (secret recommended):
        api_key_secret: Optional[secretsmanager.ISecret] = None,
        api_key: Optional[str] = None,

        site: str = "datadoghq.com",
        service: Optional[str] = None,
        env: Optional[str] = None,
        version: Optional[str] = None,

        # Enable features
        enable_logs: bool = True,
        enable_tracing: bool = True,
        merge_xray_traces: bool = True,
        capture_lambda_payload: bool = False,

        # Optional tags (e.g., {"team": "payments", "owner": "sre"})
        tags: Optional[Dict[str, str]] = None,

        # Datadog Lambda library and extension layer versions
        # You must set correct, up-to-date layer versions from Datadog docs:
        # https://docs.datadoghq.com/serverless/installation/libraries_and_layers/
        extension_layer_version: Optional[int] = None,
        python_layer_version: Optional[int] = None,  # used when runtime is Python*
        node_layer_version: Optional[int] = None,    # used when runtime is Node*

        # Whether to wrap the handler using the Datadog wrapper
        wrap_handler: bool = True,
    ):
        self.api_key_secret = api_key_secret
        self.api_key = api_key

        self.site = site
        self.service = service
        self.env = env
        self.version = version

        self.enable_logs = enable_logs
        self.enable_tracing = enable_tracing
        self.merge_xray_traces = merge_xray_traces
        self.capture_lambda_payload = capture_lambda_payload

        self.tags = tags or {}

        self.extension_layer_version = extension_layer_version
        self.python_layer_version = python_layer_version
        self.node_layer_version = node_layer_version

        self.wrap_handler = wrap_handler


def _datadog_layer_arn_for_runtime(runtime: _lambda.Runtime, region: str, dd_lib_version: int) -> str:
    """
    Returns the Datadog Lambda library layer ARN for a given runtime and region.
    Update layer naming if Datadog changes conventions.
    """
    account_id = "464622532012"

    if runtime.family == _lambda.RuntimeFamily.PYTHON:
        # Universal Python Datadog layer
        # https://docs.datadoghq.com/serverless/installation/python/?tab=layers
        layer_name = "Datadog-Python"
    elif runtime.family == _lambda.RuntimeFamily.NODEJS:
        # Choose Node layer based on runtime version
        # https://docs.datadoghq.com/serverless/installation/nodejs/?tab=layers
        if runtime == _lambda.Runtime.NODEJS_20_X:
            layer_name = "Datadog-Node20"
        elif runtime == _lambda.Runtime.NODEJS_18_X:
            layer_name = "Datadog-Node18"
        elif runtime == _lambda.Runtime.NODEJS_16_X:
            layer_name = "Datadog-Node16"
        else:
            raise ValueError(f"Unsupported Node runtime for Datadog layer: {runtime.name}")
    else:
        raise ValueError(f"Unsupported runtime family for Datadog layer: {runtime.name}")

    return f"arn:aws:lambda:{region}:{account_id}:layer:{layer_name}:{dd_lib_version}"


def _datadog_extension_layer_arn(region: str, extension_version: int) -> str:
    """
    Returns the Datadog Lambda Extension layer ARN. Version must be set from Datadog docs.
    """
    account_id = "464622532012"
    return f"arn:aws:lambda:{region}:{account_id}:layer:Datadog-Extension:{extension_version}"


def _datadog_wrapper_handler_for_runtime(runtime: _lambda.Runtime) -> str:
    """
    Returns the wrapper handler string for the Datadog Lambda library based on runtime.
    """
    if runtime.family == _lambda.RuntimeFamily.PYTHON:
        # Provided by Datadog-Python layer
        return "datadog_lambda.handler.handler"
    elif runtime.family == _lambda.RuntimeFamily.NODEJS:
        # Provided by Datadog-Node* layer
        return "datadog-lambda-js.handler"
    else:
        raise ValueError(f"Unsupported runtime family for Datadog wrapper: {runtime.name}")


class DatadogFunction(Construct):
    """
    Construct that creates a Lambda function and instruments it with Datadog.
    - Adds Datadog library layer and extension layer based on runtime
    - Wraps handler with Datadog wrapper and preserves original in DD_LAMBDA_HANDLER
    - Sets environment variables for site, logs, tracing, tags, etc.
    - Optionally reads API key from Secrets Manager
    Exposes the created function via self.function
    """

    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        *,
        function_name: Optional[str] = None,
        runtime: _lambda.Runtime,
        code: _lambda.Code,
        handler: str,
        memory_size: Optional[int] = 1024,
        timeout: Optional[Duration] = Duration.seconds(30),
        architecture: Optional[_lambda.Architecture] = None,
        environment: Optional[Dict[str, str]] = None,
        log_retention: Optional[_lambda.LogRetention] = None,  # optional, not used directly
        datadog: DatadogInstrumentationProps,
    ) -> None:
        super().__init__(scope, construct_id)

        # Create the function first with the user's handler (we'll wrap after creation)
        self.function = _lambda.Function(
            self,
            "Lambda",
            function_name=function_name,
            runtime=runtime,
            code=code,
            handler=handler,
            memory_size=memory_size,
            timeout=timeout,
            architecture=architecture or _lambda.Architecture.X86_64,
            environment=environment or {},
            tracing=_lambda.Tracing.DISABLED,  # we'll enable via override to be explicit
        )

        # Instrument with Datadog
        _instrument_with_datadog(self.function, datadog)


def _instrument_with_datadog(fn: _lambda.Function, props: DatadogInstrumentationProps) -> None:
    """
    Instruments an existing Lambda function with Datadog, modifying the handler, layers, and environment.
    """
    stack = Stack.of(fn)
    region = stack.region
    runtime = fn.runtime

    # Resolve layer versions for this runtime
    if props.extension_layer_version is None:
        raise ValueError("Datadog 'extension_layer_version' must be provided.")
    if runtime.family == _lambda.RuntimeFamily.PYTHON:
        if props.python_layer_version is None:
            raise ValueError("Datadog 'python_layer_version' must be provided for Python runtimes.")
        lib_layer_arn = _datadog_layer_arn_for_runtime(runtime, region, props.python_layer_version)
    elif runtime.family == _lambda.RuntimeFamily.NODEJS:
        if props.node_layer_version is None:
            raise ValueError("Datadog 'node_layer_version' must be provided for Node runtimes.")
        lib_layer_arn = _datadog_layer_arn_for_runtime(runtime, region, props.node_layer_version)
    else:
        raise ValueError(f"Unsupported runtime family: {runtime.name}")

    ext_layer_arn = _datadog_extension_layer_arn(region, props.extension_layer_version)

    lib_layer = _lambda.LayerVersion.from_layer_version_arn(
        stack, f"{fn.node.id}DatadogLibLayer", lib_layer_arn
    )
    ext_layer = _lambda.LayerVersion.from_layer_version_arn(
        stack, f"{fn.node.id}DatadogExtensionLayer", ext_layer_arn
    )
    fn.add_layers(lib_layer, ext_layer)

    # Build env vars
    def b(v: bool) -> str:
        return "true" if v else "false"

    # Tags as "k1:v1,k2:v2"
    dd_tags_str = ",".join(f"{k}:{v}" for k, v in (props.tags or {}).items()) if props.tags else ""

    if props.site:
        fn.add_environment("DD_SITE", props.site)

    if props.enable_logs:
        fn.add_environment("DD_LOGS_ENABLED", "true")

    if props.enable_tracing:
        fn.add_environment("DD_TRACE_ENABLED", "true")

    if props.merge_xray_traces:
        fn.add_environment("DD_MERGE_XRAY_TRACES", "true")

    if props.capture_lambda_payload:
        fn.add_environment("DD_CAPTURE_LAMBDA_PAYLOAD", "true")

    if props.service:
        fn.add_environment("DD_SERVICE", props.service)

    if props.env:
        fn.add_environment("DD_ENV", props.env)

    if props.version:
        fn.add_environment("DD_VERSION", props.version)

    if dd_tags_str:
        fn.add_environment("DD_TAGS", dd_tags_str)

    # API key handling
    if props.api_key_secret is not None:
        # The Datadog extension supports loading key from Secrets Manager via this env var
        fn.add_environment("DD_API_KEY_SECRET_ARN", props.api_key_secret.secret_arn)
        props.api_key_secret.grant_read(fn)
    elif props.api_key is not None:
        # Not recommended for production. Prefer Secrets Manager.
        fn.add_environment("DD_API_KEY", props.api_key)

    # Enable X-Ray tracing mode at the Lambda config level (helps Datadog merge traces)
    cfn: _lambda.CfnFunction = fn.node.default_child  # type: ignore
    cfn.tracing_config = _lambda.CfnFunction.TracingConfigProperty(mode="Active")

    # Wrap the handler so Datadog's library receives events and emits telemetry
    if props.wrap_handler:
        original_handler = cfn.handler
        wrapper_handler = _datadog_wrapper_handler_for_runtime(runtime)

        # Tell Datadog wrapper what the original handler is
        fn.add_environment("DD_LAMBDA_HANDLER", original_handler)

        # Swap handler to Datadog wrapper
        cfn.handler = wrapper_handler


# ------------------------------
# Example usage in a CDK stack
# ------------------------------
class ExampleDatadogStack(Stack):
    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # Retrieve your Datadog API key from Secrets Manager (recommended)
        # Create this secret beforehand or via CDK. The secret's value should be the Datadog API key string.
        dd_api_key_secret = secretsmanager.Secret.from_secret_name_v2(
            self,
            "DatadogApiKeySecret",
            secret_name="datadog/api-key",  # change to your secret name
        )

        # Set layer versions (update these with the latest from Datadog docs!)
        DD_EXTENSION_LAYER_VERSION = 60  # example only; update to latest
        DD_PYTHON_LAYER_VERSION = 90     # example only; update to latest
        DD_NODE18_LAYER_VERSION = 90     # example only; update to latest

        # Python example
        py_fn = DatadogFunction(
            self,
            "PyApp",
            function_name="py-datadog-demo",
            runtime=_lambda.Runtime.PYTHON_3_10,
            code=_lambda.Code.from_asset("lambda/python"),  # path to your Python code
            handler="app.handler",  # your original handler "module.function"
            timeout=Duration.seconds(20),
            environment={"EXAMPLE": "true"},
            datadog=DatadogInstrumentationProps(
                api_key_secret=dd_api_key_secret,
                site="datadoghq.com",
                service="payments-api",
                env="prod",
                version="1.2.3",
                enable_logs=True,
                enable_tracing=True,
                merge_xray_traces=True,
                capture_lambda_payload=False,
                tags={"team": "sre", "component": "lambda"},
                extension_layer_version=DD_EXTENSION_LAYER_VERSION,
                python_layer_version=DD_PYTHON_LAYER_VERSION,
            ),
        )

        # Node example
        node_fn = DatadogFunction(
            self,
            "NodeApp",
            function_name="node-datadog-demo",
            runtime=_lambda.Runtime.NODEJS_18_X,
            code=_lambda.Code.from_asset("lambda/node"),  # path to your Node code
            handler="app.handler",  # your original handler "file.exportedHandler"
            timeout=Duration.seconds(20),
            datadog=DatadogInstrumentationProps(
                api_key_secret=dd_api_key_secret,
                site="datadoghq.com",
                service="orders-api",
                env="staging",
                enable_logs=True,
                enable_tracing=True,
                tags={"team": "backend", "feature": "orders"},
                extension_layer_version=DD_EXTENSION_LAYER_VERSION,
                node_layer_version=DD_NODE18_LAYER_VERSION,
            ),
        )

Notes and tips:
- Always use the latest Datadog layer versions for your region and runtime. Update the DD_EXTENSION_LAYER_VERSION and runtime-specific library layer versions accordingly.
- Prefer storing the Datadog API key in AWS Secrets Manager and pass it via api_key_secret; the construct grants read permissions to the function automatically.
- The construct sets TracingConfig to Active and DD_MERGE_XRAY_TRACES to true to help Datadog correlate AWS X-Ray segments with Datadog traces.
- If you already have a Function and only want to instrument it, you can adapt _instrument_with_datadog to accept and modify your existing function directly.
------------------------------------------------------------

PROMPT 274:
"Generate Python code that uses the `colcon` library to read package metadata from files, specifically the `package.yaml` file in the root directory of the package, using the `colcon.metadata.read_package_metadata()` function."
RESPONSE:
#!/usr/bin/env python3

import sys
import json
from pathlib import Path

def main():
    try:
        from colcon.metadata import read_package_metadata
    except Exception as e:
        print(f"Error: could not import colcon.metadata.read_package_metadata: {e}", file=sys.stderr)
        sys.exit(2)

    if len(sys.argv) != 2:
        print("Usage: python read_metadata.py /path/to/package_root", file=sys.stderr)
        sys.exit(1)

    package_root = Path(sys.argv[1]).resolve()
    if not package_root.is_dir():
        print(f"Error: not a directory: {package_root}", file=sys.stderr)
        sys.exit(1)

    try:
        # This will look for package.yaml in the given directory and read metadata.
        metadata = read_package_metadata(package_root)
    except Exception as e:
        print(f"Failed to read package metadata from {package_root}: {e}", file=sys.stderr)
        sys.exit(3)

    # Print the metadata as JSON for convenience
    print(json.dumps(metadata, indent=2, sort_keys=True, default=str))

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 275:
"Generate Python code that imports the XBlocks library and creates a utility module with functions for working with XBlocks, such as creating and manipulating blocks, handling events, and interacting with the XBlocks database."
RESPONSE:
# xblock_utils.py
# Utility helpers for creating and manipulating XBlocks, handling events,
# and interacting with field data ("XBlock database") in a lightweight runtime.

import json
import uuid
from typing import Any, Dict, Iterable, Optional, Tuple, Type

# Core XBlock imports
from xblock.core import XBlock
from xblock.runtime import ScopeIds
from xblock.fields import Scope

# Prefer a multi-block key-value store if available, else fallback to a single-block dict-backed field data.
try:
    from xblock.kvs import DictKeyValueStore  # type: ignore
    from xblock.field_data import KvsFieldData, DictFieldData  # type: ignore
    HAS_KVS = True
except Exception:
    from xblock.field_data import DictFieldData  # type: ignore
    KvsFieldData = None  # type: ignore
    DictKeyValueStore = None  # type: ignore
    HAS_KVS = False

# Prefer an officially provided testing runtime; fallback to a minimal Runtime
try:
    from xblock.test.tools import TestRuntime  # type: ignore
except Exception:
    # Minimal runtime fallback with basic publish/event capture
    from xblock.runtime import Runtime  # type: ignore

    class TestRuntime(Runtime):  # type: ignore
        def __init__(self, services: Optional[Dict[str, Any]] = None):
            super().__init__(services or {})
            self.published_events = []
            self.user_id = "student"

        def handler_url(self, block, handler_name, suffix="", query=""):
            return f"/handler/{block.scope_ids.usage_id}/{handler_name}{suffix}"

        def resource_url(self, resource):
            return f"/static/{resource}"

        def local_resource_url(self, block, uri):
            return f"/static/{uri}"

        def publish(self, block, event_type, event):
            self.published_events.append(
                {"usage_id": block.scope_ids.usage_id, "type": event_type, "event": event}
            )


def _make_field_data(use_kvs: bool = True):
    """
    Create a field data provider and an optional backing store.

    Returns:
      field_data: A FieldData instance for the runtime.
      backing_store: The underlying store if using KVS, else None.
    """
    if HAS_KVS and use_kvs:
        store = DictKeyValueStore()  # supports many blocks
        field_data = KvsFieldData(store)
        return field_data, store
    # DictFieldData supports one block at a time (simple/testing use).
    return DictFieldData({}), None


def init_runtime(
    use_kvs: bool = True,
    user_id: str = "student",
    extra_services: Optional[Dict[str, Any]] = None,
) -> Tuple[TestRuntime, Any, Any]:
    """
    Initialize a TestRuntime with field-data service.

    Returns:
      (runtime, field_data, backing_store)
      - backing_store is non-None only when using KVS (DictKeyValueStore).
    """
    field_data, backing_store = _make_field_data(use_kvs=use_kvs)
    services = dict(extra_services or {})
    services["field-data"] = field_data
    runtime = TestRuntime(services=services)
    # Attach a user_id attribute for convenience
    setattr(runtime, "user_id", user_id)
    # Optional: capture published events if runtime supports it
    if not hasattr(runtime, "published_events"):
        setattr(runtime, "published_events", [])
    return runtime, field_data, backing_store


def make_scope_ids(
    user_id: str,
    block_type: str,
    def_id: Optional[str] = None,
    usage_id: Optional[str] = None,
) -> ScopeIds:
    """
    Create ScopeIds for a block instance.
    """
    def_id = def_id or f"def-{uuid.uuid4()}"
    usage_id = usage_id or f"usage-{uuid.uuid4()}"
    return ScopeIds(user_id=user_id, block_type=block_type, def_id=def_id, usage_id=usage_id)


def create_block(
    block_cls: Type[XBlock],
    runtime: TestRuntime,
    scope_ids: Optional[ScopeIds] = None,
    *,
    user_id: Optional[str] = None,
    block_type: Optional[str] = None,
    def_id: Optional[str] = None,
    usage_id: Optional[str] = None,
    initial_fields: Optional[Dict[str, Any]] = None,
) -> XBlock:
    """
    Construct an XBlock instance in the given runtime.
    """
    if scope_ids is None:
        block_type = block_type or getattr(block_cls, "entry_point", block_cls.__name__.lower())
        scope_ids = make_scope_ids(
            user_id or getattr(runtime, "user_id", "student"),
            block_type=block_type,
            def_id=def_id,
            usage_id=usage_id,
        )
    block = block_cls(runtime, scope_ids=scope_ids)
    if initial_fields:
        update_fields(block, **initial_fields)
    return block


def get_field(block: XBlock, name: str) -> Any:
    """
    Get a field value from a block.
    """
    if name not in block.fields:
        raise AttributeError(f"Field '{name}' not found in {type(block).__name__}")
    return getattr(block, name)


def set_field(block: XBlock, name: str, value: Any) -> None:
    """
    Set a field value on a block (persisted through field-data).
    """
    if name not in block.fields:
        raise AttributeError(f"Field '{name}' not found in {type(block).__name__}")
    setattr(block, name, value)


def update_fields(block: XBlock, **values: Any) -> None:
    """
    Bulk update multiple fields on a block.
    """
    for k, v in values.items():
        set_field(block, k, v)


def snapshot_fields(block: XBlock, names: Optional[Iterable[str]] = None) -> Dict[str, Any]:
    """
    Capture a dict of current field values for portability/backups.
    """
    field_names = list(names) if names is not None else list(block.fields.keys())
    snap = {}
    for name in field_names:
        if name in block.fields:
            snap[name] = getattr(block, name)
    return snap


def reset_user_state(block: XBlock) -> None:
    """
    Reset user_state scoped fields to their default values.
    """
    for name, field in block.fields.items():
        if getattr(field, "scope", None) == Scope.user_state:
            default = field.default
            setattr(block, name, default)


def send_event(block: XBlock, event_type: str, event: Optional[Dict[str, Any]] = None) -> Any:
    """
    Dispatch an event to the block's handle_event method.
    """
    event = event or {}
    handler = getattr(block, "handle_event", None)
    if callable(handler):
        return handler(event_type, event)
    return None


def publish_event(block: XBlock, event_type: str, event: Optional[Dict[str, Any]] = None) -> None:
    """
    Publish an event via the runtime (captured in runtime.published_events in TestRuntime).
    """
    event = event or {}
    if hasattr(block.runtime, "publish"):
        block.runtime.publish(block, event_type, event)


def list_children(block: XBlock) -> Tuple[str, ...]:
    """
    Return child usage_ids if the block defines a 'children' field.
    """
    if hasattr(block, "children"):
        children = getattr(block, "children")
        if isinstance(children, (list, tuple)):
            return tuple(children)
    return tuple()


def add_child(container: XBlock, child: XBlock) -> None:
    """
    Add a child block to a container that uses a 'children' field of usage_ids.
    """
    if not hasattr(container, "children"):
        raise AttributeError(f"{type(container).__name__} has no 'children' field")
    children = list(getattr(container, "children") or [])
    if child.scope_ids.usage_id not in children:
        children.append(child.scope_ids.usage_id)
        setattr(container, "children", children)


def remove_child(container: XBlock, child_usage_id: str) -> None:
    """
    Remove a child usage_id from a container's 'children' field.
    """
    if not hasattr(container, "children"):
        raise AttributeError(f"{type(container).__name__} has no 'children' field")
    children = list(getattr(container, "children") or [])
    if child_usage_id in children:
        children.remove(child_usage_id)
        setattr(container, "children", children)


def save_fields_json(block: XBlock, path: str, names: Optional[Iterable[str]] = None) -> None:
    """
    Persist a snapshot of selected field values as JSON.
    """
    snap = snapshot_fields(block, names)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(snap, f, indent=2, sort_keys=True, default=str)


def load_fields_json(block: XBlock, path: str, overwrite: bool = True) -> None:
    """
    Load field values from a JSON snapshot into the block.
    """
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    for name, value in data.items():
        if not overwrite and name in block.fields:
            continue
        if name in block.fields:
            setattr(block, name, value)


def get_published_events(runtime: TestRuntime) -> Tuple[Dict[str, Any], ...]:
    """
    Return events captured by TestRuntime.publish (if available).
    """
    events = getattr(runtime, "published_events", None)
    if isinstance(events, list):
        return tuple(events)
    return tuple()


# Optional: small convenience to create a block with fresh runtime in one call.
def new_block_with_runtime(
    block_cls: Type[XBlock],
    *,
    use_kvs: bool = True,
    user_id: str = "student",
    block_type: Optional[str] = None,
    def_id: Optional[str] = None,
    usage_id: Optional[str] = None,
    initial_fields: Optional[Dict[str, Any]] = None,
    extra_services: Optional[Dict[str, Any]] = None,
) -> Tuple[XBlock, TestRuntime]:
    """
    Create a new runtime and a single block instance in it.
    """
    runtime, _field_data, _store = init_runtime(use_kvs=use_kvs, user_id=user_id, extra_services=extra_services)
    block = create_block(
        block_cls,
        runtime,
        scope_ids=make_scope_ids(user_id=user_id, block_type=block_type or getattr(block_cls, "entry_point", block_cls.__name__.lower()), def_id=def_id, usage_id=usage_id),
        initial_fields=initial_fields,
    )
    return block, runtime


# Example usage (uncomment to test):
# if __name__ == "__main__":
#     from xblock.fields import Integer, String
#
#     class CounterBlock(XBlock):
#         display_name = String(default="Counter", scope=Scope.content)
#         count = Integer(default=0, scope=Scope.user_state)
#
#         def handle_event(self, event_type, event):
#             if event_type == "increment":
#                 self.count += int(event.get("by", 1))
#                 self.runtime.publish(self, "count_updated", {"count": self.count})
#
#     block, runtime = new_block_with_runtime(CounterBlock, user_id="alice")
#     print("Initial:", snapshot_fields(block))
#     send_event(block, "increment", {"by": 3})
#     print("After increment:", get_field(block, "count"))
#     publish_event(block, "custom_log", {"foo": "bar"})
#     print("Published events:", get_published_events(runtime))
------------------------------------------------------------

PROMPT 276:
"Generate Python code that imports the requests library and uses it to send HTTP requests to the Tinify API to compress and decompress images, including setting up API credentials, handling errors, and formatting response data."
RESPONSE:
import os
import time
from typing import Any, Dict, Optional
import requests


TINIFY_API_URL = "https://api.tinify.com/shrink"


class TinifyAPIError(Exception):
    def __init__(self, status: int, error: Optional[str], message: str, compression_count: Optional[str] = None):
        super().__init__(f"{status} {error or ''} {message}".strip())
        self.status = status
        self.error = error
        self.message = message
        self.compression_count = compression_count


class TinifyClient:
    def __init__(self, api_key: str, timeout: float = 30.0, max_retries: int = 3):
        if not api_key:
            raise ValueError("Tinify API key is required.")
        self.timeout = timeout
        self.max_retries = max_retries
        self.session = requests.Session()
        self.session.auth = (api_key, "")
        self.session.headers.update({
            "Accept": "application/json",
            "User-Agent": "TinifyRequestsClient/1.0"
        })

    def compress_file(
        self,
        input_path: str,
        output_path: str,
        resize: Optional[Dict[str, Any]] = None,
        preserve: Optional[list] = None,
        convert: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        with open(input_path, "rb") as f:
            data = f.read()

        # Step 1: Upload to /shrink
        r = self._request("POST", TINIFY_API_URL, data=data, headers={"Content-Type": "application/octet-stream"})
        self._raise_for_error(r, expected=(201,))
        shrink_meta = self._safe_json(r)
        result_url = r.headers.get("Location") or (shrink_meta.get("output") or {}).get("url")
        if not result_url:
            raise TinifyAPIError(r.status_code, "InvalidResponse", "No result URL provided by Tinify.", r.headers.get("Compression-Count"))

        # Step 2: Optionally transform (resize/preserve/convert) by POSTing to the result URL
        transform_meta = shrink_meta
        if any([resize, preserve, convert]):
            body: Dict[str, Any] = {}
            if resize:
                body["resize"] = resize
            if preserve:
                body["preserve"] = preserve
            if convert:
                body["convert"] = convert

            r2 = self._request("POST", result_url, json=body)
            self._raise_for_error(r2, expected=(201,))
            transform_meta = self._safe_json(r2)
            result_url = r2.headers.get("Location") or (transform_meta.get("output") or {}).get("url")
            if not result_url:
                raise TinifyAPIError(r2.status_code, "InvalidResponse", "No transformed result URL from Tinify.", r2.headers.get("Compression-Count"))

        # Step 3: Download the optimized image
        r3 = self._request("GET", result_url)
        self._raise_for_error(r3, expected=(200,))
        with open(output_path, "wb") as f:
            f.write(r3.content)

        return self._format_summary(transform_meta, r3.headers)

    def compress_from_url(
        self,
        source_url: str,
        output_path: str,
        resize: Optional[Dict[str, Any]] = None,
        preserve: Optional[list] = None,
        convert: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        # Step 1: Ask Tinify to fetch the image by URL
        r = self._request("POST", TINIFY_API_URL, json={"source": {"url": source_url}})
        self._raise_for_error(r, expected=(201,))
        shrink_meta = self._safe_json(r)
        result_url = r.headers.get("Location") or (shrink_meta.get("output") or {}).get("url")
        if not result_url:
            raise TinifyAPIError(r.status_code, "InvalidResponse", "No result URL provided by Tinify.", r.headers.get("Compression-Count"))

        # Step 2: Optional transform
        transform_meta = shrink_meta
        if any([resize, preserve, convert]):
            body: Dict[str, Any] = {}
            if resize:
                body["resize"] = resize
            if preserve:
                body["preserve"] = preserve
            if convert:
                body["convert"] = convert

            r2 = self._request("POST", result_url, json=body)
            self._raise_for_error(r2, expected=(201,))
            transform_meta = self._safe_json(r2)
            result_url = r2.headers.get("Location") or (transform_meta.get("output") or {}).get("url")
            if not result_url:
                raise TinifyAPIError(r2.status_code, "InvalidResponse", "No transformed result URL from Tinify.", r2.headers.get("Compression-Count"))

        # Step 3: Download the optimized image
        r3 = self._request("GET", result_url)
        self._raise_for_error(r3, expected=(200,))
        with open(output_path, "wb") as f:
            f.write(r3.content)

        return self._format_summary(transform_meta, r3.headers)

    def decompress_image(self, *args, **kwargs):
        # Tinify performs lossy optimization; there is no decompression or restoration endpoint.
        # This function is provided to illustrate the concept and will raise to avoid misuse.
        raise NotImplementedError("Tinify does not support decompression or restoring images to their original state.")

    # Internal helpers

    def _request(self, method: str, url: str, **kwargs) -> requests.Response:
        # Simple retry logic for rate limits and transient server errors
        attempts = 0
        backoff = 1.0
        while True:
            attempts += 1
            resp = self.session.request(method, url, timeout=self.timeout, **kwargs)
            if resp.status_code not in (429,) and not (500 <= resp.status_code < 600):
                return resp
            if attempts >= self.max_retries:
                return resp
            time.sleep(backoff)
            backoff *= 2

    def _raise_for_error(self, resp: requests.Response, expected=(200, 201)) -> None:
        if resp.status_code in expected:
            return
        err = None
        msg = resp.text
        try:
            data = resp.json()
            err = data.get("error")
            msg = data.get("message", msg)
        except Exception:
            pass
        raise TinifyAPIError(resp.status_code, err, msg, resp.headers.get("Compression-Count"))

    def _safe_json(self, resp: requests.Response) -> Dict[str, Any]:
        try:
            return resp.json()
        except Exception:
            return {}

    def _format_summary(self, meta_json: Dict[str, Any], headers: Dict[str, Any]) -> Dict[str, Any]:
        input_info = meta_json.get("input", {}) or {}
        output_info = meta_json.get("output", {}) or {}
        in_size = input_info.get("size")
        out_size = output_info.get("size")
        ratio = round(out_size / in_size, 4) if (in_size and out_size and in_size > 0) else None
        return {
            "input_bytes": in_size,
            "output_bytes": out_size,
            "bytes_saved": (in_size - out_size) if (in_size and out_size) else None,
            "ratio": ratio,
            "input_type": input_info.get("type"),
            "output_type": output_info.get("type"),
            "width": output_info.get("width"),
            "height": output_info.get("height"),
            "mime": headers.get("Content-Type"),
            "compression_count": headers.get("Compression-Count"),
        }


if __name__ == "__main__":
    # Setup: export TINIFY_API_KEY=your_api_key
    api_key = os.environ.get("TINIFY_API_KEY")
    client = TinifyClient(api_key=api_key)

    try:
        # Compress a local file
        summary1 = client.compress_file(
            input_path="input.jpg",
            output_path="output.jpg",
            resize={"method": "fit", "width": 1600, "height": 1200},  # optional
            preserve=["location", "creation"],  # optional, preserves metadata if present
        )
        print("Local file compression summary:", summary1)

        # Compress from a remote URL
        summary2 = client.compress_from_url(
            source_url="https://example.com/image.png",
            output_path="optimized.png",
            convert={"type": ["image/webp"]},  # optional conversion example
        )
        print("URL compression summary:", summary2)

        # Demonstrate that decompression is not supported
        try:
            client.decompress_image("output.jpg", "restored.jpg")
        except NotImplementedError as e:
            print("Decompress attempt:", e)

    except TinifyAPIError as e:
        print(f"Tinify API error: {e} (compression_count={e.compression_count})")
    except Exception as e:
        print(f"Unexpected error: {e}")
------------------------------------------------------------

PROMPT 277:
"Generate Python code that imports the necessary libraries and creates an instance of the Google Cloud Certificate Manager API client library, which can be used to manage SSL certificates and create certificate signing requests."
RESPONSE:
# Requires:
#   pip install google-cloud-certificate-manager cryptography
# Auth:
#   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

from google.cloud.certificate_manager_v1 import CertificateManagerClient

# Create a Certificate Manager client (manages SSL cert resources, mappings, DNS authorizations, etc.)
cm_client = CertificateManagerClient()

# Optional: example of building a parent resource path
# project_id = "your-gcp-project-id"
# location = "global"  # or a regional location, e.g., "us-central1"
# parent = f"projects/{project_id}/locations/{location}"

# If you also want to create a Certificate Signing Request (CSR) locally to submit to a CA,
# you can use the cryptography library:

from cryptography import x509
from cryptography.x509.oid import NameOID
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import rsa, ec


def create_csr(
    common_name: str,
    san_dns: list[str] | None = None,
    key_type: str = "RSA",
    key_size: int = 2048,
    ec_curve: str = "SECP256R1",
    key_passphrase: bytes | None = None,
) -> tuple[bytes, bytes]:
    """
    Generates a private key and CSR.

    Returns:
      (csr_pem, private_key_pem)
    """
    if key_type.upper() == "RSA":
        private_key = rsa.generate_private_key(public_exponent=65537, key_size=key_size)
    else:
        curve = {
            "SECP256R1": ec.SECP256R1,
            "SECP384R1": ec.SECP384R1,
            "SECP521R1": ec.SECP521R1,
        }.get(ec_curve.upper(), ec.SECP256R1)
        private_key = ec.generate_private_key(curve())

    name = x509.Name([x509.NameAttribute(NameOID.COMMON_NAME, common_name)])
    csr_builder = x509.CertificateSigningRequestBuilder().subject_name(name)

    if san_dns:
        csr_builder = csr_builder.add_extension(
            x509.SubjectAlternativeName([x509.DNSName(d) for d in san_dns]),
            critical=False,
        )

    csr = csr_builder.sign(private_key, hashes.SHA256())
    csr_pem = csr.public_bytes(serialization.Encoding.PEM)

    encryption = (
        serialization.BestAvailableEncryption(key_passphrase)
        if key_passphrase
        else serialization.NoEncryption()
    )
    key_pem = private_key.private_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PrivateFormat.TraditionalOpenSSL,
        encryption_algorithm=encryption,
    )
    return csr_pem, key_pem


# Example usage:
# csr_pem, key_pem = create_csr(
#     common_name="example.com",
#     san_dns=["example.com", "www.example.com"],
#     key_type="RSA",
#     key_size=2048,
# )
# Submit csr_pem to your CA. After issuance, upload the resulting certificate
# (and its chain) to Certificate Manager as a self-managed certificate using cm_client.
------------------------------------------------------------

PROMPT 278:
"Generate Python code that imports the sqlite3 module and creates a connection to a SQLite database, then defines a function that takes a query as input and executes it on the database, returning the results as a list of tuples."
RESPONSE:
import sqlite3

# Create a connection to a SQLite database file (creates it if it doesn't exist)
conn = sqlite3.connect("example.db")

def execute_query(query, params=None):
    """
    Execute a SQL query against the connected SQLite database.

    Args:
        query (str): SQL query string. Use ? placeholders for parameters.
        params (tuple|list|None): Optional parameters to bind to the query.

    Returns:
        list[tuple]: Result rows as a list of tuples (empty list if no rows).
    """
    with conn:  # Commits on success, rolls back on exception
        cur = conn.cursor()
        if params is not None:
            cur.execute(query, params)
        else:
            cur.execute(query)
        return cur.fetchall()  # For non-SELECT statements, this will return []
------------------------------------------------------------

PROMPT 279:
"Generate Python code that imports the necessary modules, configures an Exasol database connection, and uses the Airflow provider to create a DAG for fetching data from the Exasol database and loading it into an external system."
RESPONSE:
import os
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

import requests
from airflow import DAG, settings
from airflow.models import Connection
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.providers.exasol.hooks.exasol import ExasolHook


# --------------------------
# Configuration (env-driven)
# --------------------------
EXASOL_CONN_ID = os.getenv("EXASOL_CONN_ID", "exasol_default")
EXASOL_HOST = os.getenv("EXASOL_HOST", "exa-db.example.com")
EXASOL_PORT = int(os.getenv("EXASOL_PORT", "8563"))
EXASOL_USER = os.getenv("EXASOL_USER", "EXA_USER")
EXASOL_PASSWORD = os.getenv("EXASOL_PASSWORD", "EXA_PASSWORD")
EXASOL_SCHEMA = os.getenv("EXASOL_SCHEMA", "MY_SCHEMA")

# Example SQL; replace with your query
EXASOL_SQL = os.getenv(
    "EXASOL_SQL",
    """
    SELECT
        CAST(ROW_NUMBER() OVER (ORDER BY 1) AS INTEGER) AS id,
        CURRENT_DATE AS as_of_date
    FROM (SELECT 1) t
    LIMIT 10
    """,
)

# External system configuration (example: REST API endpoint)
EXTERNAL_ENDPOINT_URL = os.getenv("EXTERNAL_ENDPOINT_URL", "https://api.example.com/ingest")
EXTERNAL_API_TOKEN = os.getenv("EXTERNAL_API_TOKEN", "replace-with-token")
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "500"))


# --------------------------
# Connection bootstrap
# --------------------------
def ensure_exasol_connection() -> None:
    """
    Ensure an Airflow Connection for Exasol exists.
    Note: In production, prefer managing connections via UI/CLI or env vars.
    """
    session = settings.Session()
    try:
        existing = (
            session.query(Connection)
            .filter(Connection.conn_id == EXASOL_CONN_ID)
            .one_or_none()
        )
        if existing:
            # Optionally update existing connection if fields differ
            needs_commit = False
            if existing.conn_type != "exasol":
                existing.conn_type = "exasol"
                needs_commit = True
            if existing.host != EXASOL_HOST:
                existing.host = EXASOL_HOST
                needs_commit = True
            if existing.port != EXASOL_PORT:
                existing.port = EXASOL_PORT
                needs_commit = True
            if existing.login != EXASOL_USER:
                existing.login = EXASOL_USER
                needs_commit = True
            if existing.password != EXASOL_PASSWORD:
                existing.password = EXASOL_PASSWORD
                needs_commit = True
            if existing.schema != EXASOL_SCHEMA:
                existing.schema = EXASOL_SCHEMA
                needs_commit = True
            # Example extras (optional): compression/encryption supported by pyexasol
            default_extras = {"compression": True, "encryption": True}
            if existing.extra_dejson != default_extras:
                existing.set_extra(json.dumps(default_extras))
                needs_commit = True
            if needs_commit:
                session.add(existing)
                session.commit()
        else:
            conn = Connection(
                conn_id=EXASOL_CONN_ID,
                conn_type="exasol",
                host=EXASOL_HOST,
                port=EXASOL_PORT,
                login=EXASOL_USER,
                password=EXASOL_PASSWORD,
                schema=EXASOL_SCHEMA,
                extra=json.dumps({"compression": True, "encryption": True}),  # optional
            )
            session.add(conn)
            session.commit()
    finally:
        session.close()


# --------------------------
# Task callables
# --------------------------
def fetch_from_exasol(sql: str, exasol_conn_id: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
    """
    Fetch data from Exasol using the Airflow ExasolHook.
    Returns a list of dicts suitable for JSON serialization.
    """
    hook = ExasolHook(exasol_conn_id=exasol_conn_id)

    # Prefer pandas export for convenience; ExasolHook typically provides this.
    # If pandas is not available, you can implement a records-based approach.
    df = hook.get_pandas_df(sql)  # type: ignore[no-redef]
    if limit is not None:
        df = df.head(limit)
    records = df.to_dict(orient="records")
    return records


def _chunk(iterable: List[Any], size: int) -> List[List[Any]]:
    return [iterable[i : i + size] for i in range(0, len(iterable), size)]


def load_to_external_system_from_xcom(
    ti,
    endpoint_url: str,
    api_token: str,
    batch_size: int = 500,
) -> Dict[str, Any]:
    """
    Pull records from previous task via XCom and load them to an external system (e.g., REST API).
    """
    records: List[Dict[str, Any]] = ti.xcom_pull(task_ids="fetch_from_exasol")
    if not records:
        return {"rows_sent": 0, "status": "no_data"}

    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
    }

    total = 0
    for chunk in _chunk(records, batch_size):
        resp = requests.post(endpoint_url, json=chunk, headers=headers, timeout=60)
        resp.raise_for_status()
        total += len(chunk)

    return {"rows_sent": total, "status": "success"}


# --------------------------
# DAG definition
# --------------------------
default_args = {
    "owner": "data-eng",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="exasol_to_external_system",
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,  # trigger manually or set a cron/ preset like "@daily"
    catchup=False,
    default_args=default_args,
    description="Fetch data from Exasol and load into an external system using Airflow Exasol provider",
    tags=["exasol", "etl"],
) as dag:
    start = EmptyOperator(task_id="start")

    ensure_conn = PythonOperator(
        task_id="ensure_exasol_connection",
        python_callable=ensure_exasol_connection,
    )

    fetch_task = PythonOperator(
        task_id="fetch_from_exasol",
        python_callable=fetch_from_exasol,
        op_kwargs={
            "sql": EXASOL_SQL,
            "exasol_conn_id": EXASOL_CONN_ID,
            # Optionally cap result size during development
            "limit": int(os.getenv("FETCH_LIMIT", "1000")),
        },
    )

    load_task = PythonOperator(
        task_id="load_to_external_system",
        python_callable=load_to_external_system_from_xcom,
        op_kwargs={
            "endpoint_url": EXTERNAL_ENDPOINT_URL,
            "api_token": EXTERNAL_API_TOKEN,
            "batch_size": BATCH_SIZE,
        },
    )

    end = EmptyOperator(task_id="end")

    start >> ensure_conn >> fetch_task >> load_task >> end

# Notes:
# - Ensure the following packages are installed in your Airflow environment:
#   pip install apache-airflow-providers-exasol pyexasol requests
# - Prefer configuring the Airflow connection via UI/CLI or environment variables in production.
# - Replace EXASOL_SQL and the external system details with your actual logic.
------------------------------------------------------------

PROMPT 280:
"Generate Python code that creates a CDK construct for an AWS Lambda function using the CDK library in Python, providing a boilerplate template for a serverless function that can be customized and extended to meet specific needs."
RESPONSE:
Below is a ready-to-use boilerplate for a reusable AWS CDK construct that creates and configures a Lambda function in Python (CDK v2), plus a minimal example stack and app that uses it.

File: constructs/serverless_function.py
------------------------------------
from typing import Mapping, Optional, Sequence
from constructs import Construct
from aws_cdk import (
    Duration,
    aws_lambda as lambda_,
    aws_logs as logs,
    aws_iam as iam,
    aws_sqs as sqs,
    aws_ec2 as ec2,
)


class ServerlessFunctionProps:
    def __init__(
        self,
        *,
        code_path: str,
        handler: str,
        function_name: Optional[str] = None,
        description: Optional[str] = None,
        runtime: lambda_.Runtime = lambda_.Runtime.PYTHON_3_12,
        architecture: lambda_.Architecture = lambda_.Architecture.ARM_64,
        memory_size: int = 256,
        timeout: Duration = Duration.seconds(10),
        environment: Optional[Mapping[str, str]] = None,
        layers: Optional[Sequence[lambda_.ILayerVersion]] = None,
        reserved_concurrency: Optional[int] = None,
        log_retention: logs.RetentionDays = logs.RetentionDays.ONE_WEEK,
        tracing: lambda_.Tracing = lambda_.Tracing.ACTIVE,
        insights_version: Optional[lambda_.LambdaInsightsVersion] = None,
        # Networking (optional)
        vpc: Optional[ec2.IVpc] = None,
        security_groups: Optional[Sequence[ec2.ISecurityGroup]] = None,
        subnet_selection: Optional[ec2.SubnetSelection] = None,
        # Dead-letter queue (optional)
        dead_letter_queue_enabled: bool = False,
        dead_letter_queue: Optional[sqs.IQueue] = None,
        # Additional permissions to attach to the Lambda's role
        permissions: Optional[Sequence[iam.PolicyStatement]] = None,
        # Optional event sources (e.g., SQS, DynamoDB Streams)
        event_sources: Optional[Sequence[lambda_.IEventSource]] = None,
        # Misc
        enable_snap_start: bool = False,  # Only for Java at the moment; included as a placeholder
        ephemeral_storage_mb: Optional[int] = None,  # e.g. 1024, 2048 up to 10240
    ):
        self.code_path = code_path
        self.handler = handler
        self.function_name = function_name
        self.description = description
        self.runtime = runtime
        self.architecture = architecture
        self.memory_size = memory_size
        self.timeout = timeout
        self.environment = environment or {}
        self.layers = layers or []
        self.reserved_concurrency = reserved_concurrency
        self.log_retention = log_retention
        self.tracing = tracing
        self.insights_version = insights_version

        self.vpc = vpc
        self.security_groups = security_groups
        self.subnet_selection = subnet_selection

        self.dead_letter_queue_enabled = dead_letter_queue_enabled
        self.dead_letter_queue = dead_letter_queue

        self.permissions = permissions or []
        self.event_sources = event_sources or []

        self.enable_snap_start = enable_snap_start
        self.ephemeral_storage_mb = ephemeral_storage_mb


class ServerlessFunction(Construct):
    def __init__(self, scope: Construct, construct_id: str, *, props: ServerlessFunctionProps) -> None:
        super().__init__(scope, construct_id)

        # Optionally create a DLQ if enabled and not provided
        dlq = props.dead_letter_queue
        if props.dead_letter_queue_enabled and props.dead_letter_queue is None:
            dlq = sqs.Queue(self, "Dlq")

        # Create the Lambda function
        self.function = lambda_.Function(
            self,
            "Function",
            function_name=props.function_name,
            description=props.description,
            runtime=props.runtime,
            architecture=props.architecture,
            memory_size=props.memory_size,
            timeout=props.timeout,
            environment=props.environment,
            handler=props.handler,
            code=lambda_.Code.from_asset(props.code_path),
            layers=list(props.layers),
            reserved_concurrent_executions=props.reserved_concurrency,
            log_retention=props.log_retention,
            tracing=props.tracing,
            insights_version=props.insights_version,
            vpc=props.vpc,
            security_groups=list(props.security_groups) if props.security_groups else None,
            vpc_subnets=props.subnet_selection,
            dead_letter_queue=dlq,
            dead_letter_queue_enabled=props.dead_letter_queue_enabled,
            ephemeral_storage_size=(
                Duration.mebibytes(props.ephemeral_storage_mb)
                if props.ephemeral_storage_mb
                else None
            ),
        )

        # Attach any additional permissions to the Lambda's execution role
        for stmt in props.permissions:
            self.function.add_to_role_policy(stmt)

        # Add any provided event sources
        for es in props.event_sources:
            self.function.add_event_source(es)

    @property
    def function_arn(self) -> str:
        return self.function.function_arn

    @property
    def function_name(self) -> str:
        return self.function.function_name


File: stacks/sample_stack.py
---------------------------
from constructs import Construct
from aws_cdk import (
    Stack,
    CfnOutput,
    Duration,
    aws_lambda as lambda_,
    aws_logs as logs,
)
from constructs.serverless_function import ServerlessFunction, ServerlessFunctionProps


class SampleStack(Stack):
    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        hello_fn = ServerlessFunction(
            self,
            "HelloFunction",
            props=ServerlessFunctionProps(
                code_path="lambda/hello",         # directory containing code
                handler="app.handler",            # module.function
                function_name="hello-fn",
                description="Example serverless function",
                runtime=lambda_.Runtime.PYTHON_3_12,
                memory_size=256,
                timeout=Duration.seconds(10),
                architecture=lambda_.Architecture.ARM_64,
                log_retention=logs.RetentionDays.ONE_WEEK,
                tracing=lambda_.Tracing.ACTIVE,
                environment={
                    "STAGE": self.node.try_get_context("stage") or "dev",
                    "POWERTOOLS_SERVICE_NAME": "hello-fn",
                },
            ),
        )

        CfnOutput(self, "HelloFunctionName", value=hello_fn.function_name)
        CfnOutput(self, "HelloFunctionArn", value=hello_fn.function_arn)


File: app.py
------------
#!/usr/bin/env python3
import aws_cdk as cdk
from stacks.sample_stack import SampleStack

app = cdk.App()
SampleStack(app, "SampleServerlessStack")
app.synth()


File: lambda/hello/app.py
------------------------
def handler(event, context):
    return {
        "statusCode": 200,
        "body": "Hello from CDK Python Lambda!"
    }


requirements.txt (CDK v2)
-------------------------
aws-cdk-lib>=2.149.0,<3.0.0
constructs>=10.0.0,<11.0.0

How to use:
- Create the directory structure as shown by the file headers.
- Install dependencies: pip install -r requirements.txt
- Bootstrap your environment (once per account/region): cdk bootstrap
- Synthesize: cdk synth
- Deploy: cdk deploy

Customization ideas:
- Add event sources (e.g., SQS, EventBridge) by constructing event source objects and passing them via props.event_sources.
- Attach extra IAM permissions with props.permissions using iam.PolicyStatement.
- Run inside a VPC by passing props.vpc (and optional subnet/security group selections).
- Add Lambda Layers via props.layers.
- Adjust tracing, log retention, memory, and timeout as needed.
------------------------------------------------------------

PROMPT 281:
"Generate Python code that uses the requests library to make HTTP requests to the DocuSign REST API, allowing users to create, update, and delete documents, templates, and signatures, as well as manage user accounts and permissions."
RESPONSE:
# DocuSign REST API helper using requests
# - Create, update, delete documents (via envelopes)
# - Create, update, delete templates
# - Create, update (image), delete user signatures
# - Manage users and permission profiles
#
# Requirements:
#   pip install requests
#
# Notes:
# - You must obtain an OAuth 2.0 access token and your account_id.
# - base_uri should be your account's base URI (demo default provided).
# - Many operations require admin permissions.
# - This code targets the eSignature REST API v2.1.
#
# Official API docs: https://developers.docusign.com/docs/esign-rest-api/reference/

import base64
import json
import mimetypes
import os
from typing import Any, Dict, List, Optional, Tuple

import requests


class DocuSignClient:
    def __init__(self, access_token: str, account_id: str, base_uri: str = "https://demo.docusign.net"):
        """
        access_token: OAuth 2.0 bearer token (string)
        account_id: DocuSign account ID (GUID)
        base_uri: account's base URI, e.g. https://{server}.docusign.net
        """
        self.account_id = account_id
        self.base_uri = base_uri.rstrip("/")
        self.base_url = f"{self.base_uri}/restapi/v2.1/accounts/{self.account_id}"
        self.session = requests.Session()
        self.session.headers.update(
            {
                "Authorization": f"Bearer {access_token}",
                "Accept": "application/json",
            }
        )

    def _request(self, method: str, path: str, expected: Tuple[int, ...] = (200, 201, 202), **kwargs) -> Any:
        url = path if path.startswith("http") else f"{self.base_url}{path}"
        resp = self.session.request(method, url, **kwargs)
        if resp.status_code not in expected:
            # Try to parse DocuSign error format
            try:
                err = resp.json()
            except Exception:
                err = resp.text
            raise requests.HTTPError(f"DocuSign API error {resp.status_code}: {err}", response=resp)
        if resp.headers.get("Content-Type", "").startswith("application/json"):
            return resp.json()
        return resp.content

    @staticmethod
    def _file_to_b64(file_path: str) -> str:
        with open(file_path, "rb") as f:
            return base64.b64encode(f.read()).decode("ascii")

    # ---------------------------
    # Envelopes + Documents
    # ---------------------------

    def create_envelope_with_document(
        self,
        email_subject: str,
        file_path: str,
        signer_email: str,
        signer_name: str,
        status: str = "created",  # "created" (draft) or "sent"
        anchor_string: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Creates an envelope with a single document and a single signer.
        If status="sent", the envelope is sent immediately; otherwise draft.
        anchor_string: If provided, a SignHere tab will be anchored to this string in the document.
        """
        filename = os.path.basename(file_path)
        mime, _ = mimetypes.guess_type(filename)
        doc_b64 = self._file_to_b64(file_path)

        sign_here_tabs = []
        if anchor_string:
            sign_here_tabs.append(
                {
                    "anchorString": anchor_string,
                    "anchorUnits": "pixels",
                    "anchorXOffset": "0",
                    "anchorYOffset": "0",
                }
            )
        else:
            # Absolute position fallback top-left area (x=100,y=150 on page 1)
            sign_here_tabs.append({"pageNumber": "1", "xPosition": "100", "yPosition": "150"})

        body = {
            "emailSubject": email_subject,
            "documents": [
                {
                    "documentBase64": doc_b64,
                    "documentId": "1",
                    "fileExtension": (os.path.splitext(filename)[1][1:] or "pdf"),
                    "name": filename,
                }
            ],
            "recipients": {
                "signers": [
                    {
                        "email": signer_email,
                        "name": signer_name,
                        "recipientId": "1",
                        "routingOrder": "1",
                        "tabs": {"signHereTabs": sign_here_tabs},
                    }
                ]
            },
            "status": status,
        }
        return self._request("POST", "/envelopes", json=body, expected=(201, 200))

    def replace_envelope_document(self, envelope_id: str, document_id: str, file_path: str) -> Dict[str, Any]:
        """
        Replaces an existing document in a draft envelope with a new file.
        Note: The envelope must be in "created" (draft) state.
        """
        filename = os.path.basename(file_path)
        mime, _ = mimetypes.guess_type(filename)
        mime = mime or "application/octet-stream"
        with open(file_path, "rb") as f:
            files = {"file": (filename, f, mime)}
            # Optional additional form params can be passed via 'data' if needed
            return self._request(
                "PUT",
                f"/envelopes/{envelope_id}/documents/{document_id}",
                files=files,
                expected=(200, 201),
            )

    def delete_envelope_document(self, envelope_id: str, document_id: str) -> Dict[str, Any]:
        """
        Deletes a document from an envelope (draft or completed dependent on rules).
        """
        return self._request("DELETE", f"/envelopes/{envelope_id}/documents/{document_id}", expected=(200, 202))

    # ---------------------------
    # Templates
    # ---------------------------

    def create_template(
        self,
        name: str,
        description: str,
        file_path: str,
        role_name: str = "Signer",
        anchor_string: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Creates a reusable template with one document and one role (Signer).
        """
        filename = os.path.basename(file_path)
        doc_b64 = self._file_to_b64(file_path)

        sign_here_tabs = []
        if anchor_string:
            sign_here_tabs.append(
                {
                    "anchorString": anchor_string,
                    "anchorUnits": "pixels",
                    "anchorXOffset": "0",
                    "anchorYOffset": "0",
                }
            )
        else:
            sign_here_tabs.append({"pageNumber": "1", "xPosition": "100", "yPosition": "150"})

        body = {
            "name": name,
            "description": description,
            "documents": [
                {
                    "documentBase64": doc_b64,
                    "documentId": "1",
                    "name": filename,
                    "fileExtension": (os.path.splitext(filename)[1][1:] or "pdf"),
                }
            ],
            "recipients": {"signers": [{"roleName": role_name, "recipientId": "1", "tabs": {"signHereTabs": sign_here_tabs}}]},
            "emailSubject": f"Template: {name}",
            "shared": "true",
        }
        return self._request("POST", "/templates", json=body, expected=(201, 200))

    def update_template_metadata(self, template_id: str, name: Optional[str] = None, description: Optional[str] = None) -> Dict[str, Any]:
        """
        Updates template metadata such as name and description.
        """
        body: Dict[str, Any] = {}
        if name is not None:
            body["name"] = name
        if description is not None:
            body["description"] = description
        return self._request("PUT", f"/templates/{template_id}", json=body, expected=(200,))

    def delete_template(self, template_id: str) -> Dict[str, Any]:
        """
        Deletes a template.
        """
        return self._request("DELETE", f"/templates/{template_id}", expected=(200, 202))

    # ---------------------------
    # User Signatures (Admin)
    # ---------------------------

    def list_user_signatures(self, user_id: str) -> Dict[str, Any]:
        return self._request("GET", f"/users/{user_id}/signatures", expected=(200,))

    def create_user_signature(
        self,
        user_id: str,
        signature_name: str,
        signature_initials: Optional[str] = None,
        signature_font: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Creates a user signature definition. You can optionally specify a font to auto-generate a drawn-style signature.
        Example fonts include: BradleyHandITC, DocuSign, Pacifico, Satisfy.
        """
        body: Dict[str, Any] = {"signatureName": signature_name}
        if signature_initials:
            body["signatureInitials"] = signature_initials
        if signature_font:
            body["signatureFont"] = signature_font
        return self._request("POST", f"/users/{user_id}/signatures", json=body, expected=(201, 200))

    def upload_user_signature_image(self, user_id: str, user_signature_id: str, image_path: str) -> Dict[str, Any]:
        """
        Uploads an image file for a user signature. Supported formats typically include PNG/JPG.
        """
        filename = os.path.basename(image_path)
        mime, _ = mimetypes.guess_type(filename)
        mime = mime or "application/octet-stream"
        with open(image_path, "rb") as f:
            files = {"signatureImage": (filename, f, mime)}
            return self._request(
                "PUT",
                f"/users/{user_id}/signatures/{user_signature_id}/signature_image",
                files=files,
                expected=(200, 201),
            )

    def delete_user_signature(self, user_id: str, user_signature_id: str) -> Dict[str, Any]:
        return self._request("DELETE", f"/users/{user_id}/signatures/{user_signature_id}", expected=(200, 202))

    # ---------------------------
    # Users and Permission Profiles
    # ---------------------------

    def list_users(self, additional_params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Lists users in the account. You can pass filters in additional_params (e.g., email, status).
        """
        return self._request("GET", "/users", params=additional_params or {}, expected=(200,))

    def create_user(
        self,
        user_name: str,
        email: str,
        permission_profile_id: Optional[str] = None,
        group_ids: Optional[List[str]] = None,
        auto_activate: bool = True,
    ) -> Dict[str, Any]:
        """
        Creates a user in the account. Requires admin permissions.
        """
        body: Dict[str, Any] = {
            "newUsers": [
                {
                    "userName": user_name,
                    "email": email,
                    # autoActivate requires account settings/permissions
                    "autoActivate": "true" if auto_activate else "false",
                }
            ]
        }

        if permission_profile_id:
            body["newUsers"][0]["permissionProfileId"] = permission_profile_id
        if group_ids:
            body["newUsers"][0]["groups"] = [{"groupId": gid} for gid in group_ids]

        return self._request("POST", "/users", json=body, expected=(201, 200))

    def update_user(
        self,
        user_id: str,
        user_name: Optional[str] = None,
        email: Optional[str] = None,
        permission_profile_id: Optional[str] = None,
        group_ids: Optional[List[str]] = None,
        user_status: Optional[str] = None,  # "active", "closed", "activation", etc.
    ) -> Dict[str, Any]:
        """
        Updates user info, permission profile, and groups. Requires admin permissions.
        """
        body: Dict[str, Any] = {}
        if user_name is not None:
            body["userName"] = user_name
        if email is not None:
            body["email"] = email
        if permission_profile_id is not None:
            body["permissionProfileId"] = permission_profile_id
        if group_ids is not None:
            body["groupList"] = [{"groupId": gid} for gid in group_ids]
        if user_status is not None:
            body["userStatus"] = user_status

        return self._request("PUT", f"/users/{user_id}", json=body, expected=(200,))

    def deactivate_user(self, user_id: str) -> Dict[str, Any]:
        """
        Deactivates (closes) a user.
        """
        return self.update_user(user_id=user_id, user_status="closed")

    def list_permission_profiles(self) -> Dict[str, Any]:
        return self._request("GET", "/permission_profiles", expected=(200,))

    def set_user_permission_profile(self, user_id: str, permission_profile_id: str) -> Dict[str, Any]:
        return self.update_user(user_id=user_id, permission_profile_id=permission_profile_id)


# ---------------------------
# Example usage
# ---------------------------
if __name__ == "__main__":
    # Replace with your values
    ACCESS_TOKEN = "YOUR_OAUTH_ACCESS_TOKEN"
    ACCOUNT_ID = "YOUR_ACCOUNT_ID"
    BASE_URI = "https://demo.docusign.net"  # Or your production base URI

    docusign = DocuSignClient(ACCESS_TOKEN, ACCOUNT_ID, BASE_URI)

    # 1) Create an envelope with a document and a signer
    try:
        env = docusign.create_envelope_with_document(
            email_subject="Please sign this document",
            file_path="sample.pdf",
            signer_email="recipient@example.com",
            signer_name="Recipient Name",
            status="created",  # draft
            anchor_string=None,  # or e.g. "/sn1/"
        )
        envelope_id = env.get("envelopeId")
        print("Created envelope:", envelope_id)
    except Exception as e:
        print("Error creating envelope:", e)
        envelope_id = None

    # 2) Replace the document in the draft envelope (update)
    if envelope_id:
        try:
            upd = docusign.replace_envelope_document(envelope_id, document_id="1", file_path="updated.pdf")
            print("Replaced document:", json.dumps(upd, indent=2))
        except Exception as e:
            print("Error replacing envelope document:", e)

    # 3) Delete a document from the envelope
    if envelope_id:
        try:
            dele = docusign.delete_envelope_document(envelope_id, document_id="1")
            print("Deleted document response:", json.dumps(dele, indent=2))
        except Exception as e:
            print("Error deleting envelope document:", e)

    # 4) Create a template
    try:
        tpl = docusign.create_template(
            name="My Template",
            description="A simple signing template",
            file_path="sample.pdf",
            role_name="Signer",
        )
        template_id = tpl.get("templateId")
        print("Created template:", template_id)
    except Exception as e:
        print("Error creating template:", e)
        template_id = None

    # 5) Update template metadata
    if template_id:
        try:
            upd_tpl = docusign.update_template_metadata(template_id, name="My Updated Template")
            print("Updated template:", json.dumps(upd_tpl, indent=2))
        except Exception as e:
            print("Error updating template:", e)

    # 6) Delete template
    if template_id:
        try:
            del_tpl = docusign.delete_template(template_id)
            print("Deleted template response:", json.dumps(del_tpl, indent=2))
        except Exception as e:
            print("Error deleting template:", e)

    # 7) User signatures: create a signature and upload an image
    # You need an existing user_id within the account (can be your own)
    USER_ID = "USER_GUID_HERE"
    try:
        sig = docusign.create_user_signature(
            user_id=USER_ID,
            signature_name="John Doe",
            signature_initials="JD",
            signature_font="BradleyHandITC",
        )
        print("Created user signature:", json.dumps(sig, indent=2))
        user_signature_id = (sig.get("userSignatureId") or sig.get("signatureId"))
        if user_signature_id and os.path.exists("sig.png"):
            up = docusign.upload_user_signature_image(USER_ID, user_signature_id, "sig.png")
            print("Uploaded signature image:", json.dumps(up, indent=2))
    except Exception as e:
        print("Error managing user signature:", e)

    # 8) List users, permission profiles, and set a user's permission profile
    try:
        profiles = docusign.list_permission_profiles()
        print("Permission profiles:", json.dumps(profiles, indent=2))
        users = docusign.list_users()
        print("Users:", json.dumps(users, indent=2))
        # Example: set a permission profile on a user (replace with real IDs)
        # docusign.set_user_permission_profile(USER_ID, permission_profile_id="PERMISSION_PROFILE_ID")
    except Exception as e:
        print("Error with users/permissions:", e)
------------------------------------------------------------

PROMPT 282:
"Generate Python code that imports the requests library and uses it to make API calls to TMDb to retrieve movie information, such as titles, descriptions, and cast lists, and stores the data in easily accessible python dictionaries."
RESPONSE:
# Requires: pip install requests
# Note: You need a TMDb API key (https://www.themoviedb.org/). Set it as the TMDB_API_KEY
# environment variable or pass it directly to TMDbClient(api_key="YOUR_KEY").
# This product uses the TMDB API but is not endorsed or certified by TMDB.

import os
import time
from typing import Dict, List, Optional

import requests


class TMDbClient:
    BASE_URL = "https://api.themoviedb.org/3"

    def __init__(
        self,
        api_key: Optional[str] = None,
        language: str = "en-US",
        timeout: int = 10,
        max_retries: int = 3,
    ):
        self.api_key = api_key or os.getenv("TMDB_API_KEY")
        if not self.api_key:
            raise ValueError("TMDB API key is required. Set TMDB_API_KEY or pass api_key.")
        self.language = language
        self.timeout = timeout
        self.max_retries = max_retries
        self.session = requests.Session()
        self.session.headers.update({"Accept": "application/json"})

    def _get(self, path: str, params: Optional[Dict] = None) -> Dict:
        params = params or {}
        params.setdefault("api_key", self.api_key)
        params.setdefault("language", self.language)

        url = f"{self.BASE_URL}{path}"
        retries = 0
        backoff = 1

        while True:
            resp = self.session.get(url, params=params, timeout=self.timeout)

            # Handle TMDb rate limits
            if resp.status_code == 429 and retries < self.max_retries:
                retry_after = int(resp.headers.get("Retry-After", backoff))
                time.sleep(retry_after)
                retries += 1
                backoff = min(backoff * 2, 8)
                continue

            resp.raise_for_status()
            return resp.json()

    def search_movies(self, query: str, year: Optional[int] = None, page: int = 1) -> List[Dict]:
        """
        Returns a list of simple movie dicts for a search query.
        """
        params = {"query": query, "page": page, "include_adult": False}
        if year:
            params["year"] = year

        data = self._get("/search/movie", params)
        results = []
        for m in data.get("results", []):
            results.append(
                {
                    "id": m["id"],
                    "title": m.get("title") or m.get("name"),
                    "description": m.get("overview"),
                    "release_date": m.get("release_date"),
                    "original_language": m.get("original_language"),
                    "vote_average": m.get("vote_average"),
                    "vote_count": m.get("vote_count"),
                    "popularity": m.get("popularity"),
                }
            )
        return results

    def get_movie(self, movie_id: int, include_cast: bool = True) -> Dict:
        """
        Retrieves a detailed movie dict, optionally including the full cast list.
        """
        params = {}
        if include_cast:
            # Append credits to get cast in one call
            params["append_to_response"] = "credits"

        data = self._get(f"/movie/{movie_id}", params)

        genres = [g["name"] for g in data.get("genres", []) if "name" in g]
        cast_list = []
        if include_cast:
            for c in (data.get("credits", {}) or {}).get("cast", []):
                cast_list.append(
                    {
                        "id": c.get("id"),
                        "name": c.get("name"),
                        "character": c.get("character"),
                        "order": c.get("order"),
                        "credit_id": c.get("credit_id"),
                    }
                )

        movie = {
            "id": data.get("id"),
            "imdb_id": data.get("imdb_id"),
            "title": data.get("title") or data.get("name"),
            "description": data.get("overview"),
            "release_date": data.get("release_date"),
            "runtime": data.get("runtime"),
            "genres": genres,
            "homepage": data.get("homepage"),
            "original_language": data.get("original_language"),
            "vote_average": data.get("vote_average"),
            "vote_count": data.get("vote_count"),
            "cast": cast_list,
        }
        return movie

    def get_movies_by_ids(self, ids: List[int], include_cast: bool = True) -> Dict[int, Dict]:
        """
        Fetch multiple movies by TMDb IDs and return a dict keyed by movie ID.
        """
        out: Dict[int, Dict] = {}
        for mid in ids:
            try:
                out[mid] = self.get_movie(mid, include_cast=include_cast)
            except requests.HTTPError as e:
                out[mid] = {"error": str(e)}
        return out

    def search_and_fetch(self, query: str, limit: int = 5, year: Optional[int] = None) -> List[Dict]:
        """
        Convenience method: search for movies by query, then fetch detailed info
        (including cast) for the top N results.
        """
        basic = self.search_movies(query, year=year, page=1)
        detailed: List[Dict] = []
        for m in basic[:limit]:
            detailed.append(self.get_movie(m["id"], include_cast=True))
        return detailed


if __name__ == "__main__":
    # Example usage:
    client = TMDbClient()  # or TMDbClient(api_key="YOUR_TMDB_API_KEY")

    # 1) Search for movies and get basic info as dictionaries
    search_results = client.search_movies("Inception")
    print("Search results (basic dictionaries):")
    for r in search_results[:3]:
        print(r)

    # 2) Get one movie with title, description, and cast list as dictionaries
    if search_results:
        movie_id = search_results[0]["id"]
        movie = client.get_movie(movie_id, include_cast=True)
        print("\nDetailed movie dictionary:")
        print(movie)
        print("\nTop 5 cast names:")
        print([c["name"] for c in movie["cast"][:5]])

    # 3) Search and fetch multiple detailed movies at once
    detailed_movies = client.search_and_fetch("The Matrix", limit=2)
    print("\nDetailed movies for 'The Matrix':")
    for m in detailed_movies:
        print({"title": m["title"], "release_date": m["release_date"], "top_cast": [c["name"] for c in m["cast"][:5]]})
------------------------------------------------------------

PROMPT 283:
"Generate Python code that imports the necessary libraries for data manipulation and analysis, such as Pandas and NumPy, and provides functions for data cleaning, transformation, and visualization, similar to the Ricequant Data SDK."
RESPONSE:
# data_toolkit.py
# A lightweight data utilities module for cleaning, transformation, and visualization
# inspired by typical data SDK ergonomics.

from __future__ import annotations

import re
import warnings
from dataclasses import dataclass
from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple, Union

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# ----------------------------- Plotting setup ---------------------------------

def set_plot_style(style: str = "whitegrid", context: str = "notebook", font_scale: float = 1.1) -> None:
    """Set a consistent plot style for seaborn/matplotlib."""
    sns.set_theme(style=style, context=context)
    sns.set_context(context, font_scale=font_scale)
    plt.rcParams["figure.figsize"] = (10, 5)
    plt.rcParams["axes.titlesize"] = "medium"
    plt.rcParams["axes.labelsize"] = "medium"


# ----------------------------- General utilities -------------------------------

def memory_usage_mb(df: pd.DataFrame) -> float:
    """Return the total memory usage of a DataFrame in MB."""
    return float(df.memory_usage(deep=True).sum()) / (1024 ** 2)


def _to_snake(name: str) -> str:
    """Convert a string to snake_case."""
    s = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", name)
    s = re.sub(r"[^0-9a-zA-Z]+", "_", s)
    return s.strip("_").lower()


def standardize_columns(
    df: pd.DataFrame,
    case: str = "snake",
    strip_whitespace: bool = True,
    deduplicate: bool = True,
) -> pd.DataFrame:
    """
    Standardize column names.
    - case: 'snake' or 'lower' or 'original'
    """
    new_cols = []
    seen: Dict[str, int] = {}
    for c in df.columns:
        new = c
        if strip_whitespace and isinstance(new, str):
            new = new.strip()
        if case == "snake" and isinstance(new, str):
            new = _to_snake(new)
        elif case == "lower" and isinstance(new, str):
            new = new.lower()
        if deduplicate:
            base = new
            i = 1
            while new in seen:
                i += 1
                new = f"{base}_{i}"
            seen[new] = 1
        new_cols.append(new)
    out = df.copy()
    out.columns = new_cols
    return out


def convert_dtypes_safely(df: pd.DataFrame) -> pd.DataFrame:
    """
    Convert columns to best possible dtypes without losing information.
    Attempts numeric and datetime coercion where appropriate.
    """
    out = df.copy()
    for col in out.columns:
        s = out[col]
        if s.dtype == object:
            # Try datetime
            try:
                converted = pd.to_datetime(s, errors="raise", utc=False, infer_datetime_format=True)
                # Heuristic: only accept if min 80% parse succeeded
                mask = ~s.isna()
                ok_ratio = (~converted.isna()).sum() / max(mask.sum(), 1)
                if ok_ratio >= 0.8:
                    out[col] = converted
                    continue
            except Exception:
                pass
            # Try numeric
            num = pd.to_numeric(s, errors="coerce")
            if num.notna().mean() >= 0.8:
                out[col] = num
                continue
            # Else leave as object
        elif pd.api.types.is_integer_dtype(s):
            out[col] = pd.to_numeric(s, downcast="integer")
        elif pd.api.types.is_float_dtype(s):
            out[col] = pd.to_numeric(s, downcast="float")
        elif pd.api.types.is_bool_dtype(s):
            out[col] = s.astype("boolean")
    # Pandas strict convert for remaining
    try:
        out = out.convert_dtypes()
    except Exception:
        pass
    return out


def infer_datetime_cols(df: pd.DataFrame, min_success_ratio: float = 0.8) -> pd.DataFrame:
    """
    Attempt to convert object columns to datetime where most values parse successfully.
    """
    out = df.copy()
    for c in out.columns:
        if out[c].dtype == object:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                parsed = pd.to_datetime(out[c], errors="coerce", infer_datetime_format=True, utc=False)
            if parsed.notna().mean() >= min_success_ratio:
                out[c] = parsed
    return out


def deduplicate(df: pd.DataFrame, subset: Optional[Union[str, List[str]]] = None, keep: str = "last") -> pd.DataFrame:
    """Drop duplicate rows."""
    return df.drop_duplicates(subset=subset, keep=keep)


def drop_empty_cols(df: pd.DataFrame, threshold: float = 1.0) -> pd.DataFrame:
    """
    Drop columns with a proportion of missing values >= threshold.
    threshold=1.0 drops columns that are completely empty.
    """
    mask = df.isna().mean() >= threshold
    return df.loc[:, ~mask]


# ----------------------------- Missing values ----------------------------------

def handle_missing(
    df: pd.DataFrame,
    strategy: str = "impute",
    fill_value: Any = 0,
    method: Optional[str] = None,  # 'ffill' or 'bfill'
    impute_numeric: str = "median",  # 'mean' | 'median'
    impute_categorical: str = "mode",
) -> pd.DataFrame:
    """
    Handle missing values with one of:
      - 'drop': drop rows with any missing values
      - 'fill': constant fill_value or method='ffill'/'bfill'
      - 'impute': numeric -> mean/median, categorical -> mode
    """
    out = df.copy()
    if strategy == "drop":
        return out.dropna()
    if strategy == "fill":
        if method in {"ffill", "bfill"}:
            return out.fillna(method=method)
        return out.fillna(fill_value)
    if strategy == "impute":
        num_cols = out.select_dtypes(include=[np.number]).columns
        cat_cols = out.columns.difference(num_cols)
        if len(num_cols):
            if impute_numeric == "mean":
                out[num_cols] = out[num_cols].fillna(out[num_cols].mean())
            else:
                out[num_cols] = out[num_cols].fillna(out[num_cols].median())
        for c in cat_cols:
            mode_vals = out[c].mode(dropna=True)
            if not mode_vals.empty:
                out[c] = out[c].fillna(mode_vals.iloc[0])
            else:
                out[c] = out[c].fillna("missing")
        return out
    raise ValueError("Unknown strategy. Use 'drop', 'fill', or 'impute'.")


def impute_by_group(
    df: pd.DataFrame,
    cols: Union[str, Sequence[str]],
    by: Union[str, Sequence[str]],
    method: str = "median",
) -> pd.DataFrame:
    """
    Impute missing values within groups using group statistics.
    method: 'mean' or 'median' (numeric only)
    """
    cols = [cols] if isinstance(cols, str) else list(cols)
    out = df.copy()
    agg = "mean" if method == "mean" else "median"
    grouped = out.groupby(by)[cols].transform(agg)
    for c in cols:
        out[c] = out[c].fillna(grouped[c])
    return out


# ----------------------------- Outliers & winsor -------------------------------

def remove_outliers(
    df: pd.DataFrame,
    cols: Optional[Sequence[str]] = None,
    method: str = "iqr",  # 'iqr' or 'zscore'
    iqr_factor: float = 1.5,
    z_thresh: float = 3.0,
) -> pd.DataFrame:
    """
    Remove rows deemed outliers in specified columns.
    """
    out = df.copy()
    cols = list(cols) if cols is not None else out.select_dtypes(include=[np.number]).columns.tolist()
    mask = pd.Series(True, index=out.index)
    if method == "iqr":
        for c in cols:
            q1 = out[c].quantile(0.25)
            q3 = out[c].quantile(0.75)
            iqr = q3 - q1
            lower, upper = (q1 - iqr_factor * iqr, q3 + iqr_factor * iqr)
            mask &= out[c].between(lower, upper) | out[c].isna()
    elif method == "zscore":
        for c in cols:
            mu, sigma = out[c].mean(), out[c].std(ddof=0)
            if sigma == 0 or np.isnan(sigma):
                continue
            z = (out[c] - mu) / sigma
            mask &= (z.abs() <= z_thresh) | out[c].isna()
    else:
        raise ValueError("method must be 'iqr' or 'zscore'")
    return out.loc[mask]


def winsorize(
    df: pd.DataFrame,
    cols: Optional[Sequence[str]] = None,
    lower_q: float = 0.01,
    upper_q: float = 0.99,
) -> pd.DataFrame:
    """
    Clip extreme values to quantile bounds (winsorization).
    """
    out = df.copy()
    cols = list(cols) if cols is not None else out.select_dtypes(include=[np.number]).columns.tolist()
    bounds = out[cols].quantile([lower_q, upper_q])
    for c in cols:
        out[c] = out[c].clip(bounds.loc[lower_q, c], bounds.loc[upper_q, c])
    return out


# ----------------------------- Scaling & encoding ------------------------------

@dataclass
class ScaleParams:
    method: str
    params: Dict[str, Tuple[float, float]]  # col -> (center, scale) semantics depend on method


def scale_numeric(
    df: pd.DataFrame,
    cols: Optional[Sequence[str]] = None,
    method: str = "standard",  # 'standard' | 'minmax' | 'robust'
    return_params: bool = False,
    params: Optional[ScaleParams] = None,
    clip: Optional[Tuple[float, float]] = None,  # optional post-scale clipping
) -> Union[pd.DataFrame, Tuple[pd.DataFrame, ScaleParams]]:
    """
    Scale numeric columns.
    - standard: (x - mean) / std
    - minmax: (x - min) / (max - min)
    - robust: (x - median) / IQR
    You can provide fitted params to apply the same transform to new data.
    """
    out = df.copy()
    num_cols = out.select_dtypes(include=[np.number]).columns.tolist()
    cols = list(cols) if cols is not None else num_cols

    if params is not None:
        if params.method != method:
            raise ValueError("Provided params.method differs from requested method.")
        p = params.params
        for c in cols:
            if c not in p:
                continue
            a, b = p[c]
            if method == "standard":
                out[c] = (out[c] - a) / (b if b != 0 else 1.0)
            elif method == "minmax":
                out[c] = (out[c] - a) / (b if b != 0 else 1.0)
            elif method == "robust":
                out[c] = (out[c] - a) / (b if b != 0 else 1.0)
    else:
        p: Dict[str, Tuple[float, float]] = {}
        for c in cols:
            s = out[c].astype(float)
            if method == "standard":
                mu, sd = s.mean(), s.std(ddof=0)
                p[c] = (mu, sd)
                out[c] = (s - mu) / (sd if sd != 0 else 1.0)
            elif method == "minmax":
                mn, mx = s.min(), s.max()
                rng = mx - mn
                p[c] = (mn, rng)
                out[c] = (s - mn) / (rng if rng != 0 else 1.0)
            elif method == "robust":
                med = s.median()
                iqr = s.quantile(0.75) - s.quantile(0.25)
                p[c] = (med, iqr)
                out[c] = (s - med) / (iqr if iqr != 0 else 1.0)
            else:
                raise ValueError("Unknown method. Use 'standard', 'minmax', or 'robust'.")

        params = ScaleParams(method=method, params=p)

    if clip is not None:
        lo, hi = clip
        out[cols] = out[cols].clip(lower=lo, upper=hi)

    return (out, params) if return_params else out


def encode_categoricals(
    df: pd.DataFrame,
    cols: Optional[Sequence[str]] = None,
    method: str = "onehot",  # 'onehot' or 'ordinal'
    drop_first: bool = False,
    categories: Optional[Dict[str, Sequence[Any]]] = None,  # for ordinal order
) -> pd.DataFrame:
    """
    Encode categorical columns.
    - onehot: expands columns using pandas.get_dummies
    - ordinal: map categories to integer order (provide 'categories' for custom order)
    """
    out = df.copy()
    cat_cols = out.select_dtypes(include=["object", "category"]).columns.tolist()
    cols = list(cols) if cols is not None else cat_cols

    if method == "onehot":
        return pd.get_dummies(out, columns=cols, drop_first=drop_first)
    elif method == "ordinal":
        for c in cols:
            if categories and c in categories:
                cat_type = pd.api.types.CategoricalDtype(categories=categories[c], ordered=True)
                out[c] = out[c].astype(cat_type).cat.codes.replace({-1: np.nan})
            else:
                # Infer order by frequency
                order = out[c].value_counts(dropna=True).index.tolist()
                cat_type = pd.api.types.CategoricalDtype(categories=order, ordered=True)
                out[c] = out[c].astype(cat_type).cat.codes.replace({-1: np.nan})
        return out
    else:
        raise ValueError("method must be 'onehot' or 'ordinal'")


# ----------------------------- Time series helpers -----------------------------

def resample_time_series(
    df: pd.DataFrame,
    datetime_col: str,
    rule: str,
    agg: Optional[Union[str, Mapping[str, Union[str, List[str]]]]] = None,
    label: str = "left",
    closed: Optional[str] = None,
) -> pd.DataFrame:
    """
    Resample time-indexed data.
    - agg can be a string (e.g., 'mean') or a dict like {'value': 'sum', 'price': 'last'}
    """
    out = df.copy()
    idx = pd.to_datetime(out[datetime_col], errors="coerce")
    res = (
        out.set_index(idx)
        .drop(columns=[datetime_col])
        .resample(rule, label=label, closed=closed)
        .agg(agg if agg is not None else "mean")
    )
    res[datetime_col] = res.index
    return res.reset_index(drop=True)


def resample_ohlcv(
    df: pd.DataFrame,
    datetime_col: str,
    rule: str,
    price_cols: Dict[str, str] = None,
) -> pd.DataFrame:
    """
    Resample OHLCV-like data to a new frequency.
    price_cols maps roles to column names, defaults to common names:
      {'open':'open','high':'high','low':'low','close':'close','volume':'volume'}
    """
    if price_cols is None:
        price_cols = {"open": "open", "high": "high", "low": "low", "close": "close", "volume": "volume"}
    required = {"open", "high", "low", "close"}
    missing = required - set(price_cols)
    if missing:
        raise ValueError(f"Missing price roles: {missing}")

    out = df.copy()
    idx = pd.to_datetime(out[datetime_col], errors="coerce")
    gb = out.set_index(idx)

    agg = {
        price_cols["open"]: "first",
        price_cols["high"]: "max",
        price_cols["low"]: "min",
        price_cols["close"]: "last",
    }
    if "volume" in price_cols and price_cols["volume"] in gb.columns:
        agg[price_cols["volume"]] = "sum"

    res = gb.resample(rule).agg(agg).dropna(how="all")
    res[datetime_col] = res.index
    return res.reset_index(drop=True)


def compute_returns(
    df: pd.DataFrame,
    price_col: str,
    method: str = "log",  # 'log' or 'simple'
    group_by: Optional[Union[str, Sequence[str]]] = None,
    fillna: Optional[float] = None,
) -> pd.Series:
    """
    Compute time-series returns, optionally per group (e.g., per instrument).
    """
    if group_by is None:
        px = df[price_col].astype(float)
        prev = px.shift(1)
    else:
        prev = df.groupby(group_by)[price_col].shift(1).astype(float)
        px = df[price_col].astype(float)

    if method == "log":
        ret = np.log(px / prev)
    elif method == "simple":
        ret = (px / prev) - 1.0
    else:
        raise ValueError("method must be 'log' or 'simple'")

    if fillna is not None:
        ret = ret.fillna(fillna)
    return ret


def rolling_features(
    df: pd.DataFrame,
    cols: Sequence[str],
    windows: Sequence[int],
    funcs: Sequence[str] = ("mean", "std"),
    min_periods: Optional[int] = None,
    group_by: Optional[Union[str, Sequence[str]]] = None,
    suffix_fmt: str = "{col}_r{win}_{func}",
) -> pd.DataFrame:
    """
    Compute rolling window statistics for selected columns.
    funcs can include: 'mean','std','min','max','sum','median','skew','kurt'
    """
    out = df.copy()
    group_obj = out.groupby(group_by) if group_by is not None else [(None, out)]
    for _, g in group_obj:
        idx = g.index
        for c in cols:
            s = g[c].astype(float)
            for w in windows:
                roll = s.rolling(window=w, min_periods=min_periods or w)
                for f in funcs:
                    if not hasattr(roll, f):
                        raise ValueError(f"Unsupported rolling function: {f}")
                    new_col = suffix_fmt.format(col=c, win=w, func=f)
                    out.loc[idx, new_col] = getattr(roll, f)()
    return out


def cumulative_returns(returns: Union[pd.Series, pd.DataFrame]) -> Union[pd.Series, pd.DataFrame]:
    """
    Convert returns to cumulative growth of 1 unit of capital.
    For log returns: cumprod(exp(r)); for simple returns: cumprod(1+r).
    """
    if isinstance(returns, pd.Series):
        if returns.name and "log" in returns.name.lower():
            return np.exp(returns.fillna(0.0)).cumprod()
        # Heuristic: treat as simple
        return (1 + returns.fillna(0.0)).cumprod()
    else:
        # DataFrame
        # Heuristic per column same as above
        out = pd.DataFrame(index=returns.index)
        for c in returns.columns:
            s = returns[c]
            if c and "log" in str(c).lower():
                out[c] = np.exp(s.fillna(0.0)).cumprod()
            else:
                out[c] = (1 + s.fillna(0.0)).cumprod()
        return out


def drawdown(equity_curve: pd.Series) -> pd.DataFrame:
    """
    Compute drawdown statistics for an equity curve series.
    Returns DataFrame with columns: equity, peak, drawdown, drawdown_pct.
    """
    eq = equity_curve.astype(float).fillna(method="ffill")
    peak = eq.cummax()
    dd = eq - peak
    dd_pct = dd / peak.replace(0, np.nan)
    return pd.DataFrame({"equity": eq, "peak": peak, "drawdown": dd, "drawdown_pct": dd_pct})


# ----------------------------- Reshaping helpers -------------------------------

def wide_to_long(df: pd.DataFrame, stubnames: List[str], i: Union[str, List[str]], j: str, sep: str = "_") -> pd.DataFrame:
    """Thin wrapper around pandas.wide_to_long with sensible defaults."""
    return pd.wide_to_long(df, stubnames=stubnames, i=i, j=j, sep=sep, suffix=r"\w+").reset_index()


def long_to_wide(df: pd.DataFrame, index: Union[str, List[str]], columns: str, values: Union[str, List[str]]) -> pd.DataFrame:
    """Pivot a long DataFrame to wide format."""
    return df.pivot_table(index=index, columns=columns, values=values, aggfunc="first").reset_index()


# ----------------------------- Visualization ----------------------------------

def plot_time_series(
    df: pd.DataFrame,
    x: str,
    y: Union[str, Sequence[str]],
    hue: Optional[str] = None,
    style: Optional[str] = None,
    ax: Optional[plt.Axes] = None,
    title: Optional[str] = None,
    show: bool = True,
) -> plt.Axes:
    """Line plot for one or more series over time."""
    ax = ax or plt.gca()
    cols = [y] if isinstance(y, str) else list(y)
    for c in cols:
        if hue is not None:
            sns.lineplot(data=df, x=x, y=c, hue=hue, style=style, ax=ax)
        else:
            sns.lineplot(data=df, x=x, y=c, ax=ax, label=c)
    ax.set_title(title or "Time Series")
    ax.set_xlabel(x)
    ax.set_ylabel(", ".join(cols) if len(cols) == 1 else "value")
    if show:
        plt.show()
    return ax


def plot_distributions(
    df: pd.DataFrame,
    cols: Optional[Sequence[str]] = None,
    bins: int = 50,
    kde: bool = True,
    sharex: bool = False,
    show: bool = True,
) -> plt.Figure:
    """Histograms (optionally with KDE) for selected numeric columns."""
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cols = list(cols) if cols is not None else num_cols
    n = len(cols)
    ncols = min(3, max(1, int(np.ceil(np.sqrt(n)))))
    nrows = int(np.ceil(n / ncols))
    fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 3.5 * nrows), squeeze=False)
    axes = axes.ravel()
    for i, c in enumerate(cols):
        sns.histplot(df[c], bins=bins, kde=kde, ax=axes[i])
        axes[i].set_title(f"Distribution: {c}")
    for j in range(i + 1, len(axes)):
        axes[j].axis("off")
    plt.tight_layout()
    if show:
        plt.show()
    return fig


def plot_corr_heatmap(
    df: pd.DataFrame,
    method: str = "pearson",
    mask_upper: bool = True,
    annot: bool = False,
    cmap: str = "coolwarm",
    show: bool = True,
) -> plt.Axes:
    """Correlation heatmap for numeric columns."""
    corr = df.select_dtypes(include=[np.number]).corr(method=method)
    mask = None
    if mask_upper:
        mask = np.triu(np.ones_like(corr, dtype=bool))
    ax = sns.heatmap(corr, mask=mask, cmap=cmap, annot=annot, fmt=".2f", linewidths=0.5, square=True)
    ax.set_title(f"{method.title()} correlation")
    if show:
        plt.show()
    return ax


def plot_missingness(
    df: pd.DataFrame,
    top_n: Optional[int] = None,
    show: bool = True,
) -> plt.Axes:
    """Bar plot of missingness per column."""
    na_rate = df.isna().mean().sort_values(ascending=False)
    if top_n is not None:
        na_rate = na_rate.head(top_n)
    ax = sns.barplot(x=na_rate.values, y=na_rate.index, orient="h")
    ax.set_xlabel("Missing ratio")
    ax.set_ylabel("Column")
    ax.set_title("Missingness by column")
    if show:
        plt.show()
    return ax


def plot_categorical_counts(
    df: pd.DataFrame,
    col: str,
    top_n: Optional[int] = None,
    normalize: bool = False,
    show: bool = True,
) -> plt.Axes:
    """Count plot for a categorical column."""
    series = df[col].astype("object")
    counts = series.value_counts(normalize=normalize, dropna=False)
    if top_n is not None:
        counts = counts.head(top_n)
    ax = sns.barplot(x=counts.index.astype(str), y=counts.values)
    ax.set_xlabel(col)
    ax.set_ylabel("Proportion" if normalize else "Count")
    ax.set_title(f"Category counts: {col}")
    plt.xticks(rotation=45, ha="right")
    if show:
        plt.show()
    return ax


# ----------------------------- IO convenience ---------------------------------

def load_csv(
    path: str,
    parse_dates: Optional[Union[List[str], bool]] = None,
    dtype: Optional[Dict[str, Any]] = None,
    low_memory: bool = True,
    **read_csv_kwargs: Any,
) -> pd.DataFrame:
    """
    Load a CSV with sensible defaults. parse_dates can be:
      - True to infer, False to skip, or list of column names to parse.
    """
    if parse_dates is True:
        df = pd.read_csv(path, dtype=dtype, low_memory=low_memory, infer_datetime_format=True, **read_csv_kwargs)
        df = infer_datetime_cols(df)
    else:
        df = pd.read_csv(path, dtype=dtype, low_memory=low_memory, parse_dates=parse_dates or False, **read_csv_kwargs)
    return convert_dtypes_safely(df)


# ----------------------------- Quick pipeline ---------------------------------

def quick_clean_transform(
    df: pd.DataFrame,
    *,
    standardize_cols: bool = True,
    drop_all_null_cols: bool = True,
    missing_strategy: str = "impute",  # 'drop' | 'fill' | 'impute'
    outlier_method: Optional[str] = None,  # 'iqr' | 'zscore' | None
    winsor_q: Optional[Tuple[float, float]] = (0.01, 0.99),
    scale_method: Optional[str] = None,  # 'standard' | 'minmax' | 'robust' | None
) -> pd.DataFrame:
    """
    A convenience function to perform a common sequence of cleaning steps.
    """
    out = df.copy()
    if standardize_cols:
        out = standardize_columns(out, case="snake")
    out = convert_dtypes_safely(out)
    if drop_all_null_cols:
        out = drop_empty_cols(out, threshold=1.0)
    out = handle_missing(out, strategy=missing_strategy)
    if outlier_method:
        out = remove_outliers(out, method=outlier_method)
    if winsor_q is not None:
        out = winsorize(out, lower_q=winsor_q[0], upper_q=winsor_q[1])
    if scale_method:
        out = scale_numeric(out, method=scale_method)  # discard params, one-off scaling
    return out


# ----------------------------- Public API -------------------------------------

__all__ = [
    # Setup
    "set_plot_style",
    # Utilities
    "memory_usage_mb",
    "standardize_columns",
    "convert_dtypes_safely",
    "infer_datetime_cols",
    "deduplicate",
    "drop_empty_cols",
    # Missing values
    "handle_missing",
    "impute_by_group",
    # Outliers
    "remove_outliers",
    "winsorize",
    # Scaling/Encoding
    "ScaleParams",
    "scale_numeric",
    "encode_categoricals",
    # Time series
    "resample_time_series",
    "resample_ohlcv",
    "compute_returns",
    "rolling_features",
    "cumulative_returns",
    "drawdown",
    # Reshaping
    "wide_to_long",
    "long_to_wide",
    # Viz
    "plot_time_series",
    "plot_distributions",
    "plot_corr_heatmap",
    "plot_missingness",
    "plot_categorical_counts",
    # IO
    "load_csv",
    # Pipeline
    "quick_clean_transform",
]


# ----------------------------- Example (optional) ------------------------------
if __name__ == "__main__":
    # Minimal smoke test
    set_plot_style()
    data = pd.DataFrame(
        {
            "Date": pd.date_range("2023-01-01", periods=5, freq="D"),
            "Price": [100, 101, 99, np.nan, 103],
            "Volume": [10, 12, 9, 11, 13],
            "Category": ["A", "A", "B", None, "B"],
        }
    )
    dfc = quick_clean_transform(data, scale_method="standard")
    ret = compute_returns(dfc, "price", method="simple")
    dfc["ret_simple"] = ret
    eq = (1 + dfc["ret_simple"].fillna(0)).cumprod()
    stats = drawdown(eq)
    # Quick plots (will display if run as a script)
    plot_time_series(dfc, x="date", y="price", title="Price")
    plot_distributions(dfc)
    plot_corr_heatmap(dfc)
------------------------------------------------------------

PROMPT 284:
"Generate Python code that imports the MJML library and uses it to create email templates in Django, similar to how Django's built-in template engine works, but with the added functionality of MJML's powerful email building features."
RESPONSE:
Below is a minimal, drop-in Django template backend that renders Django templates inside MJML, then compiles the result to responsive HTML using the MJML library. It works like Django’s built-in template engine but adds MJML compilation.

File: myapp/mjml_backend.py
--------------------------------
import shutil
import subprocess

from django.template.backends.base import BaseEngine
from django.template.backends.django import DjangoTemplates
from django.template import TemplateDoesNotExist

# Try the python wrapper around MJML. If it isn't available, fall back to the CLI.
# Different mjml Python wrappers expose slightly different APIs; we try a couple.
mjml_compile = None
try:
    from mjml import mjml2html as mjml_compile  # mjml-python
except Exception:
    try:
        from mjml import to_html as mjml_compile  # alternative API
    except Exception:
        mjml_compile = None


class MjmlTemplate:
    def __init__(self, django_template, compile_func, is_mjml):
        self._template = django_template
        self._compile = compile_func
        self._is_mjml = is_mjml

    def render(self, context=None, request=None):
        rendered = self._template.render(context or {}, request)
        if self._is_mjml:
            return self._compile(rendered)
        return rendered


class MJMLBackend(BaseEngine):
    """
    A template backend that renders Django templates and, if the template
    filename ends with a configured extension (default: .mjml), compiles the
    rendered MJML into HTML.
    """
    app_dirname = "templates"

    def __init__(self, params):
        params = params.copy()
        options = params.setdefault("OPTIONS", {})
        self.file_extensions = tuple(options.pop("FILE_EXTENSIONS", (".mjml",)))
        # Options for the MJML compiler. Supports both python wrapper and CLI fallback.
        self.compile_options = options.pop("MJML_OPTIONS", {})
        super().__init__(params)
        self.django_engine = DjangoTemplates(params)

    def from_string(self, template_code):
        template = self.django_engine.from_string(template_code)
        is_mjml = "<mjml" in template_code.lower()
        return MjmlTemplate(template, self._compile_mjml, is_mjml)

    def get_template(self, template_name):
        try:
            template = self.django_engine.get_template(template_name)
        except TemplateDoesNotExist:
            raise
        is_mjml = template_name.endswith(self.file_extensions)
        return MjmlTemplate(template, self._compile_mjml, is_mjml)

    def _compile_mjml(self, mjml_source: str) -> str:
        # Prefer the Python wrapper if present
        if mjml_compile:
            result = mjml_compile(mjml_source, **self.compile_options)
            # Some wrappers return dicts with an 'html' key; others return a string.
            if isinstance(result, dict) and "html" in result:
                return result["html"]
            return result

        # Fallback: use the mjml CLI
        mjml_bin = self.compile_options.get("MJML_BINARY") or shutil.which("mjml")
        if not mjml_bin:
            raise RuntimeError("MJML binary not found. Install it with: npm i -g mjml "
                               "or set MJML_OPTIONS['MJML_BINARY'] to its path.")
        args = [mjml_bin, "-i"]  # read MJML from stdin, write HTML to stdout
        if self.compile_options.get("minify"):
            args.append("-m")
        proc = subprocess.run(
            args,
            input=mjml_source.encode("utf-8"),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        if proc.returncode != 0:
            raise RuntimeError(f"MJML compilation failed: {proc.stderr.decode('utf-8')}")
        return proc.stdout.decode("utf-8")


Example Django settings
-----------------------
# settings.py
# Add the MJML backend first so .mjml files are handled by it.
TEMPLATES = [
    {
        "BACKEND": "myapp.mjml_backend.MJMLBackend",
        "DIRS": [BASE_DIR / "templates"],
        "APP_DIRS": True,
        "OPTIONS": {
            # Treat files with these extensions as MJML templates
            "FILE_EXTENSIONS": (".mjml",),
            # Options for the wrapper or CLI
            "MJML_OPTIONS": {
                "minify": True,
                # "MJML_BINARY": "/usr/local/bin/mjml",  # if not on PATH
            },
        },
    },
    {
        "BACKEND": "django.template.backends.django.DjangoTemplates",
        "DIRS": [BASE_DIR / "templates"],
        "APP_DIRS": True,
        "OPTIONS": {
            "context_processors": [
                "django.template.context_processors.debug",
                "django.template.context_processors.request",
                "django.contrib.auth.context_processors.auth",
                "django.contrib.messages.context_processors.messages",
            ]
        },
    },
]


Example usage
-------------
# emails.py (or a Django view/task)
from django.template.loader import render_to_string
from django.core.mail import EmailMultiAlternatives

def send_welcome_email(user):
    context = {"user": user}
    # This will render Django template tags inside MJML, then compile to HTML
    html_body = render_to_string("emails/welcome.mjml", context)
    text_body = render_to_string("emails/welcome.txt", context)  # optional plaintext

    msg = EmailMultiAlternatives(
        subject="Welcome!",
        body=text_body or "Welcome!",
        from_email="no-reply@example.com",
        to=[user.email],
    )
    msg.attach_alternative(html_body, "text/html")
    msg.send()


Example MJML template (templates/emails/welcome.mjml)
-----------------------------------------------------
<mjml>
  <mj-body>
    <mj-section>
      <mj-column>
        <mj-text font-size="20px">Hi {{ user.first_name|default:"there" }},</mj-text>
        <mj-text>Welcome to our app. Thanks for joining!</mj-text>
        <mj-button href="https://example.com">Get Started</mj-button>
      </mj-column>
    </mj-section>
  </mj-body>
</mjml>

Notes
- Install the Python wrapper: pip install mjml
- Ensure the MJML CLI is available (used by the wrapper or as a fallback): npm i -g mjml
- With this backend in place, render_to_string('template.mjml', ctx) behaves like Django templates but outputs responsive HTML compiled by MJML.
------------------------------------------------------------

PROMPT 285:
"Generate Python code that implements a DB-API 2.0 interface for SQLite 3.x databases using the `sqlite3` module, allowing for connection to a database, creation of tables, insertion of data, querying of data, and closing of connections."
RESPONSE:
#!/usr/bin/env python3
"""
A minimal, DB-API 2.0-style wrapper around Python's built-in sqlite3 module,
covering connection management, table creation, insertion, querying, and closing.

This code uses parameterized queries (qmark style: ?) to avoid SQL injection
and adheres to DB-API 2.0 patterns via sqlite3.
"""

from __future__ import annotations

import sqlite3
from contextlib import contextmanager
from typing import Any, Iterable, List, Mapping, Optional, Sequence, Tuple


class SQLiteDB:
    """
    Lightweight DB-API 2.0-style interface for SQLite using the sqlite3 module.

    - Connects to an SQLite database file (or :memory:)
    - Creates tables
    - Inserts data with executemany
    - Executes queries and returns rows
    - Manages transactions and connection lifecycle
    """

    def __init__(
        self,
        database: str,
        *,
        timeout: float = 5.0,
        detect_types: int = sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,
        isolation_level: Optional[str] = "",  # empty string -> implicit transaction (autocommit off)
        row_factory: Optional[Any] = sqlite3.Row,
        uri: bool = False,
        foreign_keys: bool = True,
    ) -> None:
        """
        Args:
            database: Path to SQLite file or ':memory:' for in-memory DB. Supports URI if uri=True.
            timeout: Busy timeout in seconds.
            detect_types: Type detection flags for parsing.
            isolation_level:
                - None -> autocommit mode (no implicit transactions)
                - '' (empty string) or a string -> implicit transaction mode (default)
            row_factory: Row factory; sqlite3.Row allows dict-like row access.
            uri: Treat database as URI.
            foreign_keys: Enable PRAGMA foreign_keys = ON.
        """
        self.database = database
        self.timeout = timeout
        self.detect_types = detect_types
        self.isolation_level = isolation_level
        self.row_factory = row_factory
        self.uri = uri
        self.foreign_keys = foreign_keys

        self._conn: Optional[sqlite3.Connection] = None

    @property
    def connection(self) -> sqlite3.Connection:
        """Get or open the underlying sqlite3 connection."""
        if self._conn is None:
            self._conn = sqlite3.connect(
                self.database,
                timeout=self.timeout,
                detect_types=self.detect_types,
                isolation_level=self.isolation_level,
                uri=self.uri,
            )
            if self.row_factory is not None:
                self._conn.row_factory = self.row_factory
            if self.foreign_keys:
                self._conn.execute("PRAGMA foreign_keys = ON")
        return self._conn

    @contextmanager
    def cursor(self) -> Iterable[sqlite3.Cursor]:
        """Context-managed cursor that is closed automatically."""
        cur = self.connection.cursor()
        try:
            yield cur
        finally:
            cur.close()

    def execute(self, sql: str, params: Optional[Sequence[Any]] = None) -> sqlite3.Cursor:
        """
        Execute a single SQL statement and return the cursor.
        For SELECT statements, fetch results from the returned cursor.
        """
        with self.cursor() as cur:
            cur.execute(sql, params or [])
            return cur  # Note: cursor will be closed when leaving context

    def executemany(self, sql: str, seq_of_params: Iterable[Sequence[Any]]) -> None:
        """Execute the same SQL statement for a sequence of parameter sets."""
        with self.cursor() as cur:
            cur.executemany(sql, seq_of_params)

    def create_table(
        self,
        table_name: str,
        columns: Sequence[Tuple[str, str]],
        *,
        if_not_exists: bool = True,
    ) -> None:
        """
        Create a table from a list of (name, type/constraints) column tuples.

        Example:
            create_table('users', [
                ('id', 'INTEGER PRIMARY KEY AUTOINCREMENT'),
                ('name', 'TEXT NOT NULL'),
                ('email', 'TEXT UNIQUE')
            ])
        """
        if not columns:
            raise ValueError("columns must be a non-empty sequence of (name, type) tuples")

        clause = "IF NOT EXISTS " if if_not_exists else ""
        cols_sql = ", ".join(f"{name} {ctype}" for name, ctype in columns)
        sql = f"CREATE TABLE {clause}{table_name} ({cols_sql})"
        self.execute(sql)

    def insert(self, table_name: str, rows: Sequence[Mapping[str, Any]]) -> None:
        """
        Insert multiple rows into a table using a sequence of dict-like mappings.

        Example:
            insert('users', [
                {'name': 'Alice', 'email': 'alice@example.com'},
                {'name': 'Bob', 'email': 'bob@example.com'},
            ])
        """
        if not rows:
            return

        # Ensure consistent column order based on the first row's keys
        columns = list(rows[0].keys())
        placeholders = ", ".join("?" for _ in columns)
        col_list = ", ".join(columns)
        sql = f"INSERT INTO {table_name} ({col_list}) VALUES ({placeholders})"
        values = ([row.get(col) for col in columns] for row in rows)
        self.executemany(sql, values)

    def query(
        self, sql: str, params: Optional[Sequence[Any]] = None
    ) -> List[sqlite3.Row]:
        """
        Execute a SELECT query and return all rows.

        If row_factory is sqlite3.Row (default), rows behave like dicts:
            row['column_name'] or dict(row)
        """
        with self.cursor() as cur:
            cur.execute(sql, params or [])
            return cur.fetchall()

    def commit(self) -> None:
        """Commit the current transaction."""
        self.connection.commit()

    def rollback(self) -> None:
        """Rollback the current transaction."""
        self.connection.rollback()

    def close(self) -> None:
        """Close the database connection."""
        if self._conn is not None:
            self._conn.close()
            self._conn = None

    def __enter__(self) -> "SQLiteDB":
        # Opening connection is deferred until first use
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        # Commit on success; rollback on error
        try:
            if exc_type is None:
                # Commit only if a connection was opened
                if self._conn is not None and self.isolation_level is not None:
                    self._conn.commit()
            else:
                if self._conn is not None and self.isolation_level is not None:
                    self._conn.rollback()
        finally:
            self.close()


def demo() -> None:
    """Demonstrate usage of the SQLiteDB wrapper."""
    # Use ':memory:' for an in-memory database; replace with 'example.db' for a file
    with SQLiteDB(":memory:") as db:
        # 1) Create a table
        db.create_table(
            "users",
            [
                ("id", "INTEGER PRIMARY KEY AUTOINCREMENT"),
                ("name", "TEXT NOT NULL"),
                ("email", "TEXT UNIQUE"),
                ("created_at", "TIMESTAMP DEFAULT CURRENT_TIMESTAMP"),
            ],
        )

        # 2) Insert data
        db.insert(
            "users",
            [
                {"name": "Alice", "email": "alice@example.com"},
                {"name": "Bob", "email": "bob@example.com"},
                {"name": "Charlie", "email": "charlie@example.com"},
            ],
        )

        # 3) Query data with parameters (qmark style)
        users = db.query(
            "SELECT id, name, email, created_at FROM users WHERE id > ? ORDER BY id ASC",
            [0],
        )

        # 4) Iterate results (sqlite3.Row supports dict-like access)
        for row in users:
            print(dict(row))  # or row['name'], row['email'], etc.


if __name__ == "__main__":
    demo()
------------------------------------------------------------

PROMPT 286:
"Generate Python code that uses the `pytest` library to run unit tests for an ETL pipeline, including test cases for data quality checks, data transformation checks, and data loading checks, and provides detailed reports on test results."
RESPONSE:
# file: etl_pipeline.py
import sqlite3
import logging
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Sequence, Tuple, Any

import pandas as pd
import numpy as np

log = logging.getLogger(__name__)


class QualityCheckError(AssertionError):
    pass


@dataclass
class QualityRuleSet:
    required_columns: Sequence[str] = field(default_factory=list)
    non_null: Sequence[str] = field(default_factory=list)
    dtypes: Dict[str, Any] = field(default_factory=dict)  # pandas/numpy dtype or string alias
    ranges: Dict[str, Tuple[Optional[float], Optional[float]]] = field(default_factory=dict)  # inclusive bounds
    unique_keys: Sequence[str] = field(default_factory=list)


def extract_csv(path: str, dtype: Optional[Dict[str, Any]] = None, parse_dates: Optional[List[str]] = None) -> pd.DataFrame:
    df = pd.read_csv(path, dtype=dtype)
    if parse_dates:
        for col in parse_dates:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors="coerce", utc=False)
    return df


def run_quality_checks(df: pd.DataFrame, rules: QualityRuleSet) -> List[str]:
    failures: List[str] = []

    # Required columns present
    missing_cols = [c for c in rules.required_columns if c not in df.columns]
    if missing_cols:
        failures.append(f"Missing required columns: {missing_cols}")

    # Non-null enforcement
    for col in rules.non_null:
        if col in df.columns:
            null_count = int(df[col].isna().sum())
            if null_count > 0:
                failures.append(f"Column '{col}' contains {null_count} nulls")
        else:
            failures.append(f"Non-null required column '{col}' is missing")

    # Dtype checks
    for col, expected_dtype in rules.dtypes.items():
        if col not in df.columns:
            failures.append(f"Dtype check: column '{col}' is missing")
            continue
        try:
            # Try astype conversion to validate compatibility without mutating
            pd.Series(df[col]).astype(expected_dtype)
        except Exception as e:
            failures.append(f"Dtype check failed for '{col}': cannot cast to {expected_dtype} ({e})")

    # Range checks
    for col, (min_val, max_val) in rules.ranges.items():
        if col not in df.columns:
            failures.append(f"Range check: column '{col}' is missing")
            continue
        series = df[col]
        if pd.api.types.is_numeric_dtype(series):
            if min_val is not None:
                below = int((series < min_val).sum())
                if below > 0:
                    failures.append(f"Range check: {below} values in '{col}' are < {min_val}")
            if max_val is not None:
                above = int((series > max_val).sum())
                if above > 0:
                    failures.append(f"Range check: {above} values in '{col}' are > {max_val}")
        else:
            failures.append(f"Range check: column '{col}' is not numeric (dtype={series.dtype})")

    # Uniqueness checks
    if rules.unique_keys:
        if any(k not in df.columns for k in rules.unique_keys):
            missing = [k for k in rules.unique_keys if k not in df.columns]
            failures.append(f"Uniqueness check: key columns missing: {missing}")
        else:
            dupes = df.duplicated(subset=list(rules.unique_keys), keep=False)
            dup_count = int(dupes.sum())
            if dup_count > 0:
                failures.append(
                    f"Uniqueness check: found {dup_count} duplicate rows by keys {list(rules.unique_keys)}"
                )

    return failures


def assert_quality(df: pd.DataFrame, rules: QualityRuleSet) -> None:
    failures = run_quality_checks(df, rules)
    if failures:
        summary = "\n - ".join(failures)
        raise QualityCheckError(f"Data quality checks failed:\n - {summary}")


def transform_orders(df: pd.DataFrame, currency_rates: Optional[Dict[str, float]] = None) -> pd.DataFrame:
    # Avoid mutating input
    out = df.copy(deep=True)

    # Defaults for demo
    rates = currency_rates or {"USD": 1.0, "EUR": 1.1, "GBP": 1.25}

    # Clean strings
    for col in ["customer_name", "country", "currency"]:
        if col in out.columns:
            out[col] = out[col].astype(str).str.strip()
    if "country" in out.columns:
        out["country"] = out["country"].str.upper()

    # Coerce numeric
    for col in ["quantity", "unit_price"]:
        if col in out.columns:
            out[col] = pd.to_numeric(out[col], errors="coerce")

    # Parse dates
    if "order_date" in out.columns:
        out["order_date"] = pd.to_datetime(out["order_date"], errors="coerce")

    # Derived metrics
    if "quantity" in out.columns and "unit_price" in out.columns:
        out["total_amount"] = out["quantity"] * out["unit_price"]

    if "currency" in out.columns and "total_amount" in out.columns:
        out["amount_usd"] = out.apply(
            lambda r: r["total_amount"] * rates.get(str(r["currency"]).upper(), np.nan), axis=1
        )

    # Enforce data types (example expectations)
    dtype_map = {
        "order_id": "int64",
        "quantity": "int64",
        "unit_price": "float64",
        "total_amount": "float64",
        "amount_usd": "float64",
    }
    for col, typ in dtype_map.items():
        if col in out.columns:
            out[col] = out[col].astype(typ, errors="ignore")

    return out


def _sqlite_type_for_series(s: pd.Series) -> str:
    # Map pandas dtype to SQLite types
    if pd.api.types.is_integer_dtype(s):
        return "INTEGER"
    if pd.api.types.is_float_dtype(s):
        return "REAL"
    if pd.api.types.is_bool_dtype(s):
        return "INTEGER"
    if pd.api.types.is_datetime64_any_dtype(s):
        return "TEXT"  # store as ISO string
    return "TEXT"


def _ensure_table(conn: sqlite3.Connection, table: str, df: pd.DataFrame, keys: Optional[Sequence[str]], not_null: Sequence[str]) -> None:
    cur = conn.cursor()
    # Check existence
    cur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?;", (table,))
    exists = cur.fetchone() is not None

    if not exists:
        cols_sql = []
        for col in df.columns:
            col_type = _sqlite_type_for_series(df[col])
            not_null_sql = " NOT NULL" if col in not_null else ""
            cols_sql.append(f'"{col}" {col_type}{not_null_sql}')
        pk_sql = ""
        if keys:
            keys_quoted = ", ".join([f'"{k}"' for k in keys])
            pk_sql = f", UNIQUE ({keys_quoted})"
        create_sql = f'CREATE TABLE "{table}" ({", ".join(cols_sql)}{pk_sql});'
        cur.execute(create_sql)
        conn.commit()
    else:
        # Optionally we could validate schema; for demo we skip
        pass

    # Ensure unique index for upsert keys if provided
    if keys:
        idx_name = f"ux_{table}_" + "_".join(keys)
        keys_quoted = ", ".join([f'"{k}"' for k in keys])
        cur.execute(
            f'CREATE UNIQUE INDEX IF NOT EXISTS "{idx_name}" ON "{table}" ({keys_quoted});'
        )
        conn.commit()


def load_to_sqlite(
    conn: sqlite3.Connection,
    df: pd.DataFrame,
    table: str,
    keys: Optional[Sequence[str]] = None,
    mode: str = "append",  # append | upsert | replace
    not_null: Optional[Sequence[str]] = None,
) -> int:
    if mode not in {"append", "upsert", "replace"}:
        raise ValueError("mode must be one of: append | upsert | replace")
    not_null = not_null or []

    # Prepare data: convert datetimes to ISO
    df_to_load = df.copy()
    for col in df_to_load.columns:
        if pd.api.types.is_datetime64_any_dtype(df_to_load[col]):
            df_to_load[col] = df_to_load[col].dt.strftime("%Y-%m-%dT%H:%M:%S")

    _ensure_table(conn, table, df_to_load, keys=keys, not_null=not_null)

    cur = conn.cursor()
    try:
        # Start transaction
        cur.execute("BEGIN;")

        if mode == "replace":
            cur.execute(f'DELETE FROM "{table}";')

        cols = list(df_to_load.columns)
        placeholders = ", ".join(["?"] * len(cols))
        col_list = ", ".join([f'"{c}"' for c in cols])

        if mode == "append" or not keys:
            sql = f'INSERT INTO "{table}" ({col_list}) VALUES ({placeholders});'
            cur.executemany(sql, [tuple(row[c] for c in cols) for _, row in df_to_load.iterrows()])
        else:
            # Upsert using ON CONFLICT
            update_cols = [c for c in cols if c not in keys]
            set_clause = ", ".join([f'"{c}"=excluded."{c}"' for c in update_cols])
            conflict_cols = ", ".join([f'"{k}"' for k in keys])
            sql = (
                f'INSERT INTO "{table}" ({col_list}) VALUES ({placeholders}) '
                f'ON CONFLICT ({conflict_cols}) DO UPDATE SET {set_clause};'
            )
            cur.executemany(sql, [tuple(row[c] for c in cols) for _, row in df_to_load.iterrows()])

        rowcount = cur.rowcount  # number of last executemany; not always accurate in sqlite
        conn.commit()
        # Fallback: count rows inserted now - previous count isn't tracked here; return input size instead
        return len(df_to_load)
    except Exception:
        conn.rollback()
        raise


# file: tests/conftest.py
import os
import logging
import sqlite3
from typing import Dict, List

import pandas as pd
import pytest

from etl_pipeline import QualityRuleSet

# Configure root logging for detailed test logs
def pytest_configure(config):
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
    )

def pytest_addoption(parser):
    parser.addoption(
        "--report-dir",
        action="store",
        default=os.environ.get("REPORT_DIR", "reports"),
        help="Directory to store test reports and artifacts.",
    )

@pytest.fixture(scope="session")
def report_dir(pytestconfig) -> str:
    d = pytestconfig.getoption("--report-dir")
    os.makedirs(d, exist_ok=True)
    return d

@pytest.fixture
def sqlite_conn():
    conn = sqlite3.connect(":memory:")
    yield conn
    conn.close()

@pytest.fixture
def quality_rules() -> QualityRuleSet:
    return QualityRuleSet(
        required_columns=["order_id", "customer_name", "quantity", "unit_price", "currency", "order_date", "country"],
        non_null=["order_id", "customer_name", "quantity", "unit_price", "currency", "order_date", "country"],
        dtypes={"order_id": "int64", "quantity": "int64", "unit_price": "float64"},
        ranges={"quantity": (1, 100000), "unit_price": (0.0, None)},
        unique_keys=["order_id"],
    )

@pytest.fixture
def sample_good_df() -> pd.DataFrame:
    return pd.DataFrame(
        [
            {
                "order_id": 1,
                "customer_name": " Alice ",
                "quantity": 2,
                "unit_price": 10.0,
                "currency": "USD",
                "order_date": "2024-01-01",
                "country": "us",
            },
            {
                "order_id": 2,
                "customer_name": "Bob",
                "quantity": 5,
                "unit_price": 3.5,
                "currency": "EUR",
                "order_date": "2024-01-03",
                "country": "de",
            },
        ]
    )

@pytest.fixture
def sample_bad_df_nulls(sample_good_df) -> pd.DataFrame:
    bad = sample_good_df.copy()
    bad.loc[0, "customer_name"] = None
    bad.loc[1, "quantity"] = None
    return bad

@pytest.fixture
def sample_bad_df_duplicates(sample_good_df) -> pd.DataFrame:
    dup = pd.concat([sample_good_df, sample_good_df.iloc[[0]]], ignore_index=True)
    return dup

@pytest.fixture
def sample_bad_df_ranges(sample_good_df) -> pd.DataFrame:
    bad = sample_good_df.copy()
    bad.loc[0, "quantity"] = 0
    bad.loc[1, "unit_price"] = -1.0
    return bad

@pytest.fixture
def currency_rates() -> Dict[str, float]:
    return {"USD": 1.0, "EUR": 1.2, "GBP": 1.35}


# file: tests/test_data_quality.py
import pytest
from etl_pipeline import assert_quality, run_quality_checks, QualityCheckError

def test_quality_passes(sample_good_df, quality_rules):
    failures = run_quality_checks(sample_good_df, quality_rules)
    assert failures == [], f"Expected no failures but got {failures}"
    assert_quality(sample_good_df, quality_rules)  # should not raise

def test_quality_missing_columns(sample_good_df, quality_rules):
    df = sample_good_df.drop(columns=["currency"])
    with pytest.raises(QualityCheckError) as ei:
        assert_quality(df, quality_rules)
    msg = str(ei.value)
    assert "Missing required columns" in msg or "Non-null required column 'currency' is missing" in msg

def test_quality_nulls(sample_bad_df_nulls, quality_rules):
    with pytest.raises(QualityCheckError) as ei:
        assert_quality(sample_bad_df_nulls, quality_rules)
    assert "contains" in str(ei.value)

def test_quality_ranges(sample_bad_df_ranges, quality_rules):
    with pytest.raises(QualityCheckError) as ei:
        assert_quality(sample_bad_df_ranges, quality_rules)
    msg = str(ei.value)
    assert "Range check" in msg

def test_quality_duplicates(sample_bad_df_duplicates, quality_rules):
    with pytest.raises(QualityCheckError) as ei:
        assert_quality(sample_bad_df_duplicates, quality_rules)
    assert "duplicate" in str(ei.value).lower()


# file: tests/test_transform.py
import pandas as pd
from etl_pipeline import transform_orders

def test_transform_computes_amounts(sample_good_df, currency_rates):
    out = transform_orders(sample_good_df, currency_rates=currency_rates)
    assert "total_amount" in out.columns
    assert "amount_usd" in out.columns

    # Row 1: 2 * 10.0 = 20.0, USD rate 1.0
    r0 = out[out["order_id"] == 1].iloc[0]
    assert r0["total_amount"] == 20.0
    assert r0["amount_usd"] == 20.0

    # Row 2: 5 * 3.5 = 17.5, EUR rate 1.2
    r1 = out[out["order_id"] == 2].iloc[0]
    assert r1["total_amount"] == 17.5
    assert r1["amount_usd"] == 21.0  # 17.5 * 1.2

def test_transform_normalizes_strings(sample_good_df):
    out = transform_orders(sample_good_df)
    # Trim customer_name and uppercase country
    r0 = out[out["order_id"] == 1].iloc[0]
    assert r0["customer_name"] == "Alice"
    assert r0["country"] == "US"

def test_transform_does_not_mutate_input(sample_good_df):
    before = sample_good_df.copy(deep=True)
    _ = transform_orders(sample_good_df)
    pd.testing.assert_frame_equal(sample_good_df, before)

def test_transform_types(sample_good_df):
    out = transform_orders(sample_good_df)
    assert pd.api.types.is_integer_dtype(out["quantity"])
    assert pd.api.types.is_float_dtype(out["unit_price"])
    assert pd.api.types.is_float_dtype(out["total_amount"])
    assert pd.api.types.is_float_dtype(out["amount_usd"])
    assert pd.api.types.is_datetime64_any_dtype(out["order_date"])


# file: tests/test_load.py
import sqlite3
import pandas as pd
import pytest

from etl_pipeline import load_to_sqlite, transform_orders

TABLE = "fact_orders"

def _count_rows(conn: sqlite3.Connection, table: str) -> int:
    cur = conn.cursor()
    cur.execute(f'SELECT COUNT(*) FROM "{table}";')
    return int(cur.fetchone()[0])

def test_append_load_inserts_rows(sqlite_conn, sample_good_df):
    df = transform_orders(sample_good_df)
    inserted = load_to_sqlite(
        sqlite_conn, df, TABLE, keys=["order_id"], mode="append", not_null=["order_id", "customer_name"]
    )
    assert inserted == len(df)
    assert _count_rows(sqlite_conn, TABLE) == len(df)

def test_upsert_load_updates_existing(sqlite_conn, sample_good_df):
    df = transform_orders(sample_good_df)
    load_to_sqlite(sqlite_conn, df, TABLE, keys=["order_id"], mode="append", not_null=["order_id"])

    # Duplicate key row with updated unit_price -> total_amount/amount_usd will update if passed through transform
    updated = sample_good_df.copy()
    updated.loc[updated["order_id"] == 2, "unit_price"] = 10.0
    updated = transform_orders(updated)
    # Just upsert the changed row
    updated_row = updated[updated["order_id"] == 2]
    upserted = load_to_sqlite(sqlite_conn, updated_row, TABLE, keys=["order_id"], mode="upsert")
    assert upserted == len(updated_row)

    # Verify updated
    cur = sqlite_conn.cursor()
    cur.execute(f'SELECT unit_price, total_amount FROM "{TABLE}" WHERE order_id=2;')
    unit_price, total_amount = cur.fetchone()
    assert float(unit_price) == 10.0
    assert float(total_amount) == 5 * 10.0  # quantity 5 from source

def test_replace_load_truncates_then_inserts(sqlite_conn, sample_good_df):
    df1 = transform_orders(sample_good_df)
    load_to_sqlite(sqlite_conn, df1, TABLE, keys=["order_id"], mode="append")

    # New batch with a single row
    new_df = pd.DataFrame(
        [
            {
                "order_id": 999,
                "customer_name": "Zed",
                "quantity": 1,
                "unit_price": 1.0,
                "currency": "USD",
                "order_date": "2024-02-02",
                "country": "us",
            }
        ]
    )
    new_df = transform_orders(new_df)
    replaced = load_to_sqlite(sqlite_conn, new_df, TABLE, keys=["order_id"], mode="replace")
    assert replaced == 1
    assert _count_rows(sqlite_conn, TABLE) == 1

def test_load_transaction_rolls_back_on_error(sqlite_conn, sample_good_df):
    df = transform_orders(sample_good_df)
    load_to_sqlite(sqlite_conn, df, TABLE, keys=["order_id"], mode="append", not_null=["order_id"])

    # Create a bad frame that violates NOT NULL constraint
    bad = transform_orders(pd.DataFrame(
        [{"order_id": 3, "customer_name": None, "quantity": 1, "unit_price": 1.0, "currency": "USD", "order_date": "2024-03-01", "country": "us"}]
    ))

    with pytest.raises(Exception):
        load_to_sqlite(sqlite_conn, bad, TABLE, keys=["order_id"], mode="append", not_null=["order_id", "customer_name"])

    # Ensure row count unchanged
    assert _count_rows(sqlite_conn, TABLE) == len(df)

def test_load_respects_primary_key_unique(sqlite_conn, sample_good_df):
    df = transform_orders(sample_good_df)
    load_to_sqlite(sqlite_conn, df, TABLE, keys=["order_id"], mode="append")
    with pytest.raises(Exception):
        # Duplicate order_id without upsert should error if unique constraint triggers (depends on data shape)
        load_to_sqlite(sqlite_conn, df.iloc[[0]], TABLE, keys=["order_id"], mode="append")


# file: run_tests.py
import os
import sys
import pytest
import importlib.util

def main():
    reports_dir = os.environ.get("REPORT_DIR", "reports")
    os.makedirs(reports_dir, exist_ok=True)

    args = [
        "-vv",
        "--maxfail=1",
        f"--junitxml={os.path.join(reports_dir, 'junit.xml')}",
    ]

    # Add HTML report if pytest-html is available
    if importlib.util.find_spec("pytest_html"):
        html_path = os.path.join(reports_dir, "report.html")
        args += ["--html", html_path, "--self-contained-html"]

    # Allow passing extra args from CLI
    extra = sys.argv[1:]
    code = pytest.main(args + extra)
    sys.exit(code)

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 287:
"Generate Python code that imports the requests library and uses it to send a GET request to the Nord Pool API to retrieve the current spot prices, then parses the response data into a pandas DataFrame for easy analysis and manipulation."
RESPONSE:
import re
import requests
import pandas as pd


def _parse_price(value: str):
    if value is None or value == "-" or value == "":
        return None
    s = str(value)
    # Remove non-breaking spaces and regular spaces, normalize decimal comma to dot
    s = s.replace("\xa0", "").replace(" ", "").replace(",", ".")
    # Strip any non-numeric symbols (e.g., EUR, €/MWh)
    s = re.sub(r"[^0-9.\-]", "", s)
    return float(s) if s not in ("", "-", None) else None


def fetch_nordpool_spot_prices(currency="EUR", timeout=20) -> pd.DataFrame:
    """
    Fetch current day-ahead spot prices from Nord Pool and return as a tidy DataFrame.

    Returns columns:
      - area: bidding zone or system name (e.g., NO1, SE3, DK1, SYS)
      - price: numeric price
      - start_time: interval start (string or ISO timestamp)
      - end_time: interval end
      - unit: price unit (e.g., EUR/MWh)
      - currency: selected currency
    """
    url = "https://www.nordpoolgroup.com/api/marketdata/page/10"
    # The website typically uses ",,EUR" to force EUR as display currency
    params = {"currency": f",,{currency}"}

    headers = {
        # A browser-like UA helps avoid occasional 403s
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "application/json, text/plain, */*",
        "Referer": "https://www.nordpoolgroup.com/en/market-data/",
    }

    resp = requests.get(url, params=params, headers=headers, timeout=timeout)
    resp.raise_for_status()
    payload = resp.json()

    data = payload.get("data", {})
    rows = data.get("Rows", []) or data.get("rows", [])
    unit = data.get("Units") or data.get("Unit")
    sel_currency = payload.get("currency") or currency

    records = []
    for row in rows:
        # Skip summary rows like Min/Max/Average
        if row.get("IsExtraRow"):
            continue

        start = row.get("StartTime") or row.get("Start")
        end = row.get("EndTime") or row.get("End")
        for col in row.get("Columns", []):
            name = col.get("Name")
            value = col.get("Value")
            if not name or value in (None, "", "-"):
                continue

            price = _parse_price(value)
            if price is None:
                continue

            records.append(
                {
                    "area": name,
                    "price": price,
                    "start_time": start,
                    "end_time": end,
                    "unit": unit,
                    "currency": sel_currency,
                }
            )

    df = pd.DataFrame.from_records(records)
    # Parse timestamps if present
    if not df.empty:
        for col in ("start_time", "end_time"):
            df[col] = pd.to_datetime(df[col], errors="coerce")

    return df


if __name__ == "__main__":
    df = fetch_nordpool_spot_prices(currency="EUR")
    print(df.head())

    # Example: pivot to one column per area for analysis
    if not df.empty:
        wide = df.pivot_table(index="start_time", columns="area", values="price")
        print(wide.head())
------------------------------------------------------------

PROMPT 288:
"Generate Python code that acquires a mutex via the database in Django by using the `django.db.transaction` module to create a transaction, and then using the `threading.Lock` class to create a lock object that is stored in the database."
RESPONSE:
Short answer: you can’t store a threading.Lock in the database, and it wouldn’t work across processes even if you could. threading.Lock is a process-local synchronization primitive. To implement a database-backed mutex in Django, use a database row lock (portable) or a database advisory lock (DB-specific). Below are two correct patterns that use django.db.transaction.

Option A (portable): row-level locking with SELECT … FOR UPDATE
models.py
from django.db import models

class DbLock(models.Model):
    name = models.CharField(max_length=255, unique=True)

utils/db_mutex.py
from contextlib import contextmanager
from django.db import transaction, DatabaseError, router
from .models import DbLock

class LockNotAcquired(Exception):
    pass

@contextmanager
def db_mutex(name: str, nowait: bool = False, using: str | None = None):
    """
    Acquire a DB-backed mutex by locking a dedicated row via SELECT ... FOR UPDATE.
    - Blocks until the lock can be taken unless nowait=True.
    - The lock is released when the transaction (atomic block) exits.
    """
    db = using or router.db_for_write(DbLock)

    # Ensure the row exists to be locked (outside the atomic block is OK).
    DbLock.objects.using(db).get_or_create(name=name)

    try:
        with transaction.atomic(using=db):
            qs = DbLock.objects.using(db).select_for_update(nowait=nowait)
            try:
                qs.get(name=name)  # acquiring the row lock
            except DatabaseError as e:
                # Raised if nowait=True and another transaction holds the lock
                raise LockNotAcquired(f"Could not acquire lock '{name}': {e}") from e

            # Critical section runs while the row is locked
            yield
        # Exiting the atomic block releases the lock
    finally:
        pass

Usage
from .utils.db_mutex import db_mutex, LockNotAcquired

def do_critical_work():
    with db_mutex("my-critical-section"):
        # Your critical code here
        pass

def try_critical_work_nonblocking():
    try:
        with db_mutex("my-critical-section", nowait=True):
            # Do quick work if lock is free
            return True
    except LockNotAcquired:
        return False

Option B (PostgreSQL): transaction-scoped advisory lock
This avoids creating a table and is very efficient, but is Postgres-specific.

utils/pg_mutex.py
from contextlib import contextmanager
from django.db import transaction, connection
import zlib

@contextmanager
def pg_advisory_mutex(name: str):
    """
    Acquire a transaction-scoped advisory lock using pg_advisory_xact_lock.
    The lock is released automatically at transaction end.
    """
    # Map name to a 32-bit key; you can also use a stable bigint hash if desired.
    key = zlib.crc32(name.encode("utf-8")) & 0xFFFFFFFF

    with transaction.atomic():
        with connection.cursor() as cur:
            cur.execute("SELECT pg_advisory_xact_lock(%s)", [key])
        yield
        # No explicit unlock needed; it's released when the transaction ends

Usage
from .utils.pg_mutex import pg_advisory_mutex

def do_critical_work_pg():
    with pg_advisory_mutex("my-critical-section"):
        # Critical section
        pass

Summary
- Do not attempt to store threading.Lock in the database; it won’t work across processes.
- Use a transaction and either:
  - lock a dedicated row with select_for_update (portable), or
  - use Postgres advisory locks (efficient, Postgres-only).
------------------------------------------------------------

PROMPT 289:
"Generate Python code that creates a flexible, non-blocking web application framework with built-in support for handling HTTP requests and responses, using Twisted for asynchronous I/O and Werkzeug for parsing and processing request data."
RESPONSE:
# A tiny non-blocking web framework built on Twisted + Werkzeug
# - Twisted handles asynchronous I/O and connection lifecycles.
# - Werkzeug handles request parsing, routing, and response shaping.
#
# Features:
# - Async and sync handlers supported
# - Middleware support (async or sync)
# - Werkzeug Request/Response objects
# - Werkzeug routing (converters, methods)
# - JSON helpers and friendly return types
#
# Requirements:
#   pip install twisted werkzeug
#
# Run:
#   python this_file.py
#
# Example routes at bottom.

import sys
import json
import traceback
from io import BytesIO
from urllib.parse import urlsplit

from twisted.web.server import Site, NOT_DONE_YET
from twisted.web.resource import Resource
from twisted.internet import reactor, defer

from werkzeug.wrappers import Request as WZRequest, Response as WZResponse
from werkzeug.routing import Map, Rule
from werkzeug.exceptions import HTTPException, NotFound, MethodNotAllowed, InternalServerError


def _to_str(value):
    if value is None:
        return ""
    if isinstance(value, bytes):
        try:
            return value.decode("latin-1", "ignore")
        except Exception:
            return value.decode("utf-8", "ignore")
    return str(value)


def _maybe_bytes(x):
    if isinstance(x, bytes):
        return x
    if isinstance(x, str):
        return x.encode("utf-8")
    return bytes(x)


def _is_iterable_but_not_bytes(x):
    if isinstance(x, (bytes, bytearray)):
        return False
    try:
        iter(x)
        return True
    except TypeError:
        return False


def _deferred_from(result):
    # Normalize sync results, coroutine objects, and Twisted Deferreds into a Deferred.
    if isinstance(result, defer.Deferred):
        return result
    # Awaitable (coroutine/async gen)
    try:
        import inspect
        if inspect.isawaitable(result):
            return defer.ensureDeferred(result)
    except Exception:
        pass
    return defer.succeed(result)


class TwistedWerkApp:
    def __init__(self):
        self._rules = []
        self._map_dirty = True
        self.url_map = Map()
        self._middlewares = []

    # Public API

    def route(self, rule, methods=None, endpoint=None):
        def decorator(func):
            name = endpoint or func.__name__
            self._rules.append(Rule(rule, endpoint=name, methods=methods))
            setattr(self, name, func)
            self._map_dirty = True
            return func
        return decorator

    def add_middleware(self, middleware):
        # Middleware signature: async def middleware(request, next_handler, **values) -> ResponseLike
        # or sync version. next_handler is an async callable: await next_handler(**overrides)
        self._middlewares.append(middleware)

    def run(self, host="127.0.0.1", port=8080):
        site = Site(_RootResource(self))
        reactor.listenTCP(int(port), site, interface=host)
        print(f"Serving on http://{host}:{port}")
        reactor.run()

    # Core plumbing

    def _ensure_url_map(self):
        if self._map_dirty:
            self.url_map = Map(self._rules)
            self._map_dirty = False

    def _build_environ(self, treq, body):
        # Build a minimal but solid WSGI environ from a Twisted request.
        # This is sufficient for Werkzeug's Request/Response parsing and routing.
        uri = _to_str(treq.uri)
        parts = urlsplit(uri)

        # Detect scheme and host
        scheme = "https" if treq.isSecure() else "http"
        host_header = treq.getHeader(b"host") or treq.getHeader("host")
        if host_header:
            host_str = _to_str(host_header)
        else:
            # Fallback to server address if no Host header
            host_obj = treq.getHost()
            host_str = f"{getattr(host_obj, 'host', 'localhost')}:{getattr(host_obj, 'port', 0)}"

        # Path and query
        path_info = parts.path or "/"
        query_string = parts.query or ""

        # Remote/Server info
        remote = treq.getClientAddress()
        remote_addr = getattr(remote, "host", None) if remote else None
        server = treq.getHost()
        server_name = getattr(server, "host", "localhost")
        server_port = str(getattr(server, "port", "0"))

        # Headers to environ
        env = {
            "REQUEST_METHOD": _to_str(treq.method),
            "SCRIPT_NAME": "",
            "PATH_INFO": path_info,
            "QUERY_STRING": query_string,
            "SERVER_NAME": _to_str(server_name),
            "SERVER_PORT": _to_str(server_port),
            "SERVER_PROTOCOL": f"HTTP/{_to_str(treq.clientproto or b'1.1')}",
            "wsgi.version": (1, 0),
            "wsgi.url_scheme": scheme,
            "wsgi.input": BytesIO(body),
            "wsgi.errors": sys.stderr,
            "wsgi.multithread": False,
            "wsgi.multiprocess": False,
            "wsgi.run_once": False,
            "REMOTE_ADDR": _to_str(remote_addr) if remote_addr else "",
            "HTTP_HOST": host_str,
        }

        # Populate headers
        for name, values in treq.requestHeaders.getAllRawHeaders():
            # Twisted may provide bytes or str
            n = _to_str(name).upper()
            for v in values:
                val = _to_str(v)
                if n == "CONTENT-TYPE":
                    env["CONTENT_TYPE"] = val
                elif n == "CONTENT-LENGTH":
                    env["CONTENT_LENGTH"] = val
                else:
                    key = f"HTTP_{n.replace('-', '_')}"
                    # WSGI environ supports a single value; join multiple with commas
                    if key in env and env[key]:
                        env[key] = f"{env[key]},{val}"
                    else:
                        env[key] = val

        return env

    def _make_response(self, result, default_mimetype="text/plain; charset=utf-8"):
        # Normalize handler return values into a Werkzeug Response
        if isinstance(result, WZResponse):
            return result

        status = 200
        headers = None
        body = result

        # Flask-like tuple patterns
        if isinstance(result, tuple):
            if len(result) == 2:
                body, status = result
            elif len(result) == 3:
                body, status, headers = result
            else:
                raise TypeError("Invalid response tuple length. Use (body, status[, headers]).")

        # JSON for dict/list
        if isinstance(body, (dict, list)):
            data = json.dumps(body).encode("utf-8")
            resp = WZResponse(
                data,
                status=status,
                headers=headers,
                mimetype="application/json",
            )
            return resp

        # Iterable body (e.g. generator of bytes/str)
        if _is_iterable_but_not_bytes(body):
            # Coalesce into bytes for simplicity. Could stream-chunk in advanced version.
            chunks = []
            for chunk in body:
                chunks.append(_maybe_bytes(chunk))
            body = b"".join(chunks)

        # Now body is bytes or str or None
        if body is None:
            body = b""
        data = _maybe_bytes(body)
        resp = WZResponse(
            data,
            status=status,
            headers=headers,
            mimetype=default_mimetype if not headers else None,
        )
        return resp

    async def _call_with_middleware(self, handler, request, values):
        # Chain middlewares around the handler, returning a raw result (to be normalized later).
        async def terminal(req, **vals):
            res = handler(req, **vals)
            res = await _deferred_from(res)
            return res

        current = terminal
        for mw in reversed(self._middlewares):
            def _wrap(middle, nxt):
                async def wrapped(req, **vals):
                    async def next_handler(**override_vals):
                        merged = dict(vals)
                        merged.update(override_vals)
                        res = nxt(req, **merged)
                        return await _deferred_from(res)
                    res = middle(req, next_handler, **vals)
                    return await _deferred_from(res)
                return wrapped
            current = _wrap(mw, current)

        return await current(request, **values)

    async def _handle_request_async(self, treq):
        # Read body (Twisted buffers request body to request.content)
        body = treq.content.read() if hasattr(treq, "content") else b""
        environ = self._build_environ(treq, body)
        self._ensure_url_map()

        # Werkzeug Request from WSGI environ
        wz_req = WZRequest(environ)

        # Route and run
        try:
            adapter = self.url_map.bind_to_environ(environ)
            endpoint, values = adapter.match(method=wz_req.method)

            handler = getattr(self, endpoint, None)
            if handler is None:
                raise NotFound()

            raw_result = await self._call_with_middleware(handler, wz_req, values)
            response = self._make_response(raw_result)

        except HTTPException as he:
            # Let Werkzeug produce a default response
            response = he.get_response(environ)
        except Exception as e:
            # Fallback 500 with traceback in server logs
            traceback.print_exc()
            err = InternalServerError()
            response = err.get_response(environ)

        # Ship response to Twisted
        treq.setResponseCode(response.status_code)

        # Set headers
        # If handler returned a body already, Werkzeug may set Content-Length automatically.
        for name, value in response.headers.items():
            treq.responseHeaders.addRawHeader(_to_str(name), _to_str(value))

        # HEAD requests: no body
        if wz_req.method.upper() == "HEAD":
            treq.finish()
            return

        # Body
        # If response is a Werkzeug Response, get_data() will coalesce any iterable body
        data = response.get_data(as_text=False)
        # Ensure Content-Length is present (Twisted will chunk otherwise)
        treq.responseHeaders.setRawHeaders("Content-Length", [str(len(data))])
        treq.write(data)
        treq.finish()

    def _handle_request(self, treq):
        d = defer.ensureDeferred(self._handle_request_async(treq))
        d.addErrback(self._on_unexpected_failure, treq)
        return d

    def _on_unexpected_failure(self, failure, treq):
        try:
            traceback.print_exc()
            treq.setResponseCode(500)
            body = b"Internal Server Error"
            treq.responseHeaders.setRawHeaders("Content-Type", ["text/plain; charset=utf-8"])
            treq.responseHeaders.setRawHeaders("Content-Length", [str(len(body))])
            treq.write(body)
            treq.finish()
        except Exception:
            pass


class _RootResource(Resource):
    isLeaf = True

    def __init__(self, app: TwistedWerkApp):
        super().__init__()
        self.app = app

    def render(self, request):
        self.app._handle_request(request)
        return NOT_DONE_YET


# -----------------------------
# Example usage
# -----------------------------
app = TwistedWerkApp()


# Simple middleware example: logging + CORS
@app.add_middleware
def cors_and_logging(request, next_handler, **values):
    # Log
    print(f"{request.method} {request.path} args={dict(request.args)}")
    # Proceed
    resp = next_handler()
    # Inject CORS headers afterward if possible
    async def add_headers():
        r = await _deferred_from(resp)
        wr = app._make_response(r)
        wr.headers.setdefault("Access-Control-Allow-Origin", "*")
        wr.headers.setdefault("Access-Control-Allow-Headers", "Content-Type, Authorization")
        wr.headers.setdefault("Access-Control-Allow-Methods", "GET, POST, PUT, PATCH, DELETE, OPTIONS")
        return wr
    return add_headers()


@app.route("/", methods=["GET"])
def index(req):
    return {"message": "Hello from Twisted+Werkzeug", "query": req.args}


@app.route("/echo", methods=["POST"])
async def echo(req):
    # Werkzeug Request gives you .json, .form, .files, .data, etc.
    data = None
    if req.is_json:
        data = req.get_json(silent=True)
    if data is None:
        # fallback to raw bytes
        data = req.get_data(as_text=False).decode("utf-8", "ignore")
    return {"you_sent": data}


@app.route("/add/<int:a>/<int:b>", methods=["GET"])
def add(req, a, b):
    return {"a": a, "b": b, "sum": a + b}


@app.route("/status/<int:code>", methods=["GET"])
def with_status(req, code):
    return ("custom status body", code, [("X-Custom", "yes")])


if __name__ == "__main__":
    app.run("127.0.0.1", 8080)
------------------------------------------------------------

PROMPT 290:
"Generate Python code that imports data from various sources, such as Excel files, SQL databases, and APIs, and exports it to other formats, such as CSV files, JSON files, and SQL databases, using libraries like pandas, numpy, and requests."
RESPONSE:
#!/usr/bin/env python3
"""
Data ingestion and export utility.

- Imports data from:
  - Excel files (pandas)
  - SQL databases (pandas + SQLAlchemy)
  - HTTP APIs (requests + pandas.json_normalize)

- Exports data to:
  - CSV files (pandas)
  - JSON files (pandas)
  - SQL databases (pandas + SQLAlchemy)

Libraries used: pandas, numpy, requests, SQLAlchemy
"""

import os
import json
import logging
from typing import Any, Dict, List, Optional, Union

import numpy as np
import pandas as pd
import requests
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine


# ---------------------------
# Logging
# ---------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s"
)


# ---------------------------
# Loaders
# ---------------------------
def load_from_excel(
    path: str,
    sheet_name: Union[str, int] = 0,
    dtype: Optional[Dict[str, Any]] = None,
    na_values: Optional[List[Any]] = None
) -> pd.DataFrame:
    """
    Load data from an Excel file.

    Args:
        path: Path to the Excel file.
        sheet_name: Sheet name or index.
        dtype: Optional column dtype mapping.
        na_values: Additional strings to recognize as NA/NaN.

    Returns:
        DataFrame with the loaded data.
    """
    logging.info(f"Loading Excel: {path} (sheet={sheet_name})")
    df = pd.read_excel(path, sheet_name=sheet_name, dtype=dtype, na_values=na_values)
    logging.info(f"Loaded Excel rows={len(df)}, cols={len(df.columns)}")
    return df


def _get_engine(connection_string: str) -> Engine:
    return create_engine(connection_string, future=True)


def load_from_sql(
    connection_string: str,
    query: str,
    params: Optional[Dict[str, Any]] = None
) -> pd.DataFrame:
    """
    Load data from a SQL database using SQLAlchemy.

    Args:
        connection_string: SQLAlchemy connection string (e.g., sqlite:///example.db).
        query: SQL query (use named parameters like :param).
        params: Parameter dict to bind to the query.

    Returns:
        DataFrame with the loaded data.
    """
    logging.info("Loading from SQL database")
    engine = _get_engine(connection_string)
    with engine.connect() as conn:
        df = pd.read_sql(text(query), conn, params=params)
    logging.info(f"Loaded SQL rows={len(df)}, cols={len(df.columns)}")
    return df


def load_from_api(
    url: str,
    params: Optional[Dict[str, Any]] = None,
    headers: Optional[Dict[str, str]] = None,
    record_path: Optional[Union[str, List[Union[str, int]]]] = None,
    meta: Optional[List[str]] = None,
    timeout: int = 30,
) -> pd.DataFrame:
    """
    Load data from a REST API endpoint and normalize JSON to a flat DataFrame.

    Args:
        url: API endpoint URL.
        params: Querystring parameters.
        headers: HTTP headers (e.g., Authorization).
        record_path: Path to nested list to normalize (if needed).
        meta: Fields to keep from the parent levels when using record_path.
        timeout: Request timeout in seconds.

    Returns:
        DataFrame with the API data.
    """
    logging.info(f"Fetching API: {url}")
    resp = requests.get(url, params=params, headers=headers, timeout=timeout)
    resp.raise_for_status()

    data = resp.json()
    # Ensure data is JSON-serializable for logging safely.
    size_hint = len(json.dumps(data)) if isinstance(data, (dict, list)) else 0
    logging.info(f"API response received (approx bytes={size_hint})")

    if record_path is not None:
        df = pd.json_normalize(data, record_path=record_path, meta=meta)
    else:
        # If data is a list of objects or a dict, normalize accordingly
        if isinstance(data, list):
            df = pd.json_normalize(data)
        else:
            df = pd.json_normalize(data)
    logging.info(f"Loaded API rows={len(df)}, cols={len(df.columns)}")
    return df


# ---------------------------
# Simple transform using numpy
# ---------------------------
def simple_transform(df: pd.DataFrame) -> pd.DataFrame:
    """
    Example transform:
      - Fill numeric NaNs with 0
      - Add a boolean/int flag 'has_missing'
      - Add a sequential 'row_id' starting at 1

    Returns:
        Transformed DataFrame (copy).
    """
    out = df.copy()

    numeric_cols = out.select_dtypes(include=[np.number]).columns.tolist()
    if numeric_cols:
        out[numeric_cols] = out[numeric_cols].fillna(0)

    out["has_missing"] = np.where(out.isna().any(axis=1), 1, 0)
    out.insert(0, "row_id", np.arange(1, len(out) + 1))

    return out


# ---------------------------
# Exporters
# ---------------------------
def export_to_csv(df: pd.DataFrame, path: str, index: bool = False) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=index)
    logging.info(f"Wrote CSV: {path} (rows={len(df)})")


def export_to_json(
    df: pd.DataFrame,
    path: str,
    orient: str = "records",
    lines: bool = True,
    force_ascii: bool = False
) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_json(path, orient=orient, lines=lines, force_ascii=force_ascii)
    logging.info(f"Wrote JSON: {path} (rows={len(df)})")


def export_to_sql(
    df: pd.DataFrame,
    connection_string: str,
    table_name: str,
    if_exists: str = "replace",
    index: bool = False,
    chunksize: int = 1000
) -> None:
    engine = _get_engine(connection_string)
    with engine.begin() as conn:
        df.to_sql(
            table_name,
            con=conn,
            if_exists=if_exists,
            index=index,
            chunksize=chunksize,
            method="multi",
        )
    logging.info(f"Wrote SQL table: {table_name} (rows={len(df)})")


# ---------------------------
# Demonstration / Example usage
# ---------------------------
def demo() -> None:
    """
    Demonstrate:
      - Reading from Excel
      - Reading from SQL (SQLite demo)
      - Reading from an API (public placeholder)
      - Transforming with numpy
      - Exporting to CSV, JSON, and SQL
    """
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)

    # 1) Excel import (adjust path and sheet_name to your files)
    # If you don't have a file handy, comment this out or create a sample first.
    try:
        excel_path = "input/data.xlsx"  # change to your Excel file
        excel_df = load_from_excel(excel_path, sheet_name=0)
        excel_df_t = simple_transform(excel_df)
        export_to_csv(excel_df_t, os.path.join(output_dir, "excel_data.csv"))
        export_to_json(excel_df_t, os.path.join(output_dir, "excel_data.jsonl"))
    except FileNotFoundError:
        logging.warning("Excel demo skipped (file not found).")

    # 2) SQL import using SQLite (self-contained demo)
    sqlite_conn = "sqlite:///example.db"
    # Create a small demo table
    engine = _get_engine(sqlite_conn)
    demo_orders = pd.DataFrame(
        {
            "order_id": [1, 2, 3, 4],
            "customer": ["Alice", "Bob", "Charlie", "Dana"],
            "amount": [120.5, 49.9, 300.0, np.nan],
            "currency": ["USD", "USD", "EUR", "USD"],
        }
    )
    with engine.begin() as conn:
        demo_orders.to_sql("orders", con=conn, if_exists="replace", index=False)

    # Read from SQL with a parameterized query
    orders_df = load_from_sql(
        sqlite_conn,
        "SELECT order_id, customer, amount, currency FROM orders WHERE amount >= :min_amount",
        params={"min_amount": 50},
    )
    orders_df_t = simple_transform(orders_df)
    export_to_csv(orders_df_t, os.path.join(output_dir, "orders_filtered.csv"))
    export_to_json(orders_df_t, os.path.join(output_dir, "orders_filtered.jsonl"))
    export_to_sql(orders_df_t, sqlite_conn, table_name="orders_filtered", if_exists="replace")

    # 3) API import (public JSONPlaceholder API for demo)
    api_url = "https://jsonplaceholder.typicode.com/users"
    api_df = load_from_api(api_url)
    api_df_t = simple_transform(api_df)
    export_to_csv(api_df_t, os.path.join(output_dir, "api_users.csv"))
    export_to_json(api_df_t, os.path.join(output_dir, "api_users.jsonl"))
    export_to_sql(api_df_t, sqlite_conn, table_name="api_users", if_exists="replace")

    logging.info("Demo complete.")


if __name__ == "__main__":
    demo()
------------------------------------------------------------

PROMPT 291:
"Generate Python code that uses the `pandas` library to read in raw data from a CSV file, applies a series of transformations to clean and manipulate the data, and then writes the transformed data to a new CSV file, creating a feature store."
RESPONSE:
#!/usr/bin/env python3
"""
Feature Store Builder using pandas

Reads raw data from a CSV, cleans and transforms it into a set of machine-learning-friendly features,
and writes the result to a new CSV file (a simple, file-based feature store). Optionally writes a
JSON metadata file describing the feature schema and lineage.

Usage:
  python feature_store.py \
    --input raw_data.csv \
    --output features.csv \
    --metadata features_schema.json \
    --id-cols id_column_name \
    --top-n-categories 10 \
    --winsorize-lo 0.01 \
    --winsorize-hi 0.99

Notes:
- This script is generic and uses heuristics. Review and tailor the transformation steps to your data.
- Only pandas and numpy are required.
"""

import argparse
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd


def standardize_colnames(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    new_cols = []
    for c in df.columns:
        c2 = c.strip().lower()
        c2 = re.sub(r"[^\w]+", "_", c2)
        c2 = re.sub(r"_+", "_", c2).strip("_")
        new_cols.append(c2)
    df.columns = new_cols
    return df


def strip_whitespace_strings(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    obj_cols = df.select_dtypes(include=["object", "string"]).columns
    for c in obj_cols:
        df[c] = df[c].astype("string")
        df[c] = df[c].str.strip()
        # Normalize multiple spaces
        df[c] = df[c].str.replace(r"\s+", " ", regex=True)
        # Treat empty strings as NaN
        df[c] = df[c].replace("", pd.NA)
    return df


def coerce_booleans(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    truthy = {"true", "t", "yes", "y", "1", "on"}
    falsy = {"false", "f", "no", "n", "0", "off"}
    obj_cols = df.select_dtypes(include=["object", "string"]).columns

    for c in obj_cols:
        s = df[c].dropna().astype(str).str.lower().str.strip()
        if s.isin(truthy | falsy).mean() > 0.9 and s.nunique() <= 6:
            df[c] = df[c].astype("string").str.lower().str.strip()
            df[c] = df[c].map(lambda x: True if isinstance(x, str) and x in truthy else (False if isinstance(x, str) and x in falsy else pd.NA))
            df[c] = df[c].astype("boolean")
    return df


def parse_date_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    candidates = [c for c in df.columns if any(k in c for k in ["date", "time", "timestamp", "dt", "ts"])]
    # Also try object columns that look like dates
    candidates += [c for c in df.select_dtypes(include=["object", "string"]).columns if c not in candidates]

    for c in candidates:
        if df[c].isna().all():
            continue
        try:
            parsed = pd.to_datetime(df[c], errors="coerce", utc=False, infer_datetime_format=True)
            # Accept as datetime if we get a reasonable conversion
            if parsed.notna().mean() > 0.5:
                df[c] = parsed
        except Exception:
            pass
    return df


def deduplicate_rows(df: pd.DataFrame, id_cols: List[str]) -> pd.DataFrame:
    df = df.copy()
    if id_cols:
        df = df.sort_index()
        df = df.drop_duplicates(subset=id_cols, keep="last")
    else:
        df = df.drop_duplicates(keep="last")
    return df


def fill_missing_values(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # Numerics: median
    num_cols = df.select_dtypes(include=[np.number]).columns
    for c in num_cols:
        if df[c].isna().any():
            df[c] = df[c].fillna(df[c].median())

    # Booleans: mode or False
    bool_cols = df.select_dtypes(include=["boolean", "bool"]).columns
    for c in bool_cols:
        if df[c].isna().any():
            mode_val = df[c].mode(dropna=True)
            fill_val = bool(mode_val.iloc[0]) if not mode_val.empty else False
            df[c] = df[c].fillna(fill_val).astype(bool)

    # Datetimes: forward-fill then back-fill within groups? Keep simple: fill with median-like (most frequent day)
    dt_cols = df.select_dtypes(include=["datetime64[ns]", "datetime64[ns, UTC]"]).columns
    for c in dt_cols:
        # Leave missing as is for datetimes to avoid fabricating dates; they will be handled when deriving features
        pass

    # Categorical/text: mode or "unknown"
    cat_cols = df.select_dtypes(include=["object", "string"]).columns
    for c in cat_cols:
        if df[c].isna().any():
            mode_val = df[c].mode(dropna=True)
            fill_val = str(mode_val.iloc[0]) if not mode_val.empty else "unknown"
            df[c] = df[c].fillna(fill_val).astype("string")
    return df


def winsorize_numeric(df: pd.DataFrame, lo: float, hi: float) -> pd.DataFrame:
    df = df.copy()
    num_cols = df.select_dtypes(include=[np.number]).columns
    for c in num_cols:
        if df[c].notna().sum() == 0:
            continue
        lower = df[c].quantile(lo)
        upper = df[c].quantile(hi)
        if pd.isna(lower) or pd.isna(upper):
            continue
        if lower > upper:
            lower, upper = upper, lower
        df[c] = df[c].clip(lower=lower, upper=upper)
    return df


def remove_constant_columns(df: pd.DataFrame) -> pd.DataFrame:
    nunique = df.nunique(dropna=False)
    constant_cols = nunique[nunique <= 1].index.tolist()
    return df.drop(columns=constant_cols) if constant_cols else df


def derive_datetime_features(df: pd.DataFrame, meta: Dict) -> Tuple[pd.DataFrame, Dict]:
    df = df.copy()
    dt_cols = df.select_dtypes(include=["datetime64[ns]", "datetime64[ns, UTC]"]).columns
    for c in dt_cols:
        # Ensure timezone-naive for consistent operations
        if hasattr(df[c].dtype, "tz") and df[c].dtype.tz is not None:
            df[c] = df[c].dt.tz_convert(None)
        df[f"{c}__year"] = df[c].dt.year
        df[f"{c}__month"] = df[c].dt.month
        df[f"{c}__day"] = df[c].dt.day
        df[f"{c}__dow"] = df[c].dt.dayofweek
        df[f"{c}__week"] = df[c].dt.isocalendar().week.astype(int)
        df[f"{c}__is_weekend"] = df[c].dt.dayofweek.isin([5, 6]).astype(int)
        meta["features"].extend(
            [
                {"name": f"{c}__year", "source": c, "type": "int"},
                {"name": f"{c}__month", "source": c, "type": "int"},
                {"name": f"{c}__day", "source": c, "type": "int"},
                {"name": f"{c}__dow", "source": c, "type": "int"},
                {"name": f"{c}__week", "source": c, "type": "int"},
                {"name": f"{c}__is_weekend", "source": c, "type": "int"},
            ]
        )
    # Drop raw datetime columns to keep numeric features only (optional)
    df = df.drop(columns=list(dt_cols))
    return df, meta


def derive_text_features(df: pd.DataFrame, meta: Dict, keep_original: bool = False) -> Tuple[pd.DataFrame, Dict]:
    df = df.copy()
    text_cols = df.select_dtypes(include=["object", "string"]).columns
    for c in text_cols:
        df[f"{c}__len"] = df[c].astype("string").str.len().astype("Int64").fillna(0).astype(int)
        df[f"{c}__words"] = (
            df[c]
            .astype("string")
            .str.split()
            .apply(lambda x: len(x) if isinstance(x, list) else 0)
            .astype(int)
        )
        meta["features"].append({"name": f"{c}__len", "source": c, "type": "int"})
        meta["features"].append({"name": f"{c}__words", "source": c, "type": "int"})
    if not keep_original and len(text_cols) > 0:
        df = df.drop(columns=list(text_cols))
    return df, meta


def one_hot_encode_topn(df: pd.DataFrame, meta: Dict, top_n: int = 10) -> Tuple[pd.DataFrame, Dict]:
    df = df.copy()
    cat_cols = df.select_dtypes(include=["category"]).columns.tolist()
    # Also include small-cardinality object/string as categories
    obj_cols = df.select_dtypes(include=["object", "string"]).columns.tolist()
    for c in obj_cols:
        if df[c].nunique(dropna=True) <= max(top_n, 25):
            df[c] = df[c].astype("category")
            cat_cols.append(c)
    cat_cols = list(dict.fromkeys(cat_cols))  # deduplicate

    for c in cat_cols:
        vc = df[c].value_counts(dropna=True)
        top = vc.nlargest(top_n).index.tolist()
        for cat in top:
            new_col = f"{c}__is_{str(cat)}"
            df[new_col] = (df[c] == cat).astype(int)
            meta["features"].append({"name": new_col, "source": c, "type": "int", "category": str(cat)})
        # "Other" bucket
        new_col_other = f"{c}__is_other"
        df[new_col_other] = (~df[c].isin(top)).astype(int)
        meta["features"].append({"name": new_col_other, "source": c, "type": "int", "category": "other"})
        # Drop original categorical to keep numeric-only features
        df = df.drop(columns=[c])
    return df, meta


def zscore_numeric(df: pd.DataFrame, meta: Dict, suffix: str = "__z") -> Tuple[pd.DataFrame, Dict]:
    df = df.copy()
    num_cols = df.select_dtypes(include=[np.number]).columns
    for c in num_cols:
        mean = df[c].mean()
        std = df[c].std(ddof=0)
        if std and np.isfinite(std) and std > 0:
            zcol = f"{c}{suffix}"
            df[zcol] = (df[c] - mean) / std
            meta["features"].append({"name": zcol, "source": c, "type": "float", "transform": "zscore"})
    return df, meta


def finalize_and_validate(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # Replace remaining NaNs: numerics -> 0, booleans -> 0/False, others -> "unknown"
    for c in df.columns:
        if pd.api.types.is_numeric_dtype(df[c]):
            df[c] = df[c].fillna(0)
        elif pd.api.types.is_bool_dtype(df[c]):
            df[c] = df[c].fillna(False).astype(int)  # cast to 0/1
        elif pd.api.types.is_datetime64_any_dtype(df[c]):
            # Rare at this point; if present, convert to int timestamp or drop.
            df[c] = df[c].view("int64").fillna(0)
        else:
            df[c] = df[c].astype("string").fillna("unknown")
    # Ensure column name uniqueness and sorted order for reproducibility
    df = df.loc[:, ~df.columns.duplicated()]
    df = df.reindex(sorted(df.columns), axis=1)
    return df


def build_feature_store(
    input_csv: Path,
    output_csv: Path,
    metadata_json: Path = None,
    id_cols: List[str] = None,
    top_n_categories: int = 10,
    winsor_lo: float = 0.01,
    winsor_hi: float = 0.99,
) -> None:
    meta = {
        "input_csv": str(input_csv),
        "output_csv": str(output_csv),
        "id_columns": id_cols or [],
        "params": {
            "top_n_categories": top_n_categories,
            "winsor_lo": winsor_lo,
            "winsor_hi": winsor_hi,
        },
        "features": [],
    }

    # 1) Read
    df = pd.read_csv(input_csv)

    # 2) Standardize and clean
    df = standardize_colnames(df)
    df = strip_whitespace_strings(df)
    df = coerce_booleans(df)
    df = parse_date_columns(df)

    # 3) Deduplicate
    df = deduplicate_rows(df, id_cols=id_cols or [])

    # 4) Handle missing values
    df = fill_missing_values(df)

    # 5) Cap outliers for stability
    df = winsorize_numeric(df, lo=winsor_lo, hi=winsor_hi)

    # 6) Drop constants
    df = remove_constant_columns(df)

    # 7) Derive features
    df, meta = derive_datetime_features(df, meta)
    # Keep original text columns for one-hot? We'll derive simple text stats and then encode small-cardinality text as category.
    df, meta = derive_text_features(df, meta, keep_original=True)
    df, meta = one_hot_encode_topn(df, meta, top_n=top_n_categories)

    # 8) Numeric standardization (adds z-score columns, keeps originals)
    df, meta = zscore_numeric(df, meta)

    # 9) Finalize
    df = finalize_and_validate(df)

    # Add an auto row key if none provided
    if not id_cols:
        df.insert(0, "row_key", np.arange(len(df)))
        meta["features"].insert(0, {"name": "row_key", "source": "generated", "type": "int"})

    # 10) Write outputs
    output_csv.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_csv, index=False)

    if metadata_json is not None:
        metadata_json.parent.mkdir(parents=True, exist_ok=True)
        # Include simple dtype report
        meta["dtypes"] = {c: str(t) for c, t in df.dtypes.items()}
        with open(metadata_json, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2)


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Create a feature store CSV from raw data using pandas.")
    p.add_argument("--input", required=True, type=Path, help="Path to input CSV file.")
    p.add_argument("--output", required=True, type=Path, help="Path to output features CSV file.")
    p.add_argument("--metadata", required=False, type=Path, default=None, help="Optional path to write feature metadata JSON.")
    p.add_argument(
        "--id-cols",
        nargs="*",
        default=[],
        help="Optional list of ID columns to use for deduplication (keep last). Example: --id-cols user_id event_time",
    )
    p.add_argument("--top-n-categories", type=int, default=10, help="Top-N categories to one-hot encode per categorical column.")
    p.add_argument("--winsorize-lo", type=float, default=0.01, help="Lower quantile for numeric winsorization.")
    p.add_argument("--winsorize-hi", type=float, default=0.99, help="Upper quantile for numeric winsorization.")
    return p.parse_args()


if __name__ == "__main__":
    args = parse_args()
    build_feature_store(
        input_csv=args.input,
        output_csv=args.output,
        metadata_json=args.metadata,
        id_cols=args.id_cols,
        top_n_categories=args.top_n_categories,
        winsor_lo=args.winsorize_lo,
        winsor_hi=args.winsorize_hi,
    )
------------------------------------------------------------

PROMPT 292:
"Generate Python code that creates a library for simple interface creation, metric computation, and checkpointing; import needed modules; create classes for metrics and checkpoints; define methods in each class; run tests on created metrics."
RESPONSE:
# Simple library for interface creation, metric computation, and checkpointing,
# with basic tests for the metric implementations.

from __future__ import annotations

import argparse
import dataclasses
import hashlib
import json
import math
import os
import pickle
import shutil
import sys
import tempfile
import time
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple, Union


# =========================
# Simple interface creation
# =========================

Converter = Callable[[str], Any]


@dataclass
class ArgumentSpec:
    name: str
    type: Converter = str
    default: Any = dataclasses.field(default_factory=lambda: None)
    help: str = ""


class Interface:
    """
    A minimal interface wrapper around a Python callable. It supports:
      - Programmatic invocation via run(params=...)
      - Interactive CLI prompting via run(interactive=True)
      - Argparse wiring via to_cli() and from_cli()

    Example:
        def add(a: int, b: float) -> float:
            return a + b

        iface = Interface(
            add,
            inputs=[ArgumentSpec("a", int, help="First addend"),
                    ArgumentSpec("b", float, help="Second addend")],
            description="Add two numbers"
        )

        result = iface.run(params={"a": 1, "b": 2.5})
        print(result)  # 3.5
    """

    def __init__(
        self,
        func: Callable[..., Any],
        inputs: Sequence[ArgumentSpec],
        description: str = "",
    ) -> None:
        self.func = func
        self.inputs = list(inputs)
        self.description = description or f"Interface for {getattr(func, '__name__', 'callable')}"

    def _coerce_params(self, params: Mapping[str, Any]) -> Dict[str, Any]:
        out: Dict[str, Any] = {}
        known = {spec.name for spec in self.inputs}
        for spec in self.inputs:
            if spec.name in params:
                val = params[spec.name]
                # If already correct type, keep; if str and converter provided, convert.
                if isinstance(val, str) and spec.type is not None:
                    out[spec.name] = spec.type(val)
                else:
                    out[spec.name] = val
            else:
                if spec.default is not None:
                    out[spec.name] = spec.default
                else:
                    raise ValueError(f"Missing required parameter: {spec.name}")
        # Ignore extra params beyond spec
        extra = set(params.keys()) - known
        if extra:
            # Silently ignore extras or raise; we'll ignore to be permissive
            pass
        return out

    def run(
        self,
        params: Optional[Mapping[str, Any]] = None,
        interactive: bool = False,
    ) -> Any:
        if params is not None and interactive:
            raise ValueError("Provide either params or interactive=True, not both.")

        if interactive:
            coerced: Dict[str, Any] = {}
            print(self.description)
            for spec in self.inputs:
                prompt = f"{spec.name}"
                if spec.help:
                    prompt += f" ({spec.help})"
                if spec.default is not None:
                    prompt += f" [default={spec.default}]"
                prompt += ": "

                raw = input(prompt)
                if raw.strip() == "" and spec.default is not None:
                    coerced[spec.name] = spec.default
                else:
                    coerced[spec.name] = spec.type(raw)
            return self.func(**coerced)

        if params is None:
            raise ValueError("Either provide params or set interactive=True.")
        coerced = self._coerce_params(params)
        return self.func(**coerced)

    def to_cli(self, parser: Optional[argparse.ArgumentParser] = None) -> argparse.ArgumentParser:
        parser = parser or argparse.ArgumentParser(description=self.description)
        for spec in self.inputs:
            arg_name = f"--{spec.name}"
            kwargs: Dict[str, Any] = {"type": spec.type, "help": spec.help or ""}
            if spec.default is not None:
                kwargs["default"] = spec.default
                kwargs["required"] = False
            else:
                kwargs["required"] = True
            parser.add_argument(arg_name, **kwargs)
        return parser

    def from_cli(self, argv: Optional[Sequence[str]] = None) -> Any:
        parser = self.to_cli()
        ns = parser.parse_args(argv)
        return self.run(params=vars(ns))


class InterfaceRegistry:
    """
    Registry for named interfaces.
    """
    def __init__(self) -> None:
        self._store: Dict[str, Interface] = {}

    def register(self, name: str, iface: Interface) -> None:
        if name in self._store:
            raise ValueError(f"Interface named '{name}' already registered.")
        self._store[name] = iface

    def list(self) -> List[str]:
        return sorted(self._store.keys())

    def get(self, name: str) -> Interface:
        return self._store[name]

    def run(self, name: str, params: Optional[Mapping[str, Any]] = None, interactive: bool = False) -> Any:
        return self.get(name).run(params=params, interactive=interactive)


# ==================
# Metric computation
# ==================

class Metric:
    """
    Base streaming metric class.
    """
    def reset(self) -> None:
        raise NotImplementedError

    def update(self, preds: Iterable[Any], targets: Iterable[Any]) -> None:
        raise NotImplementedError

    def compute(self) -> Any:
        raise NotImplementedError

    @property
    def name(self) -> str:
        return self.__class__.__name__


def _to_list(x: Union[Sequence[Any], Iterable[Any]]) -> List[Any]:
    if isinstance(x, list):
        return x
    if isinstance(x, (tuple, set)):
        return list(x)
    return list(x)


def _is_sequence_of_sequences(x: Sequence[Any]) -> bool:
    return len(x) > 0 and isinstance(x[0], (list, tuple))


def _argmax(seq: Sequence[float]) -> int:
    max_idx = 0
    max_val = seq[0]
    for i in range(1, len(seq)):
        if seq[i] > max_val:
            max_val = seq[i]
            max_idx = i
    return max_idx


def _preds_to_labels(
    preds: Sequence[Any],
    targets: Sequence[Any],
    threshold: float = 0.5
) -> Tuple[List[int], List[int]]:
    """
    Convert various prediction forms to label indices.
      - If preds is sequence of sequences: use argmax per row -> multiclass
      - If preds is sequence of floats and targets are {0,1}: threshold -> binary
      - If preds is sequence of ints: assume already labels
    Targets should be ints (class indices) for multiclass or binary.
    """
    p = _to_list(preds)
    t = _to_list(targets)
    if len(p) != len(t):
        raise ValueError("preds and targets must have same length")

    # Already labels
    if len(p) > 0 and isinstance(p[0], int):
        return [int(v) for v in p], [int(v) for v in t]

    # Probabilities over classes
    if len(p) > 0 and isinstance(p[0], (list, tuple)):
        labels = [_argmax(row) for row in p]  # type: ignore[arg-type]
        return labels, [int(v) for v in t]

    # Floats -> assume binary scores
    if len(p) > 0 and isinstance(p[0], float):
        labels = [1 if float(v) >= threshold else 0 for v in p]  # type: ignore[arg-type]
        return labels, [int(v) for v in t]

    # Fallback: try to cast to ints
    try:
        return [int(v) for v in p], [int(v) for v in t]
    except Exception as e:
        raise TypeError(f"Unsupported prediction types for preds_to_labels: {type(p[0])}") from e


class Accuracy(Metric):
    def __init__(self, threshold: float = 0.5) -> None:
        self.threshold = threshold
        self.reset()

    def reset(self) -> None:
        self.correct = 0
        self.total = 0

    def update(self, preds: Iterable[Any], targets: Iterable[Any]) -> None:
        p_list, t_list = _preds_to_labels(list(preds), list(targets), threshold=self.threshold)
        for p, t in zip(p_list, t_list):
            if p == t:
                self.correct += 1
            self.total += 1

    def compute(self) -> float:
        if self.total == 0:
            return 0.0
        return self.correct / self.total


class _PRF1Base(Metric):
    """
    Base for precision, recall, F1 (multiclass and binary single-label).
    Accumulates per-class TP, FP, FN and support.
    """
    def __init__(self, average: str = "macro", threshold: float = 0.5) -> None:
        if average not in ("micro", "macro", "weighted"):
            raise ValueError("average must be one of {'micro','macro','weighted'}")
        self.average = average
        self.threshold = threshold
        self.reset()

    def reset(self) -> None:
        self.tp: Dict[int, int] = defaultdict(int)
        self.fp: Dict[int, int] = defaultdict(int)
        self.fn: Dict[int, int] = defaultdict(int)
        self.support: Dict[int, int] = defaultdict(int)
        self.labels_seen: set[int] = set()

    def update(self, preds: Iterable[Any], targets: Iterable[Any]) -> None:
        p_list, t_list = _preds_to_labels(list(preds), list(targets), threshold=self.threshold)
        for p, t in zip(p_list, t_list):
            self.labels_seen.add(p)
            self.labels_seen.add(t)
            self.support[t] += 1
            if p == t:
                self.tp[t] += 1
            else:
                self.fp[p] += 1
                self.fn[t] += 1

    def _micro(self) -> Tuple[float, float, float]:
        tp = sum(self.tp.values())
        fp = sum(self.fp.values())
        fn = sum(self.fn.values())
        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0
        return prec, rec, f1

    def _per_class(self) -> Dict[int, Tuple[float, float, float]]:
        out: Dict[int, Tuple[float, float, float]] = {}
        classes = sorted(self.labels_seen)
        for c in classes:
            tp = self.tp.get(c, 0)
            fp = self.fp.get(c, 0)
            fn = self.fn.get(c, 0)
            prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0
            out[c] = (prec, rec, f1)
        return out

    def _aggregate(self) -> Tuple[float, float, float]:
        if self.average == "micro":
            return self._micro()

        per_class = self._per_class()
        classes = list(per_class.keys())
        if not classes:
            return 0.0, 0.0, 0.0

        if self.average == "macro":
            prec = sum(per_class[c][0] for c in classes) / len(classes)
            rec = sum(per_class[c][1] for c in classes) / len(classes)
            f1 = sum(per_class[c][2] for c in classes) / len(classes)
            return prec, rec, f1

        # weighted
        total_support = sum(self.support.get(c, 0) for c in classes)
        if total_support == 0:
            return 0.0, 0.0, 0.0
        prec = sum(per_class[c][0] * self.support.get(c, 0) for c in classes) / total_support
        rec = sum(per_class[c][1] * self.support.get(c, 0) for c in classes) / total_support
        f1 = sum(per_class[c][2] * self.support.get(c, 0) for c in classes) / total_support
        return prec, rec, f1


class Precision(_PRF1Base):
    def compute(self) -> float:
        prec, _, _ = self._aggregate()
        return prec


class Recall(_PRF1Base):
    def compute(self) -> float:
        _, rec, _ = self._aggregate()
        return rec


class F1Score(_PRF1Base):
    def compute(self) -> float:
        _, _, f1 = self._aggregate()
        return f1


class MeanSquaredError(Metric):
    def __init__(self) -> None:
        self.reset()

    def reset(self) -> None:
        self._sum_sq = 0.0
        self._n = 0

    def update(self, preds: Iterable[float], targets: Iterable[float]) -> None:
        for p, t in zip(preds, targets):
            diff = float(p) - float(t)
            self._sum_sq += diff * diff
            self._n += 1

    def compute(self) -> float:
        if self._n == 0:
            return 0.0
        return self._sum_sq / self._n


class MeanAbsoluteError(Metric):
    def __init__(self) -> None:
        self.reset()

    def reset(self) -> None:
        self._sum_abs = 0.0
        self._n = 0

    def update(self, preds: Iterable[float], targets: Iterable[float]) -> None:
        for p, t in zip(preds, targets):
            diff = abs(float(p) - float(t))
            self._sum_abs += diff
            self._n += 1

    def compute(self) -> float:
        if self._n == 0:
            return 0.0
        return self._sum_abs / self._n


# =============
# Checkpointing
# =============

@dataclass
class CheckpointRecord:
    path: Path
    timestamp: float
    tag: Optional[str]
    metadata: Dict[str, Any]
    sha256: str


class CheckpointManager:
    """
    Simple file-based checkpoint manager using pickle for state persistence and
    JSON sidecars for metadata. Keeps last N checkpoints.
    """
    def __init__(self, root_dir: Union[str, Path], keep_last_n: int = 5) -> None:
        self.root_dir = Path(root_dir)
        self.keep_last_n = int(keep_last_n)
        self.root_dir.mkdir(parents=True, exist_ok=True)
        self.index_file = self.root_dir / "index.json"
        self._index: List[Dict[str, Any]] = self._load_index()

    def _load_index(self) -> List[Dict[str, Any]]:
        if self.index_file.exists():
            with open(self.index_file, "r", encoding="utf-8") as f:
                return json.load(f)
        return []

    def _save_index(self) -> None:
        with open(self.index_file, "w", encoding="utf-8") as f:
            json.dump(self._index, f, indent=2)

    def _sha256_bytes(self, data: bytes) -> str:
        h = hashlib.sha256()
        h.update(data)
        return h.hexdigest()

    def save_state(self, state: Any, tag: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None) -> Path:
        ts = time.time()
        ts_str = time.strftime("%Y%m%d-%H%M%S", time.localtime(ts))
        uid = hashlib.md5(f"{ts}-{os.getpid()}".encode()).hexdigest()[:8]
        base = f"ckpt-{ts_str}-{uid}"
        pkl_path = self.root_dir / f"{base}.pkl"
        meta_path = self.root_dir / f"{base}.json"

        # Serialize state
        payload = pickle.dumps(state, protocol=pickle.HIGHEST_PROTOCOL)
        sha = self._sha256_bytes(payload)
        with open(pkl_path, "wb") as f:
            f.write(payload)

        md = {
            "timestamp": ts,
            "tag": tag,
            "metadata": metadata or {},
            "sha256": sha,
            "state_file": pkl_path.name,
        }
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(md, f, indent=2)

        # Update index
        self._index.append(
            {
                "timestamp": ts,
                "tag": tag,
                "metadata": metadata or {},
                "sha256": sha,
                "state_file": pkl_path.name,
                "meta_file": meta_path.name,
            }
        )
        # Keep sorted by timestamp, ascending
        self._index.sort(key=lambda r: r["timestamp"])
        self._prune()
        self._save_index()
        return pkl_path

    def _prune(self) -> None:
        # Remove older checkpoints beyond keep_last_n
        if self.keep_last_n <= 0:
            # If configured to keep 0, remove all but latest?
            self.keep_last_n = 1
        excess = max(0, len(self._index) - self.keep_last_n)
        for _ in range(excess):
            rec = self._index.pop(0)  # remove oldest
            # Delete files
            try:
                (self.root_dir / rec["state_file"]).unlink(missing_ok=True)
            except Exception:
                pass
            try:
                (self.root_dir / rec["meta_file"]).unlink(missing_ok=True)
            except Exception:
                pass

    def list_checkpoints(self) -> List[CheckpointRecord]:
        out: List[CheckpointRecord] = []
        for rec in self._index:
            out.append(
                CheckpointRecord(
                    path=self.root_dir / rec["state_file"],
                    timestamp=rec["timestamp"],
                    tag=rec.get("tag"),
                    metadata=rec.get("metadata", {}),
                    sha256=rec["sha256"],
                )
            )
        return out

    def load_latest(self) -> Tuple[Any, Dict[str, Any]]:
        if not self._index:
            raise FileNotFoundError("No checkpoints available")
        rec = self._index[-1]  # latest
        return self._load_by_record(rec)

    def load_by_tag(self, tag: str) -> Tuple[Any, Dict[str, Any]]:
        # Load the latest checkpoint with this tag
        candidates = [rec for rec in self._index if rec.get("tag") == tag]
        if not candidates:
            raise FileNotFoundError(f"No checkpoint found with tag '{tag}'")
        rec = sorted(candidates, key=lambda r: r["timestamp"])[-1]
        return self._load_by_record(rec)

    def load_path(self, path: Union[str, Path]) -> Tuple[Any, Dict[str, Any]]:
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(str(path))
        # Find matching metadata
        meta_path = path.with_suffix(".json")
        if not meta_path.exists():
            # best-effort search index
            for rec in self._index:
                if (self.root_dir / rec["state_file"]) == path:
                    meta_path = self.root_dir / rec["meta_file"]
                    break
        with open(path, "rb") as f:
            payload = f.read()
        state = pickle.loads(payload)
        meta: Dict[str, Any] = {}
        if meta_path.exists():
            with open(meta_path, "r", encoding="utf-8") as f:
                m = json.load(f)
            meta = {"timestamp": m.get("timestamp"), "tag": m.get("tag"), **m.get("metadata", {})}
        return state, meta

    def _load_by_record(self, rec: Dict[str, Any]) -> Tuple[Any, Dict[str, Any]]:
        p = self.root_dir / rec["state_file"]
        with open(p, "rb") as f:
            payload = f.read()
        # Validate hash
        sha = self._sha256_bytes(payload)
        if sha != rec["sha256"]:
            raise IOError("Checkpoint SHA256 mismatch; file may be corrupted.")
        state = pickle.loads(payload)
        meta: Dict[str, Any] = {"timestamp": rec["timestamp"], "tag": rec.get("tag"), **rec.get("metadata", {})}
        return state, meta


# ============
# Basic tests
# ============

def _approx_equal(a: float, b: float, tol: float = 1e-9) -> bool:
    return abs(a - b) <= tol


def test_accuracy_binary() -> None:
    acc = Accuracy(threshold=0.5)
    preds = [0.1, 0.6, 0.9, 0.4]
    targets = [0, 1, 1, 0]
    acc.update(preds, targets)
    val = acc.compute()
    assert _approx_equal(val, 1.0), f"Expected 1.0, got {val}"


def test_accuracy_multiclass() -> None:
    acc = Accuracy()
    preds = [0, 2, 1, 2, 0, 1]
    targets = [0, 1, 1, 2, 0, 2]
    acc.update(preds, targets)
    val = acc.compute()
    expected = 4 / 6
    assert _approx_equal(val, expected), f"Expected {expected}, got {val}"


def test_precision_recall_f1_multiclass() -> None:
    preds = [0, 2, 1, 2, 0, 1]
    targets = [0, 1, 1, 2, 0, 2]

    for avg in ("micro", "macro", "weighted"):
        p = Precision(average=avg)
        r = Recall(average=avg)
        f1 = F1Score(average=avg)

        p.update(preds, targets)
        r.update(preds, targets)
        f1.update(preds, targets)

        pv = p.compute()
        rv = r.compute()
        f1v = f1.compute()

        # Manually computed for this dataset:
        # Micro: all 2/3; Macro: all 2/3; Weighted: supports are equal -> same as macro
        expected = 4 / 6
        assert _approx_equal(pv, expected), f"Precision {avg}: expected {expected}, got {pv}"
        assert _approx_equal(rv, expected), f"Recall {avg}: expected {expected}, got {rv}"
        assert _approx_equal(f1v, expected), f"F1 {avg}: expected {expected}, got {f1v}"


def test_regression_errors() -> None:
    mse = MeanSquaredError()
    mae = MeanAbsoluteError()
    preds = [0.0, 3.0, -2.0]
    targets = [0.0, 2.0, -1.0]
    mse.update(preds, targets)
    mae.update(preds, targets)
    mse_v = mse.compute()
    mae_v = mae.compute()
    expected = 2.0 / 3.0
    assert _approx_equal(mse_v, expected), f"MSE expected {expected}, got {mse_v}"
    assert _approx_equal(mae_v, expected), f"MAE expected {expected}, got {mae_v}"


def test_checkpoint_save_load() -> None:
    with tempfile.TemporaryDirectory() as tmp:
        mgr = CheckpointManager(tmp, keep_last_n=2)
        path1 = mgr.save_state({"step": 1, "weights": [1, 2, 3]}, tag="first", metadata={"epoch": 1})
        state1, meta1 = mgr.load_latest()
        assert state1["step"] == 1 and meta1.get("epoch") == 1, "Loaded state/metadata mismatch"

        time.sleep(0.01)  # ensure different timestamps
        path2 = mgr.save_state({"step": 2, "weights": [3, 2, 1]}, tag="second", metadata={"epoch": 2})
        state2, meta2 = mgr.load_latest()
        assert state2["step"] == 2 and meta2.get("epoch") == 2, "Latest checkpoint mismatch"

        time.sleep(0.01)
        path3 = mgr.save_state({"step": 3}, tag="third", metadata={"epoch": 3})
        # keep_last_n=2 -> only last two should exist (path2, path3)
        files = {p.path.name for p in mgr.list_checkpoints()}
        assert Path(path1).name not in files, "Oldest checkpoint was not pruned"
        assert Path(path2).name in files and Path(path3).name in files, "Latest checkpoints missing"

        # Load by tag
        state_by_tag, meta_by_tag = mgr.load_by_tag("third")
        assert state_by_tag["step"] == 3 and meta_by_tag.get("epoch") == 3, "load_by_tag failed"


def test_interface() -> None:
    def add(a: int, b: float) -> float:
        return a + b

    iface = Interface(
        add,
        inputs=[ArgumentSpec("a", int, help="First addend"),
                ArgumentSpec("b", float, help="Second addend")],
        description="Add two numbers",
    )
    out = iface.run(params={"a": 2, "b": 2.5})
    assert _approx_equal(out, 4.5), f"Interface result expected 4.5, got {out}"


def run_all_tests() -> None:
    tests = [
        test_accuracy_binary,
        test_accuracy_multiclass,
        test_precision_recall_f1_multiclass,
        test_regression_errors,
        test_checkpoint_save_load,
        test_interface,
    ]
    passed = 0
    for t in tests:
        name = t.__name__
        try:
            t()
            print(f"[PASS] {name}")
            passed += 1
        except AssertionError as e:
            print(f"[FAIL] {name}: {e}")
        except Exception as e:
            print(f"[ERROR] {name}: {e}")
    print(f"Passed {passed}/{len(tests)} tests.")
    if passed != len(tests):
        sys.exit(1)


if __name__ == "__main__":
    run_all_tests()
------------------------------------------------------------

PROMPT 293:
"Generate Python code that creates a command line application using the Databricks API to manage Databricks resources, including creating and deleting clusters, creating and deleting datasets, and listing all available clusters and datasets."
RESPONSE:
```python
#!/usr/bin/env python3
"""
Databricks CLI (lightweight) using the Databricks REST API.

Features:
- Clusters: create, delete (terminate or permanent), list
- Datasets: create, delete, list

Note on "datasets":
- In this CLI, a "dataset" is modeled as a Unity Catalog schema (database).
  You can create/delete schemas and list schemas across catalogs.
  If your workspace does not have Unity Catalog enabled, these dataset commands will not work.

Authentication:
- Provide host and token via environment variables or CLI flags:
  - DATABRICKS_HOST (e.g., https://adb-1234567890123.4.azuredatabricks.net)
  - DATABRICKS_TOKEN (a personal access token)
- Or use: --host https://... --token dapi...

Examples:
- List clusters:
    python dbx_cli.py clusters list
- Create cluster (fixed size with 2 workers):
    python dbx_cli.py clusters create --name my-cluster --spark-version 13.3.x-scala2.12 \
        --node-type-id i3.xlarge --num-workers 2
- Terminate a cluster:
    python dbx_cli.py clusters delete --cluster-id 0123-456789-abcd123
- Permanently delete a cluster (must be terminated first):
    python dbx_cli.py clusters delete --cluster-id 0123-456789-abcd123 --permanent
- List datasets (schemas) across all catalogs:
    python dbx_cli.py datasets list
- List datasets in a given catalog:
    python dbx_cli.py datasets list --catalog main
- Create a dataset (schema) in catalog 'main':
    python dbx_cli.py datasets create --catalog main --name demo_schema --comment "Demo schema"
- Delete a dataset (schema):
    python dbx_cli.py datasets delete --full-name main.demo_schema --force
"""

import argparse
import json
import os
import sys
from typing import Any, Dict, Optional

import requests


class DatabricksAPIError(Exception):
    pass


class DatabricksClient:
    def __init__(self, host: str, token: str, verify_ssl: bool = True, debug: bool = False) -> None:
        if not host.startswith("http"):
            host = f"https://{host}"
        self.host = host.rstrip("/")
        self.session = requests.Session()
        self.session.headers.update({"Authorization": f"Bearer {token}"})
        self.verify_ssl = verify_ssl
        self.debug = debug

    def _url(self, path: str) -> str:
        path = path if path.startswith("/") else f"/{path}"
        return f"{self.host}{path}"

    def request(self, method: str, path: str, params: Optional[Dict[str, Any]] = None, json_body: Optional[Dict[str, Any]] = None) -> Any:
        url = self._url(path)
        if self.debug:
            print(f"[DEBUG] {method} {url}", file=sys.stderr)
            if params:
                print(f"[DEBUG] params={params}", file=sys.stderr)
            if json_body:
                print(f"[DEBUG] json={json.dumps(json_body, indent=2)}", file=sys.stderr)
        resp = self.session.request(method=method, url=url, params=params, json=json_body, timeout=60, verify=self.verify_ssl)
        if self.debug:
            print(f"[DEBUG] status={resp.status_code}", file=sys.stderr)
        if not resp.ok:
            # Try to parse error message
            try:
                data = resp.json()
                message = data.get("message") or data.get("error") or data
            except Exception:
                message = resp.text
            raise DatabricksAPIError(f"HTTP {resp.status_code} error for {method} {url}: {message}")
        if resp.status_code == 204:  # No Content
            return None
        # Attempt JSON, fallback to text
        try:
            return resp.json()
        except Exception:
            return resp.text

    # -------- Clusters --------
    def clusters_list(self) -> Any:
        return self.request("GET", "/api/2.0/clusters/list")

    def clusters_create(
        self,
        cluster_name: str,
        spark_version: str,
        node_type_id: str,
        num_workers: Optional[int] = None,
        autoscale_min_workers: Optional[int] = None,
        autoscale_max_workers: Optional[int] = None,
        autotermination_minutes: Optional[int] = None,
        runtime_engine: Optional[str] = None,
        spark_conf: Optional[Dict[str, str]] = None,
        custom_tags: Optional[Dict[str, str]] = None,
    ) -> Any:
        payload: Dict[str, Any] = {
            "cluster_name": cluster_name,
            "spark_version": spark_version,
            "node_type_id": node_type_id,
        }
        if num_workers is not None and (autoscale_min_workers is not None or autoscale_max_workers is not None):
            raise ValueError("Provide either --num-workers or --autoscale-min/--autoscale-max, not both.")
        if num_workers is not None:
            if num_workers < 0:
                raise ValueError("--num-workers must be >= 0")
            payload["num_workers"] = num_workers
        else:
            if (autoscale_min_workers is None) != (autoscale_max_workers is None):
                raise ValueError("Both --autoscale-min and --autoscale-max must be provided for autoscaling.")
            if autoscale_min_workers is not None and autoscale_max_workers is not None:
                if autoscale_min_workers < 0 or autoscale_max_workers < 0 or autoscale_min_workers > autoscale_max_workers:
                    raise ValueError("Invalid autoscale bounds.")
                payload["autoscale"] = {
                    "min_workers": autoscale_min_workers,
                    "max_workers": autoscale_max_workers,
                }
            else:
                raise ValueError("You must provide either --num-workers or both --autoscale-min and --autoscale-max.")
        if autotermination_minutes is not None:
            payload["autotermination_minutes"] = autotermination_minutes
        if runtime_engine is not None:
            payload["runtime_engine"] = runtime_engine  # STANDARD or PHOTON
        if spark_conf:
            payload["spark_conf"] = spark_conf
        if custom_tags:
            payload["custom_tags"] = custom_tags
        return self.request("POST", "/api/2.0/clusters/create", json_body=payload)

    def clusters_delete(self, cluster_id: str, permanent: bool = False) -> Any:
        if permanent:
            return self.request("POST", "/api/2.0/clusters/permanent-delete", json_body={"cluster_id": cluster_id})
        else:
            return self.request("POST", "/api/2.0/clusters/delete", json_body={"cluster_id": cluster_id})

    # -------- Unity Catalog: Catalogs and Schemas (Datasets) --------
    def catalogs_list(self) -> Any:
        return self.request("GET", "/api/2.1/unity-catalog/catalogs")

    def schemas_list(self, catalog_name: str) -> Any:
        return self.request("GET", "/api/2.1/unity-catalog/schemas", params={"catalog_name": catalog_name})

    def schema_create(self, catalog_name: str, schema_name: str, comment: Optional[str] = None) -> Any:
        body: Dict[str, Any] = {"name": schema_name, "catalog_name": catalog_name}
        if comment:
            body["comment"] = comment
        return self.request("POST", "/api/2.1/unity-catalog/schemas", json_body=body)

    def schema_delete(self, full_name: str, force: bool = False) -> Any:
        # full_name is "catalog.schema"
        return self.request("DELETE", f"/api/2.1/unity-catalog/schemas/{full_name}", params={"force": str(force).lower()})


def print_clusters(data: Any) -> None:
    clusters = data.get("clusters", []) if isinstance(data, dict) else []
    if not clusters:
        print("No clusters found.")
        return
    for c in clusters:
        print(
            f"- cluster_id={c.get('cluster_id')} "
            f"name={c.get('cluster_name')} "
            f"state={c.get('state')} "
            f"spark_version={c.get('spark_version')} "
            f"node_type_id={c.get('node_type_id')}"
        )


def print_datasets_list(catalog: str, schemas_resp: Any) -> None:
    schemas = schemas_resp.get("schemas", []) if isinstance(schemas_resp, dict) else []
    if not schemas:
        print(f"No datasets (schemas) found in catalog {catalog}.")
        return
    for s in schemas:
        full_name = s.get("full_name") or f"{catalog}.{s.get('name')}"
        comment = s.get("comment") or ""
        print(f"- {full_name}{'  # ' + comment if comment else ''}")


def require_env_or_arg(value: Optional[str], env_key: str, arg_name: str) -> str:
    if value:
        return value
    v = os.environ.get(env_key)
    if v:
        return v
    raise SystemExit(f"Missing {arg_name}. Provide via --{arg_name.replace('_', '-')} or environment variable {env_key}.")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Manage Databricks clusters and datasets (Unity Catalog schemas) via REST API.")
    parser.add_argument("--host", help="Databricks workspace URL (e.g., https://adb-xxx.azuredatabricks.net). If omitted, uses DATABRICKS_HOST.")
    parser.add_argument("--token", help="Databricks personal access token. If omitted, uses DATABRICKS_TOKEN.")
    parser.add_argument("--insecure", action="store_true", help="Disable SSL verification (not recommended).")
    parser.add_argument("--debug", action="store_true", help="Enable debug logging to stderr.")

    subparsers = parser.add_subparsers(dest="resource", required=True)

    # Clusters
    clusters = subparsers.add_parser("clusters", help="Manage clusters")
    clusters_sub = clusters.add_subparsers(dest="action", required=True)

    c_list = clusters_sub.add_parser("list", help="List clusters")

    c_create = clusters_sub.add_parser("create", help="Create a cluster")
    c_create.add_argument("--name", required=True, help="Cluster name")
    c_create.add_argument("--spark-version", required=True, help="Spark runtime version (e.g., 13.3.x-scala2.12)")
    c_create.add_argument("--node-type-id", required=True, help="Node type id (e.g., i3.xlarge, Standard_DS3_v2)")
    size_group = c_create.add_mutually_exclusive_group(required=True)
    size_group.add_argument("--num-workers", type=int, help="Fixed number of workers (0 = driver only if policy allows)")
    size_group.add_argument("--autoscale", nargs=2, metavar=("MIN", "MAX"), help="Autoscale bounds (e.g., --autoscale 2 8)")
    c_create.add_argument("--autotermination-minutes", type=int, help="Auto-terminate after N minutes of inactivity")
    c_create.add_argument("--runtime-engine", choices=["STANDARD", "PHOTON"], help="Runtime engine type")
    c_create.add_argument("--tag", action="append", default=[], help="Custom tag key=value (repeatable)")
    c_create.add_argument("--conf", action="append", default=[], help="Spark conf key=value (repeatable)")

    c_delete = clusters_sub.add_parser("delete", help="Terminate or permanently delete a cluster")
    c_delete.add_argument("--cluster-id", required=True, help="Cluster ID")
    c_delete.add_argument("--permanent", action="store_true", help="Permanently delete (cluster must be terminated first)")

    # Datasets (schemas)
    datasets = subparsers.add_parser("datasets", help="Manage datasets (Unity Catalog schemas)")
    datasets_sub = datasets.add_subparsers(dest="action", required=True)

    d_list = datasets_sub.add_parser("list", help="List datasets (schemas)")
    d_list.add_argument("--catalog", help="Catalog to list. If omitted, lists all catalogs then their schemas.")

    d_create = datasets_sub.add_parser("create", help="Create a dataset (schema)")
    d_create.add_argument("--catalog", required=True, help="Catalog name (e.g., main)")
    d_create.add_argument("--name", required=True, help="Schema (dataset) name")
    d_create.add_argument("--comment", help="Optional comment")

    d_delete = datasets_sub.add_parser("delete", help="Delete a dataset (schema)")
    d_delete.add_argument("--full-name", help="Full schema name catalog.schema (preferred)")
    d_delete.add_argument("--catalog", help="Catalog name (used if --full-name not provided)")
    d_delete.add_argument("--name", help="Schema name (used if --full-name not provided)")
    d_delete.add_argument("--force", action="store_true", help="Cascade drop if non-empty")

    return parser


def parse_kv_list(items: list[str]) -> Dict[str, str]:
    out: Dict[str, str] = {}
    for it in items:
        if "=" not in it:
            raise SystemExit(f"Invalid key=value: {it}")
        k, v = it.split("=", 1)
        out[k] = v
    return out


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    host = require_env_or_arg(args.host, "DATABRICKS_HOST", "host")
    token = require_env_or_arg(args.token, "DATABRICKS_TOKEN", "token")
    client = DatabricksClient(host=host, token=token, verify_ssl=not args.insecure, debug=args.debug)

    try:
        if args.resource == "clusters":
            if args.action == "list":
                resp = client.clusters_list()
                print_clusters(resp)

            elif args.action == "create":
                num_workers = None
                autoscale_min = None
                autoscale_max = None
                if args.num_workers is not None:
                    num_workers = args.num_workers
                elif args.autoscale:
                    try:
                        autoscale_min = int(args.autoscale[0])
                        autoscale_max = int(args.autoscale[1])
                    except ValueError:
                        raise SystemExit("Autoscale bounds must be integers.")
                spark_conf = parse_kv_list(args.conf) if args.conf else None
                custom_tags = parse_kv_list(args.tag) if args.tag else None

                resp = client.clusters_create(
                    cluster_name=args.name,
                    spark_version=args.spark_version,
                    node_type_id=args.node_type_id,
                    num_workers=num_workers,
                    autoscale_min_workers=autoscale_min,
                    autoscale_max_workers=autoscale_max,
                    autotermination_minutes=args.autotermination_minutes,
                    runtime_engine=args.runtime_engine,
                    spark_conf=spark_conf,
                    custom_tags=custom_tags,
                )
                cluster_id = resp.get("cluster_id")
                print(f"Cluster creating: id={cluster_id}")

            elif args.action == "delete":
                resp = client.clusters_delete(cluster_id=args.cluster_id, permanent=args.permanent)
                if args.permanent:
                    print(f"Cluster permanently deleted: id={args.cluster_id}")
                else:
                    print(f"Cluster termination requested: id={args.cluster_id}")

        elif args.resource == "datasets":
            if args.action == "list":
                if args.catalog:
                    schemas_resp = client.schemas_list(args.catalog)
                    print_datasets_list(args.catalog, schemas_resp)
                else:
                    catalogs_resp = client.catalogs_list()
                    catalogs = catalogs_resp.get("catalogs", [])
                    if not catalogs:
                        print("No catalogs found.")
                    for c in catalogs:
                        cat_name = c.get("name")
                        print(f"Catalog: {cat_name}")
                        try:
                            schemas_resp = client.schemas_list(cat_name)
                            print_datasets_list(cat_name, schemas_resp)
                        except DatabricksAPIError as e:
                            print(f"  Error listing schemas for catalog {cat_name}: {e}")

            elif args.action == "create":
                resp = client.schema_create(catalog_name=args.catalog, schema_name=args.name, comment=args.comment)
                full_name = resp.get("full_name") or f"{args.catalog}.{args.name}"
                print(f"Dataset (schema) created: {full_name}")

            elif args.action == "delete":
                if args.full_name:
                    full_name = args.full_name
                else:
                    if not args.catalog or not args.name:
                        raise SystemExit("Provide --full-name or both --catalog and --name.")
                    full_name = f"{args.catalog}.{args.name}"
                client.schema_delete(full_name=full_name, force=args.force)
                print(f"Dataset (schema) deleted: {full_name} (force={args.force})")

    except DatabricksAPIError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(2)


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 294:
"Generate Python code that imports necessary libraries for chemoinformatics and machine learning, such as RDKit and scikit-learn, and creates a module that provides functions for handling chemical data and performing predictive modeling tasks."
RESPONSE:
# chemml_module.py
# A lightweight chemoinformatics + machine learning utility module using RDKit and scikit-learn.

from __future__ import annotations

import math
import warnings
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union

import numpy as np
import pandas as pd
from joblib import dump, load
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression, Ridge, Lasso
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    f1_score,
    precision_recall_fscore_support,
    roc_auc_score,
    r2_score,
    mean_absolute_error,
    mean_squared_error,
)
from sklearn.model_selection import (
    StratifiedKFold,
    KFold,
    train_test_split,
    cross_validate,
    GridSearchCV,
    RandomizedSearchCV,
)
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC, SVR

# RDKit imports
try:
    from rdkit import Chem, RDLogger
    from rdkit.Chem import AllChem, Descriptors
    from rdkit.DataStructs import ConvertToNumpyArray
    try:
        from rdkit.Chem import rdMolDescriptors
    except Exception:
        rdMolDescriptors = None
    try:
        # optional standardizer
        from rdkit.Chem import rdMolStandardize
    except Exception:
        rdMolStandardize = None
except ImportError as e:
    raise ImportError(
        "RDKit is required for this module. Install via conda (recommended):\n"
        "conda install -c conda-forge rdkit"
    ) from e

# Suppress RDKit warnings by default
RDLogger.DisableLog("rdApp.*")


# -----------------------------
# Exceptions and small utilities
# -----------------------------

class ChemMLException(Exception):
    pass


def set_random_seed(seed: int = 0) -> None:
    np.random.seed(seed)


def _is_int_like_array(y: Sequence[Any], max_classes: int = 50) -> bool:
    try:
        y = np.asarray(y)
        uniques = np.unique(y[~pd.isna(y)])
        if len(uniques) == 0:
            return False
        # all are integers (or cast without change)
        all_int = np.all(np.equal(np.mod(uniques, 1), 0))
        return all_int and len(uniques) <= max_classes
    except Exception:
        return False


def infer_task(y: Sequence[Any], max_classes: int = 50) -> str:
    y = np.asarray(y)
    if _is_int_like_array(y, max_classes=max_classes):
        return "classification"
    # heuristic: few unique levels but floats might also be classification (e.g., 0.0/1.0)
    uniques = np.unique(y[~pd.isna(y)])
    if len(uniques) <= 10 and np.all(np.isin(uniques, [0.0, 1.0])):
        return "classification"
    return "regression"


# -----------------------------
# Chemistry handling
# -----------------------------

def rdkit_disable_logging(disable: bool = True) -> None:
    if disable:
        RDLogger.DisableLog("rdApp.*")
    else:
        RDLogger.EnableLog("rdApp.*")


def sanitize_mol(mol: Chem.Mol) -> Optional[Chem.Mol]:
    if mol is None:
        return None
    try:
        Chem.SanitizeMol(mol)
        return mol
    except Exception:
        return None


def standardize_smiles(smiles: str, sanitize: bool = True, clear_stereo: bool = False) -> Optional[str]:
    if not isinstance(smiles, str) or not smiles.strip():
        return None
    try:
        mol = Chem.MolFromSmiles(smiles, sanitize=sanitize)
        if mol is None:
            return None
        if rdMolStandardize is not None:
            # Basic cleanup and normalization
            params = rdMolStandardize.CleanupParameters()
            parent = rdMolStandardize.Cleanup(mol, params)
            parent = rdMolStandardize.FragmentParent(parent)
            parent = rdMolStandardize.Uncharger().uncharge(parent)
            if clear_stereo:
                Chem.RemoveStereochemistry(parent)
            smiles_std = Chem.MolToSmiles(parent, canonical=True)
            return smiles_std
        else:
            if clear_stereo:
                Chem.RemoveStereochemistry(mol)
            return Chem.MolToSmiles(mol, canonical=True)
    except Exception:
        return None


def smiles_to_mol(smiles: str, sanitize: bool = True, standardize: bool = False) -> Optional[Chem.Mol]:
    if standardize:
        smiles = standardize_smiles(smiles, sanitize=sanitize)
    if smiles is None:
        return None
    try:
        mol = Chem.MolFromSmiles(smiles, sanitize=sanitize)
        return sanitize_mol(mol) if sanitize else mol
    except Exception:
        return None


def mols_from_smiles(smiles_list: Iterable[str], sanitize: bool = True, standardize: bool = False) -> List[Optional[Chem.Mol]]:
    return [smiles_to_mol(s, sanitize=sanitize, standardize=standardize) for s in smiles_list]


def read_sdf(path: str, sanitize: bool = True) -> List[Chem.Mol]:
    suppl = Chem.SDMolSupplier(path, sanitize=sanitize, removeHs=False)
    mols = [m for m in suppl if m is not None]
    return mols


# -----------------------------
# Featurization
# -----------------------------

def morgan_fingerprint(
    mol: Chem.Mol,
    radius: int = 2,
    n_bits: int = 2048,
    use_chirality: bool = True,
    use_counts: bool = False,
) -> np.ndarray:
    if mol is None:
        return np.full((n_bits,), np.nan, dtype=np.float32)
    if use_counts:
        # integer hashed counts vector -> dense array
        sv = AllChem.GetHashedMorganFingerprint(mol, radius, nBits=n_bits, useChirality=use_chirality)
        arr = np.zeros((n_bits,), dtype=np.float32)
        for idx, val in sv.GetNonzeroElements().items():
            arr[idx % n_bits] = float(val)
        return arr
    else:
        bv = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits, useChirality=use_chirality)
        arr = np.zeros((n_bits,), dtype=np.uint8)
        ConvertToNumpyArray(bv, arr)
        return arr.astype(np.float32)


def rdkit_descriptor_names() -> List[str]:
    return [name for name, _ in Descriptors._descList]


def rdkit_descriptors(mol: Chem.Mol, names: Optional[List[str]] = None) -> np.ndarray:
    if names is None:
        names = rdkit_descriptor_names()
    vals: List[float] = []
    for name in names:
        try:
            func = getattr(Descriptors, name)
            v = float(func(mol)) if mol is not None else np.nan
        except Exception:
            v = np.nan
        # guard against infs
        if isinstance(v, float) and (math.isinf(v) or math.isnan(v)):
            vals.append(np.nan)
        else:
            vals.append(v)
    return np.asarray(vals, dtype=np.float32)


class MoleculeFeaturizer(BaseEstimator, TransformerMixin):
    """
    An sklearn-compatible transformer to convert SMILES or RDKit Mol objects
    into numeric feature arrays.

    Parameters
    ----------
    method : {'morgan', 'morgan_counts', 'rdkit'}
    radius : int, for Morgan
    n_bits : int, for Morgan
    use_chirality : bool, for Morgan
    sanitize : bool, SMILES sanitization
    standardize : bool, canonicalization and cleanup
    """

    def __init__(
        self,
        method: str = "morgan",
        radius: int = 2,
        n_bits: int = 2048,
        use_chirality: bool = True,
        sanitize: bool = True,
        standardize: bool = False,
    ):
        self.method = method
        self.radius = radius
        self.n_bits = n_bits
        self.use_chirality = use_chirality
        self.sanitize = sanitize
        self.standardize = standardize
        self.feature_names_: Optional[List[str]] = None
        self.output_kind_: Optional[str] = None  # 'binary' or 'continuous'

    def fit(self, X: Sequence[Union[str, Chem.Mol]], y: Optional[Sequence[Any]] = None):
        # set feature names depending on method
        if self.method == "rdkit":
            self.feature_names_ = rdkit_descriptor_names()
            self.output_kind_ = "continuous"
        elif self.method == "morgan":
            self.feature_names_ = [f"morgan_{i}" for i in range(self.n_bits)]
            self.output_kind_ = "binary"
        elif self.method == "morgan_counts":
            self.feature_names_ = [f"morgan_count_{i}" for i in range(self.n_bits)]
            self.output_kind_ = "continuous"
        else:
            raise ValueError(f"Unknown method: {self.method}")
        return self

    def transform(self, X: Sequence[Union[str, Chem.Mol]]) -> np.ndarray:
        if not isinstance(X, (list, tuple, np.ndarray, pd.Series)):
            X = list(X)
        # convert to mols if strings
        if len(X) == 0:
            return np.empty((0, len(self.feature_names_ or [])), dtype=np.float32)
        if isinstance(X[0], str):
            mols = mols_from_smiles(X, sanitize=self.sanitize, standardize=self.standardize)
        else:
            mols = list(X)

        feats: List[np.ndarray] = []
        if self.method == "rdkit":
            names = self.feature_names_ or rdkit_descriptor_names()
            for m in mols:
                feats.append(rdkit_descriptors(m, names))
        elif self.method == "morgan":
            for m in mols:
                feats.append(morgan_fingerprint(m, radius=self.radius, n_bits=self.n_bits, use_chirality=self.use_chirality, use_counts=False))
        elif self.method == "morgan_counts":
            for m in mols:
                feats.append(morgan_fingerprint(m, radius=self.radius, n_bits=self.n_bits, use_chirality=self.use_chirality, use_counts=True))
        else:
            raise ValueError(f"Unknown method: {self.method}")

        Xf = np.vstack(feats).astype(np.float32)
        # replace inf with nan then impute later
        Xf[~np.isfinite(Xf)] = np.nan
        return Xf


# -----------------------------
# Datasets and splitting
# -----------------------------

def filter_valid_smiles(smiles: Sequence[str], return_mask: bool = False) -> Union[List[str], Tuple[List[str], np.ndarray]]:
    smiles = list(smiles)
    mask = np.array([smiles_to_mol(s) is not None for s in smiles], dtype=bool)
    filtered = [s for s, ok in zip(smiles, mask) if ok]
    if return_mask:
        return filtered, mask
    return filtered


def load_smiles_csv(path: str, smiles_col: str = "SMILES", target_cols: Union[str, Sequence[str]] = "y") -> pd.DataFrame:
    df = pd.read_csv(path)
    cols = [smiles_col] if isinstance(smiles_col, str) else list(smiles_col)
    tcols = [target_cols] if isinstance(target_cols, str) else list(target_cols)
    missing = [c for c in (cols + tcols) if c not in df.columns]
    if missing:
        raise ValueError(f"Missing columns in CSV: {missing}")
    return df[cols + tcols].copy()


def stratified_or_regular_split(
    X: Sequence[Any],
    y: Sequence[Any],
    test_size: float = 0.2,
    random_state: int = 0,
    task: Optional[str] = None,
):
    task = task or infer_task(y)
    stratify = None
    if task == "classification":
        y_arr = np.asarray(y)
        # ensure stratify feasible
        try:
            _, counts = np.unique(y_arr, return_counts=True)
            if np.all(counts > 1):
                stratify = y_arr
        except Exception:
            stratify = None
    return train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=stratify)


# -----------------------------
# Modeling utilities
# -----------------------------

def default_estimators(task: str = "regression") -> Dict[str, Tuple[BaseEstimator, Dict[str, Any]]]:
    if task == "classification":
        return {
            "rf": (RandomForestClassifier(n_estimators=300, random_state=0, n_jobs=-1, class_weight="balanced"), {}),
            "logreg": (LogisticRegression(max_iter=2000, n_jobs=-1), {"C": [0.1, 1.0, 10.0]}),
            "svc": (SVC(kernel="rbf", probability=True), {"C": [0.1, 1, 10], "gamma": ["scale", 0.1, 0.01]}),
            "knn": (KNeighborsClassifier(), {"n_neighbors": [3, 5, 11]}),
            "gbc": (GradientBoostingClassifier(), {"n_estimators": [100, 300], "learning_rate": [0.05, 0.1]}),
        }
    else:
        return {
            "rf": (RandomForestRegressor(n_estimators=400, random_state=0, n_jobs=-1), {}),
            "ridge": (Ridge(), {"alpha": [0.1, 1.0, 10.0]}),
            "lasso": (Lasso(max_iter=5000), {"alpha": [0.001, 0.01, 0.1, 1.0]}),
            "svr": (SVR(kernel="rbf"), {"C": [1, 10], "gamma": ["scale", 0.1, 0.01]}),
            "knn": (KNeighborsRegressor(), {"n_neighbors": [3, 5, 11]}),
            "gbr": (GradientBoostingRegressor(), {"n_estimators": [200, 400], "learning_rate": [0.05, 0.1]}),
        }


def build_pipeline(
    task: str,
    featurizer: Optional[MoleculeFeaturizer] = None,
    estimator: Optional[BaseEstimator] = None,
    impute: bool = True,
    scale: Optional[bool] = None,
) -> Pipeline:
    # defaults
    if featurizer is None:
        featurizer = MoleculeFeaturizer(method="morgan", radius=2, n_bits=2048, use_chirality=True)
    if estimator is None:
        est = default_estimators(task)["rf"][0]
    else:
        est = estimator

    # decide scaling:
    # - RDKit descriptors benefit from scaling
    # - Linear/SVM models benefit from scaling
    # - Fingerprints for tree models do not need scaling
    if scale is None:
        need_scaling = (getattr(featurizer, "method", "") == "rdkit") or isinstance(est, (SVR, SVC, Ridge, Lasso, KNeighborsRegressor, KNeighborsClassifier, LogisticRegression))
    else:
        need_scaling = scale

    steps: List[Tuple[str, Any]] = [("featurizer", featurizer)]
    if impute:
        steps.append(("imputer", SimpleImputer(strategy="median")))
    if need_scaling:
        steps.append(("scaler", StandardScaler(with_mean=True, with_std=True)))
    steps.append(("model", est))
    return Pipeline(steps)


def score_classification(y_true, y_pred, y_proba=None) -> Dict[str, float]:
    res = {}
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    res["accuracy"] = float(accuracy_score(y_true, y_pred))
    res["balanced_accuracy"] = float(balanced_accuracy_score(y_true, y_pred))
    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary" if len(np.unique(y_true)) == 2 else "macro", zero_division=0)
    res["precision"] = float(pr)
    res["recall"] = float(rc)
    res["f1"] = float(f1)
    if y_proba is not None:
        try:
            if y_proba.ndim == 1 or y_proba.shape[1] == 1:
                # already probabilities of positive class
                auc = roc_auc_score(y_true, y_proba.ravel())
            elif y_proba.shape[1] == 2:
                auc = roc_auc_score(y_true, y_proba[:, 1])
            else:
                # macro-average one-vs-rest
                auc = roc_auc_score(y_true, y_proba, multi_class="ovr", average="macro")
            res["roc_auc"] = float(auc)
        except Exception:
            pass
    return res


def score_regression(y_true, y_pred) -> Dict[str, float]:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    rmse = math.sqrt(mean_squared_error(y_true, y_pred))
    return {
        "r2": float(r2_score(y_true, y_pred)),
        "mae": float(mean_absolute_error(y_true, y_pred)),
        "rmse": float(rmse),
    }


def evaluate_model(
    model: Pipeline,
    X_test: Sequence[Union[str, Chem.Mol]],
    y_test: Sequence[Any],
    task: Optional[str] = None,
) -> Dict[str, float]:
    task = task or infer_task(y_test)
    y_pred = model.predict(X_test)
    if task == "classification":
        y_proba = None
        try:
            y_proba = model.predict_proba(X_test)
        except Exception:
            pass
        return score_classification(y_test, y_pred, y_proba)
    else:
        return score_regression(y_test, y_pred)


def cross_validate_model(
    model: Pipeline,
    X: Sequence[Union[str, Chem.Mol]],
    y: Sequence[Any],
    task: Optional[str] = None,
    cv: int = 5,
    n_jobs: int = -1,
) -> Dict[str, float]:
    task = task or infer_task(y)
    if task == "classification":
        kf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=0)
        scoring = ["accuracy", "balanced_accuracy", "f1"]
    else:
        kf = KFold(n_splits=cv, shuffle=True, random_state=0)
        scoring = ["r2", "neg_mean_absolute_error", "neg_root_mean_squared_error"]
    cvres = cross_validate(model, X, y, cv=kf, scoring=scoring, n_jobs=n_jobs, error_score="raise")
    # average the metrics (convert signs back)
    out: Dict[str, float] = {}
    for k, vals in cvres.items():
        if not k.startswith("test_"):
            continue
        name = k[len("test_") :]
        val = np.mean(vals)
        if name.startswith("neg_"):
            name_out = name.replace("neg_", "")
            out[name_out] = float(-val)
        else:
            out[name] = float(val)
    return out


def tune_hyperparameters(
    base_pipeline: Pipeline,
    param_grid: Dict[str, Any],
    X: Sequence[Union[str, Chem.Mol]],
    y: Sequence[Any],
    task: Optional[str] = None,
    search: str = "random",
    n_iter: int = 25,
    cv: int = 5,
    n_jobs: int = -1,
    random_state: int = 0,
) -> Tuple[Pipeline, Dict[str, Any]]:
    task = task or infer_task(y)
    if task == "classification":
        kf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)
        scoring = "roc_auc"  # reasonable default
    else:
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        scoring = "neg_root_mean_squared_error"

    if search == "grid":
        searcher = GridSearchCV(base_pipeline, param_grid=param_grid, scoring=scoring, n_jobs=n_jobs, cv=kf, refit=True)
    else:
        searcher = RandomizedSearchCV(
            base_pipeline,
            param_distributions=param_grid,
            scoring=scoring,
            n_iter=n_iter,
            n_jobs=n_jobs,
            cv=kf,
            random_state=random_state,
            refit=True,
        )
    searcher.fit(X, y)
    return searcher.best_estimator_, searcher.best_params_


def quick_fit(
    smiles: Sequence[str],
    y: Sequence[Any],
    task: Optional[str] = None,
    featurization: str = "morgan",
    test_size: float = 0.2,
    random_state: int = 0,
) -> Tuple[Pipeline, Dict[str, float], Dict[str, float]]:
    """
    Convenience function:
    - splits data
    - builds a default pipeline
    - trains and evaluates
    Returns: (fitted_model, train_metrics, test_metrics)
    """
    task = task or infer_task(y)
    X_train, X_test, y_train, y_test = stratified_or_regular_split(smiles, y, test_size=test_size, random_state=random_state, task=task)
    featurizer = MoleculeFeaturizer(method=featurization)
    pipe = build_pipeline(task=task, featurizer=featurizer, estimator=None)
    pipe.fit(X_train, y_train)
    train_metrics = evaluate_model(pipe, X_train, y_train, task=task)
    test_metrics = evaluate_model(pipe, X_test, y_test, task=task)
    return pipe, train_metrics, test_metrics


# -----------------------------
# Model persistence
# -----------------------------

def save_model(model: Pipeline, path: str) -> None:
    dump(model, path)


def load_model(path: str) -> Pipeline:
    return load(path)


# -----------------------------
# Example parameter grids
# -----------------------------

def example_param_grid(task: str = "classification", featurizer_prefix: str = "featurizer", model_prefix: str = "model") -> Dict[str, Any]:
    """
    Provide a reasonable param grid for tuning a Morgan+RandomForest pipeline.
    Use with tune_hyperparameters.
    """
    if task == "classification":
        return {
            f"{featurizer_prefix}__method": ["morgan", "morgan_counts"],
            f"{featurizer_prefix}__radius": [2, 3],
            f"{featurizer_prefix}__n_bits": [1024, 2048],
            f"{model_prefix}__n_estimators": [200, 400, 800],
            f"{model_prefix}__max_depth": [None, 10, 20],
            f"{model_prefix}__min_samples_split": [2, 5],
        }
    else:
        return {
            f"{featurizer_prefix}__method": ["morgan", "morgan_counts", "rdkit"],
            f"{featurizer_prefix}__radius": [2, 3],
            f"{featurizer_prefix}__n_bits": [1024, 2048],
            f"{model_prefix}__n_estimators": [300, 600, 1000],
            f"{model_prefix}__max_depth": [None, 12, 24],
            f"{model_prefix}__min_samples_split": [2, 5],
        }


# -----------------------------
# Optional: similarity and basic chemistry helpers
# -----------------------------

def tanimoto_similarity_from_smiles(smi_a: str, smi_b: str, radius: int = 2, n_bits: int = 2048) -> Optional[float]:
    try:
        ma = smiles_to_mol(smi_a)
        mb = smiles_to_mol(smi_b)
        if ma is None or mb is None:
            return None
        fa = AllChem.GetMorganFingerprintAsBitVect(ma, radius, nBits=n_bits)
        fb = AllChem.GetMorganFingerprintAsBitVect(mb, radius, nBits=n_bits)
        return float(Chem.DataStructs.TanimotoSimilarity(fa, fb))
    except Exception:
        return None


def add_hydrogens_and_embed(mol: Chem.Mol, num_confs: int = 1, random_seed: int = 0) -> Optional[Chem.Mol]:
    if mol is None:
        return None
    try:
        m = Chem.AddHs(Chem.Mol(mol))
        params = AllChem.ETKDGv3()
        params.randomSeed = random_seed
        AllChem.EmbedMultipleConfs(m, numConfs=num_confs, params=params)
        return m
    except Exception:
        return None


# -----------------------------
# Minimal usage example (commented)
# -----------------------------
# if __name__ == "__main__":
#     smiles = ["CCO", "CCN", "CCCl", "c1ccccc1", "CC(=O)O"]
#     y_class = [0, 1, 1, 0, 0]
#     model, train_m, test_m = quick_fit(smiles, y_class, task="classification", featurization="morgan")
#     print("Train:", train_m)
#     print("Test:", test_m)
------------------------------------------------------------

PROMPT 295:
"Generate Python code that implements a Trino adapter plugin for dbt (data build tool) to connect to Trino databases, fetch schema information, and execute SQL queries, leveraging the dbt framework to transform and load data into Trino tables."
RESPONSE:
Below is a minimal, self-contained Python adapter package that lets dbt talk to Trino: open connections, execute SQL, and fetch schema/column information. It’s intentionally small for clarity and not production-grade, but it demonstrates the key integration points with dbt’s adapter framework and the Trino Python client.

File: setup.py
--------------------------------
from setuptools import setup, find_packages

setup(
    name="dbt-trino-simple",
    version="0.1.0",
    description="A minimal Trino adapter plugin for dbt",
    packages=find_packages(),
    install_requires=[
        "dbt-core>=1.7.0",
        "trino>=0.328.0",
    ],
    entry_points={
        "dbt.adapters": [
            # Register the plugin so dbt can discover it via type: trino_simple
            "trino_simple = trino_simple:Plugin",
        ],
    },
    include_package_data=True,
)

File: trino_simple/__init__.py
--------------------------------
from dbt.adapters.base import AdapterPlugin
from .adapter import TrinoAdapter
from .credentials import TrinoCredentials

Plugin = AdapterPlugin(
    adapter=TrinoAdapter,
    credentials=TrinoCredentials,
    # include_path can be set to your macros dir if/when you add adapter-specific macros
    # include_path="dbt/include/trino_simple"
)

__all__ = ["Plugin", "TrinoAdapter", "TrinoCredentials"]

File: trino_simple/credentials.py
--------------------------------
from dataclasses import dataclass
from typing import List, Optional, Dict, Any
from dbt.adapters.base import Credentials


@dataclass
class TrinoCredentials(Credentials):
    host: str
    port: int = 8080
    user: str = "dbt"
    catalog: str = "hive"
    schema: str = "default"

    # Connection optional settings
    http_scheme: str = "http"  # http or https
    verify: Optional[bool] = True  # TLS verification if https
    session_properties: Optional[Dict[str, Any]] = None

    # Simple auth support (none or basic)
    auth_method: Optional[str] = None  # "basic" or None
    password: Optional[str] = None

    @property
    def type(self) -> str:
        # This string is what you use as `type:` in profiles.yml
        return "trino_simple"

    def _connection_keys(self) -> List[str]:
        # These keys show up in debug logs for connection info
        return [
            "host",
            "port",
            "user",
            "catalog",
            "schema",
            "http_scheme",
            "verify",
            "auth_method",
        ]

File: trino_simple/connection.py
--------------------------------
from contextlib import contextmanager
from typing import Any, Optional, Tuple
import trino
from trino.auth import BasicAuthentication
from dbt.adapters.base import BaseConnectionManager
from dbt.contracts.connection import AdapterResponse
from dbt.exceptions import RuntimeException


class TrinoConnectionManager(BaseConnectionManager):
    TYPE = "trino_simple"

    @classmethod
    def open(cls, connection):
        if connection.state == "open" and connection.handle is not None:
            return connection

        creds = connection.credentials
        auth = None
        if creds.auth_method == "basic" and creds.password:
            auth = BasicAuthentication(creds.user, creds.password)

        try:
            handle = trino.dbapi.connect(
                host=creds.host,
                port=creds.port,
                user=creds.user,
                catalog=creds.catalog,
                schema=creds.schema,
                http_scheme=creds.http_scheme,
                verify=creds.verify,
                auth=auth,
                session_properties=creds.session_properties or {},
            )
        except Exception as exc:
            raise RuntimeException(f"Failed to open Trino connection: {exc}") from exc

        connection.handle = handle
        connection.state = "open"
        return connection

    @classmethod
    def get_response(cls, cursor) -> AdapterResponse:
        # Trino's rowcount is often -1 or None; keep it None if unknown
        rows = cursor.rowcount if cursor.rowcount not in (-1, None) else None
        return AdapterResponse(_message="OK", rows_affected=rows, code="SUCCESS")

    def cancel_open(self):
        # Trino DB-API does not provide a straightforward cancel for an opening connection
        pass

    @contextmanager
    def exception_handler(self, sql: str):
        try:
            yield
        except Exception as exc:
            self.release()
            raise RuntimeException(f"Trino query error: {exc}\nSQL: {sql}") from exc

    def add_query(
        self,
        sql: str,
        auto_begin: bool = False,
        bindings: Optional[Tuple[Any, ...]] = None,
        abridge_sql_log: bool = False,
    ):
        # bindings are generally not used with trino.dbapi; prefer full SQL strings
        connection = self.get_thread_connection()
        if connection.state != "open":
            connection = self.open(connection)

        with self.exception_handler(sql):
            cursor = connection.handle.cursor()
            cursor.execute(sql)
            try:
                data = cursor.fetchall()
            except Exception:
                data = None
            response = self.get_response(cursor)

        return response, data

File: trino_simple/relations.py
--------------------------------
from dataclasses import dataclass
from dbt.adapters.base.relation import BaseRelation, Policy


@dataclass(frozen=True, eq=False, repr=False)
class TrinoRelation(BaseRelation):
    # Trino accepts quoted identifiers ("name")
    quote_policy: Policy = Policy(database=True, schema=True, identifier=True)
    include_policy: Policy = Policy(database=True, schema=True, identifier=True)

    # In Trino terms:
    # database -> catalog
    # schema -> schema
    # identifier -> table/view name

File: trino_simple/adapter.py
--------------------------------
from typing import List, Optional
from dbt.adapters.sql.impl import SQLAdapter
from dbt.adapters.base.column import Column
from dbt.adapters.events.logging import AdapterLogger
from dbt.contracts.connection import AdapterResponse
from .credentials import TrinoCredentials
from .connection import TrinoConnectionManager
from .relations import TrinoRelation

logger = AdapterLogger("TrinoAdapter")


class TrinoAdapter(SQLAdapter):
    # Core adapter hookup
    ConnectionManager = TrinoConnectionManager
    Relation = TrinoRelation
    credentials_dataclass = TrinoCredentials

    # ---------- Helpers ----------
    def quoted_ident(self, name: Optional[str]) -> str:
        if name is None:
            return "null"
        return f'"{name.replace(\'"\', \'""\')}"'

    def literal(self, value: Optional[str]) -> str:
        if value is None:
            return "null"
        return f"'{value.replace(\"'\", \"''\")}'"

    @classmethod
    def date_function(cls) -> str:
        return "current_timestamp"

    @classmethod
    def convert_text_type(cls, agnostic_type: str) -> str:
        return "varchar"

    @classmethod
    def convert_number_type(cls, agnostic_type: str, size: Optional[int] = None) -> str:
        return "double"

    @classmethod
    def convert_boolean_type(cls, agnostic_type: str) -> str:
        return "boolean"

    # ---------- Basic schema management ----------
    def create_schema(self, relation: TrinoRelation) -> None:
        # CREATE SCHEMA IF NOT EXISTS "catalog"."schema"
        catalog = relation.database or self.config.credentials.catalog
        schema = relation.schema
        sql = f"CREATE SCHEMA IF NOT EXISTS {self.quoted_ident(catalog)}.{self.quoted_ident(schema)}"
        self.connections.add_query(sql)

    def drop_schema(self, relation: TrinoRelation) -> None:
        catalog = relation.database or self.config.credentials.catalog
        schema = relation.schema
        sql = f"DROP SCHEMA IF EXISTS {self.quoted_ident(catalog)}.{self.quoted_ident(schema)} CASCADE"
        self.connections.add_query(sql)

    def list_schemas(self, database: Optional[str]) -> List[str]:
        catalog = database or self.config.credentials.catalog
        sql = f"SHOW SCHEMAS FROM {self.quoted_ident(catalog)}"
        _, rows = self.connections.add_query(sql)
        return [r[0] for r in rows] if rows else []

    def check_schema_exists(self, database: Optional[str], schema: str) -> bool:
        return schema in self.list_schemas(database)

    # ---------- Introspection ----------
    def list_relations_without_caching(self, schema_relation: TrinoRelation) -> List[TrinoRelation]:
        # Query information_schema for tables and views in the requested schema
        catalog = schema_relation.database or self.config.credentials.catalog
        schema = schema_relation.schema
        sql = f"""
        SELECT table_catalog, table_schema, table_name, table_type
        FROM {self.quoted_ident(catalog)}.information_schema.tables
        WHERE table_schema = {self.literal(schema)}
        """
        _, rows = self.connections.add_query(sql)
        results: List[TrinoRelation] = []
        if rows:
            for _catalog, _schema, name, ttype in rows:
                rel_type = "view" if ttype and "VIEW" in ttype.upper() else "table"
                results.append(
                    self.Relation.create(
                        database=catalog,
                        schema=_schema,
                        identifier=name,
                        type=rel_type,
                    )
                )
        return results

    def get_columns_in_relation(self, relation: TrinoRelation) -> List[Column]:
        catalog = relation.database or self.config.credentials.catalog
        sql = f"""
        SELECT column_name, data_type
        FROM {self.quoted_ident(catalog)}.information_schema.columns
        WHERE table_schema = {self.literal(relation.schema)}
          AND table_name   = {self.literal(relation.identifier)}
        ORDER BY ordinal_position
        """
        _, rows = self.connections.add_query(sql)
        cols: List[Column] = []
        if rows:
            for name, dtype in rows:
                cols.append(self.Column.create(name=name, data_type=dtype))
        return cols

    # ---------- Execution helpers ----------
    def execute(self, sql: str) -> AdapterResponse:
        response, _ = self.connections.add_query(sql)
        return response

    def fetchall(self, sql: str):
        _, rows = self.connections.add_query(sql)
        return rows or []


How to use
- Install this package into the same environment as dbt-core: pip install -e .
- Create a profiles.yml entry using type: trino_simple:
  my_trino_profile:
    target: dev
    outputs:
      dev:
        type: trino_simple
        host: your-trino-host
        port: 8080
        user: your-user
        catalog: hive
        schema: analytics
        http_scheme: http
        # auth_method: basic
        # password: your-password

- In your dbt project’s dbt_project.yml, set profile: my_trino_profile.
- Run dbt debug to confirm connection.
- You can create models as usual; dbt will use this adapter to connect, execute SQL, and introspect schemas.

Notes
- This example focuses on connection management, SQL execution, and basic catalog/schema/columns introspection.
- For production use, you would typically add adapter-specific macros (materializations for table/view/incremental, quoting rules, type conversions, etc.), extended authentication modes, and more robust error handling and testing.
- The adapter name and type are trino_simple to avoid colliding with the official dbt-trino package. If you install the official adapter, do not use the same type name.
------------------------------------------------------------

PROMPT 296:
"Generate Python code that uses the `requests` library to make API calls to the Materials Project website, which returns data on various materials and their properties, and stores the data in Pandas DataFrames for easy analysis and manipulation."
RESPONSE:
# Python 3.x
# Requires: requests, pandas
# pip install requests pandas

import os
import time
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import pandas as pd
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class MaterialsProjectClient:
    """
    Lightweight client for the Materials Project v2 API using requests.
    - Uses API key via environment variable MP_API_KEY (or pass explicitly).
    - Provides helpers to:
        * Run paginated searches (e.g., materials/summary) with filters/fields
        * Fetch data for specific material_ids in chunks (e.g., materials/elasticity)
    - Stores results in pandas DataFrames.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: str = "https://api.materialsproject.org",
        timeout: int = 30,
        max_retries: int = 5,
        backoff_factor: float = 0.5,
    ):
        self.api_key = api_key or os.environ.get("MP_API_KEY")
        if not self.api_key:
            raise RuntimeError(
                "No API key provided. Set MP_API_KEY env var or pass api_key=..."
            )
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout

        # Robust session with retries (handles 429, 5xx, timeouts, etc.)
        self.session = requests.Session()
        retry = Retry(
            total=max_retries,
            read=max_retries,
            connect=max_retries,
            backoff_factor=backoff_factor,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=frozenset(["GET", "POST"]),
            raise_on_status=False,
        )
        adapter = HTTPAdapter(max_retries=retry)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)
        self.headers = {
            "Accept": "application/json",
            "Content-Type": "application/json",
            "X-API-KEY": self.api_key,
        }

    def _request(
        self,
        method: str,
        endpoint: str,
        params: Optional[Dict] = None,
        json: Optional[Dict] = None,
    ) -> Dict:
        url = f"{self.base_url}/{endpoint.lstrip('/')}"
        resp = self.session.request(
            method=method, url=url, headers=self.headers, params=params, json=json, timeout=self.timeout
        )
        # Basic handling for rate limiting if Retry-After is present (complements urllib3 Retry)
        if resp.status_code == 429 and "Retry-After" in resp.headers:
            wait = int(resp.headers["Retry-After"])
            time.sleep(wait)
            resp = self.session.request(
                method=method, url=url, headers=self.headers, params=params, json=json, timeout=self.timeout
            )

        if not resp.ok:
            try:
                detail = resp.json()
            except Exception:
                detail = resp.text
            raise RuntimeError(f"API error {resp.status_code} for {url}: {detail}")

        return resp.json()

    def fetch_paginated_post(
        self,
        endpoint: str,
        criteria: Optional[Dict] = None,
        properties: Optional[Sequence[str]] = None,
        per_page: int = 200,
        max_pages: Optional[int] = None,
        extra_payload: Optional[Dict] = None,
    ) -> List[Dict]:
        """
        POST-based pagination, common for MP v2 endpoints (e.g., /materials/summary).
        """
        page = 1
        results: List[Dict] = []
        while True:
            payload = {
                "criteria": criteria or {},
                "properties": list(properties) if properties else None,
                "limit": per_page,
                "page": page,
            }
            if extra_payload:
                payload.update(extra_payload)

            data = self._request("POST", endpoint, json=payload)
            items = data.get("data", [])
            results.extend(items)

            meta = data.get("meta", {})
            total_pages = meta.get("total_pages")
            # Stop when there are no more items or we reach total_pages/max_pages
            if not items:
                break
            if max_pages and page >= max_pages:
                break
            if total_pages and page >= total_pages:
                break

            page += 1
        return results

    def fetch_by_ids_post(
        self,
        endpoint: str,
        material_ids: Sequence[str],
        properties: Optional[Sequence[str]] = None,
        chunk_size: int = 200,
        extra_payload: Optional[Dict] = None,
    ) -> List[Dict]:
        """
        POST in chunks using material_ids body field (common in many endpoints).
        """
        all_rows: List[Dict] = []
        ids = list(material_ids)
        for i in range(0, len(ids), chunk_size):
            chunk = ids[i : i + chunk_size]
            payload = {
                "material_ids": chunk,
                "properties": list(properties) if properties else None,
                "limit": chunk_size,
                "page": 1,
            }
            if extra_payload:
                payload.update(extra_payload)

            data = self._request("POST", endpoint, json=payload)
            all_rows.extend(data.get("data", []))
        return all_rows

    # Convenience helpers returning DataFrames

    def materials_summary_df(
        self,
        criteria: Optional[Dict] = None,
        properties: Optional[Sequence[str]] = None,
        per_page: int = 200,
        max_pages: Optional[int] = None,
    ) -> pd.DataFrame:
        rows = self.fetch_paginated_post(
            "materials/summary", criteria=criteria, properties=properties, per_page=per_page, max_pages=max_pages
        )
        # json_normalize flattens nested dicts (e.g., fields inside "symmetry")
        return pd.json_normalize(rows)

    def materials_thermo_df(
        self,
        material_ids: Sequence[str],
        properties: Optional[Sequence[str]] = None,
        chunk_size: int = 200,
    ) -> pd.DataFrame:
        rows = self.fetch_by_ids_post(
            "materials/thermo", material_ids=material_ids, properties=properties, chunk_size=chunk_size
        )
        return pd.json_normalize(rows)

    def materials_elasticity_df(
        self,
        material_ids: Sequence[str],
        properties: Optional[Sequence[str]] = None,
        chunk_size: int = 200,
    ) -> pd.DataFrame:
        rows = self.fetch_by_ids_post(
            "materials/elasticity", material_ids=material_ids, properties=properties, chunk_size=chunk_size
        )
        return pd.json_normalize(rows)


if __name__ == "__main__":
    """
    Example usage:
    - Query the materials summary endpoint for Li–O containing compounds with <= 3 elements.
    - Bring back common properties.
    - Then fetch elasticity properties for those materials and merge into a single DataFrame.
    """
    # Initialize client (reads MP_API_KEY from environment)
    client = MaterialsProjectClient()

    # Define search criteria (Mongo-like syntax)
    criteria = {
        # Contains both Li and O
        "elements": {"$all": ["Li", "O"]},
        # Up to ternaries to keep result sizes moderate
        "nelements": {"$lte": 3},
        # Optional: only stable materials
        # "is_stable": True,
    }

    # Select fields to pull from the summary endpoint
    summary_fields = [
        "material_id",
        "formula_pretty",
        "elements",
        "nelements",
        "band_gap",
        "e_above_hull",
        "formation_energy_per_atom",
        "density",
        "volume",
        "nsites",
        "theoretical",
        "is_stable",
        # You can include nested fields; json_normalize will flatten them:
        # "symmetry.symbol",
        # "symmetry.number",
    ]

    # Fetch summary data (increase max_pages or remove it to fetch all)
    df_summary = client.materials_summary_df(
        criteria=criteria,
        properties=summary_fields,
        per_page=200,
        max_pages=5,  # set to None to retrieve all pages (may be large)
    )
    print(f"Summary rows: {len(df_summary)}")
    print(df_summary.head(3))

    # Fetch additional datasets by material_id and merge
    material_ids = df_summary["material_id"].dropna().unique().tolist()

    # Elasticity properties (bulk modulus, shear modulus, etc.)
    elasticity_fields = [
        "material_id",
        "K_VRH",         # Bulk modulus (GPa)
        "G_VRH",         # Shear modulus (GPa)
        "poisson_ratio",
        "universal_anisotropy",
    ]
    if material_ids:
        df_elastic = client.materials_elasticity_df(material_ids, properties=elasticity_fields, chunk_size=150)
        print(f"Elasticity rows: {len(df_elastic)}")

        # Merge on material_id
        df_merged = df_summary.merge(df_elastic, on="material_id", how="left", suffixes=("", "_elastic"))
    else:
        df_merged = df_summary

    print("Merged DataFrame preview:")
    print(df_merged.head(5))

    # Optional: fetch thermodynamics and merge
    thermo_fields = [
        "material_id",
        "e_above_hull",
        "formation_energy_per_atom",
        # add more thermo fields as needed
    ]
    if material_ids:
        df_thermo = client.materials_thermo_df(material_ids, properties=thermo_fields, chunk_size=150)
        print(f"Thermo rows: {len(df_thermo)}")
        df_final = df_merged.merge(df_thermo, on="material_id", how="left", suffixes=("", "_thermo"))
    else:
        df_final = df_merged

    print("Final DataFrame columns:")
    print(df_final.columns.tolist())

    # Example: save to CSV
    # df_final.to_csv("materials_li_o.csv", index=False)
------------------------------------------------------------

PROMPT 297:
"Generate Python code that imports the deprecation module and creates a wrapper function that accepts a deprecated alias and a new name, and uses the deprecation module to deprecate the alias while still allowing access to the original function."
RESPONSE:
import inspect
import deprecation


def make_deprecated_alias(
    alias_name,
    new_func,
    *,
    deprecated_in=None,
    removed_in=None,
    current_version=None,
    details=None,
):
    """
    Create a callable alias for `new_func` that emits a deprecation warning
    when used, but still calls through to `new_func`.

    Returns the alias function; you can assign it into your module's globals()
    or any namespace as needed.
    """
    if details is None:
        details = f"Use {new_func.__name__} instead."

    decorator = deprecation.deprecated(
        deprecated_in=deprecated_in,
        removed_in=removed_in,
        current_version=current_version,
        details=details,
    )

    def _alias(*args, **kwargs):
        return new_func(*args, **kwargs)

    # Wrap with deprecation
    _alias = decorator(_alias)

    # Make it look like a real alias
    _alias.__name__ = alias_name
    _alias.__qualname__ = alias_name
    _alias.__doc__ = new_func.__doc__
    _alias.__module__ = new_func.__module__
    return _alias


def deprecate_function_alias(
    alias_name,
    new,
    *,
    deprecated_in=None,
    removed_in=None,
    current_version=None,
    namespace=None,
    details=None,
):
    """
    Create and optionally register a deprecated alias for a function.

    Parameters:
      alias_name: str name of the deprecated alias to create.
      new: the new function object or the string name of the new function.
      deprecated_in, removed_in, current_version, details:
          forwarded to deprecation.deprecated().
      namespace: dict to insert alias into (e.g., globals()).
                 If `new` is a string and `namespace` is None, the caller's
                 globals() will be used to resolve it.

    Returns:
      The alias function.
    """
    # Resolve new_func from string if needed
    if isinstance(new, str):
        if namespace is None:
            namespace = inspect.currentframe().f_back.f_globals
        new_func = namespace[new]
    else:
        new_func = new

    alias = make_deprecated_alias(
        alias_name,
        new_func,
        deprecated_in=deprecated_in,
        removed_in=removed_in,
        current_version=current_version,
        details=details,
    )

    if namespace is not None:
        namespace[alias_name] = alias

    return alias


# Example usage
if __name__ == "__main__":
    __version__ = "1.3.0"

    def new_sum(a, b):
        "Add two numbers."
        return a + b

    # Create alias 'old_sum' that warns but still calls new_sum
    old_sum = deprecate_function_alias(
        "old_sum",
        new_sum,
        deprecated_in="1.2",
        removed_in="2.0",
        current_version=__version__,
        details="old_sum is deprecated; call new_sum instead.",
    )

    print("Result:", old_sum(2, 3))  # Triggers a deprecation warning, returns 5
    print("Direct:", new_sum(2, 3))  # No warning, returns 5
------------------------------------------------------------

PROMPT 298:
"Generate Python code that imports the requests library and uses it to make an HTTP request to the Geocod.io API, passing in the necessary parameters such as API key and address information, and returns a dictionary containing the geocoded data."
RESPONSE:
import requests

def geocode(api_key, address=None, *, street=None, city=None, state=None, postal_code=None, country=None, fields=None, limit=None, timeout=10):
    """
    Geocode an address using the Geocod.io API.

    Provide either:
      - address: a single-line address string (e.g., "1109 N Highland St, Arlington, VA 22201")
    or
      - street/city/state/postal_code components.

    Returns:
      dict: Parsed JSON response from Geocod.io.
    """
    base_url = "https://api.geocod.io/v1.7/geocode"

    if not api_key:
        raise ValueError("api_key is required")

    # Build query parameters
    params = {"api_key": api_key}

    if address:
        params["q"] = address
    else:
        if not street:
            raise ValueError("Provide either a full 'address' or at least 'street' with components")
        params["street"] = street
        if city:
            params["city"] = city
        if state:
            params["state"] = state
        if postal_code:
            params["postal_code"] = postal_code
        if country:
            params["country"] = country

    if fields:
        # Accept list/tuple or comma-separated string
        params["fields"] = ",".join(fields) if isinstance(fields, (list, tuple)) else str(fields)
    if limit is not None:
        params["limit"] = int(limit)

    resp = requests.get(base_url, params=params, timeout=timeout)
    resp.raise_for_status()
    return resp.json()

# Example usage:
# result = geocode(
#     api_key="YOUR_API_KEY",
#     address="1109 N Highland St, Arlington, VA 22201"
# )
# print(result)
------------------------------------------------------------

PROMPT 299:
"Generate Python code that implements a function parameter destructuring tool, allowing for easy extraction of values from tuples, lists, and dictionaries passed into functions, using the `inspect` module to access function parameter information."
RESPONSE:
# Destructuring decorator for function parameters
# - Lets callers pass composite args (tuples/lists/dicts) under a single name
# - Automatically expands them into the function's expected parameter names
# - Uses inspect to read and build signatures, and to provide helpful errors

import inspect
from inspect import Signature, Parameter
from functools import wraps


_EMPTY = inspect._empty  # sentinel


class DestructureError(TypeError):
    pass


def _is_sequence(x):
    return isinstance(x, (list, tuple))


def _extract(value, pattern, path=""):
    """
    Recursively destructure 'value' according to 'pattern' into a flat dict.
    pattern may be:
      - str: binds the entire value to that name
      - list/tuple: sequence pattern, elements can be str or nested patterns
      - dict: mapping pattern, keys look up in value, values are names/patterns
      - None: ignore this value (useful placeholder)
    """
    bindings = {}

    def add_binding(name, v, loc):
        if name is None or name == "_":
            return
        if not isinstance(name, str):
            raise DestructureError(f"Invalid target name at {loc!r}: expected str/None, got {type(name).__name__}")
        if name in bindings and bindings[name] is not v and bindings[name] != v:
            raise DestructureError(f"Conflicting values for '{name}' at {loc!r}")
        bindings[name] = v

    if isinstance(pattern, str) or pattern is None:
        add_binding(pattern, value, path or "<root>")
        return bindings

    if _is_sequence(pattern):
        if not _is_sequence(value):
            raise DestructureError(f"At {path or '<root>'}: expected sequence, got {type(value).__name__}")
        if len(value) != len(pattern):
            raise DestructureError(f"At {path or '<root>'}: expected sequence of length {len(pattern)}, got {len(value)}")
        for i, (subv, subp) in enumerate(zip(value, pattern)):
            subpath = f"{path}[{i}]" if path else f"[{i}]"
            if isinstance(subp, (str, type(None))):
                add_binding(subp, subv, subpath)
            else:
                sub_bindings = _extract(subv, subp, subpath)
                for k, v in sub_bindings.items():
                    if k in bindings and bindings[k] is not v and bindings[k] != v:
                        raise DestructureError(f"Conflicting values for '{k}' at {subpath!r}")
                    bindings[k] = v
        return bindings

    if isinstance(pattern, dict):
        if not isinstance(value, dict):
            raise DestructureError(f"At {path or '<root>'}: expected dict, got {type(value).__name__}")
        for k, subp in pattern.items():
            if k not in value:
                raise DestructureError(f"At {path or '<root>'}: missing key {k!r}")
            subv = value[k]
            subpath = f"{path}.{k}" if path else f".{k}"
            if isinstance(subp, (str, type(None))):
                add_binding(subp, subv, subpath)
            else:
                sub_bindings = _extract(subv, subp, subpath)
                for bk, bv in sub_bindings.items():
                    if bk in bindings and bindings[bk] is not bv and bindings[bk] != bv:
                        raise DestructureError(f"Conflicting values for '{bk}' at {subpath!r}")
                    bindings[bk] = bv
        return bindings

    raise DestructureError(f"Invalid pattern at {path or '<root>'}: {pattern!r}")


def _collect_target_names(pattern):
    """
    Get flat set of target parameter names referenced in a pattern.
    """
    names = set()
    def rec(p):
        if isinstance(p, str):
            if p != "_":
                names.add(p)
        elif p is None:
            return
        elif _is_sequence(p):
            for e in p:
                rec(e)
        elif isinstance(p, dict):
            for e in p.values():
                rec(e)
        else:
            raise DestructureError(f"Invalid pattern element: {p!r}")
    rec(pattern)
    return names


def _build_exposed_signature(fn_sig, patterns):
    """
    Make a merged signature that includes the function's own parameters plus
    keyword-only composite parameters for each pattern (with default None).
    """
    params = list(fn_sig.parameters.values())

    # composite names must not clash with function params
    fn_param_names = set(fn_sig.parameters.keys())
    for cname in patterns:
        if cname in fn_param_names:
            raise DestructureError(
                f"Composite parameter name {cname!r} collides with function parameter"
            )

    # add keyword-only composites with default None (optional)
    for cname in patterns:
        params.append(
            Parameter(
                cname,
                kind=Parameter.KEYWORD_ONLY,
                default=None,
                annotation=_EMPTY
            )
        )

    return Signature(parameters=params, return_annotation=fn_sig.return_annotation)


def destructure(**patterns):
    """
    Decorator enabling destructuring of composite call arguments.

    Usage:

        @destructure(
            point=("x", "y"),                  # sequence source -> x, y
            rgb=("r", "g", "b"),               # tuple/list both ok
            meta={"id": "user_id", "ts": "t"}  # dict source -> user_id, t
        )
        def fn(x, y, r, g, b, user_id, t=0):
            ...

        # callers can pass either:
        fn(point=(1, 2), rgb=[10, 20, 30], meta={"id": 42, "ts": 99})
        # or the fully expanded form:
        fn(x=1, y=2, r=10, g=20, b=30, user_id=42, t=99)

    Notes:
      - Patterns support nesting and ignoring with None or "_".
      - Composite parameters are exposed as keyword-only arguments.
      - Unknown arguments raise an error unless the function has **kwargs.
    """
    # Pre-validate pattern targets before decorating a function
    # We cannot fully validate until we know the function's param names.
    compiled = {}
    for cname, pat in patterns.items():
        if not isinstance(cname, str):
            raise DestructureError("Composite parameter names must be strings")
        # collect target names for early sanity
        targets = _collect_target_names(pat)
        compiled[cname] = (pat, targets)

    def decorator(fn):
        fn_sig = inspect.signature(fn)
        fn_param_names = set(fn_sig.parameters.keys())

        # Ensure all target names exist in the function signature
        for cname, (pat, targets) in compiled.items():
            missing = [t for t in targets if t not in fn_param_names]
            if missing:
                raise DestructureError(
                    f"Pattern {cname!r} targets names not found in function signature: {missing}"
                )

        # Build public signature exposing composite parameters
        public_sig = _build_exposed_signature(fn_sig, {k: v for k, (v, _) in compiled.items()})

        @wraps(fn)
        def wrapper(*args, **kwargs):
            # Bind against the public signature to normalize args/kwargs
            try:
                bound = public_sig.bind_partial(*args, **kwargs)
            except TypeError as e:
                raise DestructureError(str(e)) from None

            # Work on a mutable dict of arguments
            callmap = dict(bound.arguments)

            # Apply each composite pattern if provided
            for cname, (pat, _) in compiled.items():
                if cname in callmap and callmap[cname] is not None:
                    src = callmap.pop(cname)
                    assigns = _extract(src, pat, path=cname)
                    # Merge assigns into callmap with conflict checks
                    for k, v in assigns.items():
                        if k in callmap and callmap[k] is not v and callmap[k] != v:
                            raise DestructureError(
                                f"Conflicting values for '{k}': explicit {callmap[k]!r} vs from {cname!r} -> {v!r}"
                            )
                        callmap[k] = v

            # Now route arguments into the original function per its signature
            args_out = []
            kwargs_out = {}
            leftover = dict(callmap)  # we will pop consumed

            var_kw_param = None

            for p in fn_sig.parameters.values():
                if p.kind is Parameter.POSITIONAL_ONLY:
                    if p.name in leftover:
                        args_out.append(leftover.pop(p.name))
                    else:
                        if p.default is _EMPTY:
                            raise DestructureError(f"Missing required positional-only argument: {p.name!r}")
                        # else: omit to let default apply
                elif p.kind is Parameter.VAR_POSITIONAL:
                    varpos = leftover.pop(p.name, ())
                    if varpos is None:
                        varpos = ()
                    if not _is_sequence(varpos):
                        raise DestructureError(f"*{p.name} must be a sequence, got {type(varpos).__name__}")
                    args_out.extend(varpos)
                elif p.kind is Parameter.POSITIONAL_OR_KEYWORD:
                    if p.name in leftover:
                        kwargs_out[p.name] = leftover.pop(p.name)
                    else:
                        # leave missing to default if available; otherwise function will error if not provided
                        if p.default is _EMPTY:
                            # We cannot defer to function for missing non-default PK because we're building kwargs.
                            # Raise here for clarity.
                            raise DestructureError(f"Missing required argument: {p.name!r}")
                elif p.kind is Parameter.KEYWORD_ONLY:
                    if p.name in leftover:
                        kwargs_out[p.name] = leftover.pop(p.name)
                    else:
                        if p.default is _EMPTY:
                            raise DestructureError(f"Missing required keyword-only argument: {p.name!r}")
                elif p.kind is Parameter.VAR_KEYWORD:
                    var_kw_param = p.name
                else:
                    raise DestructureError(f"Unsupported parameter kind for {p.name!r}")

            if leftover:
                if var_kw_param is None:
                    unk = ", ".join(sorted(leftover.keys()))
                    raise DestructureError(f"Got unexpected arguments: {unk}")
                # feed leftovers into **kwargs
                kwargs_out[var_kw_param] = leftover

            return fn(*args_out, **kwargs_out)

        # Expose the merged signature for introspection
        wrapper.__signature__ = public_sig
        return wrapper

    return decorator


# ------------- Examples -------------
if __name__ == "__main__":
    @destructure(
        point=("x", "y"),
        rgb=("r", "g", "b"),
        meta={"id": "user_id", "ts": "timestamp"},
    )
    def demo(x, y, r, g, b, user_id, timestamp=0):
        return {
            "sum_xy": x + y,
            "sum_rgb": r + g + b,
            "uid": user_id,
            "ts": timestamp,
        }

    print(demo(point=(3, 4), rgb=[10, 20, 30], meta={"id": 7, "ts": 99}))
    print(demo(x=3, y=4, r=10, g=20, b=30, user_id=7, timestamp=99))

    # Nested patterns with ignore slots
    @destructure(
        payload={
            "coords": ("lat", "lon"),
            "colors": ("_", "green", "blue"),
            "extra": {"inner": ("ix", None, "iz")},
        }
    )
    def nested(lat, lon, green, blue, ix, iz):
        return lat, lon, green, blue, ix, iz

    print(nested(payload={"coords": (1.2, 3.4), "colors": (255, 12, 34), "extra": {"inner": (9, 8, 7)}}))
------------------------------------------------------------

PROMPT 300:
"Generate Python code that imports the necessary libraries for data collection and analysis, such as Pandas for data manipulation and NumPy for numerical computations, and creates a framework for collecting and analyzing data from various sources."
RESPONSE:
# Minimal, extensible framework for collecting and analyzing data from various sources

from __future__ import annotations

import json
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Callable, Dict, Iterable, List, Optional, Union

import numpy as np
import pandas as pd

# Optional dependencies
try:
    import requests  # For API data
except ImportError:
    requests = None  # type: ignore

try:
    import sqlalchemy as sa  # For SQL sources
except ImportError:
    sa = None  # type: ignore

import sqlite3  # Fallback for SQL with SQLite


# -----------------------------------------------------------------------------
# Logging setup
# -----------------------------------------------------------------------------
logger = logging.getLogger("data_framework")
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
logger.setLevel(logging.INFO)


# -----------------------------------------------------------------------------
# Data Source Interfaces and Implementations
# -----------------------------------------------------------------------------
class DataSource(ABC):
    """Abstract base class for any data source that can be fetched into a Pandas DataFrame."""

    def __init__(self, name: str):
        self.name = name

    @abstractmethod
    def fetch(self) -> pd.DataFrame:
        raise NotImplementedError


class CSVSource(DataSource):
    def __init__(self, name: str, path: str, **read_csv_kwargs):
        super().__init__(name)
        self.path = path
        self.read_csv_kwargs = read_csv_kwargs

    def fetch(self) -> pd.DataFrame:
        logger.info(f"[{self.name}] Loading CSV: {self.path}")
        df = pd.read_csv(self.path, **self.read_csv_kwargs)
        return df


class ExcelSource(DataSource):
    def __init__(self, name: str, path: str, sheet_name: Union[int, str, List[Union[int, str]]] = 0, **read_excel_kwargs):
        super().__init__(name)
        self.path = path
        self.sheet_name = sheet_name
        self.read_excel_kwargs = read_excel_kwargs

    def fetch(self) -> pd.DataFrame:
        logger.info(f"[{self.name}] Loading Excel: {self.path}, sheet: {self.sheet_name}")
        df = pd.read_excel(self.path, sheet_name=self.sheet_name, **self.read_excel_kwargs)
        # If multiple sheets returned as dict, concatenate
        if isinstance(df, dict):
            df = pd.concat(df.values(), ignore_index=True)
        return df


class JSONSource(DataSource):
    def __init__(self, name: str, path_or_url: str, orient: Optional[str] = None, lines: bool = False, **read_json_kwargs):
        super().__init__(name)
        self.path_or_url = path_or_url
        self.orient = orient
        self.lines = lines
        self.read_json_kwargs = read_json_kwargs

    def fetch(self) -> pd.DataFrame:
        logger.info(f"[{self.name}] Loading JSON: {self.path_or_url}")
        df = pd.read_json(self.path_or_url, orient=self.orient, lines=self.lines, **self.read_json_kwargs)
        return df


class ParquetSource(DataSource):
    def __init__(self, name: str, path: str, **read_parquet_kwargs):
        super().__init__(name)
        self.path = path
        self.read_parquet_kwargs = read_parquet_kwargs

    def fetch(self) -> pd.DataFrame:
        logger.info(f"[{self.name}] Loading Parquet: {self.path}")
        df = pd.read_parquet(self.path, **self.read_parquet_kwargs)
        return df


class APISource(DataSource):
    """
    Fetch JSON from an HTTP API and convert to a DataFrame.

    Parameters:
      - url: API endpoint
      - params, headers: request options
      - method: 'GET' or 'POST'
      - json_path: path to records within JSON (list of keys or dot-delimited string)
      - normalize: if True, use json_normalize on the records to flatten nested structures
      - records_key: an alternative key to directly extract list-like records
    """

    def __init__(
        self,
        name: str,
        url: str,
        params: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
        method: str = "GET",
        json_path: Optional[Union[str, List[Union[str, int]]]] = None,
        normalize: bool = True,
        timeout: int = 30,
    ):
        super().__init__(name)
        self.url = url
        self.params = params or {}
        self.headers = headers or {}
        self.method = method.upper()
        self.json_path = json_path
        self.normalize = normalize
        self.timeout = timeout

    def _resolve_json_path(self, obj: Any) -> Any:
        if self.json_path is None:
            return obj
        path = self.json_path
        if isinstance(path, str):
            keys: List[Union[str, int]] = []
            for token in path.split("."):
                if token.isdigit():
                    keys.append(int(token))
                else:
                    keys.append(token)
            path = keys
        cur = obj
        for key in path:
            if isinstance(cur, (list, tuple)) and isinstance(key, int) and 0 <= key < len(cur):
                cur = cur[key]
            elif isinstance(cur, dict) and key in cur:
                cur = cur[key]
            else:
                raise KeyError(f"json_path not found at key: {key}")
        return cur

    def fetch(self) -> pd.DataFrame:
        if requests is None:
            raise ImportError("requests is required for APISource. Please install with: pip install requests")
        logger.info(f"[{self.name}] Requesting API: {self.url}")
        if self.method == "GET":
            resp = requests.get(self.url, params=self.params, headers=self.headers, timeout=self.timeout)
        elif self.method == "POST":
            resp = requests.post(self.url, json=self.params, headers=self.headers, timeout=self.timeout)
        else:
            raise ValueError("Unsupported method for APISource; use GET or POST.")
        resp.raise_for_status()

        try:
            data = resp.json()
        except json.JSONDecodeError:
            raise ValueError(f"[{self.name}] Response is not valid JSON.")

        records = self._resolve_json_path(data)
        if isinstance(records, dict):
            # Try using values if they look like record list under single key
            if len(records) == 1 and isinstance(next(iter(records.values())), list):
                records = next(iter(records.values()))
            else:
                records = [records]

        if self.normalize:
            df = pd.json_normalize(records)
        else:
            df = pd.DataFrame(records)
        return df


class SQLSource(DataSource):
    """
    Fetch data from SQL databases.

    Either provide a SQLAlchemy connection string (recommended), or a sqlite3 path.

    Examples:
      SQLSource("users", conn_str="sqlite:///db.sqlite", query="SELECT * FROM users")
      SQLSource("events", sqlite_path=":memory:", query="SELECT 1 as x")
    """

    def __init__(self, name: str, query: str, conn_str: Optional[str] = None, sqlite_path: Optional[str] = None):
        super().__init__(name)
        if not conn_str and not sqlite_path:
            raise ValueError("Provide either conn_str (SQLAlchemy URL) or sqlite_path.")
        self.conn_str = conn_str
        self.sqlite_path = sqlite_path
        self.query = query

    def fetch(self) -> pd.DataFrame:
        logger.info(f"[{self.name}] Running SQL query")
        if self.conn_str:
            if sa is None:
                raise ImportError("sqlalchemy is required for SQLSource with conn_str. Install with: pip install sqlalchemy")
            engine = sa.create_engine(self.conn_str)
            with engine.begin() as conn:
                df = pd.read_sql(self.query, conn)
            return df
        else:
            # sqlite fallback
            with sqlite3.connect(self.sqlite_path) as conn:
                df = pd.read_sql_query(self.query, conn)
            return df


# -----------------------------------------------------------------------------
# Transformation and Analysis
# -----------------------------------------------------------------------------
TransformFn = Callable[[pd.DataFrame], pd.DataFrame]


@dataclass
class DataProfile:
    name: str
    rows: int
    cols: int
    dtypes: Dict[str, str]
    missing_pct: Dict[str, float]
    numeric_summary: Optional[pd.DataFrame]
    correlations: Optional[pd.DataFrame]


def analyze_dataframe(df: pd.DataFrame, name: str = "data", compute_correlations: bool = True) -> DataProfile:
    rows, cols = df.shape
    dtypes = {c: str(t) for c, t in df.dtypes.items()}
    missing_pct = {c: float(df[c].isna().mean() * 100.0) for c in df.columns}

    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_summary = None
    correlations = None
    if numeric_cols:
        numeric_summary = df[numeric_cols].describe(include="all").T
        if compute_correlations and len(numeric_cols) >= 2:
            correlations = df[numeric_cols].corr()

    return DataProfile(
        name=name,
        rows=rows,
        cols=cols,
        dtypes=dtypes,
        missing_pct=missing_pct,
        numeric_summary=numeric_summary,
        correlations=correlations,
    )


# -----------------------------------------------------------------------------
# Data Pipeline
# -----------------------------------------------------------------------------
class DataPipeline:
    """
    A lightweight framework to:
      - register data sources
      - collect them into pandas DataFrames
      - apply transformations
      - run basic analysis
    """

    def __init__(self):
        self.sources: Dict[str, DataSource] = {}
        self.transforms: List[TransformFn] = []

    def add_source(self, source: DataSource) -> "DataPipeline":
        if source.name in self.sources:
            raise ValueError(f"Source '{source.name}' already exists.")
        self.sources[source.name] = source
        return self

    def add_transform(self, fn: TransformFn) -> "DataPipeline":
        """A transform is applied independently to each DataFrame after collection."""
        self.transforms.append(fn)
        return self

    def collect(self, only: Optional[Iterable[str]] = None) -> Dict[str, pd.DataFrame]:
        names = list(only) if only else list(self.sources.keys())
        data: Dict[str, pd.DataFrame] = {}
        for name in names:
            if name not in self.sources:
                raise KeyError(f"Unknown source: {name}")
            df = self.sources[name].fetch()
            data[name] = df
            logger.info(f"[{name}] Collected shape: {df.shape}")
        return data

    def apply_transforms(self, data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        if not self.transforms:
            return data
        out: Dict[str, pd.DataFrame] = {}
        for name, df in data.items():
            df_mod = df.copy()
            for fn in self.transforms:
                df_mod = fn(df_mod)
            out[name] = df_mod
            logger.info(f"[{name}] Transformed shape: {df_mod.shape}")
        return out

    def analyze(self, data: Dict[str, pd.DataFrame], compute_correlations: bool = True) -> Dict[str, DataProfile]:
        profiles: Dict[str, DataProfile] = {}
        for name, df in data.items():
            profiles[name] = analyze_dataframe(df, name, compute_correlations=compute_correlations)
        return profiles


# -----------------------------------------------------------------------------
# Example transforms
# -----------------------------------------------------------------------------
def standard_cleaning(columns_lower: bool = True, strip_whitespace: bool = True, drop_empty_columns: bool = True) -> TransformFn:
    def _fn(df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        if columns_lower:
            out.columns = [str(c).strip().lower() for c in out.columns]
        if strip_whitespace:
            for c in out.select_dtypes(include=["object", "string"]).columns:
                out[c] = out[c].astype("string").str.strip()
        if drop_empty_columns:
            non_all_nan = [c for c in out.columns if not out[c].isna().all()]
            out = out[non_all_nan]
        return out

    return _fn


def parse_dates(columns: List[str], dayfirst: bool = False, errors: str = "coerce") -> TransformFn:
    def _fn(df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        for c in columns:
            if c in out.columns:
                out[c] = pd.to_datetime(out[c], dayfirst=dayfirst, errors=errors)
        return out

    return _fn


def fill_missing(strategy: str = "mean", fill_value: Optional[Any] = None) -> TransformFn:
    """
    strategy: 'mean'|'median'|'mode'|'constant'
    """
    def _fn(df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        num_cols = out.select_dtypes(include=[np.number]).columns
        cat_cols = [c for c in out.columns if c not in num_cols]
        if strategy == "mean":
            out[num_cols] = out[num_cols].fillna(out[num_cols].mean())
            out[cat_cols] = out[cat_cols].fillna("missing")
        elif strategy == "median":
            out[num_cols] = out[num_cols].fillna(out[num_cols].median())
            out[cat_cols] = out[cat_cols].fillna("missing")
        elif strategy == "mode":
            for c in out.columns:
                mode_val = out[c].mode(dropna=True)
                if not mode_val.empty:
                    out[c] = out[c].fillna(mode_val.iloc[0])
        elif strategy == "constant":
            out = out.fillna(fill_value)
        else:
            raise ValueError("Unknown strategy for fill_missing")
        return out
    return _fn


# -----------------------------------------------------------------------------
# Example usage
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    # Build a pipeline
    pipeline = (
        DataPipeline()
        .add_source(CSVSource("people_csv", path="people.csv"))  # replace with your path
        # .add_source(ExcelSource("sales_xlsx", path="sales.xlsx", sheet_name=0))
        # .add_source(JSONSource("events_json", path_or_url="events.json", lines=False))
        # .add_source(ParquetSource("logs_parquet", path="logs.parquet"))
        # .add_source(APISource("posts_api", url="https://jsonplaceholder.typicode.com/posts", json_path=None))
        # .add_source(SQLSource("users_sqlite", query="SELECT * FROM users", sqlite_path="example.sqlite"))
        .add_transform(standard_cleaning())
        .add_transform(fill_missing(strategy="mode"))
    )

    # Collect
    try:
        raw_data = pipeline.collect()  # dict of DataFrame keyed by source name
    except Exception as e:
        logger.error(f"Collection failed: {e}")
        raw_data = {}

    # Transform
    transformed = pipeline.apply_transforms(raw_data)

    # Analyze
    profiles = pipeline.analyze(transformed, compute_correlations=True)

    # Print quick summaries
    for name, profile in profiles.items():
        print(f"\n=== Profile: {name} ===")
        print(f"Shape: {profile.rows} rows x {profile.cols} cols")
        print("Dtypes:", profile.dtypes)
        print("Missing % (top 5):", dict(sorted(profile.missing_pct.items(), key=lambda kv: kv[1], reverse=True)[:5]))
        if profile.numeric_summary is not None:
            print("\nNumeric summary:")
            print(profile.numeric_summary)
        if profile.correlations is not None:
            print("\nCorrelations:")
            print(profile.correlations)

    # Example: merge across sources (if you have compatible keys)
    # if "people_csv" in transformed and "users_sqlite" in transformed:
    #     merged = transformed["people_csv"].merge(transformed["users_sqlite"], on="user_id", how="left")
    #     print("\nMerged shape:", merged.shape)
------------------------------------------------------------

PROMPT 301:
"Generate Python code that imports the necessary libraries and uses the Robot Framework to create a test case that utilizes the HTTP/HTTPS client and server services provided by robotframework-httpctrl to perform an HTTP request and response cycle."
RESPONSE:
# This script generates and runs a Robot Framework test that uses robotframework-httpctrl
# to spin up a local HTTP server and perform a client request/response roundtrip.
#
# Prerequisites:
#   pip install robotframework robotframework-httpctrl
#
# Note:
#   The exact keyword names in HttpCtrl may vary by version. This example uses commonly
#   seen names; if they differ in your environment, adjust the Robot test accordingly
#   per the library’s documentation.

import socket
import tempfile
from pathlib import Path
from textwrap import dedent

import robot


def find_free_port():
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.bind(("127.0.0.1", 0))
    addr, port = s.getsockname()
    s.close()
    return port


def main():
    port = find_free_port()
    suite = dedent(f"""
    *** Settings ***
    Library    HttpCtrl

    *** Variables ***
    ${{
    HOST}}    127.0.0.1
    ${{
    PORT}}    {port}
    ${{
    BASE_URL}}    http://${{HOST}}:${{PORT}}

    *** Test Cases ***
    HTTP roundtrip with HttpCtrl
        [Documentation]    Start an embedded HTTP server, register a responder, and GET it with the client.
        # Start the embedded HTTP server
        Start HTTP Server    host=${{HOST}}    port=${{PORT}}

        # Register a simple route that returns JSON
        # If your version uses different keyword/argument names, adjust accordingly:
        # e.g., "Add Route", "Map", "Serve", "When Request Is" etc.
        Register HTTP Responder    method=GET    path=/ping    status=200    body={{"ok": true}}    headers=Content-Type=application/json

        # Perform a client request against the embedded server
        ${{
        resp}}=    HTTP GET    url=${{BASE_URL}}/ping

        # Validate response
        Should Be Equal As Integers    ${{resp.status}}    200
        Should Contain    ${{resp.body}}    "ok": true

        # Stop the server
        Stop HTTP Server
    """)

    with tempfile.TemporaryDirectory() as tmpdir:
        suite_path = Path(tmpdir) / "httpctrl_roundtrip.robot"
        suite_path.write_text(suite, encoding="utf-8")

        rc = robot.run(str(suite_path), outputdir=str(Path(tmpdir) / "results"))
        print(f"Robot run return code: {rc}")
        print(f"Suite file: {suite_path}")
        print(f"Results in: {Path(tmpdir) / 'results'}")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 302:
"Generate Python code that imports the necessary modules and uses the erl_tree module to parse an ERP format string into a nested dictionary representation, similar to how the erlang_term_to_python function in the erlang_external_term module works."
RESPONSE:
# -*- coding: utf-8 -*-
"""
Best-effort example that:
- Imports erl_tree
- Parses an ERP/Erlang-term-like string into a tree
- Converts that tree to nested Python structures (dict/list/tuple/str/int/bytes/etc.),
  similar in spirit to erlang_external_term.erlang_term_to_python.

Notes:
- Different erl_tree variants expose different APIs. This code tries a few common
  parse entry points and node attribute names. Adjust the attribute names if your
  erl_tree differs (see the helper accessors below).
"""

from typing import Any, Dict, Iterable, Tuple
import importlib


def _import_erl_tree():
    try:
        return importlib.import_module("erl_tree")
    except ImportError as e:
        raise ImportError(
            "Could not import erl_tree. Ensure it is installed and importable."
        ) from e


def _call_parser(erl_tree_mod, text: str) -> Any:
    """
    Try a few common parser entry points found in erl_tree-like modules.
    Adjust if your module uses a different name.
    """
    candidates = (
        getattr(erl_tree_mod, "parse", None),
        getattr(erl_tree_mod, "parse_term", None),
        getattr(erl_tree_mod, "loads", None),
    )
    for fn in candidates:
        if callable(fn):
            return fn(text)
    raise AttributeError(
        "Could not find a parser function in erl_tree. Tried: parse, parse_term, loads."
    )


# ------------- Node access helpers (adjust to match your erl_tree) ------------- #
def _node_type(n: Any) -> str | None:
    for attr in ("type", "tag", "node_type", "kind"):
        if hasattr(n, attr):
            t = getattr(n, attr)
            return t.name if hasattr(t, "name") else str(t)
    # Fallback: class name hint
    return type(n).__name__


def _node_value(n: Any) -> Any:
    # Common places where leaf values may live
    for attr in ("value", "val", "text", "name", "data"):
        if hasattr(n, attr):
            return getattr(n, attr)
    return None


def _node_items(n: Any) -> Iterable[Tuple[Any, Any]] | None:
    # For maps: pairs or items
    for attr in ("pairs", "items", "kvs"):
        if hasattr(n, attr):
            v = getattr(n, attr)
            # If it's a method, call it
            return v() if callable(v) else v
    # Some nodes may provide key/value as children with .key/.val
    if hasattr(n, "key") and hasattr(n, "val"):
        return [(getattr(n, "key"), getattr(n, "val"))]
    return None


def _node_children(n: Any) -> Iterable[Any] | None:
    for attr in ("elements", "elts", "items", "children", "values"):
        if hasattr(n, attr):
            v = getattr(n, attr)
            return v() if callable(v) else v
    # Many nodes also behave like sequences
    if isinstance(n, (list, tuple)):
        return n
    return None


def _is_atom_type(tname: str) -> bool:
    return tname.lower() in ("atom", "erlangatom", "atomnode")


def _is_binary_type(tname: str) -> bool:
    return tname.lower() in ("binary", "bin", "erlbinary", "binarynode")


def _is_string_type(tname: str) -> bool:
    return tname.lower() in ("string", "str", "stringnode")


def _is_integer_type(tname: str) -> bool:
    return tname.lower() in ("integer", "int", "integernode", "number")


def _is_float_type(tname: str) -> bool:
    return tname.lower() in ("float", "floatnode", "number")


def _is_list_type(tname: str) -> bool:
    return tname.lower() in ("list", "listnode")


def _is_tuple_type(tname: str) -> bool:
    return tname.lower() in ("tuple", "tuplenode")


def _is_map_type(tname: str) -> bool:
    return tname.lower() in ("map", "mapnode", "mapexpr")


def _is_record_type(tname: str) -> bool:
    return tname.lower() in ("record", "recordnode", "rec")


def _maybe_bytes(value: Any) -> bytes | None:
    # If a binary node stores raw bytes under common attributes
    if isinstance(value, (bytes, bytearray, memoryview)):
        return bytes(value)
    return None


# ------------- Conversion (tree -> Python nested structures) ------------- #
def _to_python(obj: Any) -> Any:
    # Already a standard Python type
    if obj is None or isinstance(obj, (bool, int, float, str, bytes)):
        return obj

    # Built-in containers
    if isinstance(obj, dict):
        return { _to_python(k): _to_python(v) for k, v in obj.items() }
    if isinstance(obj, (list, tuple)):
        seq = [_to_python(x) for x in obj]
        return seq if isinstance(obj, list) else tuple(seq)

    # If it looks like a mapping with items()
    if hasattr(obj, "items") and callable(getattr(obj, "items")):
        try:
            return { _to_python(k): _to_python(v) for k, v in obj.items() }
        except Exception:
            pass

    # Node-based handling
    tname = _node_type(obj) or ""

    # Atoms: convert to str
    if _is_atom_type(tname):
        val = _node_value(obj)
        return str(val)

    # Binary: convert to bytes
    if _is_binary_type(tname):
        val = _node_value(obj)
        b = _maybe_bytes(val)
        if b is not None:
            return b
        # Some ASTs store binaries as string-literals; treat as bytes via UTF-8 best-effort
        if isinstance(val, str):
            return val.encode("utf-8")
        # Or nested children that evaluate to list of ints
        children = _node_children(obj) or []
        data = bytes([int(_to_python(c)) for c in children])
        return data

    # String literal
    if _is_string_type(tname):
        return str(_node_value(obj))

    # Numbers
    if _is_integer_type(tname):
        return int(_node_value(obj))
    if _is_float_type(tname):
        return float(_node_value(obj))

    # List
    if _is_list_type(tname):
        children = _node_children(obj) or []
        return [_to_python(c) for c in children]

    # Tuple
    if _is_tuple_type(tname):
        children = _node_children(obj) or []
        return tuple(_to_python(c) for c in children)

    # Map
    if _is_map_type(tname):
        items = _node_items(obj)
        if items is None:
            # Some ASTs keep map entries as children of 2-tuples
            children = _node_children(obj) or []
            items = []
            for c in children:
                pc = _to_python(c)
                if isinstance(pc, (list, tuple)) and len(pc) == 2:
                    items.append(pc)
                else:
                    raise ValueError(f"Unexpected map child shape: {pc!r}")
        return { _to_python(k): _to_python(v) for k, v in items }

    # Record: turn into a dict {__record__: Name, ...fields...}
    if _is_record_type(tname):
        # Heuristics: record name in .name/.value, fields in children or items
        name = _node_value(obj)
        fields = {}
        # Common patterns: fields as pairs or as named field nodes
        items = _node_items(obj)
        if items:
            for k, v in items:
                fields[str(_to_python(k))] = _to_python(v)
        else:
            children = _node_children(obj) or []
            # Try tuples like {field, Value} or {Key, Value}
            for ch in children:
                p = _to_python(ch)
                if isinstance(p, (list, tuple)) and len(p) == 2:
                    fields[str(p[0])] = p[1]
        out = {"__record__": str(name) if name is not None else "record"}
        out.update(fields)
        return out

    # Fallback: try children; if none, return best-effort value or repr
    children = _node_children(obj)
    if children:
        return [_to_python(c) for c in children]
    val = _node_value(obj)
    return val if val is not None else repr(obj)


def parse_erp_to_python(erp_text: str) -> Any:
    """
    Parse an ERP/Erlang-term-like string and convert it to nested Python structures.
    """
    erl_tree = _import_erl_tree()
    tree = _call_parser(erl_tree, erp_text)
    return _to_python(tree)


# --------------------------- Example usage --------------------------- #
if __name__ == "__main__":
    # Example ERP/Erlang-like term string; adjust to your input format.
    example = '{user, #{id => 123, name => <<"Alice">>, tags => [active, paid]}}'
    result = parse_erp_to_python(example)
    print(result)
    # Example output (typical):
    # ('user', {'id': 123, 'name': b'Alice', 'tags': ['active', 'paid']})
------------------------------------------------------------

PROMPT 303:
"Generate Python code that uses the `feedparser` library to parse ATOM and RSS feeds and the `pydub` library to create podcasts, allowing users to customize feed metadata and audio settings before saving the generated files to a specified directory."
RESPONSE:
Below is a self-contained Python script that:

- Uses feedparser to parse RSS or Atom feeds.
- Downloads audio enclosures from feed entries (if available).
- Uses pydub to normalize, resample, channel-convert, add padding, and optionally combine multiple entries into a single “podcast” episode.
- Lets you customize feed metadata (album/artist/genre) and audio settings (bitrate, sample rate, channels, normalization, padding, crossfade) before saving to a specified directory.

Notes:
- This script handles feeds that already include audio enclosures (common for podcasts). It does not perform text-to-speech; entries without audio are skipped by default.
- pydub requires ffmpeg/ffprobe installed and available on your PATH.

Save this as feed_to_podcast.py and run from the command line.

```python
#!/usr/bin/env python3
"""
feed_to_podcast.py

Create podcast audio files from RSS/Atom feeds that contain audio enclosures.

Features:
- Parses RSS/Atom using feedparser.
- Downloads audio enclosures and processes them with pydub.
- Per-entry export or combine multiple entries into one long episode.
- Customizable tags (album/artist/genre) and audio settings (bitrate/sample-rate/channels).
- Normalization, padding, optional crossfade.
- Saves MP3 (default) or other formats supported by ffmpeg.

Requirements:
- feedparser
- pydub
- ffmpeg/ffprobe installed and accessible
"""

import argparse
import io
import os
import re
import sys
import urllib.request
from datetime import datetime
from typing import List, Optional, Tuple

import feedparser
from pydub import AudioSegment


# ------------------------------- Utilities ---------------------------------- #

AUDIO_MIME_PREFIXES = ("audio/",)
AUDIO_MIME_WHITELIST = {
    "audio/mpeg",
    "audio/mp3",
    "audio/mp4",
    "audio/aac",
    "audio/x-m4a",
    "audio/ogg",
    "audio/webm",
    "audio/wav",
    "audio/x-wav",
    "audio/flac",
}

DEFAULT_USER_AGENT = (
    "Mozilla/5.0 (compatible; FeedToPodcast/1.0; +https://example.com/)"
)


def sanitize_filename(name: str, replacement: str = "_") -> str:
    name = re.sub(r"[\\/:\*\?\"<>\|\r\n\t]", replacement, name).strip()
    name = re.sub(r"\s+", " ", name)
    return name[:200] if len(name) > 200 else name


def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def parse_date(entry) -> Optional[datetime]:
    # Try common date fields: published_parsed, updated_parsed
    for attr in ("published_parsed", "updated_parsed", "created_parsed"):
        if getattr(entry, attr, None):
            try:
                t = getattr(entry, attr)
                # t is time.struct_time
                return datetime(*t[:6])
            except Exception:
                pass
    # Fallback: try to parse text date if present
    for attr in ("published", "updated"):
        if getattr(entry, attr, None):
            txt = getattr(entry, attr)
            try:
                from email.utils import parsedate_to_datetime

                return parsedate_to_datetime(txt)
            except Exception:
                pass
    return None


def is_audio_link(link_dict) -> bool:
    # link_dict is usually a dict-like structure with keys: href, type, rel
    typ = link_dict.get("type")
    if typ and (typ in AUDIO_MIME_WHITELIST or typ.startswith(AUDIO_MIME_PREFIXES)):
        return True
    # Some feeds omit type; try to guess by extension
    href = link_dict.get("href", "")
    return bool(re.search(r"\.(mp3|m4a|aac|ogg|oga|opus|wav|flac|webm)$", href, re.I))


def find_audio_enclosure(entry) -> Optional[str]:
    # Prefer enclosures if available
    if hasattr(entry, "enclosures") and entry.enclosures:
        for enc in entry.enclosures:
            # feedparser normalizes to dict-like objects
            if is_audio_link(enc):
                return enc.get("href")
    # Fallback to links with rel='enclosure' or any audio-type link
    if hasattr(entry, "links"):
        for l in entry.links:
            if l.get("rel") == "enclosure" and is_audio_link(l):
                return l.get("href")
        for l in entry.links:
            if is_audio_link(l):
                return l.get("href")
    return None


def download_bytes(
    url: str, user_agent: str = DEFAULT_USER_AGENT, timeout: int = 60
) -> bytes:
    req = urllib.request.Request(url, headers={"User-Agent": user_agent})
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        return resp.read()


def load_audio_segment_from_bytes(
    data: bytes, format_hint: Optional[str] = None
) -> AudioSegment:
    # If you know the format, pass format_hint like 'mp3', 'm4a', 'wav' etc.
    bio = io.BytesIO(data)
    return AudioSegment.from_file(bio, format=format_hint)


def normalize_to_dbfs(seg: AudioSegment, target_dbfs: float) -> AudioSegment:
    if seg.dBFS == float("-inf"):
        return seg  # silence; can't normalize
    gain = target_dbfs - seg.dBFS
    return seg.apply_gain(gain)


def apply_audio_settings(
    seg: AudioSegment,
    sample_rate: Optional[int],
    channels: Optional[int],
    normalize_dbfs: Optional[float],
    pad_start_ms: int,
    pad_end_ms: int,
) -> AudioSegment:
    if normalize_dbfs is not None:
        seg = normalize_to_dbfs(seg, normalize_dbfs)
    if sample_rate is not None:
        seg = seg.set_frame_rate(sample_rate)
    if channels is not None:
        seg = seg.set_channels(channels)
    if pad_start_ms > 0:
        seg = AudioSegment.silent(duration=pad_start_ms) + seg
    if pad_end_ms > 0:
        seg = seg + AudioSegment.silent(duration=pad_end_ms)
    return seg


def export_segment(
    seg: AudioSegment,
    out_path: str,
    fmt: str,
    bitrate: Optional[str],
    tags: Optional[dict],
) -> None:
    export_kwargs = {}
    if bitrate and fmt.lower() in ("mp3", "aac", "m4a", "opus", "ogg"):
        export_kwargs["bitrate"] = bitrate
    if tags:
        export_kwargs["tags"] = {k: v for k, v in tags.items() if v is not None}
    seg.export(out_path, format=fmt, **export_kwargs)


def guess_format_from_url(url: str) -> Optional[str]:
    m = re.search(r"\.([a-z0-9]{2,4})(?:\?|#|$)", url, re.I)
    if not m:
        return None
    ext = m.group(1).lower()
    ext_to_fmt = {
        "mp3": "mp3",
        "m4a": "ipod",  # mp4 container; ffmpeg sometimes uses 'ipod' for m4a
        "aac": "adts",
        "mp4": "mp4",
        "ogg": "ogg",
        "oga": "ogg",
        "opus": "opus",
        "wav": "wav",
        "flac": "flac",
        "webm": "webm",
    }
    return ext_to_fmt.get(ext)


def entry_default_title(entry, idx: int) -> str:
    title = getattr(entry, "title", None) or f"Episode {idx}"
    return title


def build_tags_for_entry(
    entry,
    fallback_artist: Optional[str],
    fallback_album: Optional[str],
    genre: Optional[str],
    idx: int,
) -> dict:
    title = entry_default_title(entry, idx)
    date_obj = parse_date(entry)
    date_str = date_obj.strftime("%Y-%m-%d") if date_obj else None

    # Feed-level tags can be overridden via CLI; use them as fallbacks here.
    tags = {
        "title": title,
        "artist": fallback_artist,
        "album": fallback_album,
        "genre": genre,
        "date": date_str,
    }
    return tags


def combine_segments(
    segments: List[Tuple[AudioSegment, dict]],
    crossfade_ms: int = 0,
) -> Tuple[AudioSegment, dict]:
    if not segments:
        raise ValueError("No segments to combine")

    combined = segments[0][0]
    # Combine tags where possible; album/artist/genre remain, title becomes a combined title.
    first_tags = segments[0][1].copy()
    base_album = first_tags.get("album") or "Combined Podcast"
    base_artist = first_tags.get("artist") or "Various"
    base_genre = first_tags.get("genre")

    for i in range(1, len(segments)):
        seg, _ = segments[i]
        if crossfade_ms > 0:
            combined = combined.append(seg, crossfade=crossfade_ms)
        else:
            combined += seg

    tags = {
        "title": f"{base_album} - Combined ({len(segments)} parts)",
        "artist": base_artist,
        "album": base_album,
        "genre": base_genre,
    }
    return combined, tags


# ------------------------------- Main Logic ---------------------------------- #

def process_feed(
    feed_url_or_path: str,
    out_dir: str,
    per_entry: bool,
    combine: bool,
    limit: Optional[int],
    sample_rate: Optional[int],
    channels: Optional[int],
    normalize_dbfs: Optional[float],
    pad_start_ms: int,
    pad_end_ms: int,
    export_format: str,
    bitrate: Optional[str],
    override_album: Optional[str],
    override_artist: Optional[str],
    override_genre: Optional[str],
    user_agent: str,
    timeout: int,
    crossfade_ms: int,
) -> None:
    ensure_dir(out_dir)

    print(f"Parsing feed: {feed_url_or_path}")
    feed = feedparser.parse(feed_url_or_path)

    if feed.bozo:
        print(f"Warning: feed parsing encountered an issue: {feed.bozo_exception}", file=sys.stderr)

    feed_title = getattr(feed.feed, "title", None) or "Untitled Feed"
    feed_author = getattr(feed.feed, "author", None) or getattr(feed.feed, "publisher", None)

    album = override_album or feed_title
    artist = override_artist or feed_author or "Unknown"
    genre = override_genre

    entries = list(feed.entries or [])
    # Sort by date if available (oldest to newest), else keep order
    try:
        entries.sort(key=lambda e: parse_date(e) or datetime.max)
    except Exception:
        pass

    if limit is not None:
        entries = entries[:max(0, limit)]

    processed_segments: List[Tuple[AudioSegment, dict, str]] = []

    if not entries:
        print("No entries found in feed.")
        return

    for idx, entry in enumerate(entries, start=1):
        title = entry_default_title(entry, idx)
        enclosure_url = find_audio_enclosure(entry)
        if not enclosure_url:
            print(f"Skipping entry without audio: {title}")
            continue

        print(f"Downloading: {title}")
        try:
            data = download_bytes(enclosure_url, user_agent=user_agent, timeout=timeout)
        except Exception as e:
            print(f"Failed to download {enclosure_url}: {e}", file=sys.stderr)
            continue

        fmt_hint = guess_format_from_url(enclosure_url)
        try:
            seg = load_audio_segment_from_bytes(data, format_hint=fmt_hint)
        except Exception as e:
            print(f"Failed to decode audio for {title}: {e}", file=sys.stderr)
            continue

        seg = apply_audio_settings(
            seg=seg,
            sample_rate=sample_rate,
            channels=channels,
            normalize_dbfs=normalize_dbfs,
            pad_start_ms=pad_start_ms,
            pad_end_ms=pad_end_ms,
        )

        tags = build_tags_for_entry(
            entry=entry,
            fallback_artist=artist,
            fallback_album=album,
            genre=genre,
            idx=idx,
        )
        processed_segments.append((seg, tags, title))

        if per_entry and not combine:
            safe_title = sanitize_filename(tags.get("title") or f"Episode {idx}")
            out_path = os.path.join(out_dir, f"{idx:03d} - {safe_title}.{export_format}")
            print(f"Exporting entry to {out_path}")
            try:
                export_segment(seg, out_path, export_format, bitrate, tags)
            except Exception as e:
                print(f"Export failed for {title}: {e}", file=sys.stderr)

    if combine and processed_segments:
        print(f"Combining {len(processed_segments)} entries into one episode...")
        segs_for_combine = [(s, t) for (s, t, _) in processed_segments]
        combined, combined_tags = combine_segments(segs_for_combine, crossfade_ms=crossfade_ms)
        safe_album = sanitize_filename(album)
        out_path = os.path.join(out_dir, f"{safe_album} - Combined.{export_format}")
        print(f"Exporting combined episode to {out_path}")
        try:
            export_segment(combined, out_path, export_format, bitrate, combined_tags)
        except Exception as e:
            print(f"Export failed for combined episode: {e}", file=sys.stderr)

    if not processed_segments:
        print("No audio entries processed. Nothing to export.")


# ------------------------------- CLI ----------------------------------------- #

def build_arg_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        description="Parse RSS/Atom feeds and create podcast audio using pydub."
    )
    p.add_argument("feed", help="Feed URL or local feed file path")
    p.add_argument(
        "-o", "--out-dir", default="output_podcast", help="Directory to save generated files"
    )
    mode = p.add_mutually_exclusive_group()
    mode.add_argument(
        "--per-entry",
        action="store_true",
        help="Export one audio file per feed entry (default behavior if --combine is not set)",
    )
    mode.add_argument(
        "--combine",
        action="store_true",
        help="Combine multiple entries into one episode",
    )
    p.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Maximum number of entries to process (default: all)",
    )

    # Metadata overrides
    p.add_argument("--album", default=None, help="Override album tag (defaults to feed title)")
    p.add_argument("--artist", default=None, help="Override artist tag (defaults to feed author)")
    p.add_argument("--genre", default=None, help="Set genre tag")

    # Audio processing
    p.add_argument("--sample-rate", type=int, default=None, help="Target sample rate (e.g. 44100)")
    p.add_argument("--channels", type=int, choices=(1, 2), default=None, help="Number of channels")
    p.add_argument(
        "--normalize",
        type=float,
        default=None,
        help="Normalize to target dBFS (e.g. -14.0). If omitted, no normalization.",
    )
    p.add_argument(
        "--pad-start-ms",
        type=int,
        default=0,
        help="Silence padding at start of each segment (ms)",
    )
    p.add_argument(
        "--pad-end-ms",
        type=int,
        default=0,
        help="Silence padding at end of each segment (ms)",
    )
    p.add_argument(
        "--crossfade-ms",
        type=int,
        default=0,
        help="Crossfade between segments when using --combine (ms)",
    )

    # Export settings
    p.add_argument(
        "--format",
        default="mp3",
        help="Export format (default: mp3). Must be supported by ffmpeg (e.g. mp3, wav, ogg, opus, m4a).",
    )
    p.add_argument(
        "--bitrate",
        default="192k",
        help="Target bitrate for compressed formats (e.g. 128k, 192k).",
    )

    # Network
    p.add_argument(
        "--user-agent",
        default=DEFAULT_USER_AGENT,
        help="HTTP User-Agent for downloads",
    )
    p.add_argument(
        "--timeout",
        type=int,
        default=60,
        help="HTTP timeout in seconds",
    )

    return p


def main():
    parser = build_arg_parser()
    args = parser.parse_args()

    # Determine mode: default to per-entry if neither explicitly set
    per_entry = True
    combine = False
    if args.combine:
        per_entry = False
        combine = True
    elif args.per_entry:
        per_entry = True
        combine = False

    process_feed(
        feed_url_or_path=args.feed,
        out_dir=args.out_dir,
        per_entry=per_entry,
        combine=combine,
        limit=args.limit,
        sample_rate=args.sample_rate,
        channels=args.channels,
        normalize_dbfs=args.normalize,
        pad_start_ms=args.pad_start_ms,
        pad_end_ms=args.pad_end_ms,
        export_format=args.format,
        bitrate=args.bitrate,
        override_album=args.album,
        override_artist=args.artist,
        override_genre=args.genre,
        user_agent=args.user_agent,
        timeout=args.timeout,
        crossfade_ms=args.crossfade_ms,
    )


if __name__ == "__main__":
    main()
```

Examples:
- Export one MP3 per entry, normalized and resampled:
  python feed_to_podcast.py https://example.com/feed.xml -o ./pod_out --per-entry --normalize -14 --sample-rate 44100 --channels 2 --bitrate 160k --album "My Custom Album" --artist "My Brand"

- Combine first 5 entries into one episode with crossfade:
  python feed_to_podcast.py https://example.com/feed.xml -o ./pod_out --combine --limit 5 --crossfade-ms 1000 --normalize -16 --bitrate 128k --genre "News"

Troubleshooting:
- If export fails or decoding doesn’t work, ensure ffmpeg/ffprobe are installed and on your PATH.
- Entries without audio enclosures are skipped by design. If you need text-to-speech, integrate a TTS library to produce AudioSegment objects for such items.
------------------------------------------------------------

PROMPT 304:
"Generate Python code that uses the `json` and `typing` modules to serialize and deserialize JSON data, the `graphql-tag` library to generate GraphQL schema from Python types, and the `graphql-python` library to execute GraphQL queries and mutations."
RESPONSE:
Note: There isn’t a Python package named “graphql-tag,” and “graphql-python” refers to the organization that maintains graphql-core (the de facto Python GraphQL implementation). Below is a working Python example that:
- uses typing and json for serialization/deserialization
- uses Strawberry (schema-from-Python-typing) to generate a GraphQL schema
- executes queries/mutations via graphql-core (from the graphql-python org) using strawberry’s underlying GraphQL schema

# pip install strawberry-graphql graphql-core
import json
from typing import Dict, List, Optional, TypedDict

import strawberry
from graphql import graphql_sync
from dataclasses import asdict

# Domain model and GraphQL schema (generated from Python typing via Strawberry)

@strawberry.type
class Todo:
    id: int
    title: str
    completed: bool = False

@strawberry.input
class TodoInput:
    id: int
    title: str

# In-memory store
DB: Dict[int, Todo] = {}

@strawberry.type
class Query:
    @strawberry.field
    def todos(self) -> List[Todo]:
        return list(DB.values())

    @strawberry.field
    def todo(self, id: int) -> Optional[Todo]:
        return DB.get(id)

@strawberry.type
class Mutation:
    @strawberry.mutation
    def add_todo(self, input: TodoInput) -> Todo:
        todo = Todo(id=input.id, title=input.title, completed=False)
        DB[input.id] = todo
        return todo

    @strawberry.mutation
    def toggle_complete(self, id: int) -> Optional[Todo]:
        todo = DB.get(id)
        if not todo:
            return None
        todo.completed = not todo.completed
        return todo

schema = strawberry.Schema(query=Query, mutation=Mutation)

# JSON serialization/deserialization with typing

class ClientTodo(TypedDict):
    id: int
    title: str

# Example client payload serialized to JSON
new_todo_payload: ClientTodo = {"id": 1, "title": "Write docs"}
variables_json = json.dumps({"input": new_todo_payload})

# Back to Python dict for use as GraphQL variables
variables = json.loads(variables_json)

# Execute GraphQL mutation and query via graphql-core

core_schema = schema.as_graphql_schema  # graphql-core's GraphQLSchema

mutation = """
mutation Add($input: TodoInput!) {
  addTodo(input: $input) { id title completed }
}
"""

mutation_result = graphql_sync(core_schema, mutation, variable_values=variables)
mutation_response = {"data": mutation_result.data}
if mutation_result.errors:
    mutation_response["errors"] = [
        getattr(e, "formatted", {"message": str(e)}) for e in mutation_result.errors
    ]

print("Mutation result JSON:", json.dumps(mutation_response))

query = """
query {
  todos { id title completed }
}
"""

query_result = graphql_sync(core_schema, query)
query_response = {"data": query_result.data}
if query_result.errors:
    query_response["errors"] = [
        getattr(e, "formatted", {"message": str(e)}) for e in query_result.errors
    ]

print("Query result JSON:", json.dumps(query_response))

# Optionally, serialize domain data to JSON (uses dataclasses.asdict because Strawberry types are dataclasses)
todos_json = json.dumps([asdict(t) for t in DB.values()])
print("Serialized todos JSON:", todos_json)

# Notes:
# - Strawberry auto-camel-cases field names in the GraphQL schema (e.g., add_todo -> addTodo, toggle_complete -> toggleComplete).
# - graphql-core is the core GraphQL execution engine in Python, maintained by the graphql-python organization.
------------------------------------------------------------

PROMPT 305:
"Generate Python code that imports the necessary libraries to create an open-source framework for representing and manipulating geometric models, such as NumPy and PyOpenGL, and defines classes and functions to implement the framework's functionality."
RESPONSE:
# MIT License
# 
# Copyright (c) 2025
# 
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# 
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
# 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""
A minimal open-source Python framework for representing, manipulating, and rendering
geometric models using NumPy and PyOpenGL.

Features:
- Core math utilities (vectors, quaternions, transforms, matrices).
- Geometry classes (Mesh with normals, UVs, indices) and operations (transform, normals).
- OBJ loader/saver (basic).
- Scene graph basics (Transform, Model).
- Camera and Renderer using PyOpenGL.
- Example GLUT viewer.

Dependencies:
- numpy
- PyOpenGL
- PyOpenGL_accelerate (optional, recommended)
"""

from __future__ import annotations

import math
import sys
import ctypes
from dataclasses import dataclass, field
from typing import Optional, Tuple, List, Dict

import numpy as np

# OpenGL imports
from OpenGL.GL import (
    glClearColor, glClear, GL_COLOR_BUFFER_BIT, GL_DEPTH_BUFFER_BIT,
    glEnable, GL_DEPTH_TEST, glViewport, glUseProgram, glGetUniformLocation,
    glUniformMatrix4fv, glUniform3f, glUniform1i, glUniform1f,
    glGenVertexArrays, glBindVertexArray, glGenBuffers, glBindBuffer, glBufferData,
    glVertexAttribPointer, glEnableVertexAttribArray, GL_ARRAY_BUFFER,
    GL_STATIC_DRAW, GL_ELEMENT_ARRAY_BUFFER, glDrawElements, GL_TRIANGLES,
    GL_UNSIGNED_INT, glDeleteBuffers, glDeleteVertexArrays, glDeleteProgram
)
from OpenGL.GL.shaders import compileProgram, compileShader

# GLUT for example viewer (optional)
try:
    from OpenGL.GLUT import (
        glutInit, glutInitDisplayMode, glutCreateWindow, glutInitWindowSize,
        glutInitWindowPosition, glutDisplayFunc, glutIdleFunc, glutMainLoop,
        GLUT_DOUBLE, GLUT_RGB, GLUT_DEPTH, glutSwapBuffers, glutPostRedisplay,
        glutReshapeFunc
    )
    GLUT_AVAILABLE = True
except Exception:
    GLUT_AVAILABLE = False


# ------------- Math utilities -------------

def normalize(v: np.ndarray, eps: float = 1e-8) -> np.ndarray:
    n = np.linalg.norm(v)
    return v if n < eps else v / n


def mat4_identity() -> np.ndarray:
    return np.eye(4, dtype=np.float32)


def translate(t: Tuple[float, float, float]) -> np.ndarray:
    m = mat4_identity()
    m[0, 3], m[1, 3], m[2, 3] = t
    return m


def scale(s: Tuple[float, float, float]) -> np.ndarray:
    sx, sy, sz = s
    m = mat4_identity()
    m[0, 0], m[1, 1], m[2, 2] = sx, sy, sz
    return m


def quat_from_axis_angle(axis: Tuple[float, float, float], angle_rad: float) -> np.ndarray:
    ax = normalize(np.array(axis, dtype=np.float32))
    s = math.sin(angle_rad / 2.0)
    w = math.cos(angle_rad / 2.0)
    return np.array([ax[0] * s, ax[1] * s, ax[2] * s, w], dtype=np.float32)


def quat_multiply(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    ax, ay, az, aw = a
    bx, by, bz, bw = b
    return np.array([
        aw * bx + ax * bw + ay * bz - az * by,
        aw * by - ax * bz + ay * bw + az * bx,
        aw * bz + ax * by - ay * bx + az * bw,
        aw * bw - ax * bx - ay * by - az * bz
    ], dtype=np.float32)


def quat_normalize(q: np.ndarray) -> np.ndarray:
    return normalize(q)


def quat_to_mat4(q: np.ndarray) -> np.ndarray:
    x, y, z, w = q
    xx, yy, zz = x * x, y * y, z * z
    xy, xz, yz = x * y, x * z, y * z
    wx, wy, wz = w * x, w * y, w * z

    m = mat4_identity()
    m[0, 0] = 1 - 2 * (yy + zz)
    m[0, 1] = 2 * (xy - wz)
    m[0, 2] = 2 * (xz + wy)

    m[1, 0] = 2 * (xy + wz)
    m[1, 1] = 1 - 2 * (xx + zz)
    m[1, 2] = 2 * (yz - wx)

    m[2, 0] = 2 * (xz - wy)
    m[2, 1] = 2 * (yz + wx)
    m[2, 2] = 1 - 2 * (xx + yy)
    return m


def perspective(fovy_deg: float, aspect: float, znear: float, zfar: float) -> np.ndarray:
    f = 1.0 / math.tan(math.radians(fovy_deg) / 2.0)
    m = np.zeros((4, 4), dtype=np.float32)
    m[0, 0] = f / aspect
    m[1, 1] = f
    m[2, 2] = (zfar + znear) / (znear - zfar)
    m[2, 3] = (2 * zfar * znear) / (znear - zfar)
    m[3, 2] = -1.0
    return m


def look_at(eye: np.ndarray, target: np.ndarray, up: np.ndarray) -> np.ndarray:
    f = normalize(target - eye)
    s = normalize(np.cross(f, up))
    u = np.cross(s, f)

    m = mat4_identity()
    m[0, 0:3] = s
    m[1, 0:3] = u
    m[2, 0:3] = -f
    m[0, 3] = -np.dot(s, eye)
    m[1, 3] = -np.dot(u, eye)
    m[2, 3] = np.dot(f, eye)
    return m


# ------------- Core data structures -------------

@dataclass
class Transform:
    position: np.ndarray = field(default_factory=lambda: np.zeros(3, dtype=np.float32))
    rotation: np.ndarray = field(default_factory=lambda: np.array([0, 0, 0, 1], dtype=np.float32))  # quaternion x,y,z,w
    scale_v: np.ndarray = field(default_factory=lambda: np.ones(3, dtype=np.float32))

    def matrix(self) -> np.ndarray:
        t = translate(tuple(self.position))
        r = quat_to_mat4(quat_normalize(self.rotation))
        s = scale(tuple(self.scale_v))
        return t @ r @ s

    def rotate_axis_angle(self, axis: Tuple[float, float, float], angle_rad: float):
        self.rotation = quat_multiply(self.rotation, quat_from_axis_angle(axis, angle_rad))
        self.rotation = quat_normalize(self.rotation)

    def combine(self, parent: 'Transform') -> 'Transform':
        # Efficient composition: parent.matrix @ self.matrix, but keep decomposition simple here
        m = parent.matrix() @ self.matrix()
        # Not decomposing back; just return a Transform that carries the matrix via position/rotation/scale is complex
        # So we store composed matrix in a special Transform-like container
        out = Transform()
        # Store as baked matrix (hidden attr)
        setattr(out, "_baked_matrix", m)
        return out

    def baked_matrix(self) -> np.ndarray:
        if hasattr(self, "_baked_matrix"):
            return getattr(self, "_baked_matrix")
        return self.matrix()


@dataclass
class Mesh:
    vertices: np.ndarray  # shape (N,3), float32
    indices: np.ndarray   # shape (M,3), uint32
    normals: Optional[np.ndarray] = None  # shape (N,3), float32
    uvs: Optional[np.ndarray] = None      # shape (N,2), float32

    def copy(self) -> 'Mesh':
        return Mesh(
            vertices=self.vertices.copy(),
            indices=self.indices.copy(),
            normals=None if self.normals is None else self.normals.copy(),
            uvs=None if self.uvs is None else self.uvs.copy()
        )

    def compute_normals(self, weighted: bool = True):
        v = self.vertices
        f = self.indices
        n = np.zeros_like(v, dtype=np.float32)
        for i0, i1, i2 in f:
            p0, p1, p2 = v[i0], v[i1], v[i2]
            e1 = p1 - p0
            e2 = p2 - p0
            fn = np.cross(e1, e2)
            if not weighted:
                fn = normalize(fn)
            n[i0] += fn
            n[i1] += fn
            n[i2] += fn
        n = np.apply_along_axis(normalize, 1, n)
        self.normals = n.astype(np.float32)

    def bounding_box(self) -> Tuple[np.ndarray, np.ndarray]:
        vmin = np.min(self.vertices, axis=0)
        vmax = np.max(self.vertices, axis=0)
        return vmin, vmax

    def transform(self, m: np.ndarray):
        # Apply transform to vertices and normals
        v = self.vertices
        ones = np.ones((v.shape[0], 1), dtype=np.float32)
        vh = np.hstack([v, ones])  # Nx4
        vtx = (vh @ m.T)[:, :3]
        self.vertices = vtx.astype(np.float32)

        if self.normals is not None:
            n3 = self.normals
            # Normal matrix = inverse-transpose of upper-left 3x3
            nmat = np.linalg.inv(m[:3, :3]).T
            self.normals = (n3 @ nmat.T)
            self.normals = np.apply_along_axis(normalize, 1, self.normals).astype(np.float32)


@dataclass
class Material:
    color: Tuple[float, float, float] = (0.8, 0.8, 0.8)
    metallic: float = 0.0  # not used in basic shader
    roughness: float = 1.0  # not used in basic shader


@dataclass
class Model:
    mesh: Mesh
    material: Material = field(default_factory=Material)
    transform: Transform = field(default_factory=Transform)


# ------------- Geometry generators -------------

def create_box(size: Tuple[float, float, float] = (1.0, 1.0, 1.0)) -> Mesh:
    sx, sy, sz = size
    hx, hy, hz = sx / 2.0, sy / 2.0, sz / 2.0

    # 8 corners
    verts = np.array([
        [-hx, -hy, -hz], [hx, -hy, -hz], [hx,  hy, -hz], [-hx,  hy, -hz],  # back
        [-hx, -hy,  hz], [hx, -hy,  hz], [hx,  hy,  hz], [-hx,  hy,  hz],  # front
    ], dtype=np.float32)

    # 12 triangles (two per face)
    idx = np.array([
        [0, 1, 2], [0, 2, 3],  # back
        [4, 6, 5], [4, 7, 6],  # front
        [0, 4, 5], [0, 5, 1],  # bottom
        [3, 2, 6], [3, 6, 7],  # top
        [1, 5, 6], [1, 6, 2],  # right
        [0, 3, 7], [0, 7, 4],  # left
    ], dtype=np.uint32)

    mesh = Mesh(vertices=verts, indices=idx)
    mesh.compute_normals()
    return mesh


def create_uv_sphere(radius: float = 1.0, segments: int = 32, rings: int = 16) -> Mesh:
    verts = []
    uvs = []
    for y in range(rings + 1):
        v = y / rings
        theta = v * math.pi
        sin_t = math.sin(theta)
        cos_t = math.cos(theta)
        for x in range(segments + 1):
            u = x / segments
            phi = u * 2.0 * math.pi
            sin_p = math.sin(phi)
            cos_p = math.cos(phi)
            px = radius * sin_t * cos_p
            py = radius * cos_t
            pz = radius * sin_t * sin_p
            verts.append([px, py, pz])
            uvs.append([u, 1.0 - v])

    indices = []
    for y in range(rings):
        for x in range(segments):
            i0 = y * (segments + 1) + x
            i1 = i0 + 1
            i2 = i0 + (segments + 1)
            i3 = i2 + 1
            indices.append([i0, i2, i1])
            indices.append([i1, i2, i3])

    mesh = Mesh(
        vertices=np.array(verts, dtype=np.float32),
        indices=np.array(indices, dtype=np.uint32),
        uvs=np.array(uvs, dtype=np.float32)
    )
    mesh.compute_normals()
    return mesh


# ------------- OBJ I/O -------------

def load_obj(path: str, triangulate: bool = True) -> Mesh:
    vs: List[List[float]] = []
    vts: List[List[float]] = []
    vns: List[List[float]] = []
    faces: List[List[Tuple[Optional[int], Optional[int], Optional[int]]]] = []

    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip() or line.startswith('#'):
                continue
            parts = line.strip().split()
            if not parts:
                continue
            if parts[0] == 'v':
                vs.append([float(parts[1]), float(parts[2]), float(parts[3])])
            elif parts[0] == 'vt':
                vts.append([float(parts[1]), float(parts[2])])
            elif parts[0] == 'vn':
                vns.append([float(parts[1]), float(parts[2]), float(parts[3])])
            elif parts[0] == 'f':
                face = []
                for p in parts[1:]:
                    vals = p.split('/')
                    vi = int(vals[0]) if vals[0] else None
                    ti = int(vals[1]) if len(vals) > 1 and vals[1] else None
                    ni = int(vals[2]) if len(vals) > 2 and vals[2] else None
                    # OBJ indices are 1-based; handle negative indices
                    def fix_idx(i, n): return n + i + 1 if i is not None and i < 0 else i
                    vi = fix_idx(vi, len(vs))
                    ti = fix_idx(ti, len(vts))
                    ni = fix_idx(ni, len(vns))
                    face.append((vi, ti, ni))
                faces.append(face)

    # Build unique vertex map
    vert_map: Dict[Tuple[Optional[int], Optional[int], Optional[int]], int] = {}
    out_v = []
    out_vt = []
    out_vn = []
    out_idx = []

    def add_vertex(key):
        if key in vert_map:
            return vert_map[key]
        vi, ti, ni = key
        pos = vs[vi - 1] if vi else [0.0, 0.0, 0.0]
        uv = vts[ti - 1] if ti else [0.0, 0.0]
        nor = vns[ni - 1] if ni else [0.0, 0.0, 0.0]
        idx = len(out_v)
        out_v.append(pos)
        out_vt.append(uv)
        out_vn.append(nor)
        vert_map[key] = idx
        return idx

    for face in faces:
        if triangulate and len(face) > 3:
            # fan triangulation: v0, v(i), v(i+1)
            v0 = face[0]
            for i in range(1, len(face) - 1):
                tri = [v0, face[i], face[i + 1]]
                idxs = [add_vertex(k) for k in tri]
                out_idx.append(idxs)
        else:
            idxs = [add_vertex(k) for k in face]
            if len(idxs) == 3:
                out_idx.append(idxs)
            else:
                # skip non-triangle if not triangulating
                pass

    mesh = Mesh(
        vertices=np.array(out_v, dtype=np.float32),
        indices=np.array(out_idx, dtype=np.uint32),
        uvs=np.array(out_vt, dtype=np.float32) if any(out_vt) else None,
        normals=np.array(out_vn, dtype=np.float32) if any(out_vn) else None
    )
    if mesh.normals is None or np.linalg.norm(mesh.normals).sum() == 0.0:
        mesh.compute_normals()
    return mesh


def save_obj(mesh: Mesh, path: str):
    with open(path, 'w', encoding='utf-8') as f:
        for v in mesh.vertices:
            f.write(f"v {v[0]} {v[1]} {v[2]}\n")
        if mesh.uvs is not None:
            for t in mesh.uvs:
                f.write(f"vt {t[0]} {t[1]}\n")
        if mesh.normals is not None:
            for n in mesh.normals:
                f.write(f"vn {n[0]} {n[1]} {n[2]}\n")
        # Faces reference vertices in order; we won't deduplicate here.
        # Write faces as simple triangles; indices are 1-based.
        has_t = mesh.uvs is not None
        has_n = mesh.normals is not None
        for tri in mesh.indices:
            def idx_str(i):
                vi = i + 1
                ti = vi if has_t else ''
                ni = vi if has_n else ''
                if has_t and has_n:
                    return f"{vi}/{ti}/{ni}"
                elif has_t:
                    return f"{vi}/{ti}"
                elif has_n:
                    return f"{vi}//{ni}"
                else:
                    return f"{vi}"
            a, b, c = [idx_str(int(i)) for i in tri]
            f.write(f"f {a} {b} {c}\n")


# ------------- OpenGL resources -------------

class ShaderProgram:
    def __init__(self, vertex_src: str, fragment_src: str):
        self.program = compileProgram(
            compileShader(vertex_src, 0x8B31),  # GL_VERTEX_SHADER
            compileShader(fragment_src, 0x8B30)  # GL_FRAGMENT_SHADER
        )

    def use(self):
        glUseProgram(self.program)

    def uniform_mat4(self, name: str, mat: np.ndarray):
        loc = glGetUniformLocation(self.program, name)
        glUniformMatrix4fv(loc, 1, True, mat.astype(np.float32))  # transpose=True (row-major)

    def uniform_vec3(self, name: str, vec: Tuple[float, float, float]):
        loc = glGetUniformLocation(self.program, name)
        glUniform3f(loc, vec[0], vec[1], vec[2])

    def uniform_float(self, name: str, val: float):
        loc = glGetUniformLocation(self.program, name)
        glUniform1f(loc, val)

    def uniform_int(self, name: str, val: int):
        loc = glGetUniformLocation(self.program, name)
        glUniform1i(loc, val)

    def delete(self):
        glDeleteProgram(self.program)
        self.program = 0


DEFAULT_VERT = """
#version 330 core
layout(location=0) in vec3 aPos;
layout(location=1) in vec3 aNormal;
layout(location=2) in vec2 aUV;

uniform mat4 uModel;
uniform mat4 uView;
uniform mat4 uProj;

out vec3 vNormal;
out vec3 vWorldPos;

void main(){
    vec4 worldPos = uModel * vec4(aPos, 1.0);
    vWorldPos = worldPos.xyz;
    // normal matrix approximate: assume uniform scale or pre-normalized normals
    vNormal = mat3(uModel) * aNormal;
    gl_Position = uProj * uView * worldPos;
}
"""

DEFAULT_FRAG = """
#version 330 core
in vec3 vNormal;
in vec3 vWorldPos;

out vec4 FragColor;

uniform vec3 uColor;
uniform vec3 uLightDir; // world-space, should be normalized
uniform vec3 uEye;      // camera position

void main(){
    vec3 N = normalize(vNormal);
    vec3 L = normalize(-uLightDir);
    float NdotL = max(dot(N, L), 0.0);

    vec3 base = uColor;
    vec3 ambient = 0.1 * base;
    vec3 diffuse = 0.9 * base * NdotL;

    FragColor = vec4(ambient + diffuse, 1.0);
}
"""


class GLMesh:
    def __init__(self, mesh: Mesh):
        self.index_count = mesh.indices.size
        self.vao = glGenVertexArrays(1)
        glBindVertexArray(self.vao)

        # Positions
        self.vbo_pos = glGenBuffers(1)
        glBindBuffer(GL_ARRAY_BUFFER, self.vbo_pos)
        glBufferData(GL_ARRAY_BUFFER, mesh.vertices.astype(np.float32).nbytes,
                     mesh.vertices.astype(np.float32), GL_STATIC_DRAW)
        glVertexAttribPointer(0, 3, 0x1406, False, 0, ctypes.c_void_p(0))  # GL_FLOAT
        glEnableVertexAttribArray(0)

        # Normals
        self.vbo_nrm = None
        if mesh.normals is not None:
            self.vbo_nrm = glGenBuffers(1)
            glBindBuffer(GL_ARRAY_BUFFER, self.vbo_nrm)
            glBufferData(GL_ARRAY_BUFFER, mesh.normals.astype(np.float32).nbytes,
                         mesh.normals.astype(np.float32), GL_STATIC_DRAW)
            glVertexAttribPointer(1, 3, 0x1406, False, 0, ctypes.c_void_p(0))
            glEnableVertexAttribArray(1)

        # UVs
        self.vbo_uv = None
        if mesh.uvs is not None:
            self.vbo_uv = glGenBuffers(1)
            glBindBuffer(GL_ARRAY_BUFFER, self.vbo_uv)
            glBufferData(GL_ARRAY_BUFFER, mesh.uvs.astype(np.float32).nbytes,
                         mesh.uvs.astype(np.float32), GL_STATIC_DRAW)
            glVertexAttribPointer(2, 2, 0x1406, False, 0, ctypes.c_void_p(0))
            glEnableVertexAttribArray(2)

        # Indices
        self.ebo = glGenBuffers(1)
        glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, self.ebo)
        inds = mesh.indices.astype(np.uint32).flatten()
        glBufferData(GL_ELEMENT_ARRAY_BUFFER, inds.nbytes, inds, GL_STATIC_DRAW)

        glBindVertexArray(0)
        glBindBuffer(GL_ARRAY_BUFFER, 0)

    def draw(self):
        glBindVertexArray(self.vao)
        glDrawElements(GL_TRIANGLES, self.index_count, GL_UNSIGNED_INT, ctypes.c_void_p(0))
        glBindVertexArray(0)

    def delete(self):
        if self.vbo_pos:
            glDeleteBuffers(1, [self.vbo_pos])
        if self.vbo_nrm:
            glDeleteBuffers(1, [self.vbo_nrm])
        if self.vbo_uv:
            glDeleteBuffers(1, [self.vbo_uv])
        if self.ebo:
            glDeleteBuffers(1, [self.ebo])
        if self.vao:
            glDeleteVertexArrays(1, [self.vao])
        self.vbo_pos = self.vbo_nrm = self.vbo_uv = self.ebo = self.vao = 0


# ------------- Camera and Renderer -------------

@dataclass
class Camera:
    eye: np.ndarray = field(default_factory=lambda: np.array([3.0, 3.0, 3.0], dtype=np.float32))
    target: np.ndarray = field(default_factory=lambda: np.array([0.0, 0.0, 0.0], dtype=np.float32))
    up: np.ndarray = field(default_factory=lambda: np.array([0.0, 1.0, 0.0], dtype=np.float32))
    fov: float = 60.0
    aspect: float = 1.6
    near: float = 0.1
    far: float = 100.0

    def view_matrix(self) -> np.ndarray:
        return look_at(self.eye, self.target, self.up)

    def proj_matrix(self) -> np.ndarray:
        return perspective(self.fov, self.aspect, self.near, self.far)


class Renderer:
    def __init__(self, camera: Camera, shader: Optional[ShaderProgram] = None):
        self.camera = camera
        self.shader = shader or ShaderProgram(DEFAULT_VERT, DEFAULT_FRAG)
        self.light_dir = normalize(np.array([1.0, 1.0, 1.0], dtype=np.float32))
        self.clear_color = (0.05, 0.05, 0.07, 1.0)

    def render(self, models: List[Model], glmeshes: Dict[int, GLMesh]):
        glClearColor(*self.clear_color)
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
        glEnable(GL_DEPTH_TEST)

        self.shader.use()
        self.shader.uniform_mat4("uView", self.camera.view_matrix())
        self.shader.uniform_mat4("uProj", self.camera.proj_matrix())
        self.shader.uniform_vec3("uLightDir", tuple(self.light_dir))
        self.shader.uniform_vec3("uEye", (float(self.camera.eye[0]), float(self.camera.eye[1]), float(self.camera.eye[2])))

        for m in models:
            model_mat = m.transform.baked_matrix()
            self.shader.uniform_mat4("uModel", model_mat)
            self.shader.uniform_vec3("uColor", m.material.color)
            glmesh = glmeshes[id(m.mesh)]
            glmesh.draw()


# ------------- Example GLUT viewer -------------

class Viewer:
    def __init__(self, models: List[Model], camera: Optional[Camera] = None):
        if not GLUT_AVAILABLE:
            raise RuntimeError("GLUT is not available. Install PyOpenGL and use a windowing toolkit.")
        self.models = models
        self.camera = camera or Camera()
        self.renderer = Renderer(self.camera)
        self.glmeshes: Dict[int, GLMesh] = {}
        self.win_w = 960
        self.win_h = 600

    def init_gl(self):
        # Create GL resources for meshes
        unique_meshes: Dict[int, Mesh] = {}
        for m in self.models:
            unique_meshes[id(m.mesh)] = m.mesh
        for k, mesh in unique_meshes.items():
            self.glmeshes[k] = GLMesh(mesh)

    def display(self):
        self.renderer.render(self.models, self.glmeshes)
        glutSwapBuffers()
        glutPostRedisplay()

    def reshape(self, w: int, h: int):
        self.win_w, self.win_h = w, h
        glViewport(0, 0, w, h)
        self.camera.aspect = max(w / max(1, h), 1e-6)

    def run(self, title: str = "GeoFX Viewer"):
        glutInit(sys.argv)
        glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGB | GLUT_DEPTH)
        glutInitWindowSize(self.win_w, self.win_h)
        glutInitWindowPosition(100, 100)
        glutCreateWindow(title.encode('utf-8'))

        self.init_gl()
        glutDisplayFunc(self.display)
        glutIdleFunc(self.display)
        glutReshapeFunc(self.reshape)
        glutMainLoop()

    def cleanup(self):
        for gm in self.glmeshes.values():
            gm.delete()
        if self.renderer.shader:
            self.renderer.shader.delete()


# ------------- Minimal demo -------------

def demo():
    # Build scene
    box = create_box((1.5, 1.0, 1.0))
    sphere = create_uv_sphere(0.6, segments=32, rings=16)
    m1 = Model(mesh=box, material=Material(color=(0.9, 0.3, 0.2)))
    m2 = Model(mesh=sphere, material=Material(color=(0.2, 0.6, 0.9)),
               transform=Transform(position=np.array([0.0, 0.9, 0.0], dtype=np.float32)))

    # Apply a rotation to box
    m1.transform.rotate_axis_angle((0, 1, 0), math.radians(30))

    cam = Camera(eye=np.array([3.0, 2.5, 4.0], dtype=np.float32),
                 target=np.array([0.0, 0.5, 0.0], dtype=np.float32),
                 up=np.array([0.0, 1.0, 0.0], dtype=np.float32),
                 fov=60.0, aspect=16/10, near=0.1, far=100.0)

    if GLUT_AVAILABLE:
        viewer = Viewer([m1, m2], camera=cam)
        try:
            viewer.run("GeoFX Demo")
        finally:
            viewer.cleanup()
    else:
        print("GLUT not available. Create your own context or install PyOpenGL GLUT to run the demo.")


if __name__ == "__main__":
    demo()
------------------------------------------------------------

PROMPT 306:
"Generate Python code that imports the NLTK library, Tokenize text data using NLTK's word_tokenize function, Remove stopwords from tokenized data using NLTK's corpus.stopwords module, Lemmatizes remaining words using NLTK's WordNet lemmatization tool."
RESPONSE:
# Import NLTK and required modules
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Ensure required NLTK resources are available
def ensure_nltk_resources():
    resources = [
        ("tokenizers/punkt", "punkt"),
        ("corpora/stopwords", "stopwords"),
        ("corpora/wordnet", "wordnet"),
        ("corpora/omw-1.4", "omw-1.4"),
    ]
    for path, name in resources:
        try:
            nltk.data.find(path)
        except LookupError:
            nltk.download(name, quiet=True)

ensure_nltk_resources()

# Preprocessing pipeline:
# 1) Tokenize text
# 2) Remove stopwords
# 3) Lemmatize remaining words
def preprocess(text, language="english"):
    # Tokenize
    tokens = word_tokenize(text)

    # Remove stopwords (case-insensitive)
    sw = set(stopwords.words(language))
    tokens_no_sw = [t for t in tokens if t.lower() not in sw]

    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(t.lower()) for t in tokens_no_sw]

    return lemmas

# Example usage
if __name__ == "__main__":
    sample_text = "Dogs are running quickly in the park, and the kids were happier than before."
    processed = preprocess(sample_text)
    print(processed)  # -> ['dog', 'running', 'quickly', 'park', ',', 'kid', 'happier', 'before', '.']
------------------------------------------------------------

PROMPT 307:
"Generate Python code that implements a JSON Web Token middleware for Starlette using the `jsonwebtoken` library, which generates and verifies JWT tokens, and the `starlette.middleware` module, which provides a framework for creating middleware functions."
RESPONSE:
# pip install starlette uvicorn PyJWT
import os
from datetime import datetime, timedelta, timezone
from typing import Callable, Optional, Sequence, Set, Union

import jwt  # PyJWT (a.k.a. jsonwebtoken)
from jwt import ExpiredSignatureError, InvalidTokenError

from starlette.applications import Starlette
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import JSONResponse, Response
from starlette.routing import Route


class JWTMiddleware(BaseHTTPMiddleware):
    def __init__(
        self,
        app,
        *,
        secret_key: str,
        algorithms: Sequence[str] = ("HS256",),
        auto_error: bool = True,
        exempt_paths: Optional[Set[str]] = None,
        auth_header: str = "Authorization",
        auth_scheme: str = "Bearer",
        audience: Optional[str] = None,
        issuer: Optional[str] = None,
        cookie_name: Optional[str] = None,
    ):
        super().__init__(app)
        self.secret_key = secret_key
        self.algorithms = list(algorithms)
        self.auto_error = auto_error
        self.exempt_paths = set(exempt_paths or set())
        self.auth_header = auth_header
        self.auth_scheme = auth_scheme
        self.audience = audience
        self.issuer = issuer
        self.cookie_name = cookie_name

    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        path = request.url.path

        request.state.user = None
        request.state.token = None
        request.state.claims = None

        if path in self.exempt_paths:
            return await call_next(request)

        token = self._extract_token(request)

        if not token:
            if self.auto_error:
                return self._unauthorized("Missing token")
            return await call_next(request)

        try:
            options = {}
            if self.audience is None:
                options["verify_aud"] = False

            payload = jwt.decode(
                token,
                self.secret_key,
                algorithms=self.algorithms,
                audience=self.audience,
                issuer=self.issuer,
                options=options,
            )

            request.state.token = token
            request.state.claims = payload
            request.state.user = payload.get("sub", payload)

        except ExpiredSignatureError:
            if self.auto_error:
                return self._unauthorized("Token expired")
            return await call_next(request)
        except InvalidTokenError:
            if self.auto_error:
                return self._unauthorized("Invalid token")
            return await call_next(request)

        return await call_next(request)

    def _extract_token(self, request: Request) -> Optional[str]:
        if self.cookie_name:
            cookie_val = request.cookies.get(self.cookie_name)
            if cookie_val:
                return cookie_val

        header_val = request.headers.get(self.auth_header)
        if not header_val:
            return None

        parts = header_val.split()
        if len(parts) == 2 and parts[0].lower() == self.auth_scheme.lower():
            return parts[1]
        return None

    def _unauthorized(self, description: str) -> Response:
        return JSONResponse(
            {"detail": description},
            status_code=401,
            headers={
                "WWW-Authenticate": f'{self.auth_scheme} realm="api", error="invalid_token", error_description="{description}"'
            },
        )


def create_access_token(
    subject: Union[str, int],
    *,
    secret_key: str,
    expires_in: int = 3600,
    algorithm: str = "HS256",
    issuer: Optional[str] = None,
    audience: Optional[str] = None,
    extra_claims: Optional[dict] = None,
) -> str:
    now = datetime.now(timezone.utc)
    payload = {
        "sub": str(subject),
        "iat": int(now.timestamp()),
        "nbf": int(now.timestamp()),
        "exp": int((now + timedelta(seconds=expires_in)).timestamp()),
    }
    if issuer:
        payload["iss"] = issuer
    if audience:
        payload["aud"] = audience
    if extra_claims:
        payload.update(extra_claims)

    token = jwt.encode(payload, secret_key, algorithm=algorithm)
    if isinstance(token, bytes):
        token = token.decode("utf-8")
    return token


# Example Starlette app
SECRET = os.environ.get("JWT_SECRET", "change-me")


async def public_endpoint(request: Request):
    return JSONResponse({"message": "Public OK"})


async def login(request: Request):
    body = await request.json()
    user_id = body.get("username", "anonymous")
    token = create_access_token(
        user_id,
        secret_key=SECRET,
        expires_in=1800,
        issuer="example-api",
        audience="example-clients",
    )
    return JSONResponse({"access_token": token, "token_type": "Bearer"})


async def protected_endpoint(request: Request):
    if not request.state.user:
        return JSONResponse({"detail": "Unauthorized"}, status_code=401)
    return JSONResponse(
        {"message": "Protected OK", "user": request.state.user, "claims": request.state.claims}
    )


routes = [
    Route("/public", public_endpoint),
    Route("/login", login, methods=["POST"]),
    Route("/protected", protected_endpoint),
]

app = Starlette(debug=True, routes=routes)
app.add_middleware(
    JWTMiddleware,
    secret_key=SECRET,
    algorithms=["HS256"],
    auto_error=True,
    exempt_paths={"/public", "/login"},
    issuer="example-api",
    audience="example-clients",
    cookie_name=None,  # set to a cookie name if you store tokens in cookies
)
------------------------------------------------------------

PROMPT 308:
"Generate Python code that imports the re module and uses its compile function to create a regular expression object from a valid regular expression pattern, then uses the sample method of the re module to generate a random string that matches the pattern."
RESPONSE:
Python’s re module does not have a sample method. Here’s a working alternative that still uses re.compile, and uses the third‑party rstr library to generate a random matching string.

# pip install rstr

import re
import rstr

# Compile a valid regex pattern
regex = re.compile(r'^[A-Z]{2}\d{3}[a-z]{2}$')

# Generate a random string that matches the pattern
random_string = rstr.xeger(regex.pattern)

print(random_string)

# Optional: verify it matches
assert regex.fullmatch(random_string) is not None
------------------------------------------------------------

PROMPT 309:
"Generate Python code that imports the unittest module, creates a mock object for urllib3 using the patch decorator from the mockk library, and defines test functions for each of the three main methods in urllib3 - request(), response(), and get_response()."
RESPONSE:
I can generate this, but need to clarify two issues first:
- In Python, mocking is typically done with unittest.mock.patch (or the mock backport). There’s no “mockk” library for Python; mockk is a Kotlin/Java library. Did you mean unittest.mock.patch?
- urllib3 exposes urllib3.request(...) and the HTTPResponse class (urllib3.response.HTTPResponse), but it does not have response() or get_response() functions. Did you perhaps mean the stdlib’s http.client.HTTPConnection.getresponse() or something else?

If you meant “use unittest.mock.patch” and test urllib3.request plus interactions with HTTPResponse, here’s a working example you can run:

import unittest
from unittest.mock import patch, MagicMock
import urllib3


class TestUrllib3(unittest.TestCase):
    @patch("urllib3.PoolManager")
    def test_request(self, MockPoolManager):
        # Arrange
        fake_response = MagicMock(name="HTTPResponse")
        instance = MockPoolManager.return_value
        instance.request.return_value = fake_response

        # Force urllib3.request() to create a PoolManager so our patch is used
        original_default_manager = getattr(urllib3, "_default_manager", None)
        urllib3._default_manager = None
        try:
            # Act
            resp = urllib3.request("GET", "https://example.com", headers={"X": "Y"})

            # Assert
            MockPoolManager.assert_called_once()
            instance.request.assert_called_once_with("GET", "https://example.com", headers={"X": "Y"})
            self.assertIs(resp, fake_response)
        finally:
            # Cleanup
            urllib3._default_manager = original_default_manager

    @patch("urllib3.response.HTTPResponse")
    def test_response_object_usage(self, MockHTTPResponse):
        # Arrange
        fake = MockHTTPResponse.return_value
        fake.status = 200
        fake.data = b"ok"

        # Act: simulate constructing an HTTPResponse (constructor is mocked)
        r = urllib3.response.HTTPResponse(body=b"ok", status=200)

        # Assert
        MockHTTPResponse.assert_called_once_with(body=b"ok", status=200)
        self.assertEqual(r.status, 200)
        self.assertEqual(r.data, b"ok")

    # Placeholder: urllib3 has no 'get_response' function.
    # If you intended a different API (e.g., http.client.HTTPConnection.getresponse),
    # please confirm and I’ll adapt this test accordingly.
    # Example of how you'd mock a non-existent symbol if needed:
    @patch("urllib3.get_response", create=True)
    def test_get_response_placeholder(self, mock_get_response):
        mock_get_response.return_value = MagicMock(name="HTTPResponse")

        # Example call in your code under test would be something like:
        # result = urllib3.get_response("https://example.com")
        # For demonstration, we call the patched function directly:
        result = urllib3.get_response("https://example.com")

        mock_get_response.assert_called_once_with("https://example.com")
        self.assertIsNotNone(result)


if __name__ == "__main__":
    unittest.main()

If you confirm the exact API you want (and whether you indeed want unittest.mock rather than mockk), I’ll adjust the code to match precisely.
------------------------------------------------------------

PROMPT 310:
"Generate Python code that imports the necessary modules and creates a function that takes a path to a file and a value to write to that file, and then uses the atomic writes functionality to write the value to the file without overwriting any existing data."
RESPONSE:
import os

def atomic_append(path, value, encoding="utf-8", add_newline=False):
    """
    Atomically append value to a file without overwriting existing data.
    Writes are done with O_APPEND so the kernel moves the file offset to
    end-of-file for the write, making each write() atomic with respect to
    other writers.

    Args:
        path (str | bytes): Path to the file.
        value (str | bytes): Data to append.
        encoding (str): Used if value is a str.
        add_newline (bool): If True, append a trailing newline if missing.
    """
    # Normalize to bytes in one buffer so we can write in a single syscall.
    if isinstance(value, str):
        data = value.encode(encoding)
    else:
        data = bytes(value)

    if add_newline and (not data.endswith(b"\n")):
        data += b"\n"

    flags = os.O_WRONLY | os.O_CREAT | os.O_APPEND
    try:
        flags |= os.O_CLOEXEC  # best-effort, not available everywhere
    except AttributeError:
        pass

    fd = os.open(path, flags, 0o666)
    try:
        # Single write call for atomic append relative to other writers.
        written = os.write(fd, data)
        if written != len(data):
            raise OSError(f"Short write: wrote {written} of {len(data)} bytes")
        os.fsync(fd)  # ensure durability
    finally:
        os.close(fd)
------------------------------------------------------------

PROMPT 311:
"Generate Python code that creates a Jupyter extension to display resource usage, including CPU, memory, and disk usage, using the `psutil` library to gather system information and the `ipywidgets` library to create interactive widgets for the Jupyter notebook."
RESPONSE:
```python
"""
Jupyter/IPython extension: Resource usage monitor using psutil + ipywidgets.

Usage:
1) Save this code to a file, e.g. resource_monitor.py (or paste into a cell to define it in-session).
2) In a Jupyter notebook cell, run:
   %load_ext resource_monitor
   # The widget will appear. Use Start/Stop to control updates.
3) To unload/cleanup:
   %unload_ext resource_monitor
"""

import asyncio
import os
from dataclasses import dataclass
from typing import Optional

import psutil
import ipywidgets as widgets
from IPython.display import display


def _default_disk_path() -> str:
    # Prefer current working directory's drive, else fallback to root.
    try:
        cwd = os.path.abspath(os.getcwd())
        if os.name == "nt":
            # Windows: return drive root like C:\
            drive = os.path.splitdrive(cwd)[0]
            return drive + "\\" if drive else os.path.abspath(os.sep)
        else:
            # POSIX: use /
            return os.path.abspath(os.sep)
    except Exception:
        return os.path.abspath(os.sep)


@dataclass
class _TaskHandle:
    task: Optional[asyncio.Task] = None


class ResourceMonitorWidget:
    def __init__(self):
        # Header
        self.title = widgets.HTML(
            "<b>System Resource Monitor</b> <span style='color:gray'>(psutil + ipywidgets)</span>"
        )

        # CPU widgets
        self.cpu_bar = widgets.FloatProgress(
            value=0.0, min=0.0, max=100.0, description="CPU:",
            bar_style="", orientation="horizontal"
        )
        self.cpu_label = widgets.Label("0.0%")

        # Memory widgets
        self.mem_bar = widgets.FloatProgress(
            value=0.0, min=0.0, max=100.0, description="Memory:",
            bar_style="", orientation="horizontal"
        )
        self.mem_label = widgets.Label("0.0%")

        # Disk widgets
        self.disk_path = widgets.Text(
            value=_default_disk_path(), description="Disk path:", layout=widgets.Layout(width="50%")
        )
        self.disk_bar = widgets.FloatProgress(
            value=0.0, min=0.0, max=100.0, description="Disk:",
            bar_style="", orientation="horizontal"
        )
        self.disk_label = widgets.Label("0.0%")

        # Controls
        self.interval = widgets.FloatSlider(
            value=1.0, min=0.2, max=10.0, step=0.1, readout=True,
            description="Interval (s):", continuous_update=False
        )
        self.start_button = widgets.Button(description="Start", button_style="success", icon="play")
        self.stop_button = widgets.Button(description="Stop", button_style="warning", icon="stop", disabled=True)
        self.refresh_button = widgets.Button(description="Refresh once", icon="refresh")

        # Status
        self.status = widgets.HTML("<i>Idle</i>")

        # Layout
        self.ui = widgets.VBox([
            self.title,
            widgets.HBox([self.cpu_bar, self.cpu_label]),
            widgets.HBox([self.mem_bar, self.mem_label]),
            widgets.HBox([self.disk_bar, self.disk_label]),
            self.disk_path,
            widgets.HBox([self.interval, self.start_button, self.stop_button, self.refresh_button]),
            self.status,
        ])

        # Runtime
        self._running = False
        self._task_handle = _TaskHandle()
        self._last_cpu_call = None  # psutil.cpu_percent priming

        # Wire events
        self.start_button.on_click(self._on_start)
        self.stop_button.on_click(self._on_stop)
        self.refresh_button.on_click(self._on_refresh)
        self.disk_path.observe(self._on_disk_path_change, names="value")

        # Prime cpu_percent to avoid first-call averaging delay
        try:
            psutil.cpu_percent(interval=None)
        except Exception:
            pass

        # Initial update
        self._update_once()

    def _set_status(self, text: str):
        self.status.value = text

    def _on_start(self, _):
        self.start()

    def _on_stop(self, _):
        self.stop()

    def _on_refresh(self, _):
        self._update_once()

    def _on_disk_path_change(self, change):
        # Validate path quickly; update bar on change without starting loop
        self._update_disk_safely()

    def _update_bar(self, bar: widgets.FloatProgress, label: widgets.Label, pct: float):
        pct = max(0.0, min(100.0, float(pct)))
        bar.value = pct
        label.value = f"{pct:.1f}%"
        # Change bar color thresholds
        if pct < 50:
            bar.bar_style = ""
        elif pct < 75:
            bar.bar_style = "info"
        elif pct < 90:
            bar.bar_style = "warning"
        else:
            bar.bar_style = "danger"

    def _update_cpu(self):
        try:
            cpu_pct = psutil.cpu_percent(interval=None)
        except Exception:
            cpu_pct = 0.0
        self._update_bar(self.cpu_bar, self.cpu_label, cpu_pct)

    def _update_mem(self):
        try:
            mem = psutil.virtual_memory()
            mem_pct = mem.percent
        except Exception:
            mem_pct = 0.0
        self._update_bar(self.mem_bar, self.mem_label, mem_pct)

    def _disk_usage_for_path(self, path: str) -> Optional[float]:
        try:
            # psutil.disk_usage supports any valid path that resides on a partition
            usage = psutil.disk_usage(path)
            return usage.percent
        except Exception:
            return None

    def _update_disk_safely(self):
        pct = self._disk_usage_for_path(self.disk_path.value or _default_disk_path())
        if pct is None:
            # Indicate invalid path
            self.disk_bar.bar_style = "danger"
            self.disk_bar.value = 0.0
            self.disk_label.value = "Invalid path"
        else:
            self._update_bar(self.disk_bar, self.disk_label, pct)

    def _update_once(self):
        self._update_cpu()
        self._update_mem()
        self._update_disk_safely()

    async def _run(self):
        self._running = True
        self._set_status("<span style='color:green'>Running</span>")
        self.start_button.disabled = True
        self.stop_button.disabled = False
        try:
            while self._running:
                self._update_once()
                interval = max(0.1, float(self.interval.value or 1.0))
                await asyncio.sleep(interval)
        except asyncio.CancelledError:
            pass
        finally:
            self.start_button.disabled = False
            self.stop_button.disabled = True
            if self._running:
                self._set_status("<span style='color:orange'>Stopped</span>")
            else:
                self._set_status("<i>Idle</i>")

    def start(self):
        if self._task_handle.task and not self._task_handle.task.done():
            return
        # Start background task
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.get_event_loop()
        self._task_handle.task = loop.create_task(self._run())

    def stop(self):
        self._running = False
        task = self._task_handle.task
        if task and not task.done():
            task.cancel()

    def close(self):
        self.stop()
        try:
            self.ui.close()
        except Exception:
            pass


# IPython extension API
_RESOURCE_MONITOR_INSTANCE: Optional[ResourceMonitorWidget] = None

def load_ipython_extension(ipython):
    global _RESOURCE_MONITOR_INSTANCE
    if _RESOURCE_MONITOR_INSTANCE is None:
        _RESOURCE_MONITOR_INSTANCE = ResourceMonitorWidget()
        display(_RESOURCE_MONITOR_INSTANCE.ui)
        # Do not auto-start; let user click Start
        _RESOURCE_MONITOR_INSTANCE._set_status("<i>Idle (click Start)</i>")
    else:
        display(_RESOURCE_MONITOR_INSTANCE.ui)

def unload_ipython_extension(ipython):
    global _RESOURCE_MONITOR_INSTANCE
    if _RESOURCE_MONITOR_INSTANCE is not None:
        _RESOURCE_MONITOR_INSTANCE.close()
        _RESOURCE_MONITOR_INSTANCE = None
```
------------------------------------------------------------

PROMPT 312:
"Generate Python code that implements a reference implementation for Bech32 and segwit addresses, including encoding and decoding functions for each format, and a main function that demonstrates how to use the library to convert between different address formats."
RESPONSE:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Reference implementation of:
- Bech32 / Bech32m encoding and decoding (BIP-0173, BIP-0350)
- Segwit address encoding and decoding (P2WPKH, P2WSH, Taproot v1, etc.)

Includes a main() demonstrating:
- Encoding witness programs to segwit addresses (mainnet/testnet)
- Decoding segwit addresses to witness version/program
- Converting addresses across networks (re-encode decoded witness program with a different HRP)
- Producing scriptPubKey bytes from a segwit address (and parsing it back)

This module is self-contained and has no external dependencies.
"""

from typing import List, Tuple, Optional

# Bech32 charset
CHARSET = "qpzry9x8gf2tvdw0s3jn54khce6mua7l"
CHARSET_REV = {c: i for i, c in enumerate(CHARSET)}

# Generator coefficients for polymod
GEN = [0x3b6a57b2, 0x26508e6d, 0x1ea119fa, 0x3d4233dd, 0x2a1462b3]

# Checksum constants
BECH32_CONST = 1
BECH32M_CONST = 0x2bc830a3


def _bech32_hrp_expand(hrp: str) -> List[int]:
    return [ord(x) >> 5 for x in hrp] + [0] + [ord(x) & 31 for x in hrp]


def _bech32_polymod(values: List[int]) -> int:
    chk = 1
    for v in values:
        top = chk >> 25
        chk = ((chk & 0x1ffffff) << 5) ^ v
        for i in range(5):
            if (top >> i) & 1:
                chk ^= GEN[i]
    return chk


def _bech32_create_checksum(hrp: str, data: List[int], spec: str) -> List[int]:
    const = BECH32_CONST if spec == "bech32" else BECH32M_CONST
    values = _bech32_hrp_expand(hrp) + data
    polymod = _bech32_polymod(values + [0, 0, 0, 0, 0, 0]) ^ const
    return [(polymod >> (5 * (5 - i))) & 31 for i in range(6)]


def _bech32_verify_checksum(hrp: str, data: List[int]) -> Optional[str]:
    polymod = _bech32_polymod(_bech32_hrp_expand(hrp) + data)
    if polymod == BECH32_CONST:
        return "bech32"
    if polymod == BECH32M_CONST:
        return "bech32m"
    return None


def bech32_encode(hrp: str, data: List[int], spec: str = "bech32") -> str:
    """
    Encode HRP and data into a Bech32 or Bech32m string.
    spec: "bech32" (BIP173) or "bech32m" (BIP350)
    """
    if spec not in ("bech32", "bech32m"):
        raise ValueError("spec must be 'bech32' or 'bech32m'")
    if not hrp:
        raise ValueError("HRP must not be empty")
    if any((x < 0 or x > 31) for x in data):
        raise ValueError("data values must be 5-bit (0..31)")
    if any(ord(c) < 33 or ord(c) > 126 for c in hrp):
        raise ValueError("HRP contains invalid characters")
    hrp = hrp.lower()
    combined = data + _bech32_create_checksum(hrp, data, spec)
    ret = hrp + "1" + "".join(CHARSET[d] for d in combined)
    if len(ret) > 90:
        raise ValueError("Bech32 string too long (>90)")
    return ret


def bech32_decode(bech: str) -> Tuple[str, List[int], str]:
    """
    Decode a Bech32/Bech32m string into (hrp, data-without-checksum, spec)
    Raises ValueError on invalid input.
    """
    if not (8 <= len(bech) <= 90):
        raise ValueError("Invalid bech32 length")

    # Disallow mixed case
    if bech.lower() != bech and bech.upper() != bech:
        raise ValueError("Mixed-case Bech32 string")
    bech = bech.lower()

    # Only printable ASCII
    if any(ord(x) < 33 or ord(x) > 126 for x in bech):
        raise ValueError("Invalid character range in bech32")

    pos = bech.rfind("1")
    if pos < 1:
        raise ValueError("No separator '1' found or HRP missing")
    if pos + 7 > len(bech):
        raise ValueError("Too short data part (requires checksum)")

    hrp = bech[:pos]
    data_part = bech[pos + 1 :]

    data = []
    for c in data_part:
        if c not in CHARSET_REV:
            raise ValueError("Invalid character in data part")
        data.append(CHARSET_REV[c])

    spec = _bech32_verify_checksum(hrp, data)
    if spec is None:
        raise ValueError("Invalid checksum")
    # strip checksum
    return hrp, data[:-6], spec


def convertbits(data: bytes, frombits: int, tobits: int, pad: bool = True) -> Optional[List[int]]:
    """
    General power-of-2 base conversion.
    Returns list of integers (each < 2**tobits) or None on failure.
    """
    acc = 0
    bits = 0
    ret: List[int] = []
    maxv = (1 << tobits) - 1
    max_acc = (1 << (frombits + tobits - 1)) - 1
    for b in data:
        if b < 0 or (b >> frombits):
            return None
        acc = ((acc << frombits) | b) & max_acc
        bits += frombits
        while bits >= tobits:
            bits -= tobits
            ret.append((acc >> bits) & maxv)
    if pad:
        if bits:
            ret.append((acc << (tobits - bits)) & maxv)
    elif bits >= frombits or ((acc << (tobits - bits)) & maxv):
        # Non-zero padding
        return None
    return ret


def segwit_addr_encode(hrp: str, witver: int, witprog: bytes) -> str:
    """
    Encode a segwit address for given HRP ('bc', 'tb', 'bcrt', etc.),
    witness version (0..16), and witness program (bytes).
    Uses Bech32 for version 0, Bech32m for version >= 1.
    """
    if not (0 <= witver <= 16):
        raise ValueError("Invalid witness version (0..16)")
    if witver == 0 and len(witprog) not in (20, 32):
        raise ValueError("Invalid v0 witness program length (must be 20 or 32)")
    if witver != 0 and not (2 <= len(witprog) <= 40):
        raise ValueError("Invalid witness program length (2..40)")

    data = convertbits(witprog, 8, 5, pad=True)
    if data is None:
        raise ValueError("convertbits failed")

    data = [witver] + data
    spec = "bech32" if witver == 0 else "bech32m"
    return bech32_encode(hrp.lower(), data, spec)


def segwit_addr_decode(hrp: str, addr: str) -> Tuple[int, bytes]:
    """
    Decode a segwit address. Returns (witver, witprog bytes).
    Validates HRP, checksum (bech32/bech32m), version and length rules.
    """
    dec_hrp, data, spec = bech32_decode(addr)
    if dec_hrp != hrp.lower():
        raise ValueError(f"HRP mismatch: expected '{hrp}', got '{dec_hrp}'")
    if not data:
        raise ValueError("No data in segwit address")
    witver = data[0]
    if not (0 <= witver <= 16):
        raise ValueError("Invalid witness version (0..16)")

    prog5 = data[1:]
    witprog_bytes_list = convertbits(bytes(prog5), 5, 8, pad=False)
    if witprog_bytes_list is None:
        raise ValueError("convertbits failed (non-zero padding)")
    witprog = bytes(witprog_bytes_list)

    # Length checks
    if witver == 0 and len(witprog) not in (20, 32):
        raise ValueError("Invalid v0 witness program length (must be 20 or 32)")
    if witver != 0 and not (2 <= len(witprog) <= 40):
        raise ValueError("Invalid witness program length (2..40)")

    # Spec check per version: v0 -> bech32, v>=1 -> bech32m
    if witver == 0 and spec != "bech32":
        raise ValueError("v0 address must use bech32 checksum")
    if witver != 0 and spec != "bech32m":
        raise ValueError("v>=1 address must use bech32m checksum")

    return witver, witprog


def segwit_scriptpubkey(witver: int, witprog: bytes) -> bytes:
    """
    Build a segwit scriptPubKey from witness version and program:
    OP_n (version) + PUSH(len) + program
    """
    if not (0 <= witver <= 16):
        raise ValueError("Invalid witness version (0..16)")
    if witver == 0:
        ver_opcode = 0x00
    else:
        ver_opcode = 0x50 + witver  # OP_1..OP_16
    if not (2 <= len(witprog) <= 40) and not (witver == 0 and len(witprog) in (20, 32)):
        raise ValueError("Invalid witness program length")
    return bytes([ver_opcode, len(witprog)]) + witprog


def scriptpubkey_to_segwit(spk: bytes) -> Tuple[int, bytes]:
    """
    Parse a segwit scriptPubKey back into (witver, witprog).
    Raises ValueError if not a valid segwit scriptPubKey.
    """
    if len(spk) < 4:
        raise ValueError("scriptPubKey too short")
    ver_opcode = spk[0]
    if ver_opcode == 0x00:
        witver = 0
    elif 0x51 <= ver_opcode <= 0x60:
        witver = ver_opcode - 0x50
    else:
        raise ValueError("Not a segwit scriptPubKey (bad version opcode)")
    push_len = spk[1]
    if len(spk) != 2 + push_len:
        raise ValueError("Malformed push length")
    witprog = spk[2:]
    # Length rules as in segwit addresses
    if witver == 0 and len(witprog) not in (20, 32):
        raise ValueError("Invalid v0 witness program length")
    if witver != 0 and not (2 <= len(witprog) <= 40):
        raise ValueError("Invalid witness program length")
    return witver, witprog


def _hex(s: str) -> bytes:
    return bytes.fromhex(s)


def _bhex(b: bytes) -> str:
    return b.hex()


def main():
    print("=== Bech32/Bech32m + Segwit reference demo ===")
    # Example witness programs (dummy data for demo)
    p2wpkh_prog = _hex("00112233445566778899aabbccddeeff00112233")  # 20 bytes
    p2wsh_prog = _hex("11" * 32)  # 32 bytes
    taproot_xonly = _hex("ab" * 32)  # 32 bytes (taproot key)

    # Encode segwit addresses for mainnet (bc) and testnet (tb)
    bc_p2wpkh = segwit_addr_encode("bc", 0, p2wpkh_prog)
    tb_p2wpkh = segwit_addr_encode("tb", 0, p2wpkh_prog)
    bc_p2wsh = segwit_addr_encode("bc", 0, p2wsh_prog)
    bc_taproot = segwit_addr_encode("bc", 1, taproot_xonly)
    tb_taproot = segwit_addr_encode("tb", 1, taproot_xonly)

    print("Mainnet P2WPKH (v0, bech32):", bc_p2wpkh)
    print("Testnet P2WPKH (v0, bech32):", tb_p2wpkh)
    print("Mainnet P2WSH  (v0, bech32):", bc_p2wsh)
    print("Mainnet Taproot (v1, bech32m):", bc_taproot)
    print("Testnet Taproot (v1, bech32m):", tb_taproot)

    # Decode and verify
    for hrp, addr in [("bc", bc_p2wpkh), ("tb", tb_p2wpkh), ("bc", bc_p2wsh), ("bc", bc_taproot), ("tb", tb_taproot)]:
        witver, witprog = segwit_addr_decode(hrp, addr)
        print(f"Decoded {addr}: ver={witver}, proglen={len(witprog)}, prog={_bhex(witprog)}")

    # Convert "formats": re-encode decoded witness program with a different HRP (mainnet <-> testnet)
    witver, witprog = segwit_addr_decode("bc", bc_p2wpkh)
    as_testnet = segwit_addr_encode("tb", witver, witprog)
    print("Converted mainnet P2WPKH to testnet (same witness data):", as_testnet)

    # ScriptPubKey from address, and back
    wv, wp = segwit_addr_decode("bc", bc_taproot)
    spk = segwit_scriptpubkey(wv, wp)
    print("Taproot scriptPubKey (hex):", _bhex(spk))
    wv2, wp2 = scriptpubkey_to_segwit(spk)
    addr_roundtrip = segwit_addr_encode("bc", wv2, wp2)
    print("Roundtrip back to address:", addr_roundtrip)

    # Generic Bech32 example (not segwit-specific)
    hrp = "example"
    data = [0, 1, 2, 3, 4, 5, 6]  # 5-bit words
    b32 = bech32_encode(hrp, data, spec="bech32")
    print("Generic Bech32:", b32)
    dhrp, ddata, dspec = bech32_decode(b32)
    print("Decoded generic Bech32:", dhrp, ddata, dspec)

    # Demonstrate error handling
    try:
        segwit_addr_decode("bc", tb_p2wpkh)  # Wrong HRP on purpose
    except ValueError as e:
        print("Expected error (HRP mismatch):", e)


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 313:
"Generate Python code that imports the necessary libraries and uses the pyamf library to serialize and deserialize data in AMF format, similar to how the `json` module works but specifically designed for working with Adobe Media Server and its proprietary format."
RESPONSE:
# Requires: pyamf (pip install pyamf)
# Note: PyAMF is an older library primarily used with Adobe/Flash-era stacks.
# It provides AMF0/AMF3 serialization similar in spirit to json.dumps/json.loads.

from io import BytesIO
import pyamf
from pyamf import encode, decode


# Internal helper to map version selector to PyAMF encoding constant
def _to_encoding(version):
    if isinstance(version, str):
        v = version.strip().lower()
        if v in ("3", "amf3"):
            return pyamf.AMF3
        if v in ("0", "amf0"):
            return pyamf.AMF0
    if version in (3, pyamf.AMF3):
        return pyamf.AMF3
    if version in (0, pyamf.AMF0):
        return pyamf.AMF0
    # Default to AMF3
    return pyamf.AMF3


def amf_dumps(obj, version=3):
    """
    Serialize a Python object to AMF bytes.
    version: 3/'amf3' or 0/'amf0'
    """
    encoding = _to_encoding(version)
    return encode(obj, encoding=encoding).getvalue()


def amf_dump(obj, fp, version=3):
    """
    Serialize obj to AMF and write to a binary file-like object fp.
    """
    fp.write(amf_dumps(obj, version=version))


def amf_loads(b, version=3):
    """
    Deserialize AMF bytes to a Python object.
    version: 3/'amf3' or 0/'amf0'
    """
    encoding = _to_encoding(version)
    decoder = decode(BytesIO(b), encoding=encoding)
    return next(decoder)


def amf_load(fp, version=3):
    """
    Read AMF bytes from a binary file-like object fp and deserialize.
    """
    return amf_loads(fp.read(), version=version)


# Example usage
if __name__ == "__main__":
    data = {
        "user": "alice",
        "score": 1337,
        "items": ["sword", "shield"],
        "active": True,
        "meta": {"region": "NA", "level": 7},
    }

    # AMF3 (default)
    amf3_bytes = amf_dumps(data, version=3)
    round_trip_amf3 = amf_loads(amf3_bytes, version=3)
    print("AMF3 equal:", round_trip_amf3 == data)

    # AMF0
    amf0_bytes = amf_dumps(data, version=0)
    round_trip_amf0 = amf_loads(amf0_bytes, version=0)
    print("AMF0 equal:", round_trip_amf0 == data)

    # File I/O example
    with open("example.amf3", "wb") as f:
        amf_dump(data, f, version="amf3")

    with open("example.amf3", "rb") as f:
        loaded = amf_load(f, version="amf3")
        print("Loaded from file equal:", loaded == data)
------------------------------------------------------------

PROMPT 314:
"Generate Python code that imports the requests and BeautifulSoup libraries, uses the requests library to make an HTTP request to a Wikipedia API, parses the HTML response using BeautifulSoup, and extracts specific data such as article titles, summaries, and links."
RESPONSE:
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote, urljoin

WIKI_API = "https://en.wikipedia.org/w/api.php"
WIKI_REST_HTML = "https://en.wikipedia.org/api/rest_v1/page/html/"
WIKI_BASE = "https://en.wikipedia.org"
HEADERS = {
    "User-Agent": "ExampleBot/1.0 (contact: youremail@example.com)"
}

def search_titles(query, limit=5):
    params = {
        "action": "query",
        "list": "search",
        "srsearch": query,
        "srlimit": limit,
        "format": "json"
    }
    resp = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=15)
    resp.raise_for_status()
    data = resp.json()
    return [item["title"] for item in data.get("query", {}).get("search", [])]

def fetch_page_html(title):
    url = WIKI_REST_HTML + quote(title)
    resp = requests.get(url, headers=HEADERS, timeout=15)
    resp.raise_for_status()
    return resp.text

def parse_article(html, page_url):
    soup = BeautifulSoup(html, "html.parser")

    # Extract page title
    title = None
    for meta_key in ("mw:displayTitle", "mw:pageTitle", "og:title"):
        tag = soup.find("meta", attrs={"property": meta_key})
        if tag and tag.get("content"):
            title = tag["content"]
            break
    if not title and soup.title:
        title = soup.title.get_text().replace(" - Wikipedia", "").strip()

    # Get the lead section (section id 0 is the lead in REST HTML)
    lead_section = soup.select_one('section[data-mw-section-id="0"]') or soup

    # Extract a summary: first non-empty paragraph in the lead
    summary = None
    for p in lead_section.find_all("p", recursive=True):
        text = p.get_text(" ", strip=True)
        if text:
            summary = text
            break

    # Extract links from the lead section
    links = []
    seen = set()
    for a in lead_section.find_all("a", href=True):
        href = a["href"]
        if href.startswith("#"):
            continue
        if href.startswith("//"):
            abs_url = "https:" + href
        elif href.startswith("/"):
            abs_url = urljoin(WIKI_BASE, href)
        elif href.startswith("http"):
            abs_url = href
        else:
            abs_url = urljoin(page_url, href)

        text = a.get_text(" ", strip=True)
        if abs_url not in seen and text:
            seen.add(abs_url)
            links.append({"text": text, "url": abs_url})

    return {
        "title": title,
        "summary": summary,
        "url": page_url,
        "links": links
    }

def get_articles(query, limit=5):
    results = []
    for title in search_titles(query, limit=limit):
        page_url = f"{WIKI_BASE}/wiki/{title.replace(' ', '_')}"
        html = fetch_page_html(title)
        article = parse_article(html, page_url)
        results.append(article)
    return results

if __name__ == "__main__":
    query = "machine learning"
    articles = get_articles(query, limit=3)
    for i, a in enumerate(articles, 1):
        print(f"{i}. {a['title']}")
        print(f"URL: {a['url']}")
        print(f"Summary: {a['summary'][:300]}{'...' if a['summary'] and len(a['summary']) > 300 else ''}")
        print(f"First 5 links:")
        for link in a["links"][:5]:
            print(f"  - {link['text']}: {link['url']}")
        print()
------------------------------------------------------------

PROMPT 315:
"Generate Python code that imports the necessary libraries and creates a class for interacting with OpenJPEG files using the openjpeg library, including methods for reading and writing image data, retrieving metadata, and converting between different image formats."
RESPONSE:
import os
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

try:
    import glymur  # Python interface to the OpenJPEG library
except ImportError as e:
    raise ImportError(
        "glymur is required but not installed. Install with 'pip install glymur'. "
        "Note: glymur requires OpenJPEG to be installed on your system."
    ) from e

try:
    from PIL import Image
except ImportError as e:
    raise ImportError(
        "Pillow is required for format conversion. Install with 'pip install pillow'."
    ) from e


class OpenJPEGHandler:
    """
    A convenience class for interacting with JPEG 2000 files (JP2/J2K/JPT) via the OpenJPEG
    library using glymur. Provides methods for reading/writing, extracting metadata, and
    converting to/from common image formats (e.g., PNG, JPEG, TIFF).

    Requirements:
      - glymur (Python wrapper for OpenJPEG)
      - OpenJPEG (system library)
      - Pillow (for non-JPEG2000 format conversions)
    """

    JP2K_EXTS = {".jp2", ".j2k", ".jpf", ".jpx", ".jpt"}

    def __init__(self) -> None:
        # Optionally, you could set glymur options here (e.g., for library paths).
        # glymur.set_option('lib.openjpeg', '/path/to/libopenjp2.so')
        pass

    # ------------- Public API -------------

    def read_image(self, path: str, rlevel: int = 0) -> np.ndarray:
        """
        Read a JPEG 2000 image into a NumPy array.

        Parameters:
          - path: str, path to JP2/J2K file.
          - rlevel: int, optional reduced resolution level (0 = full res, 1 = half, etc.).

        Returns:
          - np.ndarray: HxW or HxWxC array, dtype typically uint8/uint16.
        """
        self._check_exists(path)
        jp2 = glymur.Jp2k(path)
        # Use glymur's read with reduced resolution level if requested.
        # Fallback to slicing if needed.
        try:
            img = jp2.read(rlevel=rlevel)
        except TypeError:
            # Older glymur versions might not support rlevel in read(), fallback to full read.
            img = jp2[:]
        return img

    def write_image(
        self,
        array: np.ndarray,
        path: str,
        cratios: Optional[List[int]] = None,
        psnr: Optional[List[float]] = None,
        irreversible: Optional[bool] = None,
        tilesize: Optional[Tuple[int, int]] = None,
        prog: Optional[str] = None,
        numres: Optional[int] = None,
        comment: Optional[str] = None,
    ) -> None:
        """
        Write a NumPy array as a JPEG 2000 file (JP2/J2K) using OpenJPEG via glymur.

        Parameters:
          - array: np.ndarray, HxW or HxWxC, dtype uint8/uint16/float32.
          - path: str, output file with .jp2 or .j2k extension.
          - cratios: list of compression ratios (e.g., [10] for 10:1). Mutually exclusive with psnr.
          - psnr: list of PSNR targets (e.g., [40.0]). Mutually exclusive with cratios.
          - irreversible: bool, True = lossy DWT, False = reversible (lossless). Defaults:
                          If none provided, glymur's default applies (often irreversible True).
          - tilesize: (tile_height, tile_width) for tiled encoding.
          - prog: progression order string, e.g., 'LRCP', 'RLCP', 'RPCL', 'PCRL', 'CPRL'.
          - numres: number of wavelet decomposition levels (resolutions).
          - comment: optional string comment.

        Note:
          - To request lossless compression, set irreversible=False and do not set cratios/psnr.
        """
        self._ensure_parent_dir(path)
        if not self._is_jp2k_path(path):
            raise ValueError("Output path must have a JPEG 2000 extension (.jp2, .j2k, .jpt, .jpf, .jpx).")

        # Build kwargs only for options provided to remain compatible across glymur versions.
        kwargs: Dict[str, Any] = {}
        if cratios is not None:
            kwargs["cratios"] = cratios
        if psnr is not None:
            kwargs["psnr"] = psnr
        if irreversible is not None:
            kwargs["irreversible"] = irreversible
        if tilesize is not None:
            kwargs["tilesize"] = tilesize
        if prog is not None:
            kwargs["prog"] = prog
        if numres is not None:
            kwargs["numres"] = numres
        if comment is not None:
            kwargs["comment"] = comment

        # Create file by passing data to constructor
        # (this writes immediately in glymur).
        glymur.Jp2k(path, data=array, **kwargs)

    def get_metadata(self, path: str) -> Dict[str, Any]:
        """
        Retrieve metadata from a JPEG 2000 file.

        Returns:
          A dictionary with keys like:
            - file
            - format
            - width, height, channels, dtype, bit_depth
            - colorspace (if present)
            - icc_profile (bytes) or None
            - resolution (if present)
            - boxes (summary list for .jp2)
        """
        self._check_exists(path)
        jp2 = glymur.Jp2k(path)
        meta: Dict[str, Any] = {
            "file": os.path.abspath(path),
            "format": self._guess_format_from_ext(path),
        }

        # Basic info via decode and/or ihdr box
        try:
            shape = jp2.shape  # (H, W) or (H, W, C)
        except Exception:
            # Fall back to decoding if jp2.shape not available (older versions).
            arr = jp2[:]
            shape = arr.shape

        if len(shape) == 2:
            height, width = shape
            channels = 1
        else:
            height, width, channels = shape

        meta["width"] = int(width)
        meta["height"] = int(height)
        meta["channels"] = int(channels)

        # Determine dtype and bit depth
        try:
            # Avoid full decode by reading a small reduced level if possible
            arr = jp2.read(rlevel=jp2.codestream.segment[1].SPcod.numres - 1)  # attempt smallest res
        except Exception:
            # Fallback to full decode if above fails
            arr = jp2[:]
        meta["dtype"] = str(arr.dtype)
        meta["bit_depth"] = int(arr.dtype.itemsize * 8)

        # Extract JP2 boxes if available (JP2-only, not J2K codestream files)
        boxes_summary: List[Dict[str, Any]] = []
        colorspace = None
        icc_profile: Optional[bytes] = None
        resolution: Optional[Dict[str, Any]] = None

        if hasattr(jp2, "box") and isinstance(jp2.box, list):
            # Flatten boxes recursively and summarize
            all_boxes = list(self._iter_boxes(jp2.box))
            for b in all_boxes:
                box_id = getattr(b, "box_id", None)
                label = getattr(b, "label", None)  # glymur may provide 'label'
                summary: Dict[str, Any] = {
                    "box_id": box_id or label or b.__class__.__name__,
                }

                # Image Header Box
                if box_id == "ihdr":
                    summary.update({
                        "height": getattr(b, "height", None),
                        "width": getattr(b, "width", None),
                        "bits_per_component": getattr(b, "bpc", None),
                        "compression_type": getattr(b, "compression_type", None),
                        "unknown_colorspace": getattr(b, "unknown_colorspace", None),
                        "intellectual_property": getattr(b, "intellectual_property", None),
                        "components": getattr(b, "nc", None),
                    })

                # Colour Specification Box
                if box_id == "colr":
                    cs = getattr(b, "colorspace", None)
                    method = getattr(b, "method", None)
                    summary.update({
                        "method": method,
                        "colorspace": cs,
                        "approximation": getattr(b, "approximation", None),
                        "icc_profile_bytes": len(getattr(b, "icc_profile", b"") or b""),
                    })
                    if cs is not None and colorspace is None:
                        colorspace = cs
                    icc = getattr(b, "icc_profile", None)
                    if isinstance(icc, (bytes, bytearray)):
                        icc_profile = bytes(icc)

                # Resolution Box
                if box_id == "res ":
                    # res box typically contains 'resc' (capture) and/or 'resd' (display)
                    capture = getattr(b, "capture_resolution", None)
                    display = getattr(b, "display_resolution", None)
                    summary.update({
                        "capture_resolution": self._format_resolution(capture),
                        "display_resolution": self._format_resolution(display),
                    })
                    if resolution is None and (capture or display):
                        resolution = {
                            "capture": self._format_resolution(capture),
                            "display": self._format_resolution(display),
                        }

                # XML/UUID boxes
                if box_id == "xml ":
                    summary["xml_length"] = len(getattr(b, "xml", "") or "")
                if box_id == "uuid":
                    summary["uuid"] = getattr(b, "uuid", None)

                boxes_summary.append(summary)

        meta["colorspace"] = colorspace
        meta["icc_profile"] = icc_profile
        meta["resolution"] = resolution
        meta["boxes"] = boxes_summary

        return meta

    def convert(
        self,
        input_path: str,
        output_path: str,
        jp2k_write_options: Optional[Dict[str, Any]] = None,
        preserve_bit_depth: bool = True,
    ) -> None:
        """
        Convert between JPEG 2000 and other formats.

        Parameters:
          - input_path: str, source file path (JP2/J2K or any format Pillow can read).
          - output_path: str, destination path (JP2/J2K or any format Pillow can write).
          - jp2k_write_options: dict of options for write_image when output is JP2/J2K.
          - preserve_bit_depth: When converting JP2K -> non-JP2K:
                - If True, grayscale 16-bit is preserved when possible.
                - RGB/RGBA 16-bit may be downcast to 8-bit if Pillow cannot handle 16-bit.
        """
        self._check_exists(input_path)
        self._ensure_parent_dir(output_path)

        in_is_jp2k = self._is_jp2k_path(input_path)
        out_is_jp2k = self._is_jp2k_path(output_path)

        if in_is_jp2k and out_is_jp2k:
            # Re-wrap: decode then re-encode
            arr = self.read_image(input_path)
            self.write_image(arr, output_path, **(jp2k_write_options or {}))
            return

        if in_is_jp2k and not out_is_jp2k:
            # Decode JP2K with OpenJPEG, then save via PIL
            arr = self.read_image(input_path)
            img = self._to_pil(arr, preserve_bit_depth=preserve_bit_depth)
            img.save(output_path)
            return

        if (not in_is_jp2k) and out_is_jp2k:
            # Read with PIL, then encode JP2K
            with Image.open(input_path) as im:
                im = im.convert("RGBA") if im.mode in ("P", "LA") else im.convert(im.mode)
                arr = np.array(im)
            self.write_image(arr, output_path, **(jp2k_write_options or {}))
            return

        # Non-JP2K -> Non-JP2K: simple Pillow transcode
        with Image.open(input_path) as im:
            im.save(output_path)

    # ------------- Helpers -------------

    def _to_pil(self, arr: np.ndarray, preserve_bit_depth: bool = True) -> Image.Image:
        """
        Convert a NumPy array to a PIL Image safely, handling common bit-depth pitfalls.
        """
        if arr.ndim == 2:
            # Grayscale
            if arr.dtype == np.uint16 and preserve_bit_depth:
                return Image.fromarray(arr, mode="I;16")
            if arr.dtype == np.float32 or arr.dtype == np.float64:
                # Normalize to 0..255
                a = self._normalize_float_to_uint8(arr)
                return Image.fromarray(a, mode="L")
            return Image.fromarray(arr)
        elif arr.ndim == 3:
            h, w, c = arr.shape
            if c not in (3, 4):
                # Fallback: collapse non-RGB component counts into grayscale preview
                arr2d = self._collapse_to_luminance(arr)
                return self._to_pil(arr2d, preserve_bit_depth)

            if arr.dtype == np.uint8:
                mode = "RGB" if c == 3 else "RGBA"
                return Image.fromarray(arr, mode=mode)

            # 16-bit multi-channel: Pillow may not fully support 16-bit RGB/RGBA across all formats.
            # If preserve_bit_depth is requested we try; otherwise we downcast to 8-bit.
            if arr.dtype == np.uint16 and preserve_bit_depth:
                try:
                    # Some Pillow builds support 16-bit RGB(A) for PNG/TIFF.
                    mode = "RGB" if c == 3 else "RGBA"
                    return Image.fromarray(arr, mode=mode)
                except Exception:
                    # Downcast gracefully
                    arr8 = (arr / 257).astype(np.uint8)
                    mode = "RGB" if c == 3 else "RGBA"
                    return Image.fromarray(arr8, mode=mode)

            if np.issubdtype(arr.dtype, np.floating):
                # Normalize float to 8-bit
                arr8 = self._normalize_float_to_uint8(arr)
                mode = "RGB" if c == 3 else "RGBA"
                return Image.fromarray(arr8, mode=mode)

            # Default: attempt and if fails, downcast to 8-bit
            try:
                mode = "RGB" if c == 3 else "RGBA"
                return Image.fromarray(arr, mode=mode)
            except Exception:
                arr8 = self._downcast_to_uint8(arr)
                mode = "RGB" if c == 3 else "RGBA"
                return Image.fromarray(arr8, mode=mode)
        else:
            raise ValueError(f"Unsupported array shape for image conversion: {arr.shape}")

    @staticmethod
    def _normalize_float_to_uint8(arr: np.ndarray) -> np.ndarray:
        a = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)
        a = a - a.min() if a.max() > a.min() else a
        denom = a.max() if a.max() != 0 else 1.0
        a = (a / denom * 255.0).round()
        return a.astype(np.uint8)

    @staticmethod
    def _downcast_to_uint8(arr: np.ndarray) -> np.ndarray:
        if arr.dtype == np.uint16:
            return (arr / 257).astype(np.uint8)
        if np.issubdtype(arr.dtype, np.floating):
            return OpenJPEGHandler._normalize_float_to_uint8(arr)
        # Generic clip-cast
        a = np.clip(arr, 0, 255)
        return a.astype(np.uint8)

    @staticmethod
    def _collapse_to_luminance(arr: np.ndarray) -> np.ndarray:
        # Simple average across channels as a fallback
        a = arr.astype(np.float32)
        y = a.mean(axis=2)
        # Downcast to uint8 for display
        return OpenJPEGHandler._downcast_to_uint8(y)

    @staticmethod
    def _is_jp2k_path(path: str) -> bool:
        ext = os.path.splitext(path)[1].lower()
        return ext in OpenJPEGHandler.JP2K_EXTS

    @staticmethod
    def _guess_format_from_ext(path: str) -> str:
        ext = os.path.splitext(path)[1].lower().lstrip(".")
        return ext

    @staticmethod
    def _iter_boxes(boxes: List[Any]):
        """
        Recursively iterate all boxes for a JP2 file.
        """
        for b in boxes:
            yield b
            child = getattr(b, "box", None)
            if isinstance(child, list):
                for c in OpenJPEGHandler._iter_boxes(child):
                    yield c

    @staticmethod
    def _format_resolution(res_box: Any) -> Optional[Dict[str, Any]]:
        """
        Format resolution info from glymur's resolution box to a dict.
        The structure can vary with glymur versions; we try to read commonly
        named attributes.
        """
        if res_box is None:
            return None
        # Try common fields:
        out: Dict[str, Any] = {}
        # Capture numerator/denominator or dots-per-unit approximate
        # glymur often exposes vertical/horizontal numerator/denominator or as 'vres', 'hres'.
        for key in ("vres", "hres", "vr_n", "vr_d", "hr_n", "hr_d"):
            if hasattr(res_box, key):
                out[key] = getattr(res_box, key)
        # A friendly string if we can compute DPI
        try:
            if hasattr(res_box, "vres") and hasattr(res_box, "hres"):
                out["friendly"] = f"{res_box.hres}x{res_box.vres} units"
        except Exception:
            pass
        return out or None

    @staticmethod
    def _ensure_parent_dir(path: str) -> None:
        parent = os.path.dirname(os.path.abspath(path))
        if parent and not os.path.exists(parent):
            os.makedirs(parent, exist_ok=True)

    @staticmethod
    def _check_exists(path: str) -> None:
        if not os.path.exists(path):
            raise FileNotFoundError(path)


# Example usage (commented):
# handler = OpenJPEGHandler()
# arr = handler.read_image("input.jp2")
# handler.write_image(arr, "output.jp2", irreversible=False)  # lossless
# meta = handler.get_metadata("input.jp2")
# handler.convert("input.jp2", "output.png")
# handler.convert("input.png", "output.jp2", jp2k_write_options={"cratios": [10]})
------------------------------------------------------------

PROMPT 316:
"Generate Python code that uses the dbfpy library to read DBF files and convert them to CSV, DataFrames, HDF5 tables, and SQL tables, using the pandas library for data manipulation and the sqlite3 library for creating SQL tables, all in a Python3 compatible manner."
RESPONSE:
#!/usr/bin/env python3
"""
DBF -> CSV / pandas.DataFrame / HDF5 / SQLite converter using dbfpy, pandas, and sqlite3.

Requirements:
  pip install dbfpy pandas
  pip install tables    # only if you want HDF5 output

Usage examples:
  python dbf_convert.py input.dbf --csv out_dir/
  python dbf_convert.py input.dbf --hdf5 out.h5 --hdf5-key mytable
  python dbf_convert.py input.dbf --sqlite out.sqlite --if-exists replace
  python dbf_convert.py input1.dbf input2.dbf --sqlite out.sqlite --csv out_dir/

Notes:
- dbfpy will read memo fields (.dbt) if present next to the .dbf file.
- HDF5 requires the 'tables' package.
- SQLite tables are created via pandas.DataFrame.to_sql + sqlite3.
"""

import argparse
import os
import sqlite3
from pathlib import Path
from decimal import Decimal

import pandas as pd
from dbfpy import dbf


def _normalize_value(value):
    """Normalize DBF values to Python/pandas-friendly values."""
    # Decode bytes to str
    if isinstance(value, bytes):
        try:
            return value.decode("utf-8")
        except UnicodeDecodeError:
            return value.decode("latin-1", errors="replace")
    # Convert Decimal to float for better interoperability (e.g., SQLite)
    if isinstance(value, Decimal):
        try:
            return float(value)
        except Exception:
            return str(value)
    return value


def dbf_to_records(dbf_path):
    """Read a DBF into a list of dict records using dbfpy."""
    db = None
    try:
        db = dbf.Dbf(str(dbf_path), readOnly=True)
        # Try to get field names from header; fall back to attribute if needed
        try:
            field_names = [f.name for f in db.header.fields]
        except Exception:
            field_names = list(getattr(db, "fieldNames", []))

        records = []
        for rec in db:
            if getattr(rec, "deleted", False):
                continue
            row = {}
            for name in field_names:
                val = rec[name]
                row[name] = _normalize_value(val)
            records.append(row)

        return records
    finally:
        if db is not None:
            db.close()


def dbf_to_dataframe(dbf_path):
    """Read a DBF file into a pandas DataFrame."""
    records = dbf_to_records(dbf_path)
    df = pd.DataFrame.from_records(records)
    return df


def df_to_csv(df, csv_path, encoding="utf-8", index=False):
    csv_path = Path(csv_path)
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(csv_path, index=index, encoding=encoding)
    return csv_path


def df_to_hdf5(df, h5_path, key="table", mode="a", format="table", complib="blosc", complevel=9):
    """
    Save DataFrame to HDF5.
    Requires the 'tables' package (PyTables).
    """
    h5_path = Path(h5_path)
    h5_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_hdf(
        h5_path,
        key=key,
        mode=mode,
        format=format,       # 'table' for append/query, 'fixed' for fast write
        complevel=complevel,
        complib=complib,
    )
    return h5_path


def df_to_sqlite(df, sqlite_path, table_name, if_exists="replace", index=False):
    sqlite_path = Path(sqlite_path)
    sqlite_path.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(sqlite_path)
    try:
        df.to_sql(table_name, conn, if_exists=if_exists, index=index)
    finally:
        conn.commit()
        conn.close()
    return sqlite_path


def derive_table_name_from_path(path):
    return Path(path).stem.replace("-", "_").replace(".", "_")


def convert_dbf(
    dbf_path,
    csv_dir=None,
    sqlite_path=None,
    table_name=None,
    if_exists="replace",
    hdf5_path=None,
    hdf5_key="table",
    return_dataframe=False,
):
    """
    Convert a single DBF file to requested outputs.
    Returns the DataFrame if return_dataframe=True, else None.
    """
    dbf_path = Path(dbf_path)
    df = dbf_to_dataframe(dbf_path)

    # CSV
    if csv_dir:
        csv_dir = Path(csv_dir)
        csv_path = csv_dir / (dbf_path.stem + ".csv")
        df_to_csv(df, csv_path)

    # HDF5
    if hdf5_path:
        df_to_hdf5(df, hdf5_path, key=hdf5_key)

    # SQLite
    if sqlite_path:
        tn = table_name or derive_table_name_from_path(dbf_path.name)
        df_to_sqlite(df, sqlite_path, tn, if_exists=if_exists)

    return df if return_dataframe else None


def main():
    parser = argparse.ArgumentParser(description="Convert DBF files to CSV, HDF5, and SQLite using dbfpy and pandas.")
    parser.add_argument("dbf_files", nargs="+", help="Input .dbf file(s)")

    # CSV options
    parser.add_argument("--csv", dest="csv_dir", help="Directory to write CSV files")

    # HDF5 options
    parser.add_argument("--hdf5", dest="hdf5_path", help="HDF5 file path to write (all DBFs into one file)")
    parser.add_argument("--hdf5-key", default="table", help="HDF5 key (group) name for single-file conversion")

    # SQLite options
    parser.add_argument("--sqlite", dest="sqlite_path", help="SQLite database file path to write")
    parser.add_argument("--table-name", help="Explicit table name (only when converting a single DBF)")
    parser.add_argument(
        "--if-exists",
        choices=["fail", "replace", "append"],
        default="replace",
        help="Behavior when the table already exists in SQLite",
    )

    # Output control
    parser.add_argument("--no-index", action="store_true", help="Do not write the DataFrame index to CSV/SQL")

    args = parser.parse_args()

    # Validate table-name usage
    if args.table_name and len(args.dbf_files) != 1:
        parser.error("--table-name can only be used when converting a single DBF file")

    if not any([args.csv_dir, args.hdf5_path, args.sqlite_path]):
        parser.error("Please specify at least one output: --csv, --hdf5, or --sqlite")

    for dbf_path in args.dbf_files:
        # Per-file HDF5 key: if multiple DBFs into one HDF5 file, use distinct keys
        hdf5_key = args.hdf5_key
        if args.hdf5_path and len(args.dbf_files) > 1 and args.hdf5_key == "table":
            # derive a unique key from file name
            hdf5_key = derive_table_name_from_path(dbf_path)

        convert_dbf(
            dbf_path=dbf_path,
            csv_dir=args.csv_dir,
            sqlite_path=args.sqlite_path,
            table_name=args.table_name,
            if_exists=args.if_exists,
            hdf5_path=args.hdf5_path,
            hdf5_key=hdf5_key,
            return_dataframe=False,
        )

    print("Done.")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 317:
"Generate Python code that creates a mock data generation factory, which takes in a dataset and returns a mock dataset with similar statistical properties but different personal information, using a combination of pandas, numpy, and perhaps other relevant libraries."
RESPONSE:
# Mock Data Generation Factory
# - Preserves marginal distributions for numeric/datetime via a Gaussian copula
# - Preserves category frequencies for categoricals
# - Replaces personal information with fake values
# - Handles missingness rates per column
#
# Requirements:
#   pip install pandas numpy faker
#
# Example:
#   factory = MockDataFactory(seed=42)
#   factory.fit(df)
#   df_synth = factory.generate(n=1000)

from __future__ import annotations

import re
import uuid
from typing import Callable, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from faker import Faker
from numpy import random as npr
from numpy import special as nps


def _norm_ppf(u: np.ndarray) -> np.ndarray:
    # Inverse CDF of standard normal using erfinv
    u = np.clip(u, 1e-12, 1 - 1e-12)
    return np.sqrt(2.0) * nps.erfinv(2.0 * u - 1.0)


def _norm_cdf(z: np.ndarray) -> np.ndarray:
    return 0.5 * (1.0 + nps.erf(z / np.sqrt(2.0)))


def _quantile(values: np.ndarray, q: np.ndarray) -> np.ndarray:
    # Wrapper for np.quantile with backward-compatible API
    try:
        return np.quantile(values, q, method="linear")
    except TypeError:
        return np.quantile(values, q, interpolation="linear")


def _is_datetime_dtype(series: pd.Series) -> bool:
    return pd.api.types.is_datetime64_any_dtype(series)


def _is_numeric_dtype(series: pd.Series) -> bool:
    return pd.api.types.is_integer_dtype(series) or pd.api.types.is_float_dtype(series)


def _safe_eig_min(cov: np.ndarray) -> float:
    try:
        vals = np.linalg.eigvalsh(cov)
        return np.min(vals)
    except np.linalg.LinAlgError:
        return -1.0


def _make_pd(cov: np.ndarray, eps_start: float = 1e-8, max_iter: int = 8) -> np.ndarray:
    cov_pd = cov.copy()
    eps = eps_start
    for _ in range(max_iter):
        if _safe_eig_min(cov_pd) > 0:
            return cov_pd
        cov_pd = cov_pd + np.eye(cov_pd.shape[0]) * eps
        eps *= 10.0
    # As a last resort, diagonalize
    return np.diag(np.clip(np.diag(cov_pd), 1e-8, None))


def _rank_to_uniform(x: np.ndarray) -> np.ndarray:
    # Rank data to Uniform(0,1), handling ties with average rank
    # Input x: 1-D float array without NaNs
    s = pd.Series(x)
    ranks = s.rank(method="average").to_numpy()
    m = len(x)
    return ranks / (m + 1.0)


class MockDataFactory:
    def __init__(self, seed: Optional[int] = None, faker_locale: str = "en_US"):
        self.seed = seed
        self.rng = npr.default_rng(seed) if seed is not None else npr.default_rng()
        self.faker = Faker(faker_locale)
        if seed is not None:
            Faker.seed(seed)

        # Learned schema/info
        self.columns: List[str] = []
        self.col_types: Dict[str, str] = {}  # numeric|datetime|categorical|string|boolean|pii
        self.missing_rates: Dict[str, float] = {}

        # Numeric/datetime via Gaussian copula
        self.numeric_cols: List[str] = []
        self.datetime_cols: List[str] = []
        self.num_col_is_int: Dict[str, bool] = {}
        self.num_col_dtype: Dict[str, str] = {}
        self.num_col_values: Dict[str, np.ndarray] = {}  # for inverse quantile mapping
        self.num_is_datetime: Dict[str, bool] = {}

        self.copula_mean: Optional[np.ndarray] = None
        self.copula_cov: Optional[np.ndarray] = None
        self.copula_available_cols: List[str] = []  # subset of numeric_cols used

        # Categorical
        self.cat_cols: List[str] = []
        self.cat_levels: Dict[str, np.ndarray] = {}
        self.cat_probs: Dict[str, np.ndarray] = {}

        # String high-card
        self.string_cols: List[str] = []
        self.string_values: Dict[str, np.ndarray] = {}  # sample with replacement

        # PII generators
        self.pii_cols: List[str] = []
        self.pii_generators: Dict[str, Callable[[int], List[object]]] = {}

        # Original row count
        self.n_rows: int = 0

    def fit(self, df: pd.DataFrame) -> "MockDataFactory":
        self.columns = list(df.columns)
        self.n_rows = len(df)

        # Detect types and PII
        for col in self.columns:
            s = df[col]
            self.missing_rates[col] = float(s.isna().mean())

            if self._is_pii_column(col, s):
                self.col_types[col] = "pii"
                self.pii_cols.append(col)
                self.pii_generators[col] = self._make_pii_generator(col, s)
                continue

            if _is_datetime_dtype(s):
                self.col_types[col] = "datetime"
                self.datetime_cols.append(col)
                self.num_is_datetime[col] = True
                self.num_col_dtype[col] = str(s.dtype)
                self.num_col_is_int[col] = False  # datetime as int64 internally
                self.num_col_values[col] = s.dropna().view("int64").to_numpy()
                continue

            if _is_numeric_dtype(s):
                self.col_types[col] = "numeric"
                self.numeric_cols.append(col)
                self.num_is_datetime[col] = False
                self.num_col_dtype[col] = str(s.dtype)
                self.num_col_is_int[col] = pd.api.types.is_integer_dtype(s)
                self.num_col_values[col] = s.dropna().to_numpy().astype(float)
                continue

            if pd.api.types.is_bool_dtype(s):
                self.col_types[col] = "boolean"
                self.cat_cols.append(col)
                levels, counts = np.unique(s.dropna().to_numpy(), return_counts=True)
                probs = counts / counts.sum() if counts.sum() > 0 else np.array([1.0])
                self.cat_levels[col] = levels
                self.cat_probs[col] = probs
                continue

            # object/string: decide categorical vs free-text
            s_non_null = s.dropna()
            n_unique = s_non_null.nunique(dropna=True)
            if n_unique <= max(50, int(0.2 * max(1, len(s_non_null)))):
                self.col_types[col] = "categorical"
                self.cat_cols.append(col)
                vc = s_non_null.value_counts(normalize=True)
                self.cat_levels[col] = vc.index.to_numpy()
                self.cat_probs[col] = vc.values
            else:
                self.col_types[col] = "string"
                self.string_cols.append(col)
                # store unique values and probabilities
                vc = s_non_null.value_counts(normalize=True)
                self.string_values[col] = np.array(vc.index.tolist())

        # Fit Gaussian copula on numeric + datetime (as numeric int)
        copula_cols = [c for c in (self.numeric_cols + self.datetime_cols) if len(self.num_col_values.get(c, [])) > 1]
        if len(copula_cols) > 0:
            # Build complete-case matrix
            df_num = df[copula_cols].copy()
            # Convert datetime to int64
            for c in self.datetime_cols:
                if c in df_num:
                    df_num[c] = df_num[c].view("int64")
            complete = df_num.dropna()
            if len(complete) >= 50 and complete.shape[1] >= 1:
                U = []
                for c in copula_cols:
                    x = complete[c].to_numpy().astype(float)
                    u = _rank_to_uniform(x)
                    U.append(u)
                U = np.column_stack(U)
                Z = _norm_ppf(U)
                mean = Z.mean(axis=0)
                cov = np.cov(Z, rowvar=False)
                cov = _make_pd(cov)
                self.copula_mean = mean
                self.copula_cov = cov
                self.copula_available_cols = copula_cols
            else:
                self.copula_mean = None
                self.copula_cov = None
                self.copula_available_cols = []

        return self

    def generate(self, n: Optional[int] = None) -> pd.DataFrame:
        if n is None:
            n = self.n_rows
        if n <= 0:
            return pd.DataFrame(columns=self.columns)

        out = pd.DataFrame(index=range(n))

        # 1) Numeric + datetime via copula or independent sampling
        if len(self.copula_available_cols) >= 1 and self.copula_mean is not None and self.copula_cov is not None:
            k = len(self.copula_available_cols)
            Z = self.rng.multivariate_normal(self.copula_mean, self.copula_cov, size=n)
            U = _norm_cdf(Z)
            for j, col in enumerate(self.copula_available_cols):
                base_vals = self.num_col_values[col]
                if len(base_vals) == 0:
                    vals = np.full(n, np.nan)
                else:
                    vals = _quantile(np.asarray(base_vals, dtype=float), U[:, j])
                vals = self._coerce_numeric_back(col, vals)
                out[col] = self._apply_missingness(col, vals)
        # Any remaining numeric/datetime columns not included in copula
        remaining_num = set(self.numeric_cols + self.datetime_cols) - set(self.copula_available_cols)
        for col in remaining_num:
            base_vals = self.num_col_values.get(col, np.array([]))
            if len(base_vals) == 0:
                vals = np.full(n, np.nan)
            else:
                # Independent sampling via empirical quantiles
                u = self.rng.uniform(low=1e-6, high=1 - 1e-6, size=n)
                vals = _quantile(np.asarray(base_vals, dtype=float), u)
            vals = self._coerce_numeric_back(col, vals)
            out[col] = self._apply_missingness(col, vals)

        # 2) Categoricals and booleans
        for col in self.cat_cols:
            levels = self.cat_levels.get(col, np.array([np.nan]))
            probs = self.cat_probs.get(col, np.array([1.0]))
            if len(levels) == 0:
                vals = np.full(n, np.nan, dtype=object)
            else:
                idx = self.rng.choice(len(levels), size=n, p=probs if probs.sum() > 0 else None)
                vals = levels[idx]
            out[col] = self._apply_missingness(col, vals)

        # 3) High-card strings (non-PII): sample existing values uniformly
        for col in self.string_cols:
            vals_source = self.string_values.get(col, np.array([], dtype=object))
            if len(vals_source) == 0:
                vals = np.array([None] * n, dtype=object)
            else:
                idx = self.rng.integers(0, len(vals_source), size=n)
                vals = vals_source[idx]
            out[col] = self._apply_missingness(col, vals)

        # 4) PII columns: replace with Faker-generated content
        for col in self.pii_cols:
            gen = self.pii_generators[col]
            vals = gen(n)
            out[col] = self._apply_missingness(col, vals)

        # Reorder columns
        out = out[self.columns]

        # Cast dtypes where possible
        for col in self.numeric_cols:
            if self.num_col_is_int.get(col, False):
                out[col] = pd.to_numeric(out[col], errors="coerce").round().astype("Int64")
            else:
                out[col] = pd.to_numeric(out[col], errors="coerce")
        for col in self.datetime_cols:
            out[col] = pd.to_datetime(out[col], errors="coerce")
        for col in self.cat_cols:
            # Preserve category dtype if original was categorical
            pass

        return out

    def _apply_missingness(self, col: str, values: np.ndarray | List[object]) -> np.ndarray:
        p = self.missing_rates.get(col, 0.0)
        values = np.array(values, dtype=object) if isinstance(values, list) else np.array(values, dtype=object)
        if p <= 0:
            return values
        mask = self.rng.random(len(values)) < p
        values[mask] = None
        return values

    def _coerce_numeric_back(self, col: str, vals: np.ndarray) -> np.ndarray:
        if self.num_is_datetime.get(col, False):
            # vals are float timestamps in ns; round to int and convert later
            return vals.astype(np.int64)
        # keep floats; integer handled during final cast
        return vals

    # ---------- PII handling ----------

    def _is_pii_column(self, col: str, s: pd.Series) -> bool:
        name = col.lower()
        pii_patterns = [
            r"\bname\b",
            r"\bfirst[_\s]?name\b",
            r"\blast[_\s]?name\b",
            r"\bfull[_\s]?name\b",
            r"\bemail\b",
            r"\b(e-)?mail\b",
            r"\bphone\b",
            r"\bmobile\b",
            r"\btel(ephone)?\b",
            r"\baddress\b",
            r"\bstreet\b",
            r"\bcity\b",
            r"\bstate\b",
            r"\bprovince\b",
            r"\bpostal\b",
            r"\bzip\b",
            r"\bcountry\b",
            r"\bssn\b",
            r"\bsocial[_\s]?security\b",
            r"\btax\b",
            r"\btin\b",
            r"\bpassport\b",
            r"\buuid\b",
            r"\bguid\b",
            r"(^|_)id$",
            r"\busername\b",
            r"\buser[_\s]?name\b",
            r"\bip(v4|v6)?\b",
            r"\burl\b",
            r"\bwebsite\b",
            r"\bcompany\b",
            r"\borg(anization)?\b",
            r"\bjob\b",
            r"\bbirth(day|date)\b",
            r"\bdob\b",
        ]
        if any(re.search(p, name) for p in pii_patterns):
            return True

        # Heuristic: if many look like emails or phone numbers in sample
        s_non_null = s.dropna().astype(str)
        if len(s_non_null) == 0:
            return False
        sample = s_non_null.sample(min(50, len(s_non_null)), random_state=self.seed)

        email_like = sample.str.contains(r"^[^@\s]+@[^@\s]+\.[^@\s]+$", regex=True, na=False).mean()
        phone_like = sample.str.contains(r"[0-9][0-9\-\s\(\)]{6,}[0-9]", regex=True, na=False).mean()
        uuid_like = sample.str.contains(r"^[0-9a-fA-F\-]{32,36}$", regex=True, na=False).mean()
        if max(email_like, phone_like, uuid_like) > 0.5:
            return True

        return False

    def _make_pii_generator(self, col: str, s: pd.Series) -> Callable[[int], List[object]]:
        name = col.lower()
        unique_ratio = (s.nunique(dropna=True) / max(1, len(s.dropna()))) if len(s) else 0.0
        use_unique = unique_ratio > 0.9

        def gen_name(n):
            return [self.faker.name() for _ in range(n)]

        def gen_first(n):
            return [self.faker.first_name() for _ in range(n)]

        def gen_last(n):
            return [self.faker.last_name() for _ in range(n)]

        def gen_email(n):
            fk = self.faker.unique if use_unique else self.faker
            return [fk.email() for _ in range(n)]

        def gen_phone(n):
            fk = self.faker.unique if use_unique else self.faker
            return [fk.phone_number() for _ in range(n)]

        def gen_address(n):
            return [self.faker.address().replace("\n", ", ") for _ in range(n)]

        def gen_city(n):
            return [self.faker.city() for _ in range(n)]

        def gen_state(n):
            # choose state name or abbreviation based on sample
            s_non_null = s.dropna().astype(str)
            abbr_ratio = (s_non_null.str.fullmatch(r"[A-Z]{2}", na=False).mean() if len(s_non_null) else 0.0)
            if abbr_ratio > 0.5:
                return [self.faker.state_abbr() for _ in range(n)]
            return [self.faker.state() for _ in range(n)]

        def gen_zip(n):
            return [self.faker.postcode() for _ in range(n)]

        def gen_country(n):
            return [self.faker.country() for _ in range(n)]

        def gen_ssn(n):
            fk = self.faker.unique if use_unique else self.faker
            return [fk.ssn() for _ in range(n)]

        def gen_uuid(n):
            fk = self.faker.unique if use_unique else self.faker
            return [str(uuid.uuid4()) for _ in range(n)]

        def gen_id(n):
            # Numeric ID -> random large ints; String ID -> uuid
            if _is_numeric_dtype(s):
                # Keep scale similar if possible
                vals = s.dropna().to_numpy()
                if len(vals) >= 2:
                    low = np.nanmin(vals)
                    high = np.nanmax(vals) + 1
                    if not np.isfinite(low) or not np.isfinite(high) or low >= high:
                        low, high = 1, 10**12
                else:
                    low, high = 1, 10**12
                # Ensure uniqueness by sampling without replacement if possible
                if use_unique and (high - low) >= n:
                    return list(self.rng.choice(np.arange(low, high, dtype=np.int64), size=n, replace=False))
                return list(self.rng.integers(low, high, size=n, dtype=np.int64))
            return [str(uuid.uuid4()) for _ in range(n)]

        def gen_username(n):
            fk = self.faker.unique if use_unique else self.faker
            return [fk.user_name() for _ in range(n)]

        def gen_ip(n):
            # Mix ipv4 and ipv6
            outs = []
            for _ in range(n):
                if self.rng.random() < 0.8:
                    outs.append(self.faker.ipv4())
                else:
                    outs.append(self.faker.ipv6())
            return outs

        def gen_url(n):
            return [self.faker.url() for _ in range(n)]

        def gen_company(n):
            return [self.faker.company() for _ in range(n)]

        def gen_job(n):
            return [self.faker.job() for _ in range(n)]

        def gen_dob(n):
            # Approximate age distribution: use observed ages if available; otherwise random adult ages
            s_non_null = pd.to_datetime(s.dropna(), errors="coerce")
            if len(s_non_null) > 10 and s_non_null.notna().mean() > 0.8:
                # Derive ages
                today = pd.Timestamp.today().normalize()
                ages = ((today - s_non_null).dt.days // 365).clip(lower=0, upper=110)
                vc = ages.value_counts(normalize=True)
                age_levels = vc.index.to_numpy()
                age_probs = vc.values
                sampled_ages = self.rng.choice(age_levels, size=n, p=age_probs)
                # Convert back to birthdates with jitter within year
                dob = [pd.Timestamp.today() - pd.to_timedelta(int(a) * 365 + int(self.rng.integers(0, 365)), unit="D") for a in sampled_ages]
                return [d.normalize().date().isoformat() for d in dob]
            else:
                return [self.faker.date_of_birth(minimum_age=18, maximum_age=90).isoformat() for _ in range(n)]

        mapping = [
            (r"\bfirst[_\s]?name\b", gen_first),
            (r"\blast[_\s]?name\b", gen_last),
            (r"\bfull[_\s]?name\b|\bname\b", gen_name),
            (r"\bemail\b|\b(e-)?mail\b", gen_email),
            (r"\bphone\b|\bmobile\b|\btel(ephone)?\b", gen_phone),
            (r"\baddress\b|\bstreet\b", gen_address),
            (r"\bcity\b", gen_city),
            (r"\bstate\b|province", gen_state),
            (r"\bpostal\b|\bzip\b", gen_zip),
            (r"\bcountry\b", gen_country),
            (r"\bssn\b|\bsocial[_\s]?security\b|\btin\b|\btax\b", gen_ssn),
            (r"\buuid\b|\bguid\b", gen_uuid),
            (r"(^|_)id$", gen_id),
            (r"\busername\b|\buser[_\s]?name\b", gen_username),
            (r"\bip(v4|v6)?\b", gen_ip),
            (r"\burl\b|\bwebsite\b", gen_url),
            (r"\bcompany\b|\borg(anization)?\b", gen_company),
            (r"\bjob\b", gen_job),
            (r"\bbirth(day|date)\b|\bdob\b", gen_dob),
        ]
        for pat, fn in mapping:
            if re.search(pat, name):
                return fn

        # Fallback
        def gen_default(n):
            return [str(uuid.uuid4()) for _ in range(n)]

        return gen_default


# -------------------------
# Minimal usage example
# -------------------------
if __name__ == "__main__":
    # Example creating a sample dataset
    rng = np.random.default_rng(0)
    n = 1000
    df_example = pd.DataFrame(
        {
            "id": np.arange(1, n + 1),
            "first_name": [""] * n,
            "last_name": [""] * n,
            "email": [""] * n,
            "age": rng.normal(40, 12, size=n).round().clip(18, 90),
            "income": rng.lognormal(mean=10.5, sigma=0.5, size=n),
            "signup_date": pd.to_datetime("2020-01-01") + pd.to_timedelta(rng.integers(0, 365 * 3, size=n), unit="D"),
            "state": rng.choice(["CA", "NY", "TX", "WA", "FL"], size=n, p=[0.2, 0.2, 0.2, 0.2, 0.2]),
            "is_active": rng.choice([True, False], size=n, p=[0.7, 0.3]),
            "notes": rng.choice(["A", "B", "C", "D", "E"], size=n),
        }
    )

    factory = MockDataFactory(seed=42)
    factory.fit(df_example)
    df_synth = factory.generate(n=1000)
    print(df_synth.head())
------------------------------------------------------------

PROMPT 318:
"Generate Python code that imports the reprlib library and uses its functions to create short and cheap string representations, similar to the built-in repr() function but with additional features such as customizable output format and support for non-string objects."
RESPONSE:
# Demonstration of reprlib for short, cheap, and customizable representations

import reprlib
from collections import deque
import array


def short_repr(obj):
    """
    Use the top-level reprlib.repr function to get a bounded-size repr.
    This works out-of-the-box for many built-in containers and non-string objects.
    """
    return reprlib.repr(obj)


def configurable_repr(obj, *, maxstring=60, maxlist=6, maxtuple=6, maxdict=6, maxset=6, maxlevel=3):
    """
    Use a configurable Repr instance to control truncation limits by type.
    """
    r = reprlib.Repr()
    r.maxstring = maxstring
    r.maxlist = maxlist
    r.maxtuple = maxtuple
    r.maxdict = maxdict
    r.maxset = maxset
    r.maxfrozenset = maxset
    r.maxdeque = maxlist
    r.maxarray = maxlist
    r.maxlevel = maxlevel
    # For unknown/other objects, some Python versions honor maxother (fallback length)
    # If present, set it for good measure:
    if hasattr(r, "maxother"):
        r.maxother = max(20, maxstring)
    return r.repr(obj)


class CustomRepr(reprlib.Repr):
    """
    Example of a custom formatter:
      - customizable size limits via __init__
      - adds type/length annotations for str/bytes
      - uses a nicer ellipsis (…)
      - sorts dict keys for stable output
      - truncates very long instance reprs using maxother
    """
    def __init__(self, *, maxstring=50, maxlist=5, maxtuple=5, maxdict=5, maxset=5, maxlevel=3, maxother=80):
        super().__init__()
        self.maxstring = maxstring
        self.maxlist = maxlist
        self.maxtuple = maxtuple
        self.maxdict = maxdict
        self.maxset = maxset
        self.maxfrozenset = maxset
        self.maxdeque = maxlist
        self.maxarray = maxlist
        self.maxlevel = maxlevel
        self.maxother = maxother

    # Strings with annotation and truncation handled by base, then annotated here
    def repr_str(self, obj, level):
        base = super().repr_str(obj, level)
        return f'str(len={len(obj)}): {base}'

    def repr_bytes(self, obj, level):
        base = super().repr_bytes(obj, level)
        return f'bytes(len={len(obj)}): {base}'

    # Replace the default "..." with a single-character ellipsis for lists/tuples/sets/etc.
    def repr_list(self, obj, level):
        return super().repr_list(obj, level).replace("...", "…")

    def repr_tuple(self, obj, level):
        return super().repr_tuple(obj, level).replace("...", "…")

    def repr_set(self, obj, level):
        return super().repr_set(obj, level).replace("...", "…")

    def repr_frozenset(self, obj, level):
        return super().repr_frozenset(obj, level).replace("...", "…")

    def repr_deque(self, obj, level):
        return super().repr_deque(obj, level).replace("...", "…")

    def repr_array(self, obj, level):
        return super().repr_array(obj, level).replace("...", "…")

    # Sort dict keys (when comparable) and use custom ellipsis
    def repr_dict(self, obj, level):
        if level <= 0:
            return "{…}"
        try:
            keys = sorted(obj.keys())
        except Exception:
            keys = list(obj.keys())
        n = min(len(keys), self.maxdict)
        shown = []
        for k in keys[:n]:
            shown.append(f"{self.repr1(k, level - 1)}: {self.repr1(obj[k], level - 1)}")
        if len(keys) > n:
            shown.append("…")
        return "{" + ", ".join(shown) + "}"

    # Truncate very long custom instance reprs
    def repr_instance(self, obj, level):
        try:
            s = repr(obj)
        except Exception:
            s = f"<{obj.__class__.__name__} at 0x{id(obj):x}>"
        if len(s) > self.maxother:
            s = s[: self.maxother] + "…"
        return s


# Example usage
if __name__ == "__main__":
    class User:
        def __init__(self, name):
            self.name = name
        def __repr__(self):
            # Intentionally long repr to show truncation behavior
            return f"User(name={self.name!r}, payload={'x'*200})"

    big_object = {
        "name": "Alice",
        "bio": "A" * 200,
        "numbers": list(range(100)),
        "nested": [{"i": i, "s": "X" * 50} for i in range(10)],
        "set": set(range(50)),
        "fset": frozenset(range(80)),
        "dq": deque(range(30)),
        "arr": array.array("I", range(40)),
        "bytes": b"0123456789" * 10,
        "user": User("alice"),
    }

    print("Built-in repr (trimmed for display):")
    print(repr(big_object)[:200] + " ...\n")

    print("reprlib.repr (default limits):")
    print(short_repr(big_object), "\n")

    print("reprlib.Repr with custom limits:")
    print(configurable_repr(big_object, maxstring=40, maxlist=4, maxtuple=4, maxdict=4, maxset=6, maxlevel=2), "\n")

    print("CustomRepr with annotations and nicer ellipsis:")
    cr = CustomRepr(maxstring=30, maxlist=4, maxtuple=4, maxdict=4, maxset=6, maxlevel=2, maxother=80)
    print(cr.repr(big_object))
------------------------------------------------------------

PROMPT 319:
"Generate Python code that imports the necessary libraries and utilizes the libcurl ffi bindings to make HTTP requests with impersonation support, similar to the functionality provided by the `requests` library, but using the faster and more lightweight libcurl library."
RESPONSE:
# pip install -U curl-cffi
# This script demonstrates a Requests-like workflow using libcurl via curl_cffi,
# including browser impersonation (TLS/JA3/ALPN/User-Agent) for better evasion and compatibility.

from pathlib import Path
from typing import Optional, Dict, Any
from curl_cffi import requests


# Choose a browser fingerprint to impersonate. Options evolve; examples:
# "chrome120", "chrome124", "chrome127", "safari17_2", "edge120", "ios15_5", etc.
BROWSER_FP = "chrome124"


def basic_get():
    r = requests.get(
        "https://httpbin.org/anything",
        params={"q": "test"},
        impersonate=BROWSER_FP,
        http2=True,
        timeout=20,
    )
    print("GET status:", r.status_code, "HTTP/", r.http_version)
    print("Server:", r.headers.get("server"))
    print("JSON:", r.json())


def post_json():
    r = requests.post(
        "https://httpbin.org/post",
        json={"hello": "world"},
        impersonate=BROWSER_FP,
        http2=True,
        timeout=20,
    )
    print("POST JSON status:", r.status_code)
    print("JSON:", r.json())


def upload_file():
    files = {
        "file": ("hello.txt", b"Hello from curl_cffi\n", "text/plain"),
    }
    r = requests.post(
        "https://httpbin.org/post",
        files=files,
        impersonate=BROWSER_FP,
        http2=True,
        timeout=20,
    )
    print("Upload status:", r.status_code)
    print("JSON keys:", list(r.json().keys()))


def stream_download(dest: Path):
    url = "https://httpbin.org/bytes/102400"  # 100 KB
    with requests.get(
        url,
        stream=True,
        impersonate=BROWSER_FP,
        http2=True,
        timeout=30,
    ) as r:
        r.raise_for_status()
        with dest.open("wb") as f:
            for chunk in r.iter_content(1 << 14):  # 16KB chunks
                if chunk:
                    f.write(chunk)
    print(f"Downloaded to {dest} ({dest.stat().st_size} bytes)")


def cookie_session():
    # Session persists cookies, connections, and headers
    with requests.Session(
        impersonate=BROWSER_FP,
        http2=True,
        timeout=20,
    ) as s:
        # Set a cookie and then read it back
        s.get("https://httpbin.org/cookies/set?foo=bar")
        r = s.get("https://httpbin.org/cookies")
        print("Cookies in session:", r.json())


def with_proxy_example():
    # Example of setting an HTTP/HTTPS proxy (commented out by default).
    # Make sure your proxy is running or remove proxies to avoid errors.
    proxies = {
        # "http": "http://127.0.0.1:8080",
        # "https": "http://127.0.0.1:8080",
    }
    r = requests.get(
        "https://httpbin.org/ip",
        impersonate=BROWSER_FP,
        proxies=proxies or None,
        timeout=20,
        http2=True,
    )
    print("IP via (maybe) proxy:", r.text)


class HttpClient:
    """
    A small convenience wrapper to feel even more like 'requests',
    while using curl_cffi under the hood for libcurl performance and impersonation.
    """

    def __init__(
        self,
        impersonate: str = BROWSER_FP,
        http2: bool = True,
        timeout: Optional[float] = 30,
        verify: bool | str = True,
        proxies: Optional[Dict[str, str]] = None,
        headers: Optional[Dict[str, str]] = None,
    ):
        self.session = requests.Session(
            impersonate=impersonate,
            http2=http2,
            timeout=timeout,
            verify=verify,
            proxies=proxies,
        )
        if headers:
            self.session.headers.update(headers)

    def get(self, url: str, **kwargs: Any):
        return self.session.get(url, **kwargs)

    def post(self, url: str, **kwargs: Any):
        return self.session.post(url, **kwargs)

    def put(self, url: str, **kwargs: Any):
        return self.session.put(url, **kwargs)

    def delete(self, url: str, **kwargs: Any):
        return self.session.delete(url, **kwargs)

    def head(self, url: str, **kwargs: Any):
        return self.session.head(url, **kwargs)

    def patch(self, url: str, **kwargs: Any):
        return self.session.patch(url, **kwargs)

    def options(self, url: str, **kwargs: Any):
        return self.session.options(url, **kwargs)

    def close(self):
        self.session.close()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc, tb):
        self.close()


def client_demo():
    with HttpClient(impersonate=BROWSER_FP, http2=True, timeout=20) as client:
        r1 = client.get("https://httpbin.org/anything", params={"a": 1})
        print("Client GET:", r1.status_code, r1.json()["args"])

        r2 = client.post("https://httpbin.org/post", json={"a": 2})
        print("Client POST status:", r2.status_code)

        # Cookies across requests
        client.get("https://httpbin.org/cookies/set?k=v")
        r3 = client.get("https://httpbin.org/cookies")
        print("Client cookies:", r3.json())


if __name__ == "__main__":
    try:
        basic_get()
        post_json()
        upload_file()
        stream_download(Path("example_download.bin"))
        cookie_session()
        with_proxy_example()
        client_demo()
    except Exception as e:
        # curl_cffi mirrors requests-style errors (e.g. Timeout, ConnectionError),
        # but to keep the demo simple we catch all and print.
        print("Error:", repr(e))
------------------------------------------------------------

PROMPT 320:
"Generate Python code that creates a badge for a tool using the Badger library, which can retrieve information about the tool from various sources such as PyPI, GitHub, and GitLab, and then create a Markdown file containing the badge image and information about the tool."
RESPONSE:
#!/usr/bin/env python3
"""
Create a badge and a Markdown info file for a tool by pulling metadata from PyPI, GitHub, and/or GitLab.

- If the "badger" library is installed, this script will try to use it.
- Otherwise it falls back to "pybadges" to generate an SVG badge.

Install dependencies (fallback path):
  pip install requests pybadges

Optional tokens to raise API limits:
  export GITHUB_TOKEN=ghp_...
  export GITLAB_TOKEN=glpat-...

Examples:
  python make_tool_badge.py --name anybadge --pypi anybadge
  python make_tool_badge.py --name numpy --github numpy/numpy
  python make_tool_badge.py --name mytool --gitlab mygroup/myproject
  python make_tool_badge.py --name fastapi --pypi fastapi --github tiangolo/fastapi --out fastapi.md
"""

from __future__ import annotations

import argparse
import dataclasses
import json
import os
import sys
import textwrap
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional, Tuple
from urllib.parse import quote_plus

import requests


# -------------- Badge generator adapter --------------

class BadgeGenerator:
    """
    Adapter that tries to use 'badger' if available; otherwise uses 'pybadges'.
    Public API: make_svg(label, value, color, style) -> svg_text
    """
    def __init__(self):
        self.impl = None
        self.impl_name = None

        # Try badger
        try:
            # Unknown real API for "badger". We provide a minimal compatibility layer if present.
            import badger  # type: ignore
            self.impl = ("badger", badger)
            self.impl_name = "badger"
        except Exception:
            self.impl = None

        if self.impl is None:
            # Fallback to pybadges
            try:
                import pybadges  # type: ignore
                self.impl = ("pybadges", pybadges)
                self.impl_name = "pybadges"
            except Exception:
                self.impl = None

        if self.impl is None:
            raise RuntimeError("Neither 'badger' nor 'pybadges' is available. Install pybadges: pip install pybadges")

    def make_svg(self, label: str, value: str, color: str = "blue", style: str = "flat") -> str:
        if self.impl_name == "badger":
            # We don't know the exact API. We’ll try a few common patterns and fall back to a simple SVG if needed.
            badger = self.impl[1]
            # Attempt 1: badger.Badge(label=..., value=..., color=..., style=...).svg()
            try:
                badge = getattr(badger, "Badge")(label=label, value=value, color=color, style=style)
                if hasattr(badge, "svg"):
                    return badge.svg() if callable(badge.svg) else badge.svg
            except Exception:
                pass
            # Attempt 2: badger.generate(label=..., value=..., color=..., style=...)
            try:
                gen = getattr(badger, "generate")
                svg = gen(label=label, value=value, color=color, style=style)
                if isinstance(svg, str):
                    return svg
            except Exception:
                pass
            # Fallback to minimal SVG
            return minimal_svg(label, value, color)
        elif self.impl_name == "pybadges":
            pybadges = self.impl[1]
            return pybadges.badge(left_text=label, right_text=value, right_color=color, whole_link=None, style=style)
        else:
            return minimal_svg(label, value, color)


def minimal_svg(label: str, value: str, color: str = "blue") -> str:
    """
    Minimalistic, not-perfect badge SVG as a last resort.
    """
    # This is a very basic placeholder and not pixel-perfect like shields.io.
    label_text = escape_xml(label)
    value_text = escape_xml(value)
    # Widths are naive approximations
    lw = max(50, 7 * len(label_text) + 20)
    rw = max(50, 7 * len(value_text) + 20)
    total = lw + rw
    svg = f"""<svg xmlns="http://www.w3.org/2000/svg" width="{total}" height="20" role="img" aria-label="{label_text}: {value_text}">
  <linearGradient id="s" x2="0" y2="100%">
    <stop offset="0" stop-color="#fff" stop-opacity=".7"/>
    <stop offset=".1" stop-opacity=".1"/>
    <stop offset=".9" stop-opacity=".3"/>
    <stop offset="1" stop-opacity=".5"/>
  </linearGradient>
  <mask id="m"><rect width="{total}" height="20" rx="3" fill="#fff"/></mask>
  <g mask="url(#m)">
    <rect width="{lw}" height="20" fill="#555"/>
    <rect x="{lw}" width="{rw}" height="20" fill="{color}"/>
    <rect width="{total}" height="20" fill="url(#s)"/>
  </g>
  <g fill="#fff" text-anchor="middle" font-family="DejaVu Sans,Verdana,Geneva,sans-serif" font-size="11">
    <text x="{lw/2}" y="15" fill="#010101" fill-opacity=".3">{label_text}</text>
    <text x="{lw/2}" y="14">{label_text}</text>
    <text x="{lw + rw/2}" y="15" fill="#010101" fill-opacity=".3">{value_text}</text>
    <text x="{lw + rw/2}" y="14">{value_text}</text>
  </g>
</svg>
"""
    return svg


def escape_xml(text: str) -> str:
    return (
        text.replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
            .replace('"', "&quot;")
            .replace("'", "&apos;")
    )


# -------------- Data model --------------

@dataclasses.dataclass
class ToolInfo:
    name: str
    source: str  # "pypi" | "github" | "gitlab"
    url: Optional[str] = None
    description: Optional[str] = None
    version: Optional[str] = None
    stars: Optional[int] = None
    license: Optional[str] = None
    last_release_date: Optional[str] = None
    homepage: Optional[str] = None
    extra: Dict[str, Any] = dataclasses.field(default_factory=dict)


# -------------- HTTP helper --------------

def request_json(url: str, headers: Optional[Dict[str, str]] = None, timeout: float = 10.0) -> Dict[str, Any]:
    r = requests.get(url, headers=headers or {}, timeout=timeout)
    r.raise_for_status()
    ct = r.headers.get("content-type", "")
    if "application/json" not in ct and "json" not in ct:
        # Attempt to parse anyway
        return json.loads(r.text)
    return r.json()


# -------------- PyPI --------------

def fetch_from_pypi(package: str) -> Optional[ToolInfo]:
    try:
        url = f"https://pypi.org/pypi/{package}/json"
        data = request_json(url)
    except Exception:
        return None

    info = data.get("info", {}) if isinstance(data, dict) else {}
    releases = data.get("releases", {}) if isinstance(data, dict) else {}

    version = info.get("version")
    license_name = normalize_license(info.get("license"))
    description = info.get("summary") or ""
    homepage = info.get("home_page") or (info.get("project_urls") or {}).get("Homepage")
    project_url = f"https://pypi.org/project/{package}/"

    last_release_date = None
    if isinstance(releases, dict) and version in releases and releases[version]:
        upload_times = [
            f.get("upload_time_iso_8601") or f.get("upload_time")
            for f in releases[version]
            if isinstance(f, dict)
        ]
        upload_times = [t for t in upload_times if t]
        if upload_times:
            # Pick the latest file upload time for the release
            try:
                last_release_date = sorted(upload_times)[-1]
            except Exception:
                last_release_date = upload_times[-1]

    return ToolInfo(
        name=info.get("name") or package,
        source="pypi",
        url=project_url,
        description=description,
        version=version,
        stars=None,
        license=license_name,
        last_release_date=last_release_date,
        homepage=homepage,
        extra={"package": package},
    )


# -------------- GitHub --------------

def github_headers() -> Dict[str, str]:
    headers = {"Accept": "application/vnd.github+json"}
    token = os.getenv("GITHUB_TOKEN")
    if token:
        headers["Authorization"] = f"Bearer {token}"
    return headers


def fetch_from_github(repo: str) -> Optional[ToolInfo]:
    """
    repo: "owner/name"
    """
    if "/" not in repo:
        return None
    owner, name = repo.split("/", 1)
    headers = github_headers()

    try:
        repo_data = request_json(f"https://api.github.com/repos/{owner}/{name}", headers=headers)
    except Exception:
        return None

    # Attempt to fetch latest release (may not exist)
    version = None
    last_release_date = None
    try:
        rel = request_json(f"https://api.github.com/repos/{owner}/{name}/releases/latest", headers=headers)
        if isinstance(rel, dict) and rel.get("tag_name"):
            version = rel.get("tag_name")
            last_release_date = rel.get("published_at") or rel.get("created_at")
    except Exception:
        pass

    license_name = normalize_license(((repo_data.get("license") or {}).get("spdx_id")) if repo_data.get("license") else None)

    return ToolInfo(
        name=repo_data.get("name") or name,
        source="github",
        url=repo_data.get("html_url") or f"https://github.com/{owner}/{name}",
        description=repo_data.get("description"),
        version=version,
        stars=repo_data.get("stargazers_count"),
        license=license_name,
        last_release_date=last_release_date or repo_data.get("pushed_at"),
        homepage=repo_data.get("homepage"),
        extra={"repo": f"{owner}/{name}"},
    )


# -------------- GitLab --------------

def gitlab_headers() -> Dict[str, str]:
    headers = {"Accept": "application/json"}
    token = os.getenv("GITLAB_TOKEN")
    if token:
        headers["PRIVATE-TOKEN"] = token
    return headers


def fetch_from_gitlab(project_path: str) -> Optional[ToolInfo]:
    """
    project_path: "group/subgroup/name" (namespace/name)
    """
    pid = quote_plus(project_path)
    headers = gitlab_headers()

    try:
        data = request_json(f"https://gitlab.com/api/v4/projects/{pid}", headers=headers)
    except Exception:
        return None

    # Try latest release
    version = None
    last_release_date = None
    try:
        rels = request_json(f"https://gitlab.com/api/v4/projects/{pid}/releases?per_page=1", headers=headers)
        if isinstance(rels, list) and rels:
            version = rels[0].get("tag_name")
            last_release_date = rels[0].get("released_at")
    except Exception:
        pass

    # License can be in "license" or "license" key (varies by API/version)
    lic = None
    if isinstance(data.get("license"), dict):
        lic = data["license"].get("name")
    elif isinstance(data.get("license"), str):
        lic = data["license"]

    return ToolInfo(
        name=data.get("name") or project_path.split("/")[-1],
        source="gitlab",
        url=data.get("web_url") or f"https://gitlab.com/{project_path}",
        description=data.get("description"),
        version=version,
        stars=data.get("star_count"),
        license=normalize_license(lic),
        last_release_date=last_release_date or data.get("last_activity_at"),
        homepage=data.get("homepage"),
        extra={"project": project_path},
    )


# -------------- Helpers --------------

def normalize_license(lic: Optional[str]) -> Optional[str]:
    if not lic:
        return None
    s = str(lic).strip()
    if s.upper() in {"UNKNOWN", "OTHER"}:
        return None
    return s


def choose_badge(tool: ToolInfo) -> Tuple[str, str, str]:
    """
    Returns tuple (label, value, color) for the badge.
    Preference:
      - If version exists: show version badge with source label.
      - Else if stars exist: show stars badge with platform label.
      - Else show source label with "info".
    """
    platform = tool.source
    if tool.version:
        label = "pypi" if platform == "pypi" else platform
        value = f"v{tool.version.lstrip('v')}"
        color = "blue"
        return label, value, color

    if isinstance(tool.stars, int):
        label = f"{platform} stars"
        value = str(tool.stars)
        color = color_by_stars(tool.stars)
        return label, value, color

    label = platform
    value = "info"
    color = "lightgrey"
    return label, value, color


def color_by_stars(stars: int) -> str:
    if stars >= 10000:
        return "brightgreen"
    if stars >= 5000:
        return "green"
    if stars >= 1000:
        return "yellowgreen"
    if stars >= 100:
        return "yellow"
    if stars >= 10:
        return "orange"
    return "lightgrey"


def write_text(path: Path, text: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(text, encoding="utf-8")


def format_iso(dt: Optional[str]) -> Optional[str]:
    if not dt:
        return None
    try:
        # Normalize some common formats
        # Examples: "2023-07-12T15:23:34Z", "2023-07-12T15:23:34.123456Z", "2020-01-01 12:00:00"
        s = dt.replace("Z", "+00:00")
        d = datetime.fromisoformat(s)
        return d.strftime("%Y-%m-%d")
    except Exception:
        return dt


def build_markdown(tool: ToolInfo, badge_rel_path: str) -> str:
    lines = []
    lines.append(tool.name)
    lines.append("")
    lines.append(f"![{tool.name} badge]({badge_rel_path})")
    lines.append("")
    if tool.description:
        lines.append(tool.description.strip())
        lines.append("")

    # Minimal details in bullet list
    if tool.url:
        lines.append(f"- Source: {tool.source} ({tool.url})")
    else:
        lines.append(f"- Source: {tool.source}")
    if tool.version:
        lines.append(f"- Latest version: {tool.version}")
    if tool.stars is not None:
        lines.append(f"- Stars: {tool.stars}")
    if tool.license:
        lines.append(f"- License: {tool.license}")
    if tool.last_release_date:
        lines.append(f"- Last release: {format_iso(tool.last_release_date)}")
    if tool.homepage:
        lines.append(f"- Homepage: {tool.homepage}")
    if tool.extra:
        for k, v in tool.extra.items():
            lines.append(f"- {k.capitalize()}: {v}")

    lines.append("")
    lines.append(f"Generated on {time.strftime('%Y-%m-%d')}.")
    return "\n".join(lines).strip() + "\n"


# -------------- Orchestration --------------

def resolve_tool_info(name: str, pypi_pkg: Optional[str], gh_repo: Optional[str], gl_project: Optional[str]) -> Optional[ToolInfo]:
    # Priority: PyPI, then GitHub, then GitLab (first successful)
    if pypi_pkg:
        t = fetch_from_pypi(pypi_pkg)
        if t:
            t.name = name or t.name
            return t
    if gh_repo:
        t = fetch_from_github(gh_repo)
        if t:
            t.name = name or t.name
            return t
    if gl_project:
        t = fetch_from_gitlab(gl_project)
        if t:
            t.name = name or t.name
            return t
    return None


def main(argv: Optional[list[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="Create a badge and Markdown info file for a tool.")
    parser.add_argument("--name", required=True, help="Display name of the tool (e.g., 'numpy').")
    parser.add_argument("--pypi", help="PyPI package name (e.g., 'numpy').")
    parser.add_argument("--github", help="GitHub repo in 'owner/name' form (e.g., 'numpy/numpy').")
    parser.add_argument("--gitlab", help="GitLab project path (e.g., 'mygroup/myproject').")
    parser.add_argument("--out", help="Output Markdown file path (default: <name>.md).")
    parser.add_argument("--badge-file", help="Output badge SVG path (default: badges/<name>-badge.svg).")
    parser.add_argument("--style", default="flat", help="Badge style (e.g., flat, flat-square).")
    args = parser.parse_args(argv)

    if not (args.pypi or args.github or args.gitlab):
        print("Error: provide at least one source: --pypi, --github, or --gitlab", file=sys.stderr)
        return 2

    tool = resolve_tool_info(args.name, args.pypi, args.github, args.gitlab)
    if not tool:
        print("Error: failed to fetch tool info from provided sources.", file=sys.stderr)
        return 1

    # Badge
    label, value, color = choose_badge(tool)
    try:
        badger = BadgeGenerator()
        svg = badger.make_svg(label=label, value=value, color=color, style=args.style)
    except Exception as e:
        print(f"Error creating badge: {e}", file=sys.stderr)
        return 1

    # Output paths
    out_md = Path(args.out) if args.out else Path(f"{sanitize_filename(tool.name)}.md")
    badge_path = Path(args.badge_file) if args.badge_file else Path("badges") / f"{sanitize_filename(tool.name)}-badge.svg"

    write_text(badge_path, svg)

    # Markdown content
    badge_rel = os.path.relpath(badge_path, start=out_md.parent.resolve())
    md = build_markdown(tool, badge_rel)

    write_text(out_md, md)

    print(f"Wrote badge: {badge_path}")
    print(f"Wrote markdown: {out_md}")
    return 0


def sanitize_filename(name: str) -> str:
    return "".join(c if c.isalnum() or c in ("-", "_", ".") else "_" for c in name).strip("_") or "tool"


if __name__ == "__main__":
    raise SystemExit(main())
------------------------------------------------------------

PROMPT 321:
"Generate Python code that imports the necessary modules for the devpi-server, devpi-client, and other related utilities, and defines a set of functions or classes that provide the shared functionality, such as string formatting, file manipulation, and network communication."
RESPONSE:
# Shared utilities for working with devpi-server/devpi-client and general-purpose helpers.
# This module is designed to run even if devpi is not installed; devpi-specific
# helpers gracefully degrade to subprocess or no-op behavior when needed.

from __future__ import annotations

import contextlib
import dataclasses
import hashlib
import json
import logging
import os
import shutil
import socket
import subprocess
import sys
import tempfile
import threading
import time
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, Iterator, Mapping, Optional, Sequence, Tuple, Union

try:
    # devpi-server imports (optional)
    import devpi_server  # type: ignore
    from devpi_server.main import main as devpi_server_main  # type: ignore
except Exception:
    devpi_server = None  # type: ignore
    devpi_server_main = None  # type: ignore

try:
    # devpi-client imports (optional)
    import devpi  # type: ignore
except Exception:
    devpi = None  # type: ignore

try:
    # common shared package used by both client and server (optional)
    import devpi_common  # type: ignore
except Exception:
    devpi_common = None  # type: ignore

try:
    import requests
except Exception as exc:  # pragma: no cover
    raise RuntimeError("The 'requests' package is required for network utilities.") from exc


# ------------------------------
# Logging
# ------------------------------

def get_logger(name: str = "devpi_utils", level: int = logging.INFO) -> logging.Logger:
    logger = logging.getLogger(name)
    if not logger.handlers:
        handler = logging.StreamHandler(stream=sys.stdout)
        formatter = logging.Formatter("%(asctime)s %(levelname)s [%(name)s] %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    logger.setLevel(level)
    return logger


log = get_logger()


# ------------------------------
# String utilities
# ------------------------------

def safe_format(template: str, mapping: Mapping[str, Any], missing: str = "") -> str:
    class DefaultDict(dict):
        def __missing__(self, key):
            return missing
    return template.format_map(DefaultDict(mapping))


def slugify(value: str, sep: str = "-") -> str:
    import re
    value = value.strip().lower()
    value = re.sub(r"[^a-z0-9]+", sep, value)
    value = re.sub(rf"{sep}+", sep, value).strip(sep)
    return value


def pluralize(n: int, singular: str, plural: Optional[str] = None) -> str:
    return singular if n == 1 else (plural or f"{singular}s")


def human_bytes(n: int) -> str:
    units = ["B", "KB", "MB", "GB", "TB"]
    size = float(n)
    for unit in units:
        if size < 1024 or unit == units[-1]:
            return f"{size:.1f} {unit}"
        size /= 1024.0


# ------------------------------
# File and path utilities
# ------------------------------

def ensure_dir(path: Union[str, Path]) -> Path:
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p


def read_text(path: Union[str, Path], encoding: str = "utf-8") -> str:
    return Path(path).read_text(encoding=encoding)


def write_text(path: Union[str, Path], data: str, encoding: str = "utf-8") -> None:
    Path(path).parent.mkdir(parents=True, exist_ok=True)
    Path(path).write_text(data, encoding=encoding)


def read_json(path: Union[str, Path], encoding: str = "utf-8") -> Any:
    return json.loads(read_text(path, encoding=encoding))


def write_json(path: Union[str, Path], data: Any, encoding: str = "utf-8", indent: int = 2) -> None:
    write_text(path, json.dumps(data, indent=indent, sort_keys=True), encoding=encoding)


@contextlib.contextmanager
def atomic_write(
    path: Union[str, Path],
    mode: str = "w",
    encoding: Optional[str] = "utf-8",
    newline: Optional[str] = None,
) -> Iterator[Tuple[Path, Any]]:
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    fd, tmp = tempfile.mkstemp(prefix=path.name, dir=str(path.parent))
    tmp_path = Path(tmp)
    f = None
    try:
        f = os.fdopen(fd, mode, encoding=encoding, newline=newline)  # type: ignore[arg-type]
        yield tmp_path, f
        f.flush()
        os.fsync(f.fileno())
        f.close()
        tmp_path.replace(path)
    except Exception:
        with contextlib.suppress(Exception):
            if f and not f.closed:
                f.close()
        with contextlib.suppress(Exception):
            tmp_path.unlink()
        raise


def copytree(src: Union[str, Path], dst: Union[str, Path], overwrite: bool = True) -> Path:
    src_p, dst_p = Path(src), Path(dst)
    if dst_p.exists() and overwrite:
        shutil.rmtree(dst_p)
    shutil.copytree(src_p, dst_p)
    return dst_p


def sha256_file(path: Union[str, Path], chunk_size: int = 1024 * 1024) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(chunk_size), b""):
            h.update(chunk)
    return h.hexdigest()


# ------------------------------
# Networking utilities
# ------------------------------

@dataclasses.dataclass
class HttpConfig:
    base_url: str
    timeout: float = 10.0
    retries: int = 3
    backoff: float = 0.5
    headers: Optional[Mapping[str, str]] = None
    verify: Union[bool, str] = True  # bool or path to CA bundle
    auth: Optional[Tuple[str, str]] = None


class HttpClient:
    def __init__(self, cfg: HttpConfig):
        self.cfg = cfg
        self.session = requests.Session()
        if cfg.headers:
            self.session.headers.update(dict(cfg.headers))

    def _request(self, method: str, url: str, **kwargs) -> requests.Response:
        timeout = kwargs.pop("timeout", self.cfg.timeout)
        verify = kwargs.pop("verify", self.cfg.verify)
        auth = kwargs.pop("auth", self.cfg.auth)
        last_exc = None
        for attempt in range(1, self.cfg.retries + 1):
            try:
                resp = self.session.request(method, url, timeout=timeout, verify=verify, auth=auth, **kwargs)
                return resp
            except requests.RequestException as exc:
                last_exc = exc
                if attempt >= self.cfg.retries:
                    break
                time.sleep(self.cfg.backoff * attempt)
        assert last_exc is not None
        raise last_exc

    def get(self, path: str, **kwargs) -> requests.Response:
        return self._request("GET", self._join(path), **kwargs)

    def post(self, path: str, **kwargs) -> requests.Response:
        return self._request("POST", self._join(path), **kwargs)

    def put(self, path: str, **kwargs) -> requests.Response:
        return self._request("PUT", self._join(path), **kwargs)

    def delete(self, path: str, **kwargs) -> requests.Response:
        return self._request("DELETE", self._join(path), **kwargs)

    def download_to(self, path: str, dest: Union[str, Path], chunk_size: int = 1024 * 1024) -> Path:
        dest = Path(dest)
        with self.get(path, stream=True) as r:
            r.raise_for_status()
            ensure_dir(dest.parent)
            with open(dest, "wb") as f:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    if chunk:
                        f.write(chunk)
        return dest

    def _join(self, path: str) -> str:
        if path.startswith("http://") or path.startswith("https://"):
            return path
        base = self.cfg.base_url.rstrip("/")
        path = path.lstrip("/")
        return f"{base}/{path}"


def is_port_open(host: str, port: int, timeout: float = 1.0) -> bool:
    with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
        sock.settimeout(timeout)
        try:
            sock.connect((host, port))
            return True
        except OSError:
            return False


def wait_for_port(host: str, port: int, timeout: float = 30.0, interval: float = 0.2) -> bool:
    start = time.time()
    while time.time() - start < timeout:
        if is_port_open(host, port, timeout=interval):
            return True
        time.sleep(interval)
    return False


def wait_for_http(url: str, timeout: float = 30.0, interval: float = 0.5) -> bool:
    start = time.time()
    while time.time() - start < timeout:
        try:
            r = requests.get(url, timeout=interval)
            if r.ok:
                return True
        except requests.RequestException:
            pass
        time.sleep(interval)
    return False


# ------------------------------
# devpi client/server helpers
# ------------------------------

@dataclasses.dataclass
class DevpiServerConfig:
    host: str = "127.0.0.1"
    port: int = 3141
    serverdir: Optional[Union[str, Path]] = None
    args: Tuple[str, ...] = ()
    debug: bool = False
    timeout: float = 30.0


class DevpiServerController:
    def __init__(self, cfg: Optional[DevpiServerConfig] = None, logger: Optional[logging.Logger] = None):
        self.cfg = cfg or DevpiServerConfig()
        self.log = logger or log
        self._proc: Optional[subprocess.Popen] = None
        self.serverdir = Path(self.cfg.serverdir or tempfile.mkdtemp(prefix="devpi-server-"))

    @property
    def base_url(self) -> str:
        return f"http://{self.cfg.host}:{self.cfg.port}"

    def start(self) -> None:
        cmd: Sequence[str]
        if devpi_server_main is not None:
            # Prefer subprocess so we can stop reliably and capture output
            cmd = [
                sys.executable, "-m", "devpi_server",
                "--serverdir", str(self.serverdir),
                "--host", self.cfg.host,
                "--port", str(self.cfg.port),
                *self.cfg.args,
            ]
        else:
            # Fallback to CLI if module import failed
            cmd = [
                "devpi-server",
                "--serverdir", str(self.serverdir),
                "--host", self.cfg.host,
                "--port", str(self.cfg.port),
                *self.cfg.args,
            ]

        stdout = None if self.cfg.debug else subprocess.PIPE
        stderr = None if self.cfg.debug else subprocess.STDOUT
        self.log.info("Starting devpi-server: %s", " ".join(cmd))
        self._proc = subprocess.Popen(cmd, stdout=stdout, stderr=stderr, text=True)
        if not wait_for_port(self.cfg.host, self.cfg.port, timeout=self.cfg.timeout):
            self.stop()
            raise RuntimeError("devpi-server did not start within timeout")
        # Extra HTTP health check
        if not wait_for_http(self.base_url, timeout=self.cfg.timeout):
            self.stop()
            raise RuntimeError("devpi-server HTTP not responding within timeout")
        self.log.info("devpi-server started at %s (serverdir=%s)", self.base_url, self.serverdir)

    def stop(self) -> None:
        if self._proc and self._proc.poll() is None:
            self.log.info("Stopping devpi-server (pid=%s)", self._proc.pid)
            with contextlib.suppress(Exception):
                self._proc.terminate()
            try:
                self._proc.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self.log.warning("Terminating devpi-server timed out; killing")
                with contextlib.suppress(Exception):
                    self._proc.kill()
            self._proc = None

    def __enter__(self):
        self.start()
        return self

    def __exit__(self, exc_type, exc, tb):
        self.stop()


@dataclasses.dataclass
class DevpiClientConfig:
    base_url: str
    index: str = "root/pypi"
    user: Optional[str] = None
    password: Optional[str] = None
    verify: Union[bool, str] = True  # TLS verify
    timeout: float = 10.0


class DevpiClient:
    def __init__(self, cfg: DevpiClientConfig, logger: Optional[logging.Logger] = None):
        self.cfg = cfg
        self.log = logger or log
        self.http = HttpClient(HttpConfig(base_url=cfg.base_url, verify=cfg.verify, timeout=cfg.timeout))

        # Determine how we'll run the client: module or CLI
        if devpi is not None:
            # We still prefer CLI to keep behavior identical to user environment,
            # but detect presence for diagnostics.
            self.log.debug("devpi client module detected")
        self._cli = self._detect_cli()

    def _detect_cli(self) -> str:
        # Prefer "devpi" script; fall back to python -m devpi if available
        candidates = [
            shutil.which("devpi"),
            f"{sys.executable} -m devpi" if devpi is not None else None,
        ]
        for c in candidates:
            if c:
                return c
        raise RuntimeError("devpi-client not found. Install with 'pip install devpi-client'.")

    def _run(self, args: Sequence[str], env: Optional[Mapping[str, str]] = None) -> Tuple[int, str]:
        cmd = []
        if self._cli.endswith(" -m devpi"):
            cmd = [sys.executable, "-m", "devpi", *args]
        elif " " in self._cli:
            # if someone passed a composed command string
            cmd = self._cli.split() + list(args)
        else:
            cmd = [self._cli, *args]

        full_env = os.environ.copy()
        if env:
            full_env.update(env)

        self.log.debug("Running: %s", " ".join(cmd))
        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=full_env)
        out, _ = proc.communicate()
        rc = proc.returncode
        if rc != 0:
            self.log.error("devpi command failed (%s): %s", rc, out.strip())
        else:
            self.log.debug("devpi output: %s", out.strip())
        return rc, out

    # Basic operations

    def use(self, index: Optional[str] = None) -> None:
        idx = index or self.cfg.index
        rc, _ = self._run(["use", f"{self.cfg.base_url}/{idx}"])
        if rc != 0:
            raise RuntimeError(f"Failed to set devpi index: {idx}")

    def login(self, user: Optional[str] = None, password: Optional[str] = None) -> None:
        u = user or self.cfg.user
        p = password or self.cfg.password
        if not u or not p:
            raise ValueError("User and password are required for login")
        rc, _ = self._run(["login", u, "--password", p])
        if rc != 0:
            raise RuntimeError("devpi login failed")

    def logout(self) -> None:
        self._run(["logout"])

    def upload(self, path: Union[str, Path]) -> None:
        p = Path(path)
        if not p.exists():
            raise FileNotFoundError(p)
        rc, _ = self._run(["upload", str(p)])
        if rc != 0:
            raise RuntimeError("devpi upload failed")

    def list_packages(self, pattern: Optional[str] = None) -> str:
        args = ["list"]
        if pattern:
            args.append(pattern)
        rc, out = self._run(args)
        if rc != 0:
            raise RuntimeError("devpi list failed")
        return out

    # Server-side REST queries (useful for scripting)

    def get_json(self, path: str) -> Any:
        r = self.http.get(path, headers={"Accept": "application/json"})
        r.raise_for_status()
        return r.json()

    def create_user(self, username: str, password: str, email: Optional[str] = None) -> None:
        # devpi REST: POST /user/{username}
        payload = {"password": password}
        if email:
            payload["email"] = email
        r = self.http.put(f"/user/{username}", json=payload)
        if r.status_code not in (200, 201):
            raise RuntimeError(f"Failed to create user '{username}': {r.status_code} {r.text}")

    def create_index(self, index: str, bases: str = "root/pypi", volatile: bool = True) -> None:
        # devpi REST: PUT /{user}/{index}
        user, idx = index.split("/", 1) if "/" in index else ("root", index)
        payload = {"bases": bases, "volatile": str(volatile).lower()}
        r = self.http.put(f"/{user}/{idx}", json=payload)
        if r.status_code not in (200, 201):
            raise RuntimeError(f"Failed to create index '{index}': {r.status_code} {r.text}")

    def server_info(self) -> Any:
        return self.get_json("/")


# ------------------------------
# Example convenience routine
# ------------------------------

def bootstrap_devpi_environment(
    server_host: str = "127.0.0.1",
    server_port: int = 3141,
    user: str = "root",
    password: Optional[str] = None,
    index: str = "root/pypi",
    serverdir: Optional[Union[str, Path]] = None,
    create_index: Optional[str] = None,
) -> Tuple[DevpiServerController, DevpiClient]:
    """
    Starts a devpi-server, waits until ready, logs in with devpi-client,
    optionally creates an index, and returns both controller and client.

    Note: Caller is responsible for stopping the server via controller.stop()
    or using the controller as a context manager.
    """
    server = DevpiServerController(DevpiServerConfig(host=server_host, port=server_port, serverdir=serverdir))
    server.start()

    client = DevpiClient(DevpiClientConfig(base_url=server.base_url, user=user, password=password or "", index=index))
    client.use(index=index)
    if user and password:
        client.login(user=user, password=password)

    if create_index:
        client.create_index(create_index)

    return server, client


__all__ = [
    # logging
    "get_logger",
    # strings
    "safe_format", "slugify", "pluralize", "human_bytes",
    # files
    "ensure_dir", "read_text", "write_text", "read_json", "write_json",
    "atomic_write", "copytree", "sha256_file",
    # networking
    "HttpConfig", "HttpClient", "is_port_open", "wait_for_port", "wait_for_http",
    # devpi
    "DevpiServerConfig", "DevpiServerController", "DevpiClientConfig", "DevpiClient",
    # bootstrap
    "bootstrap_devpi_environment",
]
------------------------------------------------------------

PROMPT 322:
"Generate Python code that imports the necessary libraries, such as Pandas and SQLite, to create a data load and catalog system that can read data from various sources, store it in a centralized database, and provide an easy-to-use interface for searching and retrieving data."
RESPONSE:
import os
import re
import io
import json
import hashlib
import sqlite3
import datetime as dt
import pandas as pd
from typing import Optional, Iterable, Dict, Any, Union, List


class DataCatalog:
    """
    A lightweight data load and catalog system using Pandas and SQLite.

    Features:
    - Load data from various sources (CSV/TSV, Excel, JSON, Parquet, Pandas DataFrame).
    - Store data centrally in a SQLite database as tables.
    - Maintain metadata catalog (name, description, tags, schema, counts, provenance).
    - Search by name, tags, columns, source, and free-text.
    - Retrieve entire datasets or filtered subsets using simple queries.

    Database layout:
    - datasets: metadata about each registered dataset/table
    - search_fts (optional): FTS5 table for fast free-text search if available in SQLite
    """

    def __init__(self, db_path: str = "catalog.db"):
        self.db_path = db_path
        self.conn = sqlite3.connect(self.db_path)
        self.conn.row_factory = sqlite3.Row
        self._apply_pragmas()
        self._init_schema()

    def close(self):
        if self.conn:
            self.conn.commit()
            self.conn.close()
            self.conn = None

    # ---------------------------
    # Initialization and schema
    # ---------------------------

    def _apply_pragmas(self):
        cur = self.conn.cursor()
        cur.execute("PRAGMA journal_mode=WAL;")
        cur.execute("PRAGMA synchronous=NORMAL;")
        cur.execute("PRAGMA foreign_keys=ON;")
        self.conn.commit()

    def _init_schema(self):
        cur = self.conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS datasets (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE NOT NULL,
                table_name TEXT UNIQUE NOT NULL,
                source_type TEXT,
                source_uri TEXT,
                format TEXT,
                tags TEXT,
                description TEXT,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                row_count INTEGER,
                column_count INTEGER,
                columns_json TEXT,
                columns_text TEXT,
                checksum TEXT,
                file_size INTEGER
            );
            """
        )
        cur.execute("CREATE INDEX IF NOT EXISTS idx_datasets_name ON datasets(name);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_datasets_tags ON datasets(tags);")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_datasets_source ON datasets(source_type);")

        # Try to create FTS5 table for free-text search; ignore if not supported
        try:
            cur.execute(
                """
                CREATE VIRTUAL TABLE IF NOT EXISTS search_fts
                USING fts5(name, description, tags, columns_text, source_uri, content='');
                """
            )
        except sqlite3.OperationalError:
            # FTS5 likely not available
            pass

        self.conn.commit()

    # ---------------------------
    # Helpers
    # ---------------------------

    @staticmethod
    def _now_iso() -> str:
        return dt.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

    @staticmethod
    def _is_url(path_or_url: str) -> bool:
        return bool(re.match(r"^https?://", str(path_or_url), re.IGNORECASE))

    @staticmethod
    def _detect_format(source: Any, format_hint: Optional[str] = None) -> str:
        if isinstance(source, pd.DataFrame):
            return "dataframe"
        if format_hint:
            return format_hint.lower()

        if isinstance(source, (str, os.PathLike)):
            s = str(source).lower()
            if s.endswith(".csv"):
                return "csv"
            if s.endswith(".tsv") or s.endswith(".tab"):
                return "tsv"
            if s.endswith(".xlsx") or s.endswith(".xls"):
                return "excel"
            if s.endswith(".json"):
                return "json"
            if s.endswith(".parquet"):
                return "parquet"
            if s.endswith(".feather"):
                return "feather"

        # Default guess
        return "csv"

    @staticmethod
    def _sanitize_table_name(name: str) -> str:
        # Convert to a safe SQLite identifier: lowercase, underscores, alnum only
        base = re.sub(r"\W+", "_", name.strip().lower()).strip("_")
        if not base:
            base = "dataset"
        # Ensure doesn't start with digit
        if re.match(r"^\d", base):
            base = f"t_{base}"
        return base

    @staticmethod
    def _quote_ident(ident: str) -> str:
        # Quote SQLite identifier safely
        return '"' + ident.replace('"', '""') + '"'

    @staticmethod
    def _hash_file_md5(path: str, chunk_size: int = 2**20) -> Optional[str]:
        try:
            md5 = hashlib.md5()
            with open(path, "rb") as f:
                for chunk in iter(lambda: f.read(chunk_size), b""):
                    md5.update(chunk)
            return md5.hexdigest()
        except Exception:
            return None

    @staticmethod
    def _file_size(path: str) -> Optional[int]:
        try:
            return os.path.getsize(path)
        except Exception:
            return None

    @staticmethod
    def _df_schema_info(df: pd.DataFrame) -> Dict[str, Any]:
        cols = []
        for c in df.columns:
            dtype = str(df[c].dtype)
            cols.append({"name": str(c), "dtype": dtype})
        cols_text = " ".join([c["name"] for c in cols])
        return {
            "column_count": len(df.columns),
            "columns_json": json.dumps(cols, ensure_ascii=False),
            "columns_text": cols_text,
        }

    def _dataset_exists(self, name: str) -> Optional[sqlite3.Row]:
        cur = self.conn.cursor()
        cur.execute("SELECT * FROM datasets WHERE name = ?;", (name,))
        return cur.fetchone()

    def _insert_or_update_metadata(
        self,
        name: str,
        table_name: str,
        df: Optional[pd.DataFrame],
        source_type: Optional[str],
        source_uri: Optional[str],
        fmt: str,
        tags: Optional[Union[str, Iterable[str]]],
        description: Optional[str],
        checksum: Optional[str],
        file_size: Optional[int],
        mode: str,
    ):
        if isinstance(tags, (list, tuple, set)):
            tags_str = ",".join(sorted([str(t).strip() for t in tags if str(t).strip()]))
        else:
            tags_str = (tags or "").strip()

        now = self._now_iso()

        if df is not None:
            schema = self._df_schema_info(df)
            row_count = int(len(df))
            column_count = int(schema["column_count"])
            columns_json = schema["columns_json"]
            columns_text = schema["columns_text"]
        else:
            # When appending via chunks we might not have a single df at the end
            # We can recompute using SQL COUNT(*) and PRAGMA table_info
            row_count = self._table_row_count(table_name)
            column_count, columns_json, columns_text = self._table_schema_info(table_name)

        existing = self._dataset_exists(name)
        cur = self.conn.cursor()

        if mode == "append" and existing:
            # Update mutable fields; keep created_at
            cur.execute(
                """
                UPDATE datasets
                SET updated_at = ?,
                    row_count = ?,
                    column_count = ?,
                    columns_json = ?,
                    columns_text = ?,
                    tags = COALESCE(?, tags),
                    description = COALESCE(?, description),
                    checksum = COALESCE(?, checksum),
                    file_size = COALESCE(?, file_size)
                WHERE name = ?;
                """,
                (
                    now,
                    row_count,
                    column_count,
                    columns_json,
                    columns_text,
                    tags_str if tags_str else None,
                    description if description else None,
                    checksum,
                    file_size,
                    name,
                ),
            )
        else:
            # Replace or new insert
            if existing:
                cur.execute("DELETE FROM datasets WHERE name = ?;", (name,))
            cur.execute(
                """
                INSERT INTO datasets (
                    name, table_name, source_type, source_uri, format, tags, description,
                    created_at, updated_at, row_count, column_count, columns_json, columns_text,
                    checksum, file_size
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
                """,
                (
                    name,
                    table_name,
                    source_type,
                    source_uri,
                    fmt,
                    tags_str,
                    description,
                    now,
                    now,
                    row_count,
                    column_count,
                    columns_json,
                    columns_text,
                    checksum,
                    file_size,
                ),
            )

        # Update FTS if present
        try:
            cur.execute("DELETE FROM search_fts WHERE rowid IN (SELECT rowid FROM search_fts WHERE name = ?);", (name,))
            cur.execute(
                "INSERT INTO search_fts (name, description, tags, columns_text, source_uri) VALUES (?, ?, ?, ?, ?);",
                (name, description or "", tags_str or "", columns_text or "", source_uri or ""),
            )
        except sqlite3.OperationalError:
            pass

        self.conn.commit()

    def _table_row_count(self, table_name: str) -> int:
        q = f"SELECT COUNT(*) as c FROM {self._quote_ident(table_name)};"
        cur = self.conn.cursor()
        cur.execute(q)
        return int(cur.fetchone()[0])

    def _table_schema_info(self, table_name: str):
        cur = self.conn.cursor()
        cur.execute(f"PRAGMA table_info({self._quote_ident(table_name)});")
        cols = [{"name": row["name"], "dtype": "UNKNOWN"} for row in cur.fetchall()]
        # dtype unknown here; best effort
        columns_json = json.dumps(cols, ensure_ascii=False)
        columns_text = " ".join([c["name"] for c in cols])
        return (len(cols), columns_json, columns_text)

    def _ensure_unique_table_name(self, base: str) -> str:
        base = self._sanitize_table_name(base)
        cur = self.conn.cursor()
        name = base
        i = 1
        while True:
            try:
                cur.execute(f"SELECT 1 FROM sqlite_master WHERE type='table' AND name=?;", (name,))
                if not cur.fetchone():
                    break
                i += 1
                name = f"{base}_{i}"
            except sqlite3.Error:
                i += 1
                name = f"{base}_{i}"
        return name

    # ---------------------------
    # Data loading
    # ---------------------------

    def _load_dataframe(
        self,
        source: Any,
        fmt: Optional[str] = None,
        read_kwargs: Optional[Dict[str, Any]] = None,
        stream_chunks: bool = False,
        chunk_size: Optional[int] = None,
    ):
        read_kwargs = read_kwargs or {}
        fmt = self._detect_format(source, fmt)

        if fmt == "dataframe":
            if not isinstance(source, pd.DataFrame):
                raise ValueError("Source format 'dataframe' requires a pandas DataFrame object.")
            return "dataframe", source

        if fmt in ("csv", "tsv"):
            if fmt == "tsv":
                read_kwargs = {"sep": "\t", **read_kwargs}
            if stream_chunks or (chunk_size and chunk_size > 0):
                read_kwargs = {"chunksize": chunk_size or 100_000, **read_kwargs}
                df_iter = pd.read_csv(source, **read_kwargs)
                return "iterator", df_iter
            else:
                df = pd.read_csv(source, **read_kwargs)
                return "dataframe", df

        if fmt == "excel":
            df = pd.read_excel(source, **read_kwargs)
            return "dataframe", df

        if fmt == "json":
            df = pd.read_json(source, **read_kwargs)
            return "dataframe", df

        if fmt == "parquet":
            df = pd.read_parquet(source, **read_kwargs)
            return "dataframe", df

        if fmt == "feather":
            df = pd.read_feather(source, **read_kwargs)
            return "dataframe", df

        # Default fall-back
        df = pd.read_csv(source, **read_kwargs)
        return "dataframe", df

    def add_dataset(
        self,
        name: str,
        source: Union[str, os.PathLike, pd.DataFrame],
        format: Optional[str] = None,
        tags: Optional[Union[str, Iterable[str]]] = None,
        description: Optional[str] = None,
        if_exists: str = "fail",  # 'fail', 'replace', 'append'
        store_table_name: Optional[str] = None,
        index: bool = False,
        chunksize: Optional[int] = None,
        dtype: Optional[Dict[str, Any]] = None,
        read_kwargs: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Load data from source and store it in SQLite, with catalog metadata.

        Returns the table_name used in SQLite.
        """
        if_exists = if_exists.lower()
        if if_exists not in ("fail", "replace", "append"):
            raise ValueError("if_exists must be one of: 'fail', 'replace', 'append'")

        existing = self._dataset_exists(name)
        if existing and if_exists == "fail":
            raise ValueError(f"Dataset '{name}' already exists. Use if_exists='replace' or 'append'.")

        source_uri = str(source) if isinstance(source, (str, os.PathLike)) else "python:DataFrame"
        source_type = "url" if isinstance(source, (str, os.PathLike)) and self._is_url(str(source)) else \
                      ("file" if isinstance(source, (str, os.PathLike)) else "in_memory")
        fmt = self._detect_format(source, format)

        # Decide table name
        if store_table_name:
            table_name = self._sanitize_table_name(store_table_name)
            # If replacing we can reuse name; else ensure unique if exists
            if if_exists in ("fail", "append"):
                # Check if table exists already in SQLite, if not catalog entry
                cur = self.conn.cursor()
                cur.execute("SELECT 1 FROM sqlite_master WHERE type='table' AND name=?;", (table_name,))
                if cur.fetchone() and not existing:
                    table_name = self._ensure_unique_table_name(table_name)
        else:
            if existing and if_exists in ("replace", "append"):
                table_name = existing["table_name"]
            else:
                table_name = self._ensure_unique_table_name(name)

        # Handle replace: drop old table and metadata
        if existing and if_exists == "replace":
            self._drop_table_if_exists(table_name)
            cur = self.conn.cursor()
            cur.execute("DELETE FROM datasets WHERE name = ?;", (name,))
            try:
                cur.execute("DELETE FROM search_fts WHERE name = ?;", (name,))
            except sqlite3.OperationalError:
                pass
            self.conn.commit()

        # Determine checksum and file size for local file sources
        checksum = None
        file_size = None
        if isinstance(source, (str, os.PathLike)) and not self._is_url(str(source)):
            p = str(source)
            checksum = self._hash_file_md5(p)
            file_size = self._file_size(p)

        # Load data (possibly in chunks)
        stream = fmt in ("csv", "tsv") and (chunksize and chunksize > 0)
        loader_type, payload = self._load_dataframe(
            source, fmt=fmt, read_kwargs=read_kwargs, stream_chunks=stream, chunk_size=chunksize
        )

        # If appending, check schema compatibility (basic: same column set)
        if existing and if_exists == "append":
            # We'll compare columns after reading the first chunk or dataframe
            pass

        # Write to SQLite
        if loader_type == "iterator":
            # Write first chunk, create table
            iterator = iter(payload)
            try:
                first = next(iterator)
            except StopIteration:
                # Empty source
                first = pd.DataFrame()
            self._to_sql(first, table_name, if_exists="replace", index=index, dtype=dtype)
            for chunk in iterator:
                self._to_sql(chunk, table_name, if_exists="append", index=index, dtype=dtype)
            # Catalog metadata from assembled table
            df_for_meta = None  # we'll derive metadata from table in _insert_or_update_metadata
            write_mode = "append" if existing and if_exists == "append" else ("replace" if existing else "new")
            self._insert_or_update_metadata(
                name=name,
                table_name=table_name,
                df=df_for_meta,
                source_type=source_type,
                source_uri=source_uri,
                fmt=fmt,
                tags=tags,
                description=description,
                checksum=checksum,
                file_size=file_size,
                mode="append" if if_exists == "append" else "replace",
            )
        else:
            df: pd.DataFrame = payload
            # If appending, check columns
            if existing and if_exists == "append":
                self._assert_compatible_schema(existing["table_name"], df)
                self._to_sql(df, table_name, if_exists="append", index=index, dtype=dtype)
                self._insert_or_update_metadata(
                    name=name,
                    table_name=table_name,
                    df=df,
                    source_type=source_type,
                    source_uri=source_uri,
                    fmt=fmt,
                    tags=tags,
                    description=description,
                    checksum=checksum,
                    file_size=file_size,
                    mode="append",
                )
            else:
                self._to_sql(df, table_name, if_exists="replace", index=index, dtype=dtype)
                self._insert_or_update_metadata(
                    name=name,
                    table_name=table_name,
                    df=df,
                    source_type=source_type,
                    source_uri=source_uri,
                    fmt=fmt,
                    tags=tags,
                    description=description,
                    checksum=checksum,
                    file_size=file_size,
                    mode="replace",
                )

        return table_name

    def _to_sql(self, df: pd.DataFrame, table_name: str, if_exists: str, index: bool, dtype: Optional[Dict[str, Any]]):
        # Normalize column names to strings
        df = df.copy()
        df.columns = [str(c) for c in df.columns]
        df.to_sql(
            name=table_name,
            con=self.conn,
            if_exists=if_exists,
            index=index,
            chunksize=50_000,
            method=None,
            dtype=dtype,
        )

    def _drop_table_if_exists(self, table_name: str):
        cur = self.conn.cursor()
        cur.execute(f"DROP TABLE IF EXISTS {self._quote_ident(table_name)};")
        self.conn.commit()

    def _assert_compatible_schema(self, table_name: str, df: pd.DataFrame):
        # Simple check: same set of column names (order-insensitive)
        cur = self.conn.cursor()
        cur.execute(f"PRAGMA table_info({self._quote_ident(table_name)});")
        existing_cols = [row["name"] for row in cur.fetchall()]
        df_cols = [str(c) for c in df.columns]
        if set(existing_cols) != set(df_cols):
            raise ValueError(
                f"Schema mismatch for append. Existing columns: {existing_cols}. New columns: {df_cols}."
            )

    # ---------------------------
    # Retrieval and querying
    # ---------------------------

    def get_dataset(self, name: str, columns: Optional[List[str]] = None, where: Optional[str] = None,
                    limit: Optional[int] = None, params: Optional[Iterable[Any]] = None) -> pd.DataFrame:
        meta = self._dataset_exists(name)
        if not meta:
            raise ValueError(f"Dataset '{name}' not found.")
        table = self._quote_ident(meta["table_name"])
        cols = "*"
        if columns:
            cols = ", ".join(self._quote_ident(c) for c in columns)
        q = f"SELECT {cols} FROM {table}"
        if where:
            q += f" WHERE {where}"
        if limit is not None:
            q += f" LIMIT {int(limit)}"
        return pd.read_sql_query(q, self.conn, params=params)

    def query_sql(self, sql: str, params: Optional[Iterable[Any]] = None) -> pd.DataFrame:
        return pd.read_sql_query(sql, self.conn, params=params)

    def list_datasets(self) -> pd.DataFrame:
        return pd.read_sql_query(
            "SELECT id, name, table_name, format, tags, description, row_count, column_count, created_at, updated_at FROM datasets ORDER BY name;",
            self.conn,
        )

    def describe_dataset(self, name: str, sample_rows: int = 10) -> Dict[str, Any]:
        meta = self._dataset_exists(name)
        if not meta:
            raise ValueError(f"Dataset '{name}' not found.")
        sample = self.get_dataset(name, limit=sample_rows)
        return {
            "metadata": dict(meta),
            "sample": sample,
        }

    # ---------------------------
    # Search
    # ---------------------------

    def search(
        self,
        text: Optional[str] = None,
        name_like: Optional[str] = None,
        tags_in: Optional[Iterable[str]] = None,
        has_columns: Optional[Iterable[str]] = None,
        source_type: Optional[str] = None,
        limit: int = 50,
    ) -> pd.DataFrame:
        params = []
        conditions = []

        # Use FTS if available for free-text
        if text:
            try:
                # Using MATCH query
                df = pd.read_sql_query(
                    """
                    SELECT d.*
                    FROM search_fts s
                    JOIN datasets d ON d.name = s.name
                    WHERE s MATCH ?
                    LIMIT ?;
                    """,
                    self.conn,
                    params=(text, limit),
                )
                return df
            except sqlite3.OperationalError:
                # FTS not available; fall back to LIKE on several fields
                like = f"%{text}%"
                for col in ("name", "description", "tags", "columns_text", "source_uri"):
                    conditions.append(f"{col} LIKE ?")
                    params.append(like)

        if name_like:
            conditions.append("name LIKE ?")
            params.append(f"%{name_like}%")

        if tags_in:
            for t in tags_in:
                conditions.append("tags LIKE ?")
                params.append(f"%{t}%")

        if has_columns:
            for c in has_columns:
                conditions.append("columns_text LIKE ?")
                params.append(f"%{c}%")

        if source_type:
            conditions.append("source_type = ?")
            params.append(source_type)

        where = f"WHERE {' AND '.join(conditions)}" if conditions else ""
        sql = f"SELECT * FROM datasets {where} ORDER BY updated_at DESC LIMIT ?;"
        params.append(limit)
        return pd.read_sql_query(sql, self.conn, params=params)

    # ---------------------------
    # Maintenance
    # ---------------------------

    def remove_dataset(self, name: str, drop_table: bool = True):
        meta = self._dataset_exists(name)
        if not meta:
            raise ValueError(f"Dataset '{name}' not found.")
        if drop_table:
            self._drop_table_if_exists(meta["table_name"])
        cur = self.conn.cursor()
        cur.execute("DELETE FROM datasets WHERE name = ?;", (name,))
        try:
            cur.execute("DELETE FROM search_fts WHERE name = ?;", (name,))
        except sqlite3.OperationalError:
            pass
        self.conn.commit()

    def rename_dataset(self, old_name: str, new_name: str):
        if self._dataset_exists(new_name):
            raise ValueError(f"Target dataset name '{new_name}' already exists.")
        meta = self._dataset_exists(old_name)
        if not meta:
            raise ValueError(f"Dataset '{old_name}' not found.")
        cur = self.conn.cursor()
        cur.execute("UPDATE datasets SET name = ?, updated_at = ? WHERE name = ?;", (new_name, self._now_iso(), old_name))
        try:
            cur.execute("UPDATE search_fts SET name = ? WHERE name = ?;", (new_name, old_name))
        except sqlite3.OperationalError:
            pass
        self.conn.commit()

    def set_tags(self, name: str, tags: Union[str, Iterable[str]]):
        if isinstance(tags, (list, tuple, set)):
            tags_str = ",".join(sorted([str(t).strip() for t in tags if str(t).strip()]))
        else:
            tags_str = (tags or "").strip()
        cur = self.conn.cursor()
        cur.execute("UPDATE datasets SET tags = ?, updated_at = ? WHERE name = ?;", (tags_str, self._now_iso(), name))
        try:
            cur.execute("UPDATE search_fts SET tags = ? WHERE name = ?;", (tags_str, name))
        except sqlite3.OperationalError:
            pass
        self.conn.commit()

    def set_description(self, name: str, description: str):
        cur = self.conn.cursor()
        cur.execute("UPDATE datasets SET description = ?, updated_at = ? WHERE name = ?;", (description, self._now_iso(), name))
        try:
            cur.execute("UPDATE search_fts SET description = ? WHERE name = ?;", (description, name))
        except sqlite3.OperationalError:
            pass
        self.conn.commit()

    # ---------------------------
    # Utilities
    # ---------------------------

    def export_dataset_to_csv(self, name: str, out_path: str):
        df = self.get_dataset(name)
        df.to_csv(out_path, index=False)

    def vacuum(self):
        cur = self.conn.cursor()
        cur.execute("VACUUM;")
        self.conn.commit()


# Example usage
if __name__ == "__main__":
    catalog = DataCatalog("catalog.db")

    # Example: add CSV
    # table = catalog.add_dataset(
    #     name="employees",
    #     source="employees.csv",
    #     tags=["hr", "people"],
    #     description="Employee master data",
    #     if_exists="replace",
    #     chunksize=100_000,  # stream large CSV
    # )

    # Example: add DataFrame directly
    # df = pd.DataFrame({"a": [1, 2, 3], "b": ["x", "y", "z"]})
    # catalog.add_dataset("tiny_df", df, description="Small in-memory dataset", if_exists="replace")

    # Example: search
    # print(catalog.search(text="employee"))
    # print(catalog.list_datasets())

    # Example: retrieve
    # result = catalog.get_dataset("employees", where="department = ?", params=("Engineering",), limit=5)
    # print(result.head())

    catalog.close()
------------------------------------------------------------

PROMPT 323:
"Generate Python code that creates a high-performance function serving system for scientific computing using the `funcX` library, allowing users to deploy and manage scalable, secure, and versioned functions for data processing, machine learning, and other scientific workloads."
RESPONSE:
# High-performance function serving with funcX / Globus Compute
# -------------------------------------------------------------
# This module provides:
# - A lightweight function registry with semantic versioning (SQLite)
# - Declarative function deployment to funcX/Globus Compute
# - Scalable execution across one or more endpoints
# - Simple security defaults (private functions by default)
#
# Requirements:
#   pip install globus-compute-sdk
#   # or (legacy) pip install funcx
#
# Notes:
# - funcX has evolved into Globus Compute. This code works with globus-compute-sdk
#   and falls back to funcX if available in your environment.
# - Endpoints must already be installed and running on your target resources.
#   See: https://www.globus.org/compute
#
# Usage:
#   1) Set up and start at least one endpoint. Record its endpoint UUID.
#   2) Use FunctionDeployer to register/version your functions.
#   3) Use FunctionRunner to run them at scale on endpoints.
#
# At the bottom of this file, you'll find an end-to-end demo.

import os
import time
import json
import sqlite3
import hashlib
import inspect
import threading
from dataclasses import dataclass
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple

# Try modern Globus Compute SDK first, fall back to legacy funcX
try:
    from globus_compute_sdk import Client as _ComputeClient
    GC_SDK = True
except Exception:
    GC_SDK = False

if not GC_SDK:
    try:
        # Legacy funcX SDK
        from funcx import FuncXClient as _ComputeClient  # type: ignore
    except Exception as e:
        raise RuntimeError(
            "Neither globus-compute-sdk nor funcx could be imported. "
            "Install with: pip install globus-compute-sdk"
        ) from e


@dataclass
class FunctionRecord:
    name: str
    version: str
    function_id: str
    description: Optional[str]
    visibility: str  # 'private' or 'public'
    source_hash: str  # sha256 of function source
    created_at: float
    extras: Dict[str, Any]


class FunctionRegistry:
    """
    A lightweight SQLite-backed registry that tracks function versions and metadata.

    This registry stores a mapping from (name, version) -> Globus Compute function_id,
    plus descriptive metadata. It enables reproducible, versioned execution.
    """

    def __init__(self, db_path: str = "func_registry.sqlite"):
        self.db_path = db_path
        self._ensure_schema()

    def _connect(self):
        conn = sqlite3.connect(self.db_path)
        conn.execute("PRAGMA journal_mode = WAL;")
        conn.execute("PRAGMA synchronous = NORMAL;")
        return conn

    def _ensure_schema(self):
        conn = self._connect()
        try:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS functions (
                    name TEXT NOT NULL,
                    version TEXT NOT NULL,
                    function_id TEXT NOT NULL,
                    description TEXT,
                    visibility TEXT NOT NULL,
                    source_hash TEXT NOT NULL,
                    created_at REAL NOT NULL,
                    extras TEXT,
                    PRIMARY KEY (name, version)
                );
                """
            )
            conn.commit()
        finally:
            conn.close()

    def upsert(self, record: FunctionRecord):
        conn = self._connect()
        try:
            conn.execute(
                """
                INSERT INTO functions (name, version, function_id, description, visibility, source_hash, created_at, extras)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(name, version) DO UPDATE SET
                    function_id=excluded.function_id,
                    description=excluded.description,
                    visibility=excluded.visibility,
                    source_hash=excluded.source_hash,
                    created_at=excluded.created_at,
                    extras=excluded.extras;
                """,
                (
                    record.name,
                    record.version,
                    record.function_id,
                    record.description,
                    record.visibility,
                    record.source_hash,
                    record.created_at,
                    json.dumps(record.extras or {}),
                ),
            )
            conn.commit()
        finally:
            conn.close()

    def resolve(self, name: str, version: Optional[str] = None) -> FunctionRecord:
        """
        Resolve a function record by name and version. If version is None or 'latest',
        returns the lexicographically greatest version string.
        """
        conn = self._connect()
        try:
            if version in (None, "latest"):
                cur = conn.execute(
                    "SELECT name, version, function_id, description, visibility, source_hash, created_at, extras "
                    "FROM functions WHERE name = ? ORDER BY version DESC LIMIT 1;",
                    (name,),
                )
            else:
                cur = conn.execute(
                    "SELECT name, version, function_id, description, visibility, source_hash, created_at, extras "
                    "FROM functions WHERE name = ? AND version = ?;",
                    (name, version),
                )
            row = cur.fetchone()
            if not row:
                raise KeyError(f"Function not found: {name}@{version or 'latest'}")
            return FunctionRecord(
                name=row[0],
                version=row[1],
                function_id=row[2],
                description=row[3],
                visibility=row[4],
                source_hash=row[5],
                created_at=row[6],
                extras=json.loads(row[7] or "{}"),
            )
        finally:
            conn.close()

    def list_functions(self) -> List[Tuple[str, str]]:
        conn = self._connect()
        try:
            cur = conn.execute("SELECT name, version FROM functions ORDER BY name, version;")
            return list(cur.fetchall())
        finally:
            conn.close()

    def list_versions(self, name: str) -> List[str]:
        conn = self._connect()
        try:
            cur = conn.execute("SELECT version FROM functions WHERE name=? ORDER BY version;", (name,))
            return [r[0] for r in cur.fetchall()]
        finally:
            conn.close()


class FunctionDeployer:
    """
    Handles function registration to funcX/Globus Compute with local version tracking.

    Security:
      - Functions are private by default. Set public=True to make them publicly discoverable
        (only if supported by your service account and SDK version).
    """

    def __init__(self, registry: Optional[FunctionRegistry] = None, client: Optional[_ComputeClient] = None):
        self.client = client or _ComputeClient()
        self.registry = registry or FunctionRegistry()

    @staticmethod
    def _source_hash(func: Callable) -> str:
        try:
            src = inspect.getsource(func).encode()
        except OSError:
            # Builtins or dynamically generated code may not have source
            src = repr(func).encode()
        return hashlib.sha256(src).hexdigest()

    def register(
        self,
        func: Callable,
        name: str,
        version: str,
        description: Optional[str] = None,
        public: bool = False,
        extras: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Register a Python function with funcX/Globus Compute and store it in the local registry.

        Args:
          func: Python callable to deploy. Must be picklable and importable on workers.
          name: Logical function name (e.g., "ml.infer" or "data.normalize").
          version: Semantic version string, e.g., "1.0.0".
          description: Optional human-readable description.
          public: If True, attempts to register as publicly discoverable (if supported).
          extras: Arbitrary metadata dict to store locally (e.g., container hints, owners).

        Returns:
          function_id (UUID string) assigned by the service.
        """
        # Create a unique function_name for registration. We include version for clarity.
        reg_name = f"{name}@{version}"
        kwargs = {"function_name": reg_name}
        if description:
            kwargs["description"] = description

        # The 'public' kwarg may not exist on older SDKs; guard with try/except
        try:
            kwargs["public"] = bool(public)
        except Exception:
            pass

        function_id = self.client.register_function(func, **kwargs)

        record = FunctionRecord(
            name=name,
            version=version,
            function_id=function_id,
            description=description,
            visibility="public" if public else "private",
            source_hash=self._source_hash(func),
            created_at=time.time(),
            extras=extras or {},
        )
        self.registry.upsert(record)
        return function_id


class FunctionRunner:
    """
    Scalable submit/get for versioned functions across one or more endpoints.

    - run(): submit a single task and wait for result
    - submit(): submit and return task_id
    - gather(): collect results for multiple task_ids
    - map(): bulk submission over an iterable of args with bounded in-flight requests
    """

    def __init__(self, registry: FunctionRegistry, client: Optional[_ComputeClient] = None):
        self.client = client or _ComputeClient()
        self.registry = registry

    def resolve_function_id(self, name: str, version: Optional[str] = None) -> str:
        return self.registry.resolve(name, version).function_id

    def submit(
        self,
        name: str,
        endpoint_id: str,
        *fargs,
        version: Optional[str] = None,
        timeout: Optional[float] = None,
        **fkwargs,
    ) -> str:
        """
        Submit a task asynchronously.

        Returns: task_id (UUID string)
        """
        func_id = self.resolve_function_id(name, version)
        # Note: client.run returns a task UUID
        task_id = self.client.run(*fargs, endpoint_id=endpoint_id, function_id=func_id, **fkwargs)
        return task_id

    def get_result(self, task_id: str, timeout: Optional[float] = None) -> Any:
        """
        Block until result is available or timeout (seconds) elapses.
        Raises:
          - TimeoutError
          - Remote exception re-raised from the function if it failed
        """
        return self.client.get_result(task_id, timeout=timeout)

    def run(
        self,
        name: str,
        endpoint_id: str,
        *fargs,
        version: Optional[str] = None,
        result_timeout: Optional[float] = None,
        **fkwargs,
    ) -> Any:
        """
        Convenience: submit and wait for result.
        """
        task_id = self.submit(name, endpoint_id, *fargs, version=version, **fkwargs)
        return self.get_result(task_id, timeout=result_timeout)

    def gather(self, task_ids: List[str], poll_interval: float = 0.5, timeout: Optional[float] = None) -> List[Any]:
        """
        Gather results for many tasks. Returns results in the same order as task_ids.
        """
        start = time.time()
        results: Dict[str, Any] = {}
        remaining = set(task_ids)
        while remaining:
            done = []
            for tid in list(remaining):
                try:
                    res = self.client.get_result(tid, timeout=0.0)
                    results[tid] = res
                    done.append(tid)
                except Exception:
                    # Not ready yet or raised remote; we cannot distinguish here without SDK introspection.
                    # We'll try again shortly. If an exception is a remote error, it will be raised when ready.
                    pass
            for tid in done:
                remaining.remove(tid)
            if remaining:
                if timeout is not None and (time.time() - start) > timeout:
                    raise TimeoutError(f"Timed out waiting for tasks: {sorted(remaining)}")
                time.sleep(poll_interval)
        return [results[tid] for tid in task_ids]

    def map(
        self,
        name: str,
        endpoint_id: str,
        args_iterable: Iterable[Tuple[Tuple[Any, ...], Dict[str, Any]]],
        version: Optional[str] = None,
        max_in_flight: int = 256,
        poll_interval: float = 0.5,
    ) -> List[Any]:
        """
        Map a versioned function across many argument sets with bounded parallelism.

        args_iterable: yields tuples of (args, kwargs) to pass to the function
        """
        in_flight: Dict[str, int] = {}
        order: List[str] = []
        results: Dict[str, Any] = {}
        args_iter = iter(args_iterable)

        def submit_next(n: int):
            submitted = 0
            for _ in range(n):
                try:
                    a, kw = next(args_iter)
                except StopIteration:
                    break
                tid = self.submit(name, endpoint_id, *a, version=version, **kw)
                in_flight[tid] = 1
                order.append(tid)
                submitted += 1
            return submitted

        # Seed initial batch
        submit_next(max_in_flight)

        while in_flight:
            # Probe results
            done_now = []
            for tid in list(in_flight.keys()):
                try:
                    res = self.client.get_result(tid, timeout=0.0)
                    results[tid] = res
                    done_now.append(tid)
                except Exception:
                    pass
            # Remove completed
            for tid in done_now:
                in_flight.pop(tid, None)
            # Backfill
            submit_next(max_in_flight - len(in_flight))
            if in_flight:
                time.sleep(poll_interval)

        return [results[tid] for tid in order]


# Example scientific functions
# ----------------------------

def normalize_array(arr, axis=None):
    """
    Example data-processing function.
    Normalizes a NumPy array along an axis: (x - mean) / std
    """
    import numpy as np
    x = np.asarray(arr)
    mu = x.mean(axis=axis, keepdims=True)
    sigma = x.std(axis=axis, keepdims=True) + 1e-12
    return ((x - mu) / sigma).tolist()


def batched_matmul(a, b):
    """
    HPC-friendly batched matrix multiply using NumPy, returns list-of-lists.
    """
    import numpy as np
    A = np.asarray(a)
    B = np.asarray(b)
    return (A @ B).tolist()


def ml_infer_sklearn(model_bytes, x_batch):
    """
    Stateless ML inference: provide a pickled scikit-learn model and feature batch.

    This avoids requiring the model file to exist on the endpoint.
    """
    import pickle
    import numpy as np
    model = pickle.loads(model_bytes)
    X = np.asarray(x_batch)
    y_pred = model.predict(X)
    # Return JSON-serializable types
    try:
        return y_pred.tolist()
    except Exception:
        return [float(v) for v in y_pred]


# Endpoint config helper (optional)
# ---------------------------------
def generate_endpoint_config_template(
    endpoint_name: str = "hpc-endpoint",
    max_workers: int = 64,
    heartbeat_period: float = 30.0,
) -> str:
    """
    Returns a minimal endpoint configuration template as a string.

    Save this to:
      ~/.globus_compute/<endpoint_name>/config.yaml  (for Globus Compute)
      or
      ~/.funcx/<endpoint_name>/config.py             (legacy funcX; format differs)

    For advanced HPC integration (SLURM/LSF/PBS, containers, conda envs),
    consult the official docs and adapt accordingly.
    """
    # This template targets Globus Compute's YAML config style.
    # It may require adjustment depending on your installed version.
    cfg = f"""
# Endpoint: {endpoint_name}
display_name: "{endpoint_name}"
engine:
  type: HighThroughputEngine
  max_workers: {max_workers}
  heartbeat_period: {heartbeat_period}
  worker_debug: false
network:
  # Adjust if behind firewalls/NATs; see docs for options
  address: null
  ports: null
logging:
  level: INFO
    """.strip()
    return cfg


# Demonstration
# -------------
if __name__ == "__main__":
    """
    End-to-end demonstration:
      - Registers three versioned scientific functions
      - Runs them on a designated endpoint
      - Performs a scalable map() task

    Before running:
      - Ensure you've installed and logged in: pip install globus-compute-sdk
      - Ensure an endpoint is installed and running; set ENDPOINT_ID below.
    """
    ENDPOINT_ID = os.environ.get("GLOBUS_COMPUTE_ENDPOINT", "").strip()  # set or export this
    if not ENDPOINT_ID:
        print("Please set environment variable GLOBUS_COMPUTE_ENDPOINT to your endpoint UUID.")
        print("Skipping demo run; functions will still be registered locally.")
    deployer = FunctionDeployer()
    registry = deployer.registry
    runner = FunctionRunner(registry)

    # Register functions with semantic versions
    f1_id = deployer.register(
        normalize_array,
        name="data.normalize",
        version="1.0.0",
        description="Normalize an array along an axis: (x - mean) / std",
        public=False,
        extras={"category": "data-processing"},
    )
    print(f"Registered data.normalize@1.0.0 -> {f1_id}")

    f2_id = deployer.register(
        batched_matmul,
        name="linalg.bmm",
        version="1.0.0",
        description="Batched matrix multiply using NumPy",
        public=False,
        extras={"category": "linalg"},
    )
    print(f"Registered linalg.bmm@1.0.0 -> {f2_id}")

    f3_id = deployer.register(
        ml_infer_sklearn,
        name="ml.infer_sklearn",
        version="0.1.0",
        description="Stateless sklearn inference from pickled model bytes",
        public=False,
        extras={"category": "ml"},
    )
    print(f"Registered ml.infer_sklearn@0.1.0 -> {f3_id}")

    if ENDPOINT_ID:
        # Single-run examples
        import numpy as np
        arr = np.random.randn(4, 3).tolist()
        norm = runner.run("data.normalize", ENDPOINT_ID, arr, version="1.0.0", result_timeout=120)
        print("Normalized array (first row):", norm[0])

        A = np.random.randn(256, 512).tolist()
        B = np.random.randn(512, 128).tolist()
        C = runner.run("linalg.bmm", ENDPOINT_ID, A, B, version="1.0.0", result_timeout=300)
        print("Matmul result shape:", len(C), "x", len(C[0]) if C else 0)

        # Map() example: many matmuls
        def gen_matmul_args(n: int, m: int, k: int, count: int):
            for _ in range(count):
                a = np.random.randn(n, m).tolist()
                b = np.random.randn(m, k).tolist()
                yield ( (a, b), {} )

        results = runner.map(
            name="linalg.bmm",
            endpoint_id=ENDPOINT_ID,
            args_iterable=gen_matmul_args(64, 64, 64, count=32),
            version="1.0.0",
            max_in_flight=32,
        )
        print("Completed batched map() of 32 matmuls; first result dims:",
              len(results[0]), "x", len(results[0][0]) if results and results[0] else 0)

        # Stateless ML inference example
        try:
            from sklearn.linear_model import LogisticRegression
        except Exception:
            print("scikit-learn not available locally; skipping ML demo.")
        else:
            import pickle
            X = np.random.randn(200, 8)
            y = (X[:, 0] + 0.5 * X[:, 1] > 0).astype(int)
            clf = LogisticRegression(max_iter=200).fit(X, y)
            model_bytes = pickle.dumps(clf)
            preds = runner.run("ml.infer_sklearn", ENDPOINT_ID, model_bytes, X[:16].tolist(), version="0.1.0")
            print("ML predictions (first 10):", preds[:10])

    else:
        print("Endpoint not set; skipped remote execution demo.")

    # Show what's in the registry
    print("Registered functions:")
    for name, ver in registry.list_functions():
        print(" -", name, "@", ver)

    # Generate a config template for convenience
    template = generate_endpoint_config_template(endpoint_name="my-hpc-endpoint", max_workers=64)
    print("\nEndpoint config template (save and adapt as needed):\n")
    print(template)
------------------------------------------------------------

PROMPT 324:
"Generate Python code that imports the dataclasses module and uses it to define a serializer class that inherits from Django REST Framework's Serializer, using the Meta-class from dataclasses to automatically generate the __init__ and __annotations__ methods for the serializer."
RESPONSE:
import dataclasses
from dataclasses import make_dataclass, field, fields, MISSING
from typing import Any, get_origin, get_args, Union
from rest_framework import serializers


class DataclassBackedSerializer(serializers.Serializer):
    # Generic serializer implementation that infers fields from dataclass fields

    def to_internal_value(self, data):
        if not isinstance(data, dict):
            raise serializers.ValidationError("Expected a JSON object.")

        validated = {}
        errors = {}

        for f in fields(self):
            name = f.name

            if name not in data:
                # Required if no default/default_factory
                if f.default is MISSING and f.default_factory is MISSING:
                    errors[name] = "This field is required."
                continue

            raw = data[name]
            try:
                validated[name] = self._coerce_type(raw, f.type)
            except (TypeError, ValueError) as exc:
                errors[name] = f"Invalid value: {exc}"

        if errors:
            raise serializers.ValidationError(errors)

        return validated

    def to_representation(self, instance):
        result = {}
        for f in fields(self):
            if isinstance(instance, dict):
                value = instance.get(f.name, None)
            else:
                value = getattr(instance, f.name, None)
            result[f.name] = value
        return result

    def create(self, validated_data):
        # Default passthrough create; customize as needed
        return validated_data

    def update(self, instance, validated_data):
        # Default naive update; customize as needed
        for k, v in validated_data.items():
            if isinstance(instance, dict):
                instance[k] = v
            else:
                setattr(instance, k, v)
        return instance

    def _coerce_type(self, value: Any, tp: Any) -> Any:
        origin = get_origin(tp)
        args = get_args(tp)

        # Handle Optional/Union
        if origin is Union:
            # Optional[T] is Union[T, NoneType]
            if value is None and type(None) in args:
                return None
            # Try to coerce into one of the union types
            last_exc = None
            for a in args:
                if a is type(None):
                    continue
                try:
                    return self._coerce_type(value, a)
                except (TypeError, ValueError) as exc:
                    last_exc = exc
            if last_exc:
                raise last_exc
            return value

        # Built-ins
        if tp in (str, int, float, bool):
            # Special-case bool to avoid bool("false") == True surprises
            if tp is bool and isinstance(value, str):
                lowered = value.strip().lower()
                if lowered in ("true", "1", "yes", "y", "t"):
                    return True
                if lowered in ("false", "0", "no", "n", "f"):
                    return False
                raise ValueError("Expected a boolean-like string")
            return tp(value)

        # Containers
        if origin is list:
            (item_type,) = args or (Any,)
            if not isinstance(value, list):
                raise TypeError("Expected a list")
            return [self._coerce_type(v, item_type) for v in value]

        if origin is dict:
            key_type, val_type = args or (Any, Any)
            if not isinstance(value, dict):
                raise TypeError("Expected a dict")
            return {
                self._coerce_type(k, key_type): self._coerce_type(v, val_type)
                for k, v in value.items()
            }

        # Fallback: return as-is
        return value


# Define the serializer class using dataclasses.make_dataclass.
# This creates a class that:
# - Inherits from DRF's Serializer (via DataclassBackedSerializer base)
# - Has auto-generated __init__ and __annotations__ based on the field spec
UserSerializer = make_dataclass(
    "UserSerializer",
    fields=[
        ("id", int),
        ("username", str),
        ("email", Union[str, None], field(default=None)),
        ("age", int, field(default=0)),
        ("is_active", bool, field(default=True)),
    ],
    bases=(DataclassBackedSerializer,),
    frozen=False,
    repr=False,
    eq=False,
)


# Example usage:
# serializer = UserSerializer(data={"id": 1, "username": "alice", "email": "a@ex.com", "age": "30"})
# serializer.is_valid(raise_exception=True)
# print(serializer.validated_data)
# print(serializer.data)  # if serializer = UserSerializer(instance=serializer.validated_data)
------------------------------------------------------------

PROMPT 325:
"Generate Python code that uses the requests library to make an API call to the Delphi COVIDcast API and retrieve the latest COVID-19 data, then parse the JSON response using the json library and extract the desired information such as case counts, deaths, and hospitalizations."
RESPONSE:
# Requires: requests (pip install requests)
import requests
import json
from typing import Tuple, Optional

BASE_URL = "https://api.delphi.cmu.edu/epidata"

def fetch_json(url: str, params: dict) -> dict:
    """GET a URL with params using requests and parse JSON using json library."""
    resp = requests.get(url, params=params, timeout=20)
    resp.raise_for_status()
    try:
        return json.loads(resp.text)
    except json.JSONDecodeError as e:
        raise RuntimeError(f"Failed to parse JSON: {e}\nResponse text: {resp.text[:500]}")

def get_latest_time_value(
    data_source: str,
    signal: str,
    time_type: str = "day",
    geo_type: str = "nation",
) -> int:
    """
    Query covidcast_meta to find the most recent time_value available for the
    given data_source/signal/time_type/geo_type combination.
    """
    meta_url = f"{BASE_URL}/covidcast_meta"
    params = {
        "data_source": data_source,
        "signal": signal,
        "time_type": time_type,
        "geo_type": geo_type,
    }
    meta = fetch_json(meta_url, params)
    if meta.get("result") != 1 or not meta.get("epidata"):
        raise RuntimeError(
            f"Meta query failed or empty. Message: {meta.get('message')}, data_source={data_source}, signal={signal}"
        )

    # There can be multiple rows; choose the maximum max_time
    max_time = max(row["max_time"] for row in meta["epidata"])
    return max_time

def get_covidcast_value(
    data_source: str,
    signal: str,
    geo_type: str,
    geo_value: str,
    time_value: int,
    time_type: str = "day",
) -> Optional[float]:
    """
    Fetch a single covidcast value for the given parameters and time.
    Returns the 'value' field (float) or None if missing.
    """
    url = f"{BASE_URL}/covidcast"
    params = {
        "data_source": data_source,
        "signal": signal,
        "time_type": time_type,
        "geo_type": geo_type,
        "time_values": time_value,
        "geo_value": geo_value,
    }
    data = fetch_json(url, params)
    if data.get("result") != 1 or not data.get("epidata"):
        # Could be missing for that geo/time; return None to indicate no data
        return None

    # Expect a single record when querying a single time_value + single geo_value
    record = data["epidata"][0]
    # 'value' may be missing or NaN; handle gracefully
    return record.get("value")

def main():
    # Define the signals we want:
    # - Case counts (7-day average of new cases): jhu-csse, confirmed_7dav_incidence_num
    # - Deaths (7-day average of new deaths): jhu-csse, deaths_7dav_incidence_num
    # - Hospital admissions (7-day average): hhs, confirmed_admissions_covid_7dav
    signals = {
        "cases": ("jhu-csse", "confirmed_7dav_incidence_num"),
        "deaths": ("jhu-csse", "deaths_7dav_incidence_num"),
        "hospitalizations": ("hhs", "confirmed_admissions_covid_7dav"),
    }

    # Choose geography. Example: national US totals.
    geo_type = "nation"
    geo_value = "us"
    time_type = "day"

    results = {}
    latest_date_used = None

    for label, (data_source, signal) in signals.items():
        try:
            latest_time = get_latest_time_value(
                data_source=data_source,
                signal=signal,
                time_type=time_type,
                geo_type=geo_type,
            )
            val = get_covidcast_value(
                data_source=data_source,
                signal=signal,
                geo_type=geo_type,
                geo_value=geo_value,
                time_value=latest_time,
                time_type=time_type,
            )
            results[label] = {
                "value": val,
                "data_source": data_source,
                "signal": signal,
                "time_value": latest_time,
                "geo_type": geo_type,
                "geo_value": geo_value,
            }
            # Track the most recent date across signals (they may differ)
            if latest_date_used is None or latest_time > latest_date_used:
                latest_date_used = latest_time
        except Exception as e:
            results[label] = {"error": str(e)}

    # Pretty-print the extracted information
    # time_value is in YYYYMMDD format as an integer.
    print("Latest COVID-19 data (by signal; dates may differ per signal):")
    for label, info in results.items():
        if "error" in info:
            print(f"- {label}: ERROR - {info['error']}")
        else:
            date_str = str(info["time_value"])
            date_fmt = f"{date_str[0:4]}-{date_str[4:6]}-{date_str[6:8]}"
            value = info["value"]
            print(
                f"- {label.capitalize()}: {value} "
                f"(source={info['data_source']}/{info['signal']}, date={date_fmt}, geo={info['geo_value']})"
            )

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 326:
"Generate Python code that imports the necessary libraries and creates a client object to interact with Google Cloud BigQuery, allowing users to list and query datasets, create and delete tables, and perform data manipulation operations such as filtering, grouping, and joining."
RESPONSE:
# pip install --upgrade google-cloud-bigquery
# Auth: set GOOGLE_APPLICATION_CREDENTIALS or use Application Default Credentials (gcloud auth application-default login)

import os
from decimal import Decimal
from typing import Iterable, Optional, Sequence

from google.api_core.exceptions import NotFound, Conflict, BadRequest
from google.cloud import bigquery


def get_bq_client(project_id: Optional[str] = None, location: Optional[str] = None) -> bigquery.Client:
    """
    Create a BigQuery client. If project_id is None, uses default project from environment/ADC.
    If location is set, it's used as the default job location (e.g., "US", "EU").
    """
    client = bigquery.Client(project=project_id, location=location)
    return client


def list_datasets(client: bigquery.Client) -> list[str]:
    """
    List dataset IDs in the current project.
    """
    datasets = [ds.dataset_id for ds in client.list_datasets()]
    return datasets


def list_tables(client: bigquery.Client, dataset_id: str) -> list[str]:
    """
    List tables in a dataset (returns fully-qualified table IDs).
    """
    dataset_ref = f"{client.project}.{dataset_id}"
    tables = [f"{t.project}.{t.dataset_id}.{t.table_id}" for t in client.list_tables(dataset_ref)]
    return tables


def ensure_dataset(client: bigquery.Client, dataset_id: str, location: str = "US") -> bigquery.Dataset:
    """
    Ensure a dataset exists; create if missing.
    """
    ds_ref = bigquery.Dataset(f"{client.project}.{dataset_id}")
    ds_ref.location = location
    try:
        return client.get_dataset(ds_ref)
    except NotFound:
        return client.create_dataset(ds_ref)


def create_table(
    client: bigquery.Client,
    table_fqid: str,
    schema: Sequence[bigquery.SchemaField],
) -> bigquery.Table:
    """
    Create a table with the provided schema. If it exists, returns the existing table.
    """
    table = bigquery.Table(table_fqid, schema=schema)
    try:
        return client.create_table(table)
    except Conflict:
        return client.get_table(table_fqid)


def delete_table(client: bigquery.Client, table_fqid: str, not_found_ok: bool = True) -> None:
    """
    Delete a table.
    """
    client.delete_table(table_fqid, not_found_ok=not_found_ok)


def insert_rows_json(client: bigquery.Client, table_fqid: str, rows: Iterable[dict]) -> None:
    """
    Insert rows using JSON streaming insert.
    """
    errors = client.insert_rows_json(table_fqid, list(rows))
    if errors:
        raise RuntimeError(f"Insert errors for {table_fqid}: {errors}")


def run_query(
    client: bigquery.Client,
    sql: str,
    parameters: Optional[Sequence[bigquery.ScalarQueryParameter]] = None,
    use_legacy_sql: bool = False,
) -> list[dict]:
    """
    Run a SQL query and return rows as dicts.
    """
    job_config = bigquery.QueryJobConfig(
        query_parameters=parameters or [],
        use_legacy_sql=use_legacy_sql,
    )
    job = client.query(sql, job_config=job_config)
    try:
        result = job.result()
    except BadRequest as e:
        # Surface query errors with context
        raise RuntimeError(f"Query failed: {e.message}\n{getattr(job, 'errors', '')}") from e
    return [dict(row) for row in result]


def demo_operations():
    # Configuration (set via env or change defaults)
    project_id = os.environ.get("GOOGLE_CLOUD_PROJECT")  # or None for default
    location = os.environ.get("BQ_LOCATION", "US")
    dataset_id = os.environ.get("BQ_DATASET", "demo_ds")
    users_table = "users"
    orders_table = "orders"

    client = get_bq_client(project_id=project_id, location=location)
    project = client.project

    # Ensure dataset exists
    ensure_dataset(client, dataset_id, location=location)

    # List datasets
    print("Datasets in project:", project)
    for ds in list_datasets(client):
        print(" -", ds)

    # Define table FQIDs
    users_fqid = f"{project}.{dataset_id}.{users_table}"
    orders_fqid = f"{project}.{dataset_id}.{orders_table}"

    # Create tables
    users_schema = [
        bigquery.SchemaField("id", "INT64", mode="REQUIRED"),
        bigquery.SchemaField("name", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("created_at", "TIMESTAMP", mode="NULLABLE"),
    ]
    orders_schema = [
        bigquery.SchemaField("order_id", "INT64", mode="REQUIRED"),
        bigquery.SchemaField("user_id", "INT64", mode="REQUIRED"),
        bigquery.SchemaField("amount", "NUMERIC", mode="REQUIRED"),
        bigquery.SchemaField("status", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("ts", "TIMESTAMP", mode="NULLABLE"),
    ]

    create_table(client, users_fqid, users_schema)
    create_table(client, orders_fqid, orders_schema)

    # List tables in dataset
    print(f"Tables in {dataset_id}:")
    for t in list_tables(client, dataset_id):
        print(" -", t)

    # Insert sample data
    # Note: NUMERIC should be provided as Decimal or string
    insert_rows_json(
        client,
        users_fqid,
        [
            {"id": 1, "name": "Alice"},
            {"id": 2, "name": "Bob"},
            {"id": 3, "name": "Charlie"},
        ],
    )
    insert_rows_json(
        client,
        orders_fqid,
        [
            {"order_id": 101, "user_id": 1, "amount": Decimal("19.99"), "status": "PAID"},
            {"order_id": 102, "user_id": 1, "amount": Decimal("5.00"), "status": "PAID"},
            {"order_id": 103, "user_id": 2, "amount": Decimal("42.50"), "status": "PENDING"},
            {"order_id": 104, "user_id": 3, "amount": Decimal("12.00"), "status": "PAID"},
        ],
    )

    # Filtering example
    print("\nFilter: users with id >= 2")
    rows = run_query(
        client,
        f"""
        SELECT id, name
        FROM `{users_fqid}`
        WHERE id >= @min_id
        ORDER BY id
        """,
        parameters=[bigquery.ScalarQueryParameter("min_id", "INT64", 2)],
    )
    for r in rows:
        print(r)

    # Grouping example
    print("\nGroup: order counts and totals by user_id")
    rows = run_query(
        client,
        f"""
        SELECT
          user_id,
          COUNT(*) AS num_orders,
          SUM(amount) AS total_amount
        FROM `{orders_fqid}`
        GROUP BY user_id
        ORDER BY total_amount DESC
        """,
    )
    for r in rows:
        print(r)

    # Join example
    print("\nJoin: users with orders over a minimum amount")
    rows = run_query(
        client,
        f"""
        SELECT
          u.name,
          o.order_id,
          o.amount,
          o.status
        FROM `{users_fqid}` AS u
        JOIN `{orders_fqid}` AS o
        ON u.id = o.user_id
        WHERE o.amount > @min_amount
        ORDER BY o.amount DESC
        """,
        parameters=[bigquery.ScalarQueryParameter("min_amount", "NUMERIC", Decimal("10"))],
    )
    for r in rows:
        print(r)

    # Optional: Delete tables (uncomment to clean up)
    # delete_table(client, users_fqid)
    # delete_table(client, orders_fqid)
    # print("Deleted tables.")


if __name__ == "__main__":
    demo_operations()
------------------------------------------------------------

PROMPT 327:
"Generate Python code that imports the necessary libraries and sets up a client for interacting with the Google Analytics Data API to retrieve data from a user's Google Analytics account, including authentication and authorization, making API requests, and parsing response data."
RESPONSE:
# Requires:
#   pip install google-analytics-data google-auth google-auth-oauthlib
#
# Before running:
#   1) In Google Cloud Console, enable the "Google Analytics Data API" for your project.
#   2) Create OAuth 2.0 Client ID (Desktop app) and download client_secret.json to this script's folder.
#   3) Make sure the Google account used in the OAuth flow has access (Viewer/Analyst or higher)
#      to the GA4 property you want to query.
#   4) Set GA4_PROPERTY_ID in your environment (just the numeric ID, e.g. 123456789),
#      or pass it as a command-line argument.

import os
import sys
import json
from typing import List, Dict

from google.analytics.data_v1 import AnalyticsDataClient
from google.analytics.data_v1.types import (
    RunReportRequest,
    DateRange,
    Dimension,
    Metric,
)
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow


SCOPES = ["https://www.googleapis.com/auth/analytics.readonly"]
CLIENT_SECRETS_FILE = "client_secret.json"  # OAuth 2.0 client (Desktop) downloaded from Cloud Console
TOKEN_FILE = "token.json"  # Stored user credentials


def get_user_credentials() -> Credentials:
    """
    Load cached user credentials from token.json if present; otherwise run the OAuth flow.
    """
    creds = None

    # Load cached credentials if available
    if os.path.exists(TOKEN_FILE):
        creds = Credentials.from_authorized_user_file(TOKEN_FILE, SCOPES)

    # If no valid credentials available, let the user sign in.
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            # Refresh expired token
            creds.refresh(Request())
        else:
            # Start local server OAuth flow to get new credentials
            flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)
            creds = flow.run_local_server(port=0)
        # Cache credentials for next time
        with open(TOKEN_FILE, "w", encoding="utf-8") as token:
            token.write(creds.to_json())

    return creds


def build_client(creds: Credentials) -> AnalyticsDataClient:
    """
    Instantiate the GA4 Analytics Data API client with the provided user credentials.
    """
    return AnalyticsDataClient(credentials=creds)


def run_sample_report(client: AnalyticsDataClient, property_id: str):
    """
    Example: request active users by country for the last 7 days.
    property_id: numeric GA4 property ID as a string (e.g., "123456789").
    """
    request = RunReportRequest(
        property=f"properties/{property_id}",
        dimensions=[Dimension(name="country")],
        metrics=[Metric(name="activeUsers")],
        date_ranges=[DateRange(start_date="7daysAgo", end_date="today")],
        limit=10000,
    )
    return client.run_report(request)


def response_to_dicts(response) -> List[Dict[str, str]]:
    """
    Convert a RunReport response to a list of dictionaries:
    - Keys are dimension and metric names.
    - Values are strings as returned by the API (convert if you need numbers).
    """
    dims = [h.name for h in response.dimension_headers]
    mets = [h.name for h in response.metric_headers]
    rows = []

    for row in response.rows:
        record = {}
        for i, dim_val in enumerate(row.dimension_values):
            record[dims[i]] = dim_val.value
        for i, met_val in enumerate(row.metric_values):
            record[mets[i]] = met_val.value
        rows.append(record)

    return rows


def main():
    # Get property ID from argv or environment
    property_id = None
    if len(sys.argv) > 1:
        property_id = sys.argv[1].strip()
    if not property_id:
        property_id = os.getenv("GA4_PROPERTY_ID")
    if not property_id:
        print("Usage: python script.py <GA4_PROPERTY_ID>\nOr set environment variable GA4_PROPERTY_ID.")
        sys.exit(1)

    # Authenticate and build client
    creds = get_user_credentials()
    client = build_client(creds)

    # Make an API request
    response = run_sample_report(client, property_id)

    # Parse and print results
    data = response_to_dicts(response)
    print(json.dumps(data, indent=2))


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 328:
"Generate Python code that imports the necessary libraries and creates an async client for testing ASGI web applications, using the `asgi` and `aiohttp` libraries to create a simple asynchronous client that can send HTTP requests to the ASGI application and verify its responses."
RESPONSE:
# Requires: aiohttp (pip install aiohttp)
# Optional: asgi (if you have a package named 'asgi' installed; not strictly required here)

import asyncio
from typing import Callable, List, Tuple, Optional

try:
    import asgi  # Optional: present for alignment with request wording
except Exception:  # noqa: BLE001
    asgi = None

import aiohttp
from aiohttp import web


def asgi_to_aiohttp_handler(asgi_app: Callable):
    async def handler(request: web.Request) -> web.StreamResponse:
        # Build an ASGI HTTP scope from aiohttp's request
        peername = request.transport.get_extra_info("peername")
        client = (peername[0], peername[1]) if isinstance(peername, tuple) else None

        sockname = request.transport.get_extra_info("sockname")
        server = (sockname[0], sockname[1]) if isinstance(sockname, tuple) else None

        http_version = f"{request.version.major}.{request.version.minor}"
        raw_query = request.rel_url.raw_query_string

        scope = {
            "type": "http",
            "asgi": {"version": "3.0", "spec_version": "2.3"},
            "http_version": http_version,
            "method": request.method,
            "scheme": request.scheme,
            "path": request.rel_url.path,
            "raw_path": None,  # optional
            "query_string": raw_query if isinstance(raw_query, bytes) else raw_query.encode("ascii"),
            "headers": list(request.raw_headers),  # already in (name: bytes, value: bytes)
            "client": client,
            "server": server,
            "root_path": "",
            "extensions": {},
        }

        body = await request.read()
        got_first = False

        async def receive():
            nonlocal got_first
            if not got_first:
                got_first = True
                return {"type": "http.request", "body": body, "more_body": False}
            # After sending request body, the next receive may be disconnect or noop
            await asyncio.sleep(0)
            return {"type": "http.disconnect"}

        status_code: Optional[int] = None
        response_headers: List[Tuple[bytes, bytes]] = []
        chunks: List[bytes] = []
        done = asyncio.Event()

        async def send(message):
            nonlocal status_code, response_headers
            msg_type = message.get("type")
            if msg_type == "http.response.start":
                status_code = int(message["status"])
                response_headers = list(message.get("headers", []))
            elif msg_type == "http.response.body":
                chunks.append(message.get("body", b""))
                if not message.get("more_body", False):
                    done.set()

        # Invoke the ASGI application
        await asgi_app(scope, receive, send)

        # Ensure we have a completed response
        if status_code is None:
            status_code = 200
        if not done.is_set():
            done.set()

        # Build the aiohttp response
        resp = web.Response(
            status=status_code,
            body=b"".join(chunks),
        )
        for name_b, value_b in response_headers:
            name = name_b.decode("latin-1")
            value = value_b.decode("latin-1")
            # Append headers (allow duplicates like Set-Cookie)
            resp.headers.add(name, value)
        return resp

    return handler


class ASGIAioHTTPTestClient:
    def __init__(self, asgi_app: Callable, host: str = "127.0.0.1", port: int = 0):
        self._asgi_app = asgi_app
        self._host = host
        self._port = port
        self._app: Optional[web.Application] = None
        self._runner: Optional[web.AppRunner] = None
        self._site: Optional[web.TCPSite] = None
        self._session: Optional[aiohttp.ClientSession] = None
        self.base_url: Optional[str] = None

    async def start(self):
        handler = asgi_to_aiohttp_handler(self._asgi_app)
        self._app = web.Application()
        self._app.router.add_route("*", "/{tail:.*}", handler)

        self._runner = web.AppRunner(self._app)
        await self._runner.setup()

        self._site = web.TCPSite(self._runner, self._host, self._port)
        await self._site.start()

        # Discover the actual bound port (if 0 was used)
        sockets = getattr(self._site, "_server", None)
        if sockets and getattr(sockets, "sockets", None):
            port = sockets.sockets[0].getsockname()[1]
        else:
            # Fallback for older aiohttp versions
            addrs = getattr(self._runner, "addresses", None)
            port = addrs[0][1] if addrs else self._port or 8000

        self.base_url = f"http://{self._host}:{port}"
        self._session = aiohttp.ClientSession()

    async def close(self):
        if self._session:
            await self._session.close()
            self._session = None
        if self._runner:
            await self._runner.cleanup()
            self._runner = None
            self._site = None
            self._app = None

    async def __aenter__(self):
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc, tb):
        await self.close()

    async def request(self, method: str, path: str, **kwargs) -> aiohttp.ClientResponse:
        assert self.base_url and self._session, "Client not started"
        url = self.base_url + path
        return await self._session.request(method, url, **kwargs)

    async def get(self, path: str, **kwargs) -> aiohttp.ClientResponse:
        return await self.request("GET", path, **kwargs)

    async def post(self, path: str, **kwargs) -> aiohttp.ClientResponse:
        return await self.request("POST", path, **kwargs)


# Example ASGI application under test
async def sample_asgi_app(scope, receive, send):
    assert scope["type"] == "http"
    path = scope["path"]
    method = scope["method"]

    if path == "/hello" and method == "GET":
        body = b"Hello, world!"
        headers = [(b"content-type", b"text/plain; charset=utf-8")]
        await send({"type": "http.response.start", "status": 200, "headers": headers})
        await send({"type": "http.response.body", "body": body, "more_body": False})
        return

    if path == "/echo" and method == "POST":
        data = b""
        while True:
            event = await receive()
            if event["type"] == "http.request":
                data += event.get("body", b"")
                if not event.get("more_body", False):
                    break
            else:
                break
        headers = [(b"content-type", b"text/plain; charset=utf-8")]
        await send({"type": "http.response.start", "status": 200, "headers": headers})
        await send({"type": "http.response.body", "body": data, "more_body": False})
        return

    await send({"type": "http.response.start", "status": 404, "headers": []})
    await send({"type": "http.response.body", "body": b"Not Found", "more_body": False})


# Demonstration of making requests and verifying responses
async def main():
    async with ASGIAioHTTPTestClient(sample_asgi_app) as client:
        # GET /hello
        resp = await client.get("/hello")
        text = await resp.text()
        assert resp.status == 200, f"Expected 200, got {resp.status}"
        assert text == "Hello, world!", f"Unexpected body: {text}"
        print("GET /hello ->", resp.status, text)

        # POST /echo
        resp2 = await client.post("/echo", data=b"ping")
        body2 = await resp2.text()
        assert resp2.status == 200
        assert body2 == "ping"
        print("POST /echo ->", resp2.status, body2)

        # 404 path
        resp3 = await client.get("/missing")
        body3 = await resp3.text()
        assert resp3.status == 404
        assert "Not Found" in body3
        print("GET /missing ->", resp3.status, body3)


if __name__ == "__main__":
    asyncio.run(main())
------------------------------------------------------------

PROMPT 329:
"Generate Python code that imports the ammonia HTML sanitization library and creates a function that takes in a string of HTML content and returns a sanitized version of the HTML, using the ammonia library's APIs to remove harmful code and scripts, and outputting the cleaned HTML."
RESPONSE:
# Requires: pip install nh3
# Note: In Python, the Ammonia HTML sanitizer is provided via the 'nh3' package.

import nh3

def sanitize_html(html: str) -> str:
    """
    Return a sanitized version of the given HTML string, removing scripts and
    other potentially harmful content using the ammonia-based 'nh3' library.
    """
    return nh3.clean(html)
------------------------------------------------------------

PROMPT 330:
"Generate Python code that uses the `requests` library to send a GET request to the Farama Foundation API to retrieve a list of all libraries maintained by the foundation, and then uses the `py notifications` library to send a notification to the user for each library on the list."
RESPONSE:
```python
"""
Fetch the list of Farama Foundation libraries and send a desktop notification for each.

Requirements:
  pip install requests notifypy

Notes:
- This uses the GitHub API for the Farama-Foundation org as the source of truth.
- Optionally set GITHUB_TOKEN in your environment to raise rate limits:
    export GITHUB_TOKEN=ghp_your_token_here
"""

import os
import sys
import time
from typing import Iterable, Dict, Any, List

import requests

try:
    from notifypy import Notify
except ImportError:
    sys.exit("Missing dependency: notifypy. Install with: pip install notifypy")


GITHUB_API_URL = "https://api.github.com/orgs/Farama-Foundation/repos?per_page=100&type=public"


def _headers() -> Dict[str, str]:
    headers = {"Accept": "application/vnd.github+json", "User-Agent": "farama-notifier/1.0"}
    token = os.getenv("GITHUB_TOKEN")
    if token:
        headers["Authorization"] = f"Bearer {token}"
    return headers


def _fetch_all_pages(url: str) -> Iterable[List[Dict[str, Any]]]:
    """Yield JSON arrays from paginated GitHub API responses."""
    while url:
        resp = requests.get(url, headers=_headers(), timeout=20)
        resp.raise_for_status()
        yield resp.json()
        # Parse pagination
        link = resp.headers.get("Link", "")
        next_url = None
        if link:
            parts = [p.strip() for p in link.split(",")]
            for p in parts:
                if 'rel="next"' in p:
                    start = p.find("<") + 1
                    end = p.find(">")
                    next_url = p[start:end]
                    break
        url = next_url


def get_farama_libraries() -> List[Dict[str, Any]]:
    """Return a list of repo dicts representing Farama-maintained libraries."""
    libraries: List[Dict[str, Any]] = []
    for page in _fetch_all_pages(GITHUB_API_URL):
        # Filter out archived repos; you can adjust this as needed
        for repo in page:
            if not isinstance(repo, dict):
                continue
            if repo.get("archived"):
                continue
            libraries.append(repo)
    return libraries


def notify_library(name: str, description: str, url: str) -> None:
    n = Notify()
    n.title = f"Farama library: {name}"
    desc = description or "No description provided."
    n.message = f"{desc}\n{url}"
    try:
        n.send()
    except Exception as e:
        # Avoid crashing if notifications fail on this system
        print(f"Notification failed for {name}: {e}", file=sys.stderr)


def main():
    try:
        repos = get_farama_libraries()
    except requests.HTTPError as e:
        sys.exit(f"HTTP error while fetching libraries: {e}")
    except requests.RequestException as e:
        sys.exit(f"Network error while fetching libraries: {e}")

    if not repos:
        print("No libraries found.")
        return

    for repo in repos:
        name = repo.get("name", "Unknown")
        desc = repo.get("description") or ""
        html_url = repo.get("html_url") or ""
        notify_library(name, desc, html_url)
        # Small delay to avoid overwhelming the notification center
        time.sleep(0.3)


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 331:
"Generate Python code that imports the necessary libraries and defines functions for converting data between columnar and row-based formats, handling particle physics data, and interacting with the Collider HEP analysis framework, allowing for not-too-alien syntax in the analysis."
RESPONSE:
# Minimal, composable utilities for collider HEP analysis
# - Columnar <-> row conversions
# - Particle helpers (Lorentz vectors, ΔR, invariant mass, matching)
# - Optional adapters for common HEP frameworks (uproot, coffea)
#
# Dependencies:
#   Required: numpy, pandas, awkward, vector
#   Optional: uproot, coffea, hist
# Install with:
#   pip install numpy pandas awkward vector uproot coffea hist

from __future__ import annotations

import math
from dataclasses import dataclass
from typing import Any, Dict, Generator, Iterable, List, Mapping, Optional, Sequence, Tuple, Union

import numpy as np
import pandas as pd
import awkward as ak
import vector

# Optional imports
try:
    import uproot  # type: ignore
except Exception:  # pragma: no cover
    uproot = None

try:
    from coffea.nanoevents import NanoEventsFactory, NanoAODSchema  # type: ignore
    _has_coffea = True
except Exception:  # pragma: no cover
    _has_coffea = False

try:
    import hist  # type: ignore
    _has_hist = True
except Exception:  # pragma: no cover
    _has_hist = False

# Register awkward support for vector
vector.register_awkward()


# -------------------------
# Columnar <-> row utilities
# -------------------------

def ak_from_pandas(df: pd.DataFrame, highlevel: bool = True) -> ak.Array:
    """
    Convert a flat pandas DataFrame to an Awkward Array. Supports NaNs as missing values.
    """
    return ak.from_pandas(df, highlevel=highlevel)


def ak_to_pandas(arr: ak.Array, flatten_columns: bool = True) -> pd.DataFrame:
    """
    Convert an Awkward Array to a pandas DataFrame.
    If flatten_columns=True, joins MultiIndex columns with '__'.
    """
    df = ak.to_dataframe(arr)
    if flatten_columns and isinstance(df.columns, pd.MultiIndex):
        df = df.copy()
        df.columns = ["__".join([str(c) for c in col if c != ""]) for col in df.columns.values]
    return df


def rows_to_ak(rows: Union[Iterable[Mapping[str, Any]], List[Mapping[str, Any]]]) -> ak.Array:
    """
    Convert a sequence of Python dicts (event records) into an Awkward Array.
    The dicts can contain nested dicts/lists for jagged collections.
    """
    return ak.Array(list(rows))


def ak_to_rows(arr: ak.Array) -> List[Any]:
    """
    Convert an Awkward Array to a nested Python list/dict structure (row-based).
    Suitable for serialization or classic event loops with Python objects.
    """
    return ak.to_list(arr)


def df_to_rows(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    Convert a pandas DataFrame to a list of dicts (row-based).
    """
    return df.to_dict(orient="records")


def rows_to_df(rows: Union[Iterable[Mapping[str, Any]], List[Mapping[str, Any]]]) -> pd.DataFrame:
    """
    Convert a list of dicts to a pandas DataFrame (best for flat schemas).
    """
    return pd.DataFrame(rows)


def to_columnar(data: Any) -> ak.Array:
    """
    Convert common inputs to columnar Awkward Array:
      - pandas.DataFrame
      - list/iterable of dict rows
      - already-awkward arrays (returned as-is)
    """
    if isinstance(data, ak.Array):
        return data
    if isinstance(data, pd.DataFrame):
        return ak_from_pandas(data)
    if isinstance(data, (list, tuple)) and (len(data) == 0 or isinstance(data[0], Mapping)):
        return rows_to_ak(data)  # list of dicts
    if hasattr(data, "__iter__"):
        data_list = list(data)
        if len(data_list) == 0 or isinstance(data_list[0], Mapping):
            return rows_to_ak(data_list)
    raise TypeError("Unsupported type for to_columnar; expected ak.Array, pandas.DataFrame, or list of dicts")


def to_rows(data: Any) -> List[Any]:
    """
    Convert common inputs to row-based Python data:
      - Awkward Array -> nested lists/dicts
      - pandas.DataFrame -> list of dicts
      - list/iterable (returned as list)
    """
    if isinstance(data, ak.Array):
        return ak_to_rows(data)
    if isinstance(data, pd.DataFrame):
        return df_to_rows(data)
    if isinstance(data, list):
        return data
    if hasattr(data, "__iter__"):
        return list(data)
    raise TypeError("Unsupported type for to_rows")


# -------------------------
# Particle helpers
# -------------------------

def ensure_p4(
    record: ak.Array,
    *,
    prefer: str = "pteta",  # "pteta" or "pxpy"
    pt: str = "pt",
    eta: str = "eta",
    phi: str = "phi",
    mass: str = "mass",
    px: str = "px",
    py: str = "py",
    pz: str = "pz",
    energy: str = "energy",
    out_field: str = "p4",
) -> ak.Array:
    """
    Ensure a Momentum 4-vector field exists on a particle record.
    Adds record[out_field] as a vector.Momentum4D awkward record.

    prefer="pteta": use (pt, eta, phi, mass) if present, otherwise try (px,py,pz,energy).
    prefer="pxpy": reverse preference.

    Returns a new awkward record with the p4 field attached.
    """
    avail = set(record.fields) if hasattr(record, "fields") else set()
    use_pteta = prefer == "pteta"
    if use_pteta and {pt, eta, phi, mass}.issubset(avail):
        p4 = ak.zip({pt: record[pt], eta: record[eta], phi: record[phi], mass: record[mass]}, with_name="Momentum4D")
    elif not use_pteta and {px, py, pz, energy}.issubset(avail):
        p4 = ak.zip({px: record[px], py: record[py], pz: record[pz], "E": record[energy]}, with_name="Momentum4D")
    elif {px, py, pz, energy}.issubset(avail):
        p4 = ak.zip({px: record[px], py: record[py], pz: record[pz], "E": record[energy]}, with_name="Momentum4D")
    elif {pt, eta, phi, mass}.issubset(avail):
        p4 = ak.zip({pt: record[pt], eta: record[eta], phi: record[phi], mass: record[mass]}, with_name="Momentum4D")
    else:
        raise KeyError("Could not find required kinematic fields to build 4-vectors")
    return ak.with_field(record, p4, out_field)


def ensure_p4_for_collections(events: ak.Array, collections: Sequence[str], out_field: str = "p4") -> ak.Array:
    """
    For each named collection in events (e.g. 'Jet', 'Muon'), attach a Momentum4D field at events[collection][out_field].
    """
    out = events
    for name in collections:
        if name not in events.fields:
            continue
        out = ak.with_field(out, ensure_p4(out[name], out_field=out_field), name)
    return out


def invariant_mass(p4s: ak.Array, axis: Optional[int] = None) -> ak.Array:
    """
    Compute invariant mass. p4s should be a vector.Momentum4D awkward record or a sum of them.
    If axis is None and p4s is jagged, returns mass per element.
    If you want event-level mass from a collection, you likely want axis=1 after summing:
      mass = ak.sum(collection.p4, axis=1).mass
    """
    if hasattr(p4s, "mass"):
        return p4s.mass
    raise TypeError("Input to invariant_mass must be a vector.Momentum4D awkward record")


def delta_r(a: ak.Array, b: ak.Array) -> ak.Array:
    """
    Compute ΔR between two collections/records with .eta and .phi (Momentum4D).
    Broadcasting follows awkward semantics.
    """
    return a.deltaR(b)


def match_by_deltaR(
    a: ak.Array,
    b: ak.Array,
    max_dr: float = 0.4,
    per_a: bool = True,
) -> Tuple[ak.Array, ak.Array, ak.Array]:
    """
    Simple nearest-neighbor ΔR matching from a to b.

    Returns:
      - matched_b_idx: indices in b matched to each element of a (or None if no match)
      - matched_dr: ΔR values for the matches (or inf if no match)
      - is_matched: boolean mask for each element of a
    """
    dr = a.deltaR(b)  # shape (len(a), len(b)) with proper broadcasting/jaggedness
    # Find minimum ΔR partner for each a
    min_dr = ak.min(dr, axis=-1, initial=np.inf)
    idx = ak.argmin(dr, axis=-1, keepdims=False)
    is_match = min_dr < max_dr
    # For elements with no match, idx can be arbitrary; mask it out
    idx = ak.where(is_match, idx, -1)
    return idx, min_dr, is_match


def n_combinations_mass(p4s: ak.Array, n: int = 2) -> ak.Array:
    """
    For each event, take combinations of 'n' elements from 'p4s' and return the invariant masses of the sums.
    Example:
      dijet_masses = n_combinations_mass(events.Jet.p4, 2)
    """
    combos = ak.combinations(p4s, n, axis=1, fields=[f"p{i}" for i in range(n)])
    summed = None
    for i in range(n):
        summed = combos[f"p{i}"] if i == 0 else (summed + combos[f"p{i}"])
    return summed.mass


# -------------------------
# "Not-too-alien" helpers
# -------------------------

class Particles:
    """
    Thin convenience wrapper for per-event particle collections with common HEP syntax.
    Expects an awkward record with fields like pt, eta, phi, mass, plus optional p4.

    Example usage:
      jets = Particles(events["Jet"])
      jets = jets.with_p4()           # ensure 4-vectors
      good = (jets.pt > 30) & (abs(jets.eta) < 2.5)
      leading = jets[ak.argmax(jets.pt, axis=1, keepdims=True)]
      dr = jets.p4.deltaR(leading.p4)
    """

    def __init__(self, record: ak.Array, p4_field: str = "p4"):
        self._rec = record
        self._p4_field = p4_field

    def with_p4(self) -> "Particles":
        return Particles(ensure_p4(self._rec, out_field=self._p4_field), p4_field=self._p4_field)

    @property
    def record(self) -> ak.Array:
        return self._rec

    @property
    def p4(self) -> ak.Array:
        if self._p4_field in self._rec.fields:
            return self._rec[self._p4_field]
        return ensure_p4(self._rec, out_field=self._p4_field)[self._p4_field]

    # common accessors (work even if p4 hasn't been added yet when fields exist)
    @property
    def pt(self) -> ak.Array:
        return self._rec.pt if "pt" in self._rec.fields else self.p4.pt

    @property
    def eta(self) -> ak.Array:
        return self._rec.eta if "eta" in self._rec.fields else self.p4.eta

    @property
    def phi(self) -> ak.Array:
        return self._rec.phi if "phi" in self._rec.fields else self.p4.phi

    @property
    def mass(self) -> ak.Array:
        return self._rec.mass if "mass" in self._rec.fields else self.p4.mass

    def __getitem__(self, item) -> "Particles":
        return Particles(self._rec[item], p4_field=self._p4_field)

    def __getattr__(self, name: str) -> Any:
        # Delegate other fields to underlying record
        if name in getattr(self._rec, "fields", []):
            return self._rec[name]
        raise AttributeError(name)

    def counts(self) -> ak.Array:
        return ak.num(self._rec, axis=1)

    def select(self, mask: ak.Array) -> "Particles":
        return Particles(self._rec[mask], p4_field=self._p4_field)

    def deltaR(self, other: "Particles") -> ak.Array:
        return self.p4.deltaR(other.p4)

    def sum_p4(self, axis: int = 1) -> ak.Array:
        return ak.sum(self.p4, axis=axis)

    def pair_masses(self) -> ak.Array:
        return n_combinations_mass(self.p4, n=2)


class Events:
    """
    Wrapper around an events awkward Array to provide attribute access to collections
    and a consistent path to conversion.
    """

    def __init__(self, events: ak.Array):
        self._ev = events

    def __getattr__(self, name: str) -> Any:
        if name in getattr(self._ev, "fields", []):
            field = self._ev[name]
            # If this looks like a particle collection (jagged records with pt/eta/phi/mass), wrap it
            if isinstance(field, ak.highlevel.Array) and ak.type(field).ndim >= 2 and isinstance(field.type.content, ak.types.RecordType):
                return Particles(field)
            return field
        raise AttributeError(name)

    @property
    def array(self) -> ak.Array:
        return self._ev

    def to_rows(self) -> List[Any]:
        return ak_to_rows(self._ev)

    def select_events(self, mask: ak.Array) -> "Events":
        return Events(self._ev[mask])


# -------------------------
# I/O and framework adapters
# -------------------------

def read_root_awkward(
    file: Union[str, Sequence[str]],
    tree: str = "Events",
    branches: Optional[Sequence[str]] = None,
    entry_stop: Optional[int] = None,
) -> ak.Array:
    """
    Read ROOT TTree into Awkward Array using uproot.
    """
    if uproot is None:
        raise RuntimeError("uproot is not installed. Please `pip install uproot`.")
    arrays = uproot.concatenate(file, treepath=tree, filter_name=branches, library="ak", entry_stop=entry_stop)
    return arrays


def read_root_coffea(
    file: Union[str, Sequence[str]],
    tree: str = "Events",
    schema=NanoAODSchema,
    metadata: Optional[Dict[str, Any]] = None,
    entry_stop: Optional[int] = None,
):
    """
    Read ROOT using coffea.nanoevents for rich NanoAOD-like access (if available).
    Returns a NanoEvents object which behaves similarly to an awkward Array.
    """
    if not _has_coffea:
        raise RuntimeError("coffea is not installed. Please `pip install coffea`.")
    factory = NanoEventsFactory.from_root(
        file,
        treepath=tree,
        schemaclass=schema,
        metadata=metadata,
        entry_stop=entry_stop,
    )
    return factory.events()


def load_events(
    file: Union[str, Sequence[str]],
    tree: str = "Events",
    prefer: str = "coffea",  # "coffea" or "awkward"
    branches: Optional[Sequence[str]] = None,
    entry_stop: Optional[int] = None,
) -> Union[ak.Array, Any]:
    """
    Load events using coffea (if available) or uproot->awkward.
    """
    if prefer == "coffea" and _has_coffea:
        return read_root_coffea(file, tree=tree, entry_stop=entry_stop)
    return read_root_awkward(file, tree=tree, branches=branches, entry_stop=entry_stop)


# -------------------------
# Simple histogram helpers
# -------------------------

def make_hist(name: str, bins: int, lo: float, hi: float, label: str = ""):
    """
    Create a 1D histogram. If 'hist' is installed, returns a hist.Hist; otherwise falls back to numpy histogram config.
    """
    if _has_hist:
        return hist.Hist.new.Reg(bins, lo, hi, name=name, label=label).Double()
    else:
        # fallback: return a tuple that can be used with np.histogram later
        return {"name": name, "bins": bins, "lo": lo, "hi": hi, "label": label, "counts": None, "edges": None}


def fill_hist(h, values: np.ndarray, weights: Optional[np.ndarray] = None):
    """
    Fill histogram object returned by make_hist.
    """
    if _has_hist and isinstance(h, hist.Hist):
        h.fill(values, weight=weights)
        return h
    else:
        counts, edges = np.histogram(values, bins=h["bins"], range=(h["lo"], h["hi"]), weights=weights)
        h["counts"], h["edges"] = (counts, edges)
        return h


# -------------------------
# Example high-level API for analyses
# -------------------------

@dataclass
class ColliderSession:
    """
    A light facade for typical collider analyses with not-too-alien syntax.
    Holds events (awkward or coffea NanoEvents), and provides a few convenience methods.
    """
    events: Any

    @classmethod
    def from_root(
        cls,
        file: Union[str, Sequence[str]],
        tree: str = "Events",
        prefer: str = "coffea",
        branches: Optional[Sequence[str]] = None,
        entry_stop: Optional[int] = None,
    ) -> "ColliderSession":
        ev = load_events(file, tree=tree, prefer=prefer, branches=branches, entry_stop=entry_stop)
        return cls(ev)

    def ak(self) -> ak.Array:
        """
        Get an awkward Array view. For coffea NanoEvents, returns the underlying awkward array snapshot.
        """
        if _has_coffea and hasattr(self.events, "layout"):
            # NanoEvents -> awkward HighLevel Array
            return self.events.behavior  # this keeps behavior, but to be safe:
        if _has_coffea and hasattr(self.events, "to_awkward"):
            return self.events.to_awkward()
        if isinstance(self.events, ak.Array):
            return self.events
        # Best effort: convert via rows
        return to_columnar(to_rows(self.events))

    def wrap(self) -> Events:
        """
        Wrap in Events helper for convenience.
        """
        if isinstance(self.events, ak.Array):
            return Events(self.events)
        if _has_coffea:
            # coffea NanoEvents behaves like awkward; wrap directly
            return Events(self.events)
        return Events(self.ak())

    def define_p4(self, collections: Sequence[str], out_field: str = "p4") -> "ColliderSession":
        """
        Attach p4 to specified collections for awkward/NanoEvents-like objects.
        """
        if isinstance(self.events, ak.Array):
            self.events = ensure_p4_for_collections(self.events, collections, out_field=out_field)
        else:
            # On NanoEvents, these often already behave as Momentum4D; if not, try to attach
            try:
                arr = self.ak()
                arr = ensure_p4_for_collections(arr, collections, out_field=out_field)
                self.events = arr
            except Exception:
                pass
        return self

    def select(self, mask: ak.Array) -> "ColliderSession":
        """
        Apply an event-level mask and return a new session.
        """
        if isinstance(self.events, ak.Array):
            return ColliderSession(self.events[mask])
        if _has_coffea:
            try:
                return ColliderSession(self.events[mask])
            except Exception:
                pass
        # Fallback: convert to awkward, mask, keep as awkward
        arr = self.ak()
        return ColliderSession(arr[mask])

    def to_rows(self) -> List[Any]:
        return to_rows(self.events)

    def to_pandas(self, flatten_columns: bool = True) -> pd.DataFrame:
        return ak_to_pandas(self.ak(), flatten_columns=flatten_columns)


# -------------------------
# Convenience selectors
# -------------------------

def select_n_objects(collection: ak.Array, n_min: int = 0, n_max: Optional[int] = None) -> ak.Array:
    """
    Return an event mask for number of objects in a jagged collection.
    """
    counts = ak.num(collection, axis=1)
    mask = counts >= n_min
    if n_max is not None:
        mask = mask & (counts <= n_max)
    return mask


def basic_jet_selection(jets: Particles, pt_min: float = 30.0, abs_eta_max: float = 2.5) -> ak.Array:
    """
    Return per-element mask for jets passing kinematic cuts.
    """
    return (jets.pt > pt_min) & (ak.abs(jets.eta) < abs_eta_max)


def event_mask_from_element_mask(mask: ak.Array) -> ak.Array:
    """
    From a per-element mask on a collection, produce an event-level mask with any passing element -> True.
    """
    return ak.any(mask, axis=1)
------------------------------------------------------------

PROMPT 332:
"Generate Python code that imports the necessary libraries and sets up a client for using Cloud TPUs, such as `import google.auth, google.cloud.tpu; create_credentials = None; client = google.cloud.tpu.TPUClient(project=PROJECT_ID, location=LOCATION, credentials=create_credentials)`."
RESPONSE:
# pip install google-cloud-tpu

import os
import google.auth
from google.cloud import tpu_v2

# Set your project and location
PROJECT_ID = os.environ.get("PROJECT_ID", "your-gcp-project-id")
LOCATION = os.environ.get("LOCATION", "us-central1")

# Get application default credentials
credentials, _ = google.auth.default(scopes=["https://www.googleapis.com/auth/cloud-platform"])

# Create a TPU client (use regional endpoint)
client = tpu_v2.TpuClient(
    credentials=credentials,
    client_options={"api_endpoint": f"{LOCATION}-tpu.googleapis.com"},
)

# Example parent resource for requests (e.g., list_nodes)
parent = f"projects/{PROJECT_ID}/locations/{LOCATION}"
------------------------------------------------------------

PROMPT 333:
"Generate Python code that imports the retrying module and creates a decorator that wraps a function, allowing it to retry the function up to 3 times with increasing time intervals between retries, using the `retrying.retry` function, and returning the result of the successful attempt."
RESPONSE:
import functools
import retrying

def retry_with_backoff(func):
    # Up to 3 retries (total attempts = 4 including the initial try)
    # Exponential backoff between retries (e.g., 1s, 2s, 4s, capped at 4s)
    retried_func = retrying.retry(
        stop_max_attempt_number=4,
        wait_exponential_multiplier=1000,  # base wait = 1 second
        wait_exponential_max=4000          # cap wait at 4 seconds
    )(func)

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        return retried_func(*args, **kwargs)

    return wrapper

# Example usage:
# @retry_with_backoff
# def flaky_operation():
#     ...
# result = flaky_operation()
------------------------------------------------------------

PROMPT 334:
"Generate Python code that uses the `requests` library to send HTTP requests to Script Launchpad's web services interfaces, allowing you to interact with the platform and perform various tasks such as creating, reading, updating, and deleting scripts, as well as managing users and groups."
RESPONSE:
#!/usr/bin/env python3
"""
Script Launchpad REST client using 'requests'.

This client provides convenience methods to interact with a hypothetical
Script Launchpad API for:
- Scripts: create, read, update, delete, execute
- Users: create, read, update, delete
- Groups: create, read, update, delete, membership management

Note: Endpoints and payload shapes are examples based on common REST conventions.
Adjust paths and fields to match your Script Launchpad instance.

Requirements:
- requests
- urllib3 (bundled with requests)
"""

from __future__ import annotations

import json
import os
import time
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Tuple, Union

import requests
from requests import Response, Session
from requests.adapters import HTTPAdapter
from urllib.parse import urljoin
from urllib3.util.retry import Retry


@dataclass
class APIErrorDetail:
    code: Optional[str] = None
    message: Optional[str] = None
    fields: Optional[Dict[str, Any]] = None


class ScriptLaunchpadAPIError(Exception):
    def __init__(
        self,
        message: str,
        status_code: Optional[int] = None,
        error_code: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None,
        response: Optional[Response] = None,
    ):
        super().__init__(message)
        self.status_code = status_code
        self.error_code = error_code
        self.details = details or {}
        self.response = response

    def __str__(self):
        base = f"{super().__str__()} (status={self.status_code}"
        if self.error_code:
            base += f", code={self.error_code}"
        return base + ")"


class ScriptLaunchpadClient:
    """
    A simple REST client for Script Launchpad.

    Authentication:
    - Bearer token (recommended): pass api_token or set SL_API_TOKEN env var
    - Basic auth: pass username and password (less secure)

    Example:
        client = ScriptLaunchpadClient(
            base_url="https://your-script-launchpad.example.com",
            api_token="YOUR_TOKEN"
        )
        scripts = client.list_scripts()
    """

    def __init__(
        self,
        base_url: str,
        api_token: Optional[str] = None,
        username: Optional[str] = None,
        password: Optional[str] = None,
        timeout: Union[float, Tuple[float, float]] = 30.0,
        verify: Union[bool, str] = True,
        user_agent: Optional[str] = None,
        retries: int = 3,
        backoff_factor: float = 0.5,
        proxies: Optional[Dict[str, str]] = None,
        extra_headers: Optional[Dict[str, str]] = None,
        api_prefix: str = "/api/v1/",
    ):
        if not base_url:
            raise ValueError("base_url is required")
        self.base_url = base_url if base_url.endswith("/") else base_url + "/"
        self.api_prefix = api_prefix.lstrip("/")
        self.timeout = timeout
        self.verify = verify

        # Session with retry/backoff
        self.session: Session = requests.Session()
        retry = Retry(
            total=retries,
            read=retries,
            connect=retries,
            status=retries,
            backoff_factor=backoff_factor,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=frozenset(["GET", "POST", "PUT", "PATCH", "DELETE", "HEAD", "OPTIONS"]),
            raise_on_status=False,
            respect_retry_after_header=True,
        )
        adapter = HTTPAdapter(max_retries=retry, pool_connections=20, pool_maxsize=50)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # Headers
        self.session.headers.update({
            "Accept": "application/json",
            "Content-Type": "application/json",
            "User-Agent": user_agent or "script-launchpad-client/1.0",
        })

        # Proxies
        if proxies:
            self.session.proxies.update(proxies)

        # Authentication
        token = api_token or os.getenv("SL_API_TOKEN")
        if token:
            self.session.headers["Authorization"] = f"Bearer {token}"
        elif username and password:
            self.session.auth = (username, password)
        else:
            # Allow unauthenticated; server may have public endpoints
            pass

        if extra_headers:
            self.session.headers.update(extra_headers)

    # --------------------------
    # Core request helpers
    # --------------------------
    def _api_url(self, path: str) -> str:
        path = path.lstrip("/")
        return urljoin(self.base_url, f"{self.api_prefix}{path}")

    def _handle_error(self, resp: Response) -> None:
        try:
            data = resp.json()
        except ValueError:
            # Non-JSON response
            data = None

        msg = None
        err_code = None
        details = None

        if isinstance(data, dict):
            # Common error shapes: {"error": "...", "message": "...", "code": "...", "details": {...}}
            msg = data.get("message") or data.get("error") or resp.text
            err_code = data.get("code") or data.get("error_code")
            details = data.get("details") or data
        else:
            msg = resp.text

        raise ScriptLaunchpadAPIError(
            message=msg or f"HTTP {resp.status_code}",
            status_code=resp.status_code,
            error_code=err_code,
            details=details,
            response=resp,
        )

    def request(
        self,
        method: str,
        path: str,
        *,
        params: Optional[Dict[str, Any]] = None,
        json_body: Optional[Dict[str, Any]] = None,
        data: Optional[Union[Dict[str, Any], str, bytes]] = None,
        files: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
        expected: Iterable[int] = (200, 201, 202, 204),
        timeout: Optional[Union[float, Tuple[float, float]]] = None,
        stream: bool = False,
    ) -> Response:
        url = self._api_url(path)
        req_headers = dict(self.session.headers)
        if headers:
            req_headers.update(headers)

        # Adjust content-type for file uploads
        if files:
            # Let requests set multipart/form-data boundary automatically
            req_headers.pop("Content-Type", None)

        resp = self.session.request(
            method=method.upper(),
            url=url,
            params=params,
            json=json_body,
            data=data,
            files=files,
            headers=req_headers,
            timeout=timeout or self.timeout,
            verify=self.verify,
            stream=stream,
        )

        if resp.status_code not in expected:
            self._handle_error(resp)

        return resp

    def get_json(self, method: str, path: str, **kwargs) -> Any:
        resp = self.request(method, path, **kwargs)
        if resp.status_code == 204:
            return None
        try:
            return resp.json()
        except ValueError:
            raise ScriptLaunchpadAPIError(
                f"Expected JSON response but got: {resp.text[:200]}",
                status_code=resp.status_code,
                response=resp,
            )

    # --------------------------
    # Health / utility
    # --------------------------
    def health_check(self) -> Dict[str, Any]:
        # Adjust to your instance health endpoint
        return self.get_json("GET", "health")

    # --------------------------
    # Scripts
    # --------------------------
    def list_scripts(
        self,
        query: Optional[str] = None,
        page: Optional[int] = None,
        per_page: Optional[int] = None,
        tags: Optional[List[str]] = None,
        owner_id: Optional[str] = None,
        folder_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        params: Dict[str, Any] = {}
        if query:
            params["q"] = query
        if page is not None:
            params["page"] = page
        if per_page is not None:
            params["per_page"] = per_page
        if tags:
            params["tags"] = ",".join(tags)
        if owner_id:
            params["owner_id"] = owner_id
        if folder_id:
            params["folder_id"] = folder_id
        return self.get_json("GET", "scripts", params=params)

    def get_script(self, script_id: Union[str, int]) -> Dict[str, Any]:
        return self.get_json("GET", f"scripts/{script_id}")

    def create_script(
        self,
        name: str,
        content: Optional[str] = None,
        language: str = "python",
        description: Optional[str] = None,
        visibility: str = "private",
        tags: Optional[List[str]] = None,
        folder_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        body: Dict[str, Any] = {
            "name": name,
            "language": language,
            "visibility": visibility,
        }
        if content is not None:
            body["content"] = content
        if description:
            body["description"] = description
        if tags:
            body["tags"] = tags
        if folder_id:
            body["folder_id"] = folder_id
        if metadata:
            body["metadata"] = metadata
        return self.get_json("POST", "scripts", json_body=body, expected=(201,))

    def create_script_from_file(
        self,
        file_path: str,
        name: Optional[str] = None,
        language: str = "python",
        description: Optional[str] = None,
        visibility: str = "private",
        tags: Optional[List[str]] = None,
        folder_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        # Some APIs support multipart upload
        files = {"file": open(file_path, "rb")}
        payload = {
            "name": name or os.path.basename(file_path),
            "language": language,
            "visibility": visibility,
        }
        if description:
            payload["description"] = description
        if tags:
            payload["tags"] = json.dumps(tags)
        if folder_id:
            payload["folder_id"] = folder_id
        if metadata:
            payload["metadata"] = json.dumps(metadata)

        try:
            return self.get_json("POST", "scripts", data=payload, files=files, expected=(201,))
        finally:
            files["file"].close()

    def update_script(
        self,
        script_id: Union[str, int],
        name: Optional[str] = None,
        content: Optional[str] = None,
        description: Optional[str] = None,
        visibility: Optional[str] = None,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        body: Dict[str, Any] = {}
        if name is not None:
            body["name"] = name
        if content is not None:
            body["content"] = content
        if description is not None:
            body["description"] = description
        if visibility is not None:
            body["visibility"] = visibility
        if tags is not None:
            body["tags"] = tags
        if metadata is not None:
            body["metadata"] = metadata
        return self.get_json("PATCH", f"scripts/{script_id}", json_body=body)

    def delete_script(self, script_id: Union[str, int]) -> None:
        self.request("DELETE", f"scripts/{script_id}", expected=(204,))

    def execute_script(
        self,
        script_id: Union[str, int],
        parameters: Optional[Dict[str, Any]] = None,
        wait: bool = True,
        timeout_seconds: Optional[int] = None,
    ) -> Dict[str, Any]:
        body: Dict[str, Any] = {"parameters": parameters or {}}
        if timeout_seconds is not None:
            body["timeout_seconds"] = timeout_seconds
        result = self.get_json("POST", f"scripts/{script_id}/execute", json_body=body, expected=(200, 202))
        # If async execution is returned (202), you could poll until complete
        if wait and isinstance(result, dict) and result.get("status") in {"queued", "running"}:
            job_id = result.get("job_id")
            if job_id:
                return self._poll_job(job_id)
        return result

    def _poll_job(self, job_id: Union[str, int], interval: float = 1.5, max_wait: float = 300.0) -> Dict[str, Any]:
        deadline = time.time() + max_wait
        while time.time() < deadline:
            job = self.get_json("GET", f"jobs/{job_id}")
            status = (job or {}).get("status")
            if status in {"succeeded", "failed", "cancelled"}:
                return job
            time.sleep(interval)
        raise ScriptLaunchpadAPIError(f"Job {job_id} did not complete within {max_wait} seconds")

    # --------------------------
    # Users
    # --------------------------
    def list_users(
        self,
        query: Optional[str] = None,
        page: Optional[int] = None,
        per_page: Optional[int] = None,
        is_active: Optional[bool] = None,
        role: Optional[str] = None,
    ) -> Dict[str, Any]:
        params: Dict[str, Any] = {}
        if query:
            params["q"] = query
        if page is not None:
            params["page"] = page
        if per_page is not None:
            params["per_page"] = per_page
        if is_active is not None:
            params["is_active"] = str(is_active).lower()
        if role:
            params["role"] = role
        return self.get_json("GET", "users", params=params)

    def get_user(self, user_id: Union[str, int]) -> Dict[str, Any]:
        return self.get_json("GET", f"users/{user_id}")

    def create_user(
        self,
        username: str,
        email: str,
        password: Optional[str] = None,
        full_name: Optional[str] = None,
        roles: Optional[List[str]] = None,
        is_active: bool = True,
    ) -> Dict[str, Any]:
        body: Dict[str, Any] = {
            "username": username,
            "email": email,
            "is_active": is_active,
        }
        if password:
            body["password"] = password
        if full_name:
            body["full_name"] = full_name
        if roles:
            body["roles"] = roles
        return self.get_json("POST", "users", json_body=body, expected=(201,))

    def update_user(
        self,
        user_id: Union[str, int],
        email: Optional[str] = None,
        full_name: Optional[str] = None,
        roles: Optional[List[str]] = None,
        is_active: Optional[bool] = None,
        password: Optional[str] = None,
    ) -> Dict[str, Any]:
        body: Dict[str, Any] = {}
        if email is not None:
            body["email"] = email
        if full_name is not None:
            body["full_name"] = full_name
        if roles is not None:
            body["roles"] = roles
        if is_active is not None:
            body["is_active"] = is_active
        if password is not None:
            body["password"] = password
        return self.get_json("PATCH", f"users/{user_id}", json_body=body)

    def delete_user(self, user_id: Union[str, int]) -> None:
        self.request("DELETE", f"users/{user_id}", expected=(204,))

    # --------------------------
    # Groups
    # --------------------------
    def list_groups(
        self,
        query: Optional[str] = None,
        page: Optional[int] = None,
        per_page: Optional[int] = None,
    ) -> Dict[str, Any]:
        params: Dict[str, Any] = {}
        if query:
            params["q"] = query
        if page is not None:
            params["page"] = page
        if per_page is not None:
            params["per_page"] = per_page
        return self.get_json("GET", "groups", params=params)

    def get_group(self, group_id: Union[str, int]) -> Dict[str, Any]:
        return self.get_json("GET", f"groups/{group_id}")

    def create_group(
        self,
        name: str,
        description: Optional[str] = None,
        visibility: str = "private",
    ) -> Dict[str, Any]:
        body: Dict[str, Any] = {
            "name": name,
            "visibility": visibility,
        }
        if description:
            body["description"] = description
        return self.get_json("POST", "groups", json_body=body, expected=(201,))

    def update_group(
        self,
        group_id: Union[str, int],
        name: Optional[str] = None,
        description: Optional[str] = None,
        visibility: Optional[str] = None,
    ) -> Dict[str, Any]:
        body: Dict[str, Any] = {}
        if name is not None:
            body["name"] = name
        if description is not None:
            body["description"] = description
        if visibility is not None:
            body["visibility"] = visibility
        return self.get_json("PATCH", f"groups/{group_id}", json_body=body)

    def delete_group(self, group_id: Union[str, int]) -> None:
        self.request("DELETE", f"groups/{group_id}", expected=(204,))

    def list_group_members(self, group_id: Union[str, int]) -> Dict[str, Any]:
        return self.get_json("GET", f"groups/{group_id}/members")

    def add_user_to_group(
        self,
        group_id: Union[str, int],
        user_id: Union[str, int],
        role: str = "member",
    ) -> Dict[str, Any]:
        body = {"user_id": user_id, "role": role}
        return self.get_json("POST", f"groups/{group_id}/members", json_body=body, expected=(201,))

    def update_group_member(
        self,
        group_id: Union[str, int],
        user_id: Union[str, int],
        role: Optional[str] = None,
    ) -> Dict[str, Any]:
        body: Dict[str, Any] = {}
        if role is not None:
            body["role"] = role
        return self.get_json("PATCH", f"groups/{group_id}/members/{user_id}", json_body=body)

    def remove_user_from_group(self, group_id: Union[str, int], user_id: Union[str, int]) -> None:
        self.request("DELETE", f"groups/{group_id}/members/{user_id}", expected=(204,))

    # --------------------------
    # Pagination helper (optional)
    # --------------------------
    def iter_paginated(
        self,
        path: str,
        params: Optional[Dict[str, Any]] = None,
        item_key: str = "items",
        page_key: str = "page",
        per_page_key: str = "per_page",
        next_key: Optional[str] = "next",
        start_page: int = 1,
        per_page: int = 100,
    ):
        """
        Iterate over all items across paginated responses.
        Supports either "next" link or page/per_page pattern.
        """
        params = dict(params or {})
        params.setdefault(per_page_key, per_page)
        params.setdefault(page_key, start_page)

        while True:
            payload = self.get_json("GET", path, params=params)
            if not isinstance(payload, dict):
                raise ScriptLaunchpadAPIError("Unexpected pagination response format")
            items = payload.get(item_key) or []
            for it in items:
                yield it

            # Prefer next link if available
            if next_key and payload.get(next_key):
                next_url = payload[next_key]
                resp = self.session.get(next_url, timeout=self.timeout, verify=self.verify)
                if resp.status_code not in (200,):
                    self._handle_error(resp)
                try:
                    payload = resp.json()
                except ValueError:
                    raise ScriptLaunchpadAPIError("Expected JSON in paginated next response")
                # Continue loop using new payload without changing params
                # Emit remaining items
                items = payload.get(item_key) or []
                for it in items:
                    yield it
                # Now check if another next exists; if not, break
                if not payload.get(next_key):
                    break
                else:
                    # Continue fetching via next link in a loop
                    while payload.get(next_key):
                        next_url = payload[next_key]
                        resp = self.session.get(next_url, timeout=self.timeout, verify=self.verify)
                        if resp.status_code not in (200,):
                            self._handle_error(resp)
                        payload = resp.json()
                        items = payload.get(item_key) or []
                        for it in items:
                            yield it
                    break
            else:
                # Page-based
                curr_page = int(payload.get(page_key, params.get(page_key, 1)))
                total_pages = payload.get("total_pages")
                if total_pages is None:
                    # Fallback: stop when fewer than requested items are returned
                    if len(items) < params.get(per_page_key, per_page):
                        break
                    params[page_key] = curr_page + 1
                else:
                    if curr_page >= int(total_pages):
                        break
                    params[page_key] = curr_page + 1


# --------------------------
# Example usage
# --------------------------
if __name__ == "__main__":
    # Configure client
    client = ScriptLaunchpadClient(
        base_url=os.getenv("SL_BASE_URL", "https://your-script-launchpad.example.com"),
        api_token=os.getenv("SL_API_TOKEN", "REPLACE_WITH_TOKEN"),
        timeout=30,
        verify=True,
    )

    # Scripts CRUD
    try:
        created = client.create_script(
            name="Hello World",
            content='print("Hello, Script Launchpad!")',
            language="python",
            description="My first script",
            visibility="private",
            tags=["example", "hello"],
        )
        print("Created script:", created)

        script_id = created.get("id")
        fetched = client.get_script(script_id)
        print("Fetched script:", fetched)

        updated = client.update_script(
            script_id,
            description="Updated description",
            tags=["example", "hello", "updated"],
        )
        print("Updated script:", updated)

        # Optionally execute
        result = client.execute_script(script_id, parameters={"name": "World"}, wait=True)
        print("Execution result:", result)

        # List scripts
        listing = client.list_scripts(query="hello", per_page=10)
        print("Scripts list:", listing)

    except ScriptLaunchpadAPIError as e:
        print("Script operation failed:", e)

    # Users CRUD
    try:
        new_user = client.create_user(
            username="jdoe",
            email="jdoe@example.com",
            full_name="John Doe",
            roles=["developer"],
            is_active=True,
        )
        print("Created user:", new_user)
        user_id = new_user.get("id")

        fetched_user = client.get_user(user_id)
        print("Fetched user:", fetched_user)

        updated_user = client.update_user(user_id, full_name="Johnny Doe", roles=["developer", "reviewer"])
        print("Updated user:", updated_user)

        # client.delete_user(user_id)
        # print("Deleted user:", user_id)

    except ScriptLaunchpadAPIError as e:
        print("User operation failed:", e)

    # Groups CRUD and membership
    try:
        group = client.create_group(name="Data Team", description="Data engineering team", visibility="private")
        print("Created group:", group)
        group_id = group.get("id")

        members = client.list_group_members(group_id)
        print("Group members:", members)

        # Add user to group (replace with actual user_id)
        if 'user_id' in locals():
            member = client.add_user_to_group(group_id, user_id, role="member")
            print("Added member:", member)

            member_updated = client.update_group_member(group_id, user_id, role="admin")
            print("Updated member role:", member_updated)

            client.remove_user_from_group(group_id, user_id)
            print("Removed member:", user_id)

        updated_group = client.update_group(group_id, description="Updated description")
        print("Updated group:", updated_group)

        # client.delete_group(group_id)
        # print("Deleted group:", group_id)

    except ScriptLaunchpadAPIError as e:
        print("Group operation failed:", e)
------------------------------------------------------------

PROMPT 335:
"Generate Python code that imports the necessary modules for profiling and optimization, creates a profiling object, starts the profiling process, collects data on CPU, GPU, and memory usage, generates reports and visualizations of the data, and provides AI-powered optimization suggestions."
RESPONSE:
# Python Profiler with CPU/GPU/Memory sampling, reports, visualizations, and AI suggestions
# Optional dependencies: psutil (required), GPUtil (GPU), matplotlib (plots), pandas (CSV/DataFrame), openai (AI suggestions)
# Install as needed:
#   pip install psutil gputil matplotlib pandas openai

import os
import io
import json
import time
import math
import queue
import atexit
import shutil
import psutil
import tracemalloc
import threading
import platform
import datetime
import statistics
import cProfile
import pstats

# Optional imports
try:
    import GPUtil  # For GPU monitoring (NVIDIA)
except Exception:
    GPUtil = None

try:
    import pandas as pd
except Exception:
    pd = None

try:
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt
except Exception:
    plt = None

# Optional OpenAI client for AI suggestions
# To enable, set environment variable OPENAI_API_KEY and install the 'openai' package
try:
    from openai import OpenAI
except Exception:
    OpenAI = None


def _ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)


def _now_iso():
    return datetime.datetime.now().isoformat(timespec="seconds")


class SystemProfiler:
    """
    Profile system and process resource usage while code executes.
    - Samples CPU (system and process), memory (system and process), optional GPU
    - Captures Python memory allocations with tracemalloc
    - Runs cProfile to gather function-level CPU profile
    - Generates CSVs and plots
    - Provides AI-powered optimization suggestions (optional)
    """

    def __init__(self, sample_interval=0.5, output_dir="profiler_output", target_pid=None, name=None, verbose=True):
        self.sample_interval = float(sample_interval)
        self.output_dir = os.path.abspath(output_dir)
        self.name = name or f"run_{int(time.time())}"
        self.verbose = verbose

        self._running = threading.Event()
        self._sampler_thread = None
        self._start_ts = None
        self._stop_ts = None
        self._samples = []  # list of dicts
        self._per_gpu_samples = []  # list of dicts (one row per sample with per-GPU info)
        self._process = psutil.Process(target_pid) if target_pid else psutil.Process(os.getpid())
        self._profiler = None  # cProfile.Profile
        self._tracemalloc_started = False
        self._tracemalloc_snapshot = None
        self._warnings = []

        # Pre-flight checks
        if GPUtil is None:
            self._warn("GPUtil not available. GPU metrics will be skipped.")
        if plt is None:
            self._warn("matplotlib not available. Plots will be skipped.")
        if pd is None:
            self._warn("pandas not available. DataFrames/CSV convenience will be limited.")
        if OpenAI is None:
            self._warn("OpenAI client not available. AI suggestions fallback to heuristic rules.")

        _ensure_dir(self.output_dir)
        atexit.register(self._safe_stop)

    def _warn(self, msg):
        self._warnings.append(msg)
        if self.verbose:
            print(f"[Profiler] Warning: {msg}")

    def _info(self, msg):
        if self.verbose:
            print(f"[Profiler] {msg}")

    def start(self):
        if self._running.is_set():
            return
        self._info(f"Starting profiling session '{self.name}'")
        self._start_ts = time.time()
        self._running.set()

        # Initialize process CPU percent baseline
        try:
            self._process.cpu_percent(interval=None)
        except Exception:
            pass

        # tracemalloc
        try:
            tracemalloc.start()
            self._tracemalloc_started = True
        except Exception as e:
            self._warn(f"Failed to start tracemalloc: {e}")

        # cProfile
        self._profiler = cProfile.Profile()
        self._profiler.enable()

        # Sampler thread
        self._sampler_thread = threading.Thread(target=self._sampler_loop, name="ProfilerSampler", daemon=True)
        self._sampler_thread.start()

    def stop(self):
        if not self._running.is_set():
            return
        self._info(f"Stopping profiling session '{self.name}'")
        self._running.clear()
        if self._sampler_thread and self._sampler_thread.is_alive():
            self._sampler_thread.join(timeout=5)
        self._stop_ts = time.time()

        # cProfile stop
        if self._profiler:
            self._profiler.disable()

        # tracemalloc snapshot
        if self._tracemalloc_started:
            try:
                self._tracemalloc_snapshot = tracemalloc.take_snapshot()
            except Exception as e:
                self._warn(f"Failed to capture tracemalloc snapshot: {e}")
            finally:
                try:
                    tracemalloc.stop()
                except Exception:
                    pass

    def _safe_stop(self):
        # In case program exits without explicit stop
        if self._running.is_set():
            try:
                self.stop()
                # Save what we can
                self.generate_reports()
            except Exception:
                pass

    def _sampler_loop(self):
        next_time = time.perf_counter()
        while self._running.is_set():
            self._sample_once()
            next_time += self.sample_interval
            sleep_for = next_time - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)
            else:
                # If we are behind schedule significantly, reset the schedule
                next_time = time.perf_counter()

    def _sample_once(self):
        ts = time.time()
        # System CPU
        try:
            cpu_percent = psutil.cpu_percent(interval=None)
        except Exception:
            cpu_percent = None

        # Per-core CPU (optional detail)
        try:
            per_core = psutil.cpu_percent(interval=None, percpu=True)
        except Exception:
            per_core = None

        # System memory
        try:
            vm = psutil.virtual_memory()
            mem_used_mb = (vm.total - vm.available) / (1024 * 1024)
            mem_total_mb = vm.total / (1024 * 1024)
            mem_percent = vm.percent
        except Exception:
            mem_used_mb = mem_total_mb = mem_percent = None

        # Process resource usage
        try:
            p_cpu = self._process.cpu_percent(interval=None)  # % of single CPU (can exceed 100 on multi-core)
        except Exception:
            p_cpu = None

        try:
            with self._process.oneshot():
                mem_info = self._process.memory_info()
                rss_mb = mem_info.rss / (1024 * 1024)
                vms_mb = mem_info.vms / (1024 * 1024)
                num_threads = self._process.num_threads()
                io_counters = self._process.io_counters() if hasattr(self._process, "io_counters") else None
                read_mb = (io_counters.read_bytes / (1024 * 1024)) if io_counters else None
                write_mb = (io_counters.write_bytes / (1024 * 1024)) if io_counters else None
        except Exception:
            rss_mb = vms_mb = num_threads = read_mb = write_mb = None

        # GPU metrics
        gpu_util_avg = None
        gpu_mem_used_total_mb = None
        gpu_mem_total_total_mb = None
        per_gpu_row = None

        if GPUtil is not None:
            try:
                gpus = GPUtil.getGPUs()
                if gpus:
                    utils = []
                    mem_used = []
                    mem_total = []
                    per_gpu_row = {"timestamp": ts}
                    for idx, g in enumerate(gpus):
                        # GPUtil util is fraction [0..1], memoryUsed/Total in MB
                        utils.append(g.load * 100.0)
                        mem_used.append(g.memoryUsed)
                        mem_total.append(g.memoryTotal)
                        per_gpu_row[f"gpu{idx}_util_percent"] = g.load * 100.0
                        per_gpu_row[f"gpu{idx}_mem_used_mb"] = g.memoryUsed
                        per_gpu_row[f"gpu{idx}_mem_total_mb"] = g.memoryTotal
                        per_gpu_row[f"gpu{idx}_name"] = g.name
                    gpu_util_avg = statistics.mean(utils) if utils else None
                    gpu_mem_used_total_mb = sum(mem_used) if mem_used else None
                    gpu_mem_total_total_mb = sum(mem_total) if mem_total else None
            except Exception:
                pass

        row = {
            "timestamp": ts,
            "cpu_system_percent": cpu_percent,
            "cpu_per_core": per_core,
            "mem_system_used_mb": mem_used_mb,
            "mem_system_total_mb": mem_total_mb,
            "mem_system_percent": mem_percent,
            "proc_cpu_percent": p_cpu,
            "proc_rss_mb": rss_mb,
            "proc_vms_mb": vms_mb,
            "proc_num_threads": num_threads,
            "proc_read_mb": read_mb,
            "proc_write_mb": write_mb,
            "gpu_util_avg_percent": gpu_util_avg,
            "gpu_mem_used_total_mb": gpu_mem_used_total_mb,
            "gpu_mem_total_total_mb": gpu_mem_total_total_mb,
        }
        self._samples.append(row)
        if per_gpu_row:
            self._per_gpu_samples.append(per_gpu_row)

    def profile_callable(self, func, *args, **kwargs):
        """
        Profile a callable with system sampling and cProfile enabled.
        """
        self.start()
        try:
            return func(*args, **kwargs)
        finally:
            self.stop()

    def session(self):
        """
        Context manager to profile an arbitrary code block.

        Usage:
            with profiler.session():
                # your code
        """
        class _Session:
            def __init__(self, outer):
                self.outer = outer
            def __enter__(self):
                self.outer.start()
                return self.outer
            def __exit__(self, exc_type, exc, tb):
                self.outer.stop()
                # Do not suppress exceptions
                return False
        return _Session(self)

    def _save_json(self, path, data):
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)

    def _save_text(self, path, text):
        with open(path, "w", encoding="utf-8") as f:
            f.write(text)

    def _to_dataframe(self, rows):
        if pd is not None:
            return pd.DataFrame(rows)
        else:
            return rows  # Fallback: return raw list

    def _save_csv(self, path, rows):
        if pd is not None:
            df = pd.DataFrame(rows)
            df.to_csv(path, index=False)
        else:
            # Minimal CSV save
            if not rows:
                return
            cols = sorted(rows[0].keys())
            with open(path, "w", encoding="utf-8") as f:
                f.write(",".join(cols) + "\n")
                for r in rows:
                    vals = []
                    for c in cols:
                        v = r.get(c, "")
                        if isinstance(v, list):
                            v = ";".join(str(x) for x in v)
                        vals.append(str(v) if v is not None else "")
                    f.write(",".join(vals) + "\n")

    def _system_info(self):
        info = {
            "timestamp": _now_iso(),
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "machine": platform.machine(),
            "processor": platform.processor(),
            "cpu_count_logical": psutil.cpu_count(logical=True),
            "cpu_count_physical": psutil.cpu_count(logical=False),
            "memory_total_mb": psutil.virtual_memory().total / (1024 * 1024),
            "pid": self._process.pid,
            "process_name": self._process.name(),
            "warnings": self._warnings,
        }
        # GPU info snapshot
        if GPUtil is not None:
            try:
                gpus = GPUtil.getGPUs()
                info["gpus"] = [
                    {
                        "id": g.id,
                        "name": g.name,
                        "memory_total_mb": g.memoryTotal,
                        "uuid": getattr(g, "uuid", None),
                        "driver": getattr(g, "driver", None),
                    } for g in gpus
                ]
            except Exception:
                info["gpus"] = []
        else:
            info["gpus"] = []
        return info

    def _cprofile_reports(self, out_dir):
        paths = {}
        if not self._profiler:
            return paths
        # Text reports
        s = io.StringIO()
        ps = pstats.Stats(self._profiler, stream=s).strip_dirs()
        ps.sort_stats("cumulative")
        ps.print_stats(50)
        cum_path = os.path.join(out_dir, "cprofile_top_cumulative.txt")
        self._save_text(cum_path, s.getvalue())
        paths["cprofile_top_cumulative"] = cum_path

        s = io.StringIO()
        ps = pstats.Stats(self._profiler, stream=s).strip_dirs()
        ps.sort_stats("tottime")
        ps.print_stats(50)
        tot_path = os.path.join(out_dir, "cprofile_top_tottime.txt")
        self._save_text(tot_path, s.getvalue())
        paths["cprofile_top_tottime"] = tot_path

        # Raw stats file
        raw_path = os.path.join(out_dir, "cprofile_stats.prof")
        try:
            self._profiler.dump_stats(raw_path)
            paths["cprofile_raw"] = raw_path
        except Exception as e:
            self._warn(f"Failed to dump cProfile stats: {e}")

        return paths

    def _tracemalloc_report(self, out_dir, limit=50):
        paths = {}
        if not self._tracemalloc_snapshot:
            return paths

        try:
            top_stats = self._tracemalloc_snapshot.statistics("lineno")
            s = io.StringIO()
            s.write("Top memory allocations by line (lineno):\n")
            for idx, stat in enumerate(top_stats[:limit], start=1):
                s.write(f"#{idx}: {stat}\n")
            s.write("\n")
            s.write(f"Other {len(top_stats) - limit} lines\n" if len(top_stats) > limit else "")
            path1 = os.path.join(out_dir, "tracemalloc_top_by_lineno.txt")
            self._save_text(path1, s.getvalue())
            paths["tracemalloc_top_by_lineno"] = path1
        except Exception as e:
            self._warn(f"Failed to create tracemalloc lineno report: {e}")

        try:
            top_stats = self._tracemalloc_snapshot.statistics("traceback")
            s = io.StringIO()
            s.write("Top memory allocations by traceback:\n")
            for idx, stat in enumerate(top_stats[:10], start=1):
                s.write(f"#{idx}: {stat.size / (1024*1024):.2f} MB\n")
                for line in stat.traceback.format():
                    s.write(line)
                s.write("\n")
            path2 = os.path.join(out_dir, "tracemalloc_top_by_traceback.txt")
            self._save_text(path2, s.getvalue())
            paths["tracemalloc_top_by_traceback"] = path2
        except Exception as e:
            self._warn(f"Failed to create tracemalloc traceback report: {e}")

        return paths

    def _plots(self, out_dir):
        paths = {}
        if plt is None or not self._samples:
            return paths

        # Common time axis
        ts = [r["timestamp"] - self._start_ts for r in self._samples]

        # CPU and Memory plot
        try:
            fig, ax1 = plt.subplots(figsize=(10, 5))
            cpu = [r["cpu_system_percent"] for r in self._samples]
            p_cpu = [r["proc_cpu_percent"] for r in self._samples]
            ax1.plot(ts, cpu, label="CPU (system) %", color="tab:blue")
            ax1.plot(ts, p_cpu, label="CPU (process) %", color="tab:cyan", alpha=0.7)
            ax1.set_xlabel("Time (s)")
            ax1.set_ylabel("CPU %")
            ax1.set_ylim(0, max(100, math.ceil(max([x for x in cpu + p_cpu if x is not None] + [0])) + 10))

            ax2 = ax1.twinx()
            rss = [r["proc_rss_mb"] for r in self._samples]
            ax2.plot(ts, rss, label="RSS (process) MB", color="tab:red")
            ax2.set_ylabel("Process RSS (MB)")

            lines, labels = ax1.get_legend_handles_labels()
            lines2, labels2 = ax2.get_legend_handles_labels()
            ax1.legend(lines + lines2, labels + labels2, loc="upper left")
            ax1.grid(True, alpha=0.3)

            path = os.path.join(out_dir, "cpu_mem.png")
            fig.tight_layout()
            fig.savefig(path, dpi=150)
            plt.close(fig)
            paths["cpu_mem_plot"] = path
        except Exception as e:
            self._warn(f"Failed to create CPU/Mem plot: {e}")

        # GPU plot
        try:
            if any(r.get("gpu_util_avg_percent") is not None for r in self._samples):
                fig, ax = plt.subplots(figsize=(10, 5))
                gpu_util = [r["gpu_util_avg_percent"] for r in self._samples]
                ax.plot(ts, gpu_util, label="GPU util avg %", color="tab:green")
                ax.set_xlabel("Time (s)")
                ax.set_ylabel("GPU Utilization (%)")
                ax.set_ylim(0, 100)
                ax.grid(True, alpha=0.3)
                ax.legend()
                path = os.path.join(out_dir, "gpu_util.png")
                fig.tight_layout()
                fig.savefig(path, dpi=150)
                plt.close(fig)
                paths["gpu_util_plot"] = path
        except Exception as e:
            self._warn(f"Failed to create GPU plot: {e}")

        return paths

    def _summaries(self):
        if not self._samples:
            return {}

        def col(name):
            return [r[name] for r in self._samples if r.get(name) is not None]

        summary = {
            "duration_sec": (self._stop_ts or time.time()) - (self._start_ts or time.time()),
            "num_samples": len(self._samples),
        }

        for key in ["cpu_system_percent", "proc_cpu_percent", "proc_rss_mb", "mem_system_percent",
                    "gpu_util_avg_percent", "gpu_mem_used_total_mb"]:
            arr = col(key)
            if arr:
                summary[key] = {
                    "avg": float(statistics.fmean(arr)),
                    "p95": float(self._percentile(arr, 95)),
                    "max": float(max(arr)),
                }
        return summary

    @staticmethod
    def _percentile(arr, p):
        if not arr:
            return None
        if pd is not None:
            return float(pd.Series(arr).quantile(p / 100.0))
        arr_sorted = sorted(arr)
        k = (len(arr_sorted) - 1) * (p / 100.0)
        f = math.floor(k)
        c = math.ceil(k)
        if f == c:
            return arr_sorted[int(k)]
        d0 = arr_sorted[f] * (c - k)
        d1 = arr_sorted[c] * (k - f)
        return d0 + d1

    def generate_reports(self):
        self._info(f"Generating reports in {self.output_dir}")
        run_dir = os.path.join(self.output_dir, self.name)
        _ensure_dir(run_dir)

        # Save raw samples
        samples_path = os.path.join(run_dir, "samples.csv")
        self._save_csv(samples_path, self._samples)

        if self._per_gpu_samples:
            gpu_samples_path = os.path.join(run_dir, "per_gpu_samples.csv")
            self._save_csv(gpu_samples_path, self._per_gpu_samples)

        # System info
        sys_info = self._system_info()
        self._save_json(os.path.join(run_dir, "system_info.json"), sys_info)

        # Summaries
        summary = self._summaries()
        self._save_json(os.path.join(run_dir, "summary.json"), summary)

        # cProfile reports
        cprof_paths = self._cprofile_reports(run_dir)

        # tracemalloc reports
        tm_paths = self._tracemalloc_report(run_dir)

        # Plots
        plot_paths = self._plots(run_dir)

        index = {
            "samples_csv": samples_path,
            "per_gpu_samples_csv": os.path.join(run_dir, "per_gpu_samples.csv") if self._per_gpu_samples else None,
            "system_info_json": os.path.join(run_dir, "system_info.json"),
            "summary_json": os.path.join(run_dir, "summary.json"),
            "cprofile": cprof_paths,
            "tracemalloc": tm_paths,
            "plots": plot_paths,
        }
        self._save_json(os.path.join(run_dir, "index.json"), index)
        self._info("Reports generated.")
        return index

    def ai_optimization_suggestions(self, model="gpt-4o-mini", temperature=0.2, max_tokens=800):
        """
        Provide AI-powered suggestions using OpenAI if available and API key is set.
        Falls back to heuristic rule-based suggestions otherwise.
        """
        summary = self._summaries()
        cprof_text = ""
        cprof_stream = io.StringIO()
        if self._profiler:
            ps = pstats.Stats(self._profiler, stream=cprof_stream).strip_dirs()
            ps.sort_stats("cumulative")
            ps.print_stats(30)
            cprof_text = cprof_stream.getvalue()

        gpu_present = any("gpus" in self._system_info() and self._system_info()["gpus"])

        prompt = self._build_ai_prompt(summary, cprof_text, gpu_present)

        # Try OpenAI if available
        if OpenAI is not None and os.environ.get("OPENAI_API_KEY"):
            try:
                client = OpenAI()
                resp = client.chat.completions.create(
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    messages=[
                        {"role": "system", "content": "You are a senior performance engineer. Provide precise, actionable optimization advice."},
                        {"role": "user", "content": prompt},
                    ],
                )
                text = resp.choices[0].message.content.strip()
                return text
            except Exception as e:
                self._warn(f"OpenAI suggestion failed: {e}")

        # Fallback: heuristic suggestions
        return self._heuristic_suggestions(summary, gpu_present, cprof_text)

    def _build_ai_prompt(self, summary, cprof_text, gpu_present):
        lines = []
        lines.append("Analyze the performance profile and provide concrete optimization suggestions.")
        lines.append("Return a concise, prioritized bullet list, grouped by CPU, Memory, GPU, I/O, and Code-level tips.")
        lines.append("")
        lines.append("Summary metrics (JSON):")
        lines.append(json.dumps(summary, indent=2))
        lines.append("")
        lines.append("Top cProfile (cumulative):")
        lines.append(cprof_text[:6000])  # avoid overlong prompt
        lines.append("")
        lines.append(f"GPU present: {gpu_present}")
        return "\n".join(lines)

    def _heuristic_suggestions(self, summary, gpu_present, cprof_text):
        sugg = []
        cpu = summary.get("cpu_system_percent", {})
        pcpu = summary.get("proc_cpu_percent", {})
        rss = summary.get("proc_rss_mb", {})
        mem_pct = summary.get("mem_system_percent", {})
        gpu_util = summary.get("gpu_util_avg_percent", {})
        duration = summary.get("duration_sec", 0.0)

        # CPU
        cpu_high = (pcpu.get("avg", 0) > 200) or (cpu.get("avg", 0) > 70)
        cpu_spiky = (pcpu.get("max", 0) > (pcpu.get("avg", 0) + 100))
        if cpu_high:
            sugg.append("- CPU: High CPU usage detected. Consider algorithmic optimizations, vectorization (NumPy), reducing Python loops, and profiling hot functions with cProfile/line_profiler.")
            sugg.append("- CPU: Use multiprocessing or joblib to parallelize CPU-bound work; for numeric Python, try Numba or Cython.")
        if cpu_spiky:
            sugg.append("- CPU: Spiky CPU usage; check for periodic tasks, synchronization waits, or batch-oriented work causing bursts.")

        # Memory
        if rss.get("max", 0) > 1024:
            sugg.append("- Memory: High process RSS; reduce peak memory with batching/chunking, in-place operations, and by deleting or reusing large intermediates.")
        if mem_pct.get("avg", 0) > 80:
            sugg.append("- Memory: System memory pressure is high; consider using memory-mapped files, generators, or streaming I/O.")

        # GPU
        if gpu_present:
            if gpu_util.get("avg", 0) < 30:
                sugg.append("- GPU: Low GPU utilization; likely CPU/dataloader bottleneck or small batch sizes. Increase batch size, overlap transfers (pinned memory), and prefetch data.")
                sugg.append("- GPU: Ensure kernels are fused or use vendor libraries (cuBLAS/cuDNN). Check for frequent host-device syncs.")
            if gpu_util.get("max", 0) > 95 and rss.get("avg", 0) > 0:
                sugg.append("- GPU: Near-peak GPU utilization observed; verify memory bandwidth vs compute bottlenecks with a profiler (Nsight Systems/Compute).")
        else:
            sugg.append("- GPU: No GPU detected by GPUtil. If applicable, ensure NVIDIA drivers/CUDA are installed and accessible.")

        # I/O
        # We have limited io counters; suggest general advice
        sugg.append("- I/O: If I/O is significant, use asynchronous I/O, caching, faster storage (NVMe), and avoid small-file thrashing.")

        # Code-level hints from cProfile
        if cprof_text:
            sugg.append("- Code: Based on cProfile, focus on the top cumulative time functions. Consider memoization (functools.lru_cache) and reducing repeated computations.")

        # General
        if duration and duration > 10 and len(sugg) < 2:
            sugg.append("- General: For longer runs, consider continuous sampling profilers (py-spy, scalene) for low overhead insights.")

        return "AI suggestions (heuristic):\n" + "\n".join(sugg)


# Example usage demonstrating end-to-end profiling
if __name__ == "__main__":
    import random

    def sample_workload(seconds=5):
        # A synthetic workload mixing CPU and memory activity; optional GPU if torch available
        start = time.time()

        try:
            import numpy as np
            arr = np.random.rand(2000, 2000)
        except Exception:
            arr = None

        # Optional torch GPU workload
        used_torch = False
        try:
            import torch
            if torch.cuda.is_available():
                used_torch = True
                x = torch.randn(2048, 2048, device="cuda")
                for _ in range(50):
                    y = x @ x  # GPU matmul
                    torch.cuda.synchronize()
        except Exception:
            pass

        # CPU crunch
        def cpu_task(n=2000000):
            s = 0
            for i in range(n):
                s += (i % 7) * (i % 11)
            return s

        res = 0
        while time.time() - start < seconds:
            res ^= cpu_task(200000)  # smaller chunk to allow sampling
            # Memory bloat and release
            tmp = [bytearray(2_000_000) for _ in range(3)]
            del tmp
            time.sleep(0.05)
            if arr is not None:
                # Some NumPy compute
                _ = arr.T @ (arr[:,:100])

        return res, used_torch

    profiler = SystemProfiler(sample_interval=0.5, output_dir="profiler_output", name="demo_run", verbose=True)

    # Option 1: profile a callable
    result = profiler.profile_callable(sample_workload, 6)

    # Option 2: profile an arbitrary block
    # with profiler.session():
    #     result = sample_workload(6)

    index = profiler.generate_reports()

    print("Reports written to:", os.path.join(profiler.output_dir, profiler.name))
    print("Index:", json.dumps(index, indent=2))

    # AI-powered suggestions (uses OpenAI if available, otherwise heuristic)
    suggestions = profiler.ai_optimization_suggestions()
    suggestions_path = os.path.join(profiler.output_dir, profiler.name, "ai_suggestions.txt")
    with open(suggestions_path, "w", encoding="utf-8") as f:
        f.write(suggestions)
    print("Optimization suggestions saved to:", suggestions_path)
    print(suggestions)
------------------------------------------------------------

PROMPT 336:
"Generate Python code that defines a class called `MyView` that inherits from Flask's `RenderTemplateMixin`, uses `Flask` as its template engine, and has a `render()` method that takes a template name and dictionary of context data as arguments, rendering the template with the given context."
RESPONSE:
from flask import Flask, render_template

app = Flask(__name__)

# Flask does not provide a RenderTemplateMixin; this is a lightweight stand-in.
class RenderTemplateMixin:
    pass

class MyView(RenderTemplateMixin):
    def render(self, template_name: str, context: dict):
        return render_template(template_name, **context)
------------------------------------------------------------

PROMPT 337:
"Generate Python code that creates a developers tool for datadog checks by defining a class that inherits from datadogs check base class, overrides the necessary methods and defines a custom check function that utilizes datadogs API to retrieve metrics and display them in an interactive dashboard."
RESPONSE:
# Developer tool for Datadog checks:
# - Defines an AgentCheck-derived class (works inside Datadog Agent).
# - Provides a dev-mode dashboard (Streamlit) that uses Datadog's API to fetch and plot metrics.
#
# Requirements:
#   pip install datadog-api-client streamlit plotly
#
# Environment variables:
#   DATADOG_API_KEY, DATADOG_APP_KEY, DATADOG_SITE (e.g., datadoghq.com, datadoghq.eu)
#
# Run dashboard (interactive):
#   streamlit run this_file.py
#
# Use as a standalone CLI fetcher:
#   python this_file.py --query "avg:system.cpu.user{*} by {host}" --minutes 60
#
# Use within the Datadog Agent:
#   - Package as a custom check and configure with instances:
#     instances:
#       - queries:
#           - "avg:system.cpu.user{*} by {host}"
#         minutes: 15
#         submit_as_gauges: true

import os
import sys
import time
import argparse
import datetime
from typing import List, Dict, Any, Optional, Tuple

# Optional dependencies used in dashboard/plotting mode
try:
    import streamlit as st
    import plotly.graph_objects as go
except Exception:
    st = None
    go = None

# Datadog API client
try:
    from datadog_api_client import Configuration, ApiClient
    from datadog_api_client.v1.api.metrics_api import MetricsApi
except Exception:
    Configuration = None
    ApiClient = None
    MetricsApi = None

# Try importing AgentCheck; if not available, provide a dev stub
try:
    from datadog_checks.base import AgentCheck
except Exception:
    class _Log:
        def info(self, msg, *args, **kwargs):
            print("INFO:", msg % args if args else msg)

        def warning(self, msg, *args, **kwargs):
            print("WARN:", msg % args if args else msg)

        def error(self, msg, *args, **kwargs):
            print("ERROR:", msg % args if args else msg)

        def debug(self, msg, *args, **kwargs):
            print("DEBUG:", msg % args if args else msg)

    class AgentCheck:
        def __init__(self, name, init_config, instances):
            self.log = _Log()

        def gauge(self, metric: str, value: float, tags: Optional[List[str]] = None):
            print(f"[GAUGE] {metric}={value} tags={tags or []}")


class DatadogMetricsClient:
    """
    Thin wrapper around Datadog Metrics API v1.
    Reads credentials from env unless explicitly provided.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        app_key: Optional[str] = None,
        site: Optional[str] = None,
        timeout_s: int = 30,
    ):
        if Configuration is None:
            raise RuntimeError("datadog-api-client is not installed. pip install datadog-api-client")

        self.api_key = api_key or os.getenv("DATADOG_API_KEY")
        self.app_key = app_key or os.getenv("DATADOG_APP_KEY")
        self.site = site or os.getenv("DATADOG_SITE", "datadoghq.com")

        if not self.api_key or not self.app_key:
            raise ValueError("Missing DATADOG_API_KEY or DATADOG_APP_KEY environment variables")

        self.config = Configuration()
        # Configure site (e.g., datadoghq.com, datadoghq.eu, us3.datadoghq.com, us5.datadoghq.com, ap1.datadoghq.com)
        self.config.server_variables["site"] = self.site
        # API keys
        self.config.api_key["apiKeyAuth"] = self.api_key
        self.config.api_key["appKeyAuth"] = self.app_key
        # Timeouts
        self.config.timeout = timeout_s

    def query_timeseries(
        self,
        query: str,
        start: int,
        end: int,
    ) -> Dict[str, Any]:
        """
        Query metrics timeseries via v1 API.
        Returns raw API response.
        """
        with ApiClient(self.config) as api_client:
            api_instance = MetricsApi(api_client)
            return api_instance.query_metrics(_from=start, to=end, query=query)


def _now_epoch() -> int:
    return int(time.time())


def _parse_time_window(
    instance: Dict[str, Any],
    default_minutes: int = 15,
) -> Tuple[int, int]:
    """
    Parse time window from instance config:
      - minutes: N
      - or: from_seconds, to_seconds (unix epochs)
    """
    now = _now_epoch()
    if "from_seconds" in instance and "to_seconds" in instance:
        return int(instance["from_seconds"]), int(instance["to_seconds"])
    minutes = int(instance.get("minutes", default_minutes))
    start = now - minutes * 60
    end = now
    return start, end


def _series_to_lines(series: Dict[str, Any]) -> Tuple[str, List[Tuple[int, float]], List[str]]:
    """
    Convert a series element to a plottable line and tags list.
    Returns (label, [(ts, val), ...], tags)
    """
    metric_name = series.get("metric", "unknown.metric")
    # Scope often resembles "host:foo env:bar"; tag_set is a list if present
    tags = series.get("tag_set") or []
    scope = series.get("scope") or ""
    if scope and not tags:
        tags = [t.strip() for t in scope.split() if t.strip()]

    label = metric_name
    if tags:
        label = f"{metric_name} | {' '.join(tags)}"

    pointlist = series.get("pointlist") or []
    # Filter out None values
    points = [(int(ts / 1000), float(val)) for ts, val in pointlist if val is not None]
    return label, points, tags


class DevDatadogCheck(AgentCheck):
    """
    A developer-friendly Datadog Agent check that can also run standalone.

    Instance configuration (examples):
      - queries: list of metric queries (v1 query syntax)
      - minutes: lookback window (int)
      - submit_as_gauges: if True, submit the most recent point of each series to the Agent
      - api_key, app_key, site: optionally override env vars for API auth in dev mode
    """

    def __init__(self, name: str, init_config: Dict[str, Any], instances: List[Dict[str, Any]]):
        super().__init__(name, init_config, instances)
        self.name = name

    def check(self, instance: Dict[str, Any]) -> None:
        queries: List[str] = instance.get("queries", [])
        if not queries:
            self.log.warning("No 'queries' provided in instance config.")
            return

        start, end = _parse_time_window(instance)
        submit_as_gauges = bool(instance.get("submit_as_gauges", True))

        # Initialize API client for dev mode or when explicitly requested
        api_key = instance.get("api_key")
        app_key = instance.get("app_key")
        site = instance.get("site")
        try:
            client = DatadogMetricsClient(api_key=api_key, app_key=app_key, site=site)
        except Exception as e:
            self.log.error("Failed to initialize Datadog API client: %s", e)
            return

        for q in queries:
            try:
                resp = client.query_timeseries(q, start, end)
            except Exception as e:
                self.log.error("Query failed for '%s': %s", q, e)
                continue

            series_list = (resp or {}).get("series") or []
            if not series_list:
                self.log.info("No series returned for query: %s", q)
                continue

            for s in series_list:
                label, points, tags = _series_to_lines(s)
                if not points:
                    continue
                # Submit the most recent point as a gauge (Agent behavior)
                if submit_as_gauges:
                    metric_name = s.get("metric", "custom.dev.metric")
                    latest_ts, latest_val = points[-1]
                    try:
                        self.gauge(metric_name, latest_val, tags=tags)
                    except Exception as e:
                        self.log.error("Failed to submit gauge for %s: %s", metric_name, e)

                # Log summary for dev visibility
                self.log.debug("Fetched %d points for %s", len(points), label)

    # Optional helper: fetch data for the dashboard without Agent involvement
    def fetch_for_dashboard(
        self,
        queries: List[str],
        minutes: int = 15,
        api_key: Optional[str] = None,
        app_key: Optional[str] = None,
        site: Optional[str] = None,
    ) -> Dict[str, List[Tuple[str, List[Tuple[int, float]], List[str]]]]:
        start, end = _parse_time_window({"minutes": minutes})
        client = DatadogMetricsClient(api_key=api_key, app_key=app_key, site=site)
        result: Dict[str, List[Tuple[str, List[Tuple[int, float]], List[str]]]] = {}
        for q in queries:
            resp = client.query_timeseries(q, start, end)
            lines = []
            for s in (resp or {}).get("series") or []:
                lines.append(_series_to_lines(s))
            result[q] = lines
        return result


def _dashboard_app():
    if st is None or go is None:
        raise RuntimeError("Dashboard dependencies missing. Install: pip install streamlit plotly")

    st.set_page_config(page_title="Datadog Metrics Dev Dashboard", layout="wide")
    st.title("Datadog Metrics Developer Dashboard")

    # Sidebar controls
    st.sidebar.header("Datadog Auth")
    api_key = st.sidebar.text_input("API Key", value=os.getenv("DATADOG_API_KEY", ""), type="password")
    app_key = st.sidebar.text_input("APP Key", value=os.getenv("DATADOG_APP_KEY", ""), type="password")
    site = st.sidebar.selectbox(
        "Site",
        options=[
            os.getenv("DATADOG_SITE", "datadoghq.com"),
            "datadoghq.com",
            "datadoghq.eu",
            "us3.datadoghq.com",
            "us5.datadoghq.com",
            "ap1.datadoghq.com",
        ],
        index=0,
    )

    st.sidebar.header("Query")
    default_query = "avg:system.cpu.user{*} by {host}"
    queries_text = st.sidebar.text_area("Metric queries (one per line)", value=default_query, height=100)
    minutes = st.sidebar.slider("Lookback (minutes)", min_value=5, max_value=240, value=30, step=5)
    auto_refresh = st.sidebar.checkbox("Auto refresh (every 30s)", value=False)
    run_btn = st.sidebar.button("Run")

    if auto_refresh:
        st.experimental_set_query_params(refresh=str(int(time.time())))  # force rerun
        st.experimental_rerun()

    if run_btn or queries_text:
        try:
            check = DevDatadogCheck("dev_dd_check", {}, [])
            queries = [q.strip() for q in queries_text.splitlines() if q.strip()]
            data = check.fetch_for_dashboard(
                queries=queries, minutes=minutes, api_key=api_key or None, app_key=app_key or None, site=site
            )
        except Exception as e:
            st.error(f"Failed to fetch metrics: {e}")
            st.stop()

        # Render charts
        for q in queries:
            lines = data.get(q, [])
            st.subheader(q)
            if not lines:
                st.info("No series returned.")
                continue

            fig = go.Figure()
            for label, points, tags in lines:
                if not points:
                    continue
                xs = [datetime.datetime.utcfromtimestamp(ts) for ts, _ in points]
                ys = [v for _, v in points]
                fig.add_trace(
                    go.Scatter(
                        x=xs,
                        y=ys,
                        mode="lines",
                        name=label,
                        hovertemplate="%{x|%Y-%m-%d %H:%M:%S} UTC<br>%{y}<extra></extra>",
                    )
                )
            fig.update_layout(
                height=400,
                margin=dict(l=10, r=10, t=30, b=10),
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            )
            st.plotly_chart(fig, use_container_width=True)


def _cli():
    parser = argparse.ArgumentParser(description="Datadog Metrics Dev Tool")
    parser.add_argument("--query", action="append", help="Datadog metric query (v1). Can be repeated.", default=[])
    parser.add_argument("--minutes", type=int, default=30, help="Lookback window in minutes")
    parser.add_argument("--site", default=os.getenv("DATADOG_SITE", "datadoghq.com"), help="Datadog site")
    parser.add_argument("--api-key", default=os.getenv("DATADOG_API_KEY"), help="Datadog API key")
    parser.add_argument("--app-key", default=os.getenv("DATADOG_APP_KEY"), help="Datadog APP key")
    parser.add_argument("--dashboard", action="store_true", help="Launch Streamlit dashboard")
    args = parser.parse_args()

    if args.dashboard:
        # When launched via CLI, attempt to run the Streamlit app
        if st is None:
            print("Streamlit is not installed. pip install streamlit")
            sys.exit(1)
        _dashboard_app()
        return

    if not args.query:
        print("No --query provided. Example: --query 'avg:system.cpu.user{*} by {host}'")
        sys.exit(1)

    check = DevDatadogCheck("dev_dd_check", {}, [])
    data = check.fetch_for_dashboard(
        queries=args.query, minutes=args.minutes, api_key=args.api_key, app_key=args.app_key, site=args.site
    )

    # Print a brief textual summary and last value per series
    print(f"Fetched data for window: last {args.minutes} minutes (site={args.site})")
    for q, lines in data.items():
        print(f"\nQuery: {q}")
        if not lines:
            print("  No series")
            continue
        for label, points, tags in lines:
            if not points:
                continue
            latest_ts, latest_val = points[-1]
            ts_str = datetime.datetime.utcfromtimestamp(latest_ts).strftime("%Y-%m-%d %H:%M:%S UTC")
            print(f"  {label}: latest={latest_val} @ {ts_str}")

    # Demonstrate Agent-like submission (print via stub if AgentCheck not present)
    instance = {
        "queries": args.query,
        "minutes": args.minutes,
        "submit_as_gauges": True,
        "api_key": args.api_key,
        "app_key": args.app_key,
        "site": args.site,
    }
    print("\nSubmitting latest points as gauges (dev/stub mode if not in Agent)...")
    check.check(instance)


if __name__ == "__main__":
    # If running via `streamlit run`, Streamlit manages execution and __name__ won't be '__main__' for the script code section.
    # For plain CLI execution:
    _cli()
------------------------------------------------------------

PROMPT 338:
"Generate Python code that uses the osm library to download OpenStreetMap data, the networkx library to model the streets as a graph, the geopandas library to analyze the spatial relationships between streets and other features, and the folium library to visualize the results on an interactive map."
RESPONSE:
# Requires: pip install osmnx networkx geopandas shapely folium
# Note: OSMnx downloads OpenStreetMap data and produces a NetworkX graph plus GeoPandas GeoDataFrames.

import warnings
warnings.filterwarnings("ignore")

import os
import json
import osmnx as ox
import networkx as nx
import geopandas as gpd
from shapely.geometry import Point
import folium
import branca.colormap as cm

def main():
    # -----------------------------
    # Settings
    # -----------------------------
    place = "Cambridge, Massachusetts, USA"
    network_type = "drive"  # options include 'drive', 'walk', 'bike', 'all'
    school_buffer_m = 200
    park_buffer_m = 150
    output_html = "osm_street_analysis.html"

    ox.settings.use_cache = True
    ox.settings.log_console = False

    # -----------------------------
    # 1) Download OSM data (via OSMnx) and model streets as a NetworkX graph
    # -----------------------------
    print("Downloading street network...")
    G = ox.graph_from_place(place, network_type=network_type)

    # Keep a WGS84 copy for mapping (lat/lon), and a projected copy for analysis/distances
    G_wgs = G.copy()
    G_proj = ox.project_graph(G)

    # Convert to GeoDataFrames
    print("Converting graph to GeoDataFrames...")
    nodes_proj, edges_proj = ox.graph_to_gdfs(G_proj)
    nodes_wgs = nodes_proj.to_crs(epsg=4326)
    edges_wgs = edges_proj.to_crs(epsg=4326)

    # -----------------------------
    # 2) Use NetworkX for graph analysis
    # -----------------------------
    print("Computing simple graph measures...")
    G_undirected = ox.utils_graph.get_undirected(G_proj)
    # Degree for each node
    degree_dict = dict(G_undirected.degree())
    nx.set_node_attributes(G_undirected, degree_dict, "degree")
    nodes_proj["degree"] = nodes_proj.index.map(degree_dict).fillna(0)

    # Shortest path example between two corners of the place boundary
    print("Computing a sample shortest path...")
    boundary_wgs = ox.geocode_to_gdf(place).to_crs(epsg=4326)
    minx, miny, maxx, maxy = boundary_wgs.total_bounds  # x=lon, y=lat
    # northwest and southeast corners in lat/lon
    orig_lat, orig_lon = maxy, minx
    dest_lat, dest_lon = miny, maxx

    orig_node = ox.nearest_nodes(G_wgs, X=orig_lon, Y=orig_lat)
    dest_node = ox.nearest_nodes(G_wgs, X=dest_lon, Y=dest_lat)

    # Shortest path using edge length (meters in projected graph)
    # Map nodes (WGS) to projected node ids via same ids
    route = nx.shortest_path(G_proj, orig_node, dest_node, weight="length")

    # Build a simple list of (lat, lon) along the route using WGS graph
    route_latlon = [(G_wgs.nodes[n]["y"], G_wgs.nodes[n]["x"]) for n in route]

    # -----------------------------
    # 3) Use GeoPandas to analyze spatial relationships with other features
    # -----------------------------
    print("Downloading POIs (schools, parks) and running spatial analysis...")
    # Download POIs from OSM
    schools = ox.geometries_from_place(place, tags={"amenity": ["school", "college", "university"]})
    parks = ox.geometries_from_place(place, tags={"leisure": "park"})

    # Project POIs to match analysis CRS
    schools_proj = schools.to_crs(edges_proj.crs) if len(schools) else gpd.GeoDataFrame(geometry=[], crs="EPSG:4326").to_crs(edges_proj.crs)
    parks_proj = parks.to_crs(edges_proj.crs) if len(parks) else gpd.GeoDataFrame(geometry=[], crs="EPSG:4326").to_crs(edges_proj.crs)

    # Create buffers and test street segments that intersect them
    def boolean_intersects_buffer(lines_gdf, features_gdf, buffer_m):
        if features_gdf is None or features_gdf.empty:
            return [False] * len(lines_gdf)
        buff = features_gdf.buffer(buffer_m)
        union = buff.unary_union
        return lines_gdf.geometry.intersects(union)

    edges_proj["near_school"] = boolean_intersects_buffer(edges_proj, schools_proj, school_buffer_m)
    edges_proj["near_park"] = boolean_intersects_buffer(edges_proj, parks_proj, park_buffer_m)

    # Convert analytical outputs back to WGS84 for mapping
    edges_wgs = edges_proj.to_crs(epsg=4326)
    schools_wgs = schools_proj.to_crs(epsg=4326) if len(schools_proj) else schools_proj
    parks_wgs = parks_proj.to_crs(epsg=4326) if len(parks_proj) else parks_proj

    # -----------------------------
    # 4) Visualize with Folium
    # -----------------------------
    print("Building interactive map with Folium...")
    # Map center = place centroid
    center_lat = boundary_wgs.geometry.iloc[0].centroid.y
    center_lon = boundary_wgs.geometry.iloc[0].centroid.x

    m = folium.Map(location=[center_lat, center_lon], zoom_start=13, tiles="cartodbpositron")

    # Style streets: highlight those near schools, else default
    def street_style_function(feature):
        props = feature["properties"]
        if props.get("near_school"):
            return {"color": "#e41a1c", "weight": 3, "opacity": 0.9}  # red
        elif props.get("near_park"):
            return {"color": "#4daf4a", "weight": 2, "opacity": 0.8}  # green
        else:
            return {"color": "#5a5a5a", "weight": 1, "opacity": 0.6}  # gray

    streets_layer = folium.FeatureGroup(name="Streets")
    folium.GeoJson(
        data=json.loads(edges_wgs.to_json()),
        name="Streets",
        style_function=street_style_function,
        tooltip=folium.GeoJsonTooltip(fields=["highway", "name", "length", "near_school", "near_park"], aliases=["Type", "Name", "Length (m)", "Near school", "Near park"], localize=True, sticky=False),
    ).add_to(streets_layer)
    streets_layer.add_to(m)

    # Schools layer
    if schools_wgs is not None and not schools_wgs.empty:
        schools_points = schools_wgs.copy()
        # Normalize to points for visualization (polygons to centroids)
        schools_points["geometry"] = schools_points.geometry.representative_point()
        schools_fg = folium.FeatureGroup(name="Schools/Colleges/Universities", show=True)
        for _, row in schools_points.iterrows():
            geom = row.geometry
            folium.CircleMarker(
                location=[geom.y, geom.x],
                radius=5,
                color="#e41a1c",
                fill=True,
                fill_opacity=0.7,
                popup=str(row.get("name", "School/College/University")),
            ).add_to(schools_fg)
        schools_fg.add_to(m)

    # Parks layer
    if parks_wgs is not None and not parks_wgs.empty:
        parks_fg = folium.FeatureGroup(name="Parks", show=False)
        folium.GeoJson(
            data=json.loads(parks_wgs.to_json()),
            style_function=lambda f: {"color": "#4daf4a", "weight": 1, "fillColor": "#4daf4a", "fillOpacity": 0.3},
            tooltip=folium.GeoJsonTooltip(fields=["name"], aliases=["Park"], sticky=False),
        ).add_to(parks_fg)
        parks_fg.add_to(m)

    # Route layer
    route_fg = folium.FeatureGroup(name="Sample shortest path", show=True)
    folium.PolyLine(route_latlon, color="#377eb8", weight=5, opacity=0.9).add_to(route_fg)
    # Mark endpoints
    folium.CircleMarker(location=route_latlon[0], radius=6, color="#08519c", fill=True, fill_opacity=1, popup="Origin").add_to(route_fg)
    folium.CircleMarker(location=route_latlon[-1], radius=6, color="#08306b", fill=True, fill_opacity=1, popup="Destination").add_to(route_fg)
    route_fg.add_to(m)

    folium.LayerControl(collapsed=False).add_to(m)

    # -----------------------------
    # 5) Print a few summary stats
    # -----------------------------
    total_km = edges_proj.length.sum() / 1000
    pct_near_school = 100 * edges_proj.loc[edges_proj["near_school"], "length"].sum() / edges_proj["length"].sum()
    pct_near_park = 100 * edges_proj.loc[edges_proj["near_park"], "length"].sum() / edges_proj["length"].sum()

    print(f"Total street length: {total_km:,.1f} km")
    print(f"Percent of street length within {school_buffer_m} m of a school: {pct_near_school:.1f}%")
    print(f"Percent of street length within {park_buffer_m} m of a park: {pct_near_park:.1f}%")

    # -----------------------------
    # 6) Save map
    # -----------------------------
    m.save(output_html)
    print(f"Saved interactive map to: {os.path.abspath(output_html)}")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 339:
"Generate Python code that imports the required libraries, creates a function that takes a list of header names and values as input, and returns a dictionary with the headers as keys and their values as values, which can then be used to set the headers for an HTTP request using the `requests` library."
RESPONSE:
import requests

def build_headers(items):
    """
    Build a headers dictionary suitable for requests from various input forms.

    Accepts:
      - A dict-like mapping of {name: value}
      - An iterable of (name, value) pairs
      - An iterable of strings in the form "Name: Value"

    Returns:
      dict: headers dictionary
    """
    headers = {}
    if not items:
        return headers

    if isinstance(items, dict):
        for k, v in items.items():
            if v is None:
                continue
            headers[str(k).strip()] = str(v).strip()
        return headers

    for item in items:
        if item is None:
            continue

        if isinstance(item, str):
            if ":" not in item:
                raise ValueError(f'Invalid header string: {item!r}. Expected "Name: Value".')
            name, value = item.split(":", 1)
        else:
            try:
                name, value = item
            except Exception as exc:
                raise TypeError("Each item must be a 'Name: Value' string or a (name, value) pair.") from exc

        headers[str(name).strip()] = str(value).strip()

    return headers

# Example usage:
if __name__ == "__main__":
    header_input = [
        ("Accept", "application/json"),
        ("User-Agent", "MyClient/1.0"),
        "Authorization: Bearer YOUR_TOKEN_HERE"
    ]

    headers = build_headers(header_input)

    # Use with requests
    response = requests.get("https://httpbin.org/headers", headers=headers, timeout=10)
    print(response.status_code)
    print(response.json())
------------------------------------------------------------

PROMPT 340:
"Generate Python code that imports the necessary libraries and creates an instance of the Coinbase API client library, allowing you to interact with the Coinbase API to perform various cryptocurrency-related tasks such as retrieving account information, placing orders, and fetching real-time market data."
RESPONSE:
```python
# Requires: pip install coinbase-advanced-py
# Docs: https://github.com/coinbase/coinbase-advanced-py

import os
from coinbase.rest import RESTClient
# Optional (for real-time market data):
# from coinbase.websocket import WSClient

# Load your Coinbase Advanced Trade API credentials from environment variables
API_KEY = os.environ.get("COINBASE_API_KEY")
API_SECRET = os.environ.get("COINBASE_API_SECRET")

if not API_KEY or not API_SECRET:
    raise RuntimeError("Please set COINBASE_API_KEY and COINBASE_API_SECRET environment variables.")

# Create a REST client instance for authenticated Coinbase Advanced Trade API calls
client = RESTClient(api_key=API_KEY, api_secret=API_SECRET)

# Now `client` can be used to call methods such as:
# - client.get_accounts()                 # Retrieve account information
# - client.place_order(...)               # Place spot orders
# - client.get_products() / get_market... # Market/product data (HTTP)

# Optional: Set up a WebSocket client for real-time market data (ticker, level 2, etc.)
# def on_message(message):
#     print(message)
#
# ws = WSClient(on_message=on_message)
# ws.subscribe(product_ids=["BTC-USD"], channels=["ticker"])
# ws.run_forever()
```
------------------------------------------------------------

PROMPT 341:
"Generate Python code that uses the `subprocess` module to parse the output of `git log` and `git diff` commands to extract information about changes in the CPython Misc/NEWS.d file between two versions, and then uses the `datetime` module to format the extracted information into a neatly organized table."
RESPONSE:
```python
#!/usr/bin/env python3
"""
Summarize CPython Misc/NEWS.d changes between two refs using `git diff` and `git log`.

- Uses subprocess to invoke git.
- Parses `git diff --name-status` and `--numstat` to learn what changed.
- Parses `git log` to attribute each change to the most recent commit in the range.
- Formats dates using `datetime` and prints a neat, aligned table.

Example:
    python summarize_news_changes.py --repo /path/to/cpython --base v3.12.0 --head v3.12.1
"""

from __future__ import annotations

import argparse
import os
import subprocess
import sys
from datetime import datetime, timezone
from typing import Dict, List, Optional, Tuple

NEWS_PATHSPEC = "Misc/NEWS.d"


def run_git(args: List[str], cwd: str) -> str:
    """Run a git command and return stdout as text. Raises on error."""
    # Ensure we're actually calling git; prepend if needed
    if args and args[0] != "git":
        args = ["git"] + args
    try:
        res = subprocess.run(
            args,
            cwd=cwd,
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8",
        )
        return res.stdout
    except subprocess.CalledProcessError as e:
        sys.stderr.write(f"Error running {' '.join(args)}\n{e.stderr}\n")
        raise


def parse_name_status(out: str) -> List[Dict]:
    """
    Parse `git diff --name-status` output.

    Returns a list of dicts:
      {
        'status': 'A'|'M'|'D'|'R...'|...,
        'path': 'path' or 'old -> new' for renames,
        'old_path': Optional[str],
        'new_path': Optional[str],
      }
    """
    changes = []
    for line in out.splitlines():
        if not line.strip():
            continue
        parts = line.split("\t")
        status = parts[0]
        if status.startswith("R"):  # rename with score, e.g., R100
            if len(parts) < 3:
                # Unexpected format; skip
                continue
            old_path, new_path = parts[1], parts[2]
            changes.append(
                {
                    "status": status,
                    "path": f"{old_path} -> {new_path}",
                    "old_path": old_path,
                    "new_path": new_path,
                }
            )
        else:
            path = parts[1] if len(parts) > 1 else ""
            changes.append(
                {
                    "status": status,
                    "path": path,
                    "old_path": None,
                    "new_path": None,
                }
            )
    return changes


def parse_numstat(out: str) -> Dict[str, Tuple[Optional[int], Optional[int]]]:
    """
    Parse `git diff --numstat` output.

    Returns dict mapping:
      key (path or 'old -> new' for renames) -> (added, removed)
    Note: added/removed may be None ('-' for binary).
    """
    stats: Dict[str, Tuple[Optional[int], Optional[int]]] = {}
    for line in out.splitlines():
        if not line.strip():
            continue
        # Format: <added>\t<removed>\t<path>   OR for renames: <added>\t<removed>\told -> new
        parts = line.split("\t")
        if len(parts) < 3:
            continue
        a, r, path = parts[0], parts[1], parts[2]
        added = None if a == "-" else int(a)
        removed = None if r == "-" else int(r)
        stats[path] = (added, removed)
    return stats


def collect_diff(base: str, head: str, repo: str) -> List[Dict]:
    """
    Collect changes limited to NEWS path between base..head.

    Returns list of dicts with:
      'status', 'path', 'old_path', 'new_path', 'added', 'removed'
    """
    ns_out = run_git(["git", "diff", "--name-status", f"{base}..{head}", "--", NEWS_PATHSPEC], repo)
    num_out = run_git(["git", "diff", "--numstat", f"{base}..{head}", "--", NEWS_PATHSPEC], repo)

    name_status = parse_name_status(ns_out)
    numstats = parse_numstat(num_out)

    # Merge stats into name_status entries by best-effort key
    merged = []
    for entry in name_status:
        key_candidates = []
        if entry["old_path"] and entry["new_path"]:
            key_candidates = [
                f"{entry['old_path']} -> {entry['new_path']}",
                entry["new_path"],
                entry["old_path"],
            ]
        else:
            key_candidates = [entry["path"]]

        added = removed = None
        for k in key_candidates:
            if k in numstats:
                added, removed = numstats[k]
                break

        entry = dict(entry)  # copy
        entry["added"] = added
        entry["removed"] = removed
        merged.append(entry)
    return merged


def collect_commits(base: str, head: str, repo: str) -> List[Dict]:
    """
    Get commits affecting NEWS path in range base..head.

    Returns list of commits in reverse-chronological order:
      { 'hash', 'author', 'date' (ISO), 'subject', 'files': [paths] }
    """
    # Use a sentinel to clearly delineate commits
    fmt = "COMMIT%x09%H%x09%an%x09%ad%x09%s"
    out = run_git(
        [
            "git",
            "log",
            "--no-merges",
            "--date=iso-strict",
            f"--pretty=format:{fmt}",
            "--name-only",
            f"{base}..{head}",
            "--",
            NEWS_PATHSPEC,
        ],
        repo,
    )

    commits: List[Dict] = []
    current: Optional[Dict] = None
    for line in out.splitlines():
        if not line.strip():
            continue
        if line.startswith("COMMIT\t"):
            # Finish previous
            if current:
                commits.append(current)
            parts = line.split("\t", 5)
            # parts: ['COMMIT', hash, author, date, subject...]
            # There can be tabs in subject; split max 4 to keep subject intact:
            # Using maxsplit=4 above keeps subject in parts[4]
            if len(parts) < 5:
                # Fallback if unexpected
                continue
            _, h, author, date, subject = parts
            current = {"hash": h, "author": author, "date": date, "subject": subject, "files": []}
        else:
            # file line
            if current is not None:
                # Guard: some lines may be outside NEWS_PATHSPEC due to rename diffs; still capture
                current["files"].append(line.strip())
    if current:
        commits.append(current)
    return commits


def map_latest_commit_per_file(commits: List[Dict]) -> Dict[str, Dict]:
    """
    For each file seen in commits, record the first (most recent) commit touching it.
    Returns map: path -> commit_info
    """
    mapping: Dict[str, Dict] = {}
    for c in commits:
        for p in c.get("files", []):
            if p not in mapping:
                mapping[p] = c
    return mapping


def pick_commit_for_entry(entry: Dict, file_commit_map: Dict[str, Dict]) -> Optional[Dict]:
    """
    Choose the most relevant commit for an entry based on available path variants.
    """
    paths = []
    if entry.get("old_path") and entry.get("new_path"):
        paths = [
            entry["new_path"],
            f"{entry['old_path']} -> {entry['new_path']}",
            entry["old_path"],
        ]
    else:
        paths = [entry["path"]]

    for p in paths:
        if p in file_commit_map:
            return file_commit_map[p]
    return None


def fmt_int(n: Optional[int]) -> str:
    return "-" if n is None else str(n)


def parse_git_iso_date(d: str) -> Optional[datetime]:
    try:
        # e.g., '2023-09-14T10:22:33+00:00'
        return datetime.fromisoformat(d)
    except Exception:
        return None


def format_date_for_table(d: Optional[datetime]) -> str:
    if not d:
        return "N/A"
    # Convert to UTC for consistency
    try:
        d_utc = d.astimezone(timezone.utc)
    except Exception:
        d_utc = d
    return d_utc.strftime("%Y-%m-%d %H:%M UTC")


def trunc(s: str, width: int) -> str:
    if len(s) <= width:
        return s
    if width <= 1:
        return s[:width]
    if width <= 3:
        return s[:width]
    return s[: width - 1 - 1] + "…"


def make_table(rows: List[Dict]) -> str:
    """
    Create an aligned ASCII table.
    Rows entries must contain:
      date_str, status, added, removed, file, author, subject, commit_short
    """
    headers = ["Date (UTC)", "Status", "+", "-", "File", "Author", "Subject", "Commit"]
    # Compute column widths with sensible caps
    caps = {
        "Date (UTC)": 20,
        "Status": 8,
        "+": 6,
        "-": 6,
        "File": 60,
        "Author": 24,
        "Subject": 80,
        "Commit": 10,
    }

    # Start with header lengths
    widths = {h: len(h) for h in headers}

    # Compute content widths
    for r in rows:
        widths["Date (UTC)"] = max(widths["Date (UTC)"], len(r["date_str"]))
        widths["Status"] = max(widths["Status"], len(r["status"]))
        widths["+"] = max(widths["+"], len(r["added"]))
        widths["-"] = max(widths["-"], len(r["removed"]))
        widths["File"] = max(widths["File"], len(r["file"]))
        widths["Author"] = max(widths["Author"], len(r["author"]))
        widths["Subject"] = max(widths["Subject"], len(r["subject"]))
        widths["Commit"] = max(widths["Commit"], len(r["commit_short"]))

    # Apply caps
    for k, cap in caps.items():
        widths[k] = min(widths[k], cap)

    def fmt_row(vals: List[Tuple[str, str]]) -> str:
        # vals: list of (header, value)
        out_cells = []
        for h, v in vals:
            w = widths[h]
            if h in {"+", "-", "Status"}:
                cell = trunc(v, w).rjust(w)
            else:
                cell = trunc(v, w).ljust(w)
            out_cells.append(cell)
        return " | ".join(out_cells)

    # Build table
    lines = []
    lines.append(fmt_row([(h, h) for h in headers]))
    sep = "-+-".join("-" * widths[h] for h in headers)
    lines.append(sep)
    for r in rows:
        line = fmt_row(
            [
                ("Date (UTC)", r["date_str"]),
                ("Status", r["status"]),
                ("+", r["added"]),
                ("-", r["removed"]),
                ("File", r["file"]),
                ("Author", r["author"]),
                ("Subject", r["subject"]),
                ("Commit", r["commit_short"]),
            ]
        )
        lines.append(line)
    return "\n".join(lines)


def build_rows(diff_entries: List[Dict], commit_map: Dict[str, Dict]) -> List[Dict]:
    rows: List[Dict] = []
    for e in diff_entries:
        c = pick_commit_for_entry(e, commit_map)
        dt = parse_git_iso_date(c["date"]) if c else None
        row = {
            "date_str": format_date_for_table(dt),
            "status": e["status"],
            "added": fmt_int(e["added"]),
            "removed": fmt_int(e["removed"]),
            "file": e["path"],
            "author": (c["author"] if c else "N/A"),
            "subject": (c["subject"] if c else "N/A"),
            "commit_short": (c["hash"][:10] if c else "N/A"),
        }
        rows.append(row)

    # Sort rows by date descending (fallback by file)
    def sort_key(r):
        # parse back from string is clumsy; instead embed raw sort info
        return r["date_str"], r["file"]

    # Better: sort by actual datetime; we have dt for that in construction
    # Let's reconstruct dt safely:
    def parse_dt_from_row(row):
        # We used format 'YYYY-MM-DD HH:MM UTC' or 'N/A'
        if row["date_str"] == "N/A":
            return datetime.min.replace(tzinfo=timezone.utc)
        try:
            return datetime.strptime(row["date_str"], "%Y-%m-%d %H:%M UTC").replace(tzinfo=timezone.utc)
        except Exception:
            return datetime.min.replace(tzinfo=timezone.utc)

    rows.sort(key=lambda r: (parse_dt_from_row(r), r["file"]), reverse=True)
    return rows


def ensure_repo(path: str) -> None:
    # Basic check to ensure directory is a git repo
    if not os.path.isdir(path):
        raise SystemExit(f"Not a directory: {path}")
    try:
        run_git(["git", "rev-parse", "--is-inside-work-tree"], path)
    except Exception:
        raise SystemExit(f"Not a git repository: {path}")


def main():
    ap = argparse.ArgumentParser(description="Summarize CPython Misc/NEWS.d changes between two refs.")
    ap.add_argument("--repo", required=True, help="Path to CPython git repository")
    ap.add_argument("--base", required=True, help="Base ref (e.g., v3.12.0)")
    ap.add_argument("--head", required=True, help="Head ref (e.g., v3.12.1)")
    args = ap.parse_args()

    repo = os.path.abspath(args.repo)
    ensure_repo(repo)

    try:
        diff_entries = collect_diff(args.base, args.head, repo)
    except Exception:
        raise SystemExit(2)

    if not diff_entries:
        print(f"No changes under {NEWS_PATHSPEC} between {args.base}..{args.head}.")
        return

    try:
        commits = collect_commits(args.base, args.head, repo)
    except Exception:
        raise SystemExit(2)

    commit_map = map_latest_commit_per_file(commits)
    rows = build_rows(diff_entries, commit_map)
    table = make_table(rows)

    print(f"Changes in {NEWS_PATHSPEC} between {args.base}..{args.head} in {repo}:\n")
    print(table)


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 342:
"Generate Python code that imports the necessary libraries for an all-in-one Python library, including modules for data manipulation, visualization, machine learning, web scraping, and more, and provides functions for various tasks such as data cleaning, feature engineering, model training, and deployment."
RESPONSE:
# all_in_one.py
# A compact utility module that aggregates common data tasks:
# - Data manipulation and cleaning
# - Feature engineering
# - Model training, evaluation, and persistence
# - Visualization helpers
# - Web scraping helpers
# - Simple FastAPI deployment scaffold
# Optional dependencies are handled gracefully.

from __future__ import annotations

import os
import re
import sys
import math
import json
import time
import types
import pickle
import logging
import warnings
from typing import Any, Dict, List, Optional, Tuple, Iterable, Union

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("all_in_one")


# -------- Optional import helper --------
def _try_import(mod_name: str, alias: Optional[str] = None):
    try:
        mod = __import__(mod_name, fromlist=["*"])
        if alias:
            globals()[alias] = mod
        else:
            globals()[mod_name.split(".")[0]] = mod
        return mod
    except Exception:
        globals()[alias or mod_name.split(".")[0]] = None
        return None


def _require(obj: Any, name: str, pip_name: Optional[str] = None):
    if obj is None:
        raise ImportError(
            f"Optional dependency '{name}' is not installed. "
            f"Install via: pip install {pip_name or name}"
        )


# -------- Attempt optional imports up-front (kept optional) --------
pd = _try_import("pandas", "pd")
np = _try_import("numpy", "np")
sp = _try_import("scipy", "sp")

plt = _try_import("matplotlib.pyplot", "plt")
sns = _try_import("seaborn", "sns")
px = _try_import("plotly.express", "px")

requests = _try_import("requests")
bs4 = _try_import("bs4")
BeautifulSoup = bs4.BeautifulSoup if bs4 else None
lxml = _try_import("lxml")
selenium = _try_import("selenium")

joblib = _try_import("joblib")
fastapi = _try_import("fastapi")
uvicorn = _try_import("uvicorn")

xgb = _try_import("xgboost", "xgb")
lgb = _try_import("lightgbm", "lgb")
catboost = _try_import("catboost")

sklearn = _try_import("sklearn")


# -------- Utilities --------
def get_versions() -> Dict[str, Optional[str]]:
    def v(mod):
        try:
            return getattr(mod, "__version__", None)
        except Exception:
            return None

    return {
        "python": ".".join(map(str, sys.version_info[:3])),
        "pandas": v(pd),
        "numpy": v(np),
        "scipy": v(sp),
        "scikit-learn": v(sklearn),
        "matplotlib": v(plt),
        "seaborn": v(sns),
        "plotly": v(px),
        "requests": v(requests),
        "bs4": v(bs4),
        "lxml": v(lxml),
        "selenium": v(selenium),
        "joblib": v(joblib),
        "xgboost": v(xgb),
        "lightgbm": v(lgb),
        "catboost": v(catboost),
        "fastapi": v(fastapi),
        "uvicorn": v(uvicorn),
    }


def set_seed(seed: int = 42):
    if np is not None:
        np.random.seed(seed)
    try:
        import random

        random.seed(seed)
    except Exception:
        pass
    try:
        import torch  # type: ignore

        torch.manual_seed(seed)  # noqa
        if torch.cuda.is_available():  # type: ignore
            torch.cuda.manual_seed_all(seed)  # type: ignore
    except Exception:
        pass


# -------- Data IO --------
def load_csv(path: str, **kwargs):
    _require(pd, "pandas")
    return pd.read_csv(path, **kwargs)


def save_csv(df, path: str, index: bool = False, **kwargs):
    _require(pd, "pandas")
    df.to_csv(path, index=index, **kwargs)


def load_excel(path: str, sheet_name: Union[str, int] = 0, **kwargs):
    _require(pd, "pandas")
    return pd.read_excel(path, sheet_name=sheet_name, **kwargs)


def load_json(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_json(obj: Any, path: str, indent: int = 2):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=indent)


# -------- Data cleaning --------
def clean_column_names(df, case: str = "snake"):
    """
    Normalize column names.
    case: "snake" | "lower" | "upper" | "title"
    """
    _require(pd, "pandas")
    cols = []
    for c in df.columns:
        c2 = str(c).strip()
        c2 = re.sub(r"[^\w\s\-]+", "", c2)
        c2 = c2.replace("-", " ")
        if case == "snake":
            c2 = re.sub(r"\s+", "_", c2.lower())
        elif case == "lower":
            c2 = c2.lower()
        elif case == "upper":
            c2 = c2.upper()
        elif case == "title":
            c2 = c2.title()
        else:
            c2 = c2
        cols.append(c2)
    df.columns = cols
    return df


def deduplicate_rows(df, keep: str = "first"):
    _require(pd, "pandas")
    return df.drop_duplicates(keep=keep)


def remove_outliers_zscore(df, cols: Optional[List[str]] = None, threshold: float = 3.0):
    _require(pd, "pandas")
    _require(np, "numpy")
    d = df.copy()
    cols = cols or d.select_dtypes(include=[np.number]).columns.tolist()
    for c in cols:
        mu = d[c].mean()
        sd = d[c].std(ddof=0)
        if sd == 0:
            continue
        z = (d[c] - mu) / sd
        d = d[z.abs() <= threshold]
    return d


def impute_missing(
    df,
    numeric_strategy: str = "median",
    categorical_strategy: str = "most_frequent",
):
    """
    Simple, in-place imputation. For advanced usage, prefer build_preprocessor().
    """
    _require(pd, "pandas")
    _require(sklearn, "scikit-learn", "scikit-learn")
    from sklearn.impute import SimpleImputer

    d = df.copy()
    num_cols = d.select_dtypes(include=["number"]).columns.tolist()
    cat_cols = d.select_dtypes(exclude=["number", "datetime"]).columns.tolist()

    if num_cols:
        imp_num = SimpleImputer(strategy=numeric_strategy)
        d[num_cols] = imp_num.fit_transform(d[num_cols])
    if cat_cols:
        imp_cat = SimpleImputer(strategy=categorical_strategy)
        d[cat_cols] = imp_cat.fit_transform(d[cat_cols])
    return d


# -------- Feature engineering --------
def infer_column_types(df) -> Dict[str, List[str]]:
    _require(pd, "pandas")
    _require(np, "numpy")
    numeric = df.select_dtypes(include=[np.number]).columns.tolist()
    datetime_cols = df.select_dtypes(include=["datetime", "datetimetz"]).columns.tolist()
    bool_cols = df.select_dtypes(include=["bool"]).columns.tolist()
    remaining = [c for c in df.columns if c not in set(numeric + datetime_cols + bool_cols)]
    # Heuristic for text vs categorical
    text = []
    categorical = []
    for c in remaining:
        if df[c].dtype == "object":
            avg_len = df[c].dropna().astype(str).str.len().mean()
            nunique = df[c].nunique(dropna=True)
            if avg_len and avg_len > 50 and nunique > 20:
                text.append(c)
            else:
                categorical.append(c)
        else:
            categorical.append(c)
    return dict(numeric=numeric, categorical=categorical, text=text, datetime=datetime_cols, boolean=bool_cols)


def create_date_parts(df, columns: List[str], drop_original: bool = False, prefix: Optional[str] = None):
    _require(pd, "pandas")
    d = df.copy()
    for c in columns:
        s = pd.to_datetime(d[c], errors="coerce")
        pfx = prefix or c
        d[f"{pfx}_year"] = s.dt.year
        d[f"{pfx}_month"] = s.dt.month
        d[f"{pfx}_day"] = s.dt.day
        d[f"{pfx}_dow"] = s.dt.dayofweek
        d[f"{pfx}_week"] = s.dt.isocalendar().week.astype("Int64")
        d[f"{pfx}_hour"] = s.dt.hour
        if drop_original:
            d.drop(columns=[c], inplace=True)
    return d


def encode_categoricals(df, strategy: str = "onehot"):
    _require(pd, "pandas")
    if strategy not in {"onehot", "ordinal"}:
        raise ValueError("strategy must be 'onehot' or 'ordinal'")
    _require(sklearn, "scikit-learn", "scikit-learn")
    from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

    d = df.copy()
    cats = d.select_dtypes(exclude=["number", "datetime"]).columns.tolist()
    if not cats:
        return d
    if strategy == "onehot":
        enc = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
        arr = enc.fit_transform(d[cats])
        cols = enc.get_feature_names_out(cats)
        d = d.drop(columns=cats).reset_index(drop=True)
        _require(pd, "pandas")
        d_enc = pd.DataFrame(arr, columns=cols, index=d.index)
        d = pd.concat([d, d_enc], axis=1)
    else:
        enc = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
        d[cats] = enc.fit_transform(d[cats])
    return d


def scale_features(df, columns: Optional[List[str]] = None, method: str = "standard"):
    _require(pd, "pandas")
    _require(sklearn, "scikit-learn", "scikit-learn")
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

    d = df.copy()
    cols = columns or d.select_dtypes(include=["number"]).columns.tolist()
    if not cols:
        return d
    if method == "standard":
        scaler = StandardScaler()
    elif method == "minmax":
        scaler = MinMaxScaler()
    elif method == "robust":
        scaler = RobustScaler()
    else:
        raise ValueError("method must be 'standard'|'minmax'|'robust'")
    d[cols] = scaler.fit_transform(d[cols])
    return d


def text_vectorize(series, method: str = "tfidf", max_features: int = 20000):
    _require(sklearn, "scikit-learn", "scikit-learn")
    _require(np, "numpy")
    if method == "tfidf":
        from sklearn.feature_extraction.text import TfidfVectorizer

        vec = TfidfVectorizer(max_features=max_features)
    elif method == "count":
        from sklearn.feature_extraction.text import CountVectorizer

        vec = CountVectorizer(max_features=max_features)
    else:
        raise ValueError("method must be 'tfidf' or 'count'")
    X = vec.fit_transform(series.astype(str).fillna(""))
    return X, vec


# -------- Model preprocessing and training --------
def build_preprocessor(
    df,
    target: Optional[str] = None,
    numeric_strategy: str = "median",
    categorical_strategy: str = "most_frequent",
    scale_numeric: bool = True,
    one_hot: bool = True,
    text_cols: Optional[List[str]] = None,
):
    _require(sklearn, "scikit-learn", "scikit-learn")
    _require(pd, "pandas")
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import OneHotEncoder, StandardScaler
    from sklearn.feature_extraction.text import TfidfVectorizer

    dtypes = infer_column_types(df.drop(columns=[target]) if target else df)
    numeric = dtypes["numeric"]
    categorical = dtypes["categorical"]
    text_columns = text_cols or dtypes["text"]

    transformers = []

    if numeric:
        steps = [("imputer", SimpleImputer(strategy=numeric_strategy))]
        if scale_numeric:
            steps.append(("scaler", StandardScaler()))
        transformers.append(("num", Pipeline(steps), numeric))

    if categorical:
        if one_hot:
            transformers.append(
                (
                    "cat",
                    Pipeline(
                        steps=[
                            ("imputer", SimpleImputer(strategy=categorical_strategy)),
                            ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False)),
                        ]
                    ),
                    categorical,
                )
            )
        else:
            transformers.append(
                (
                    "cat",
                    Pipeline(
                        steps=[
                            ("imputer", SimpleImputer(strategy=categorical_strategy)),
                        ]
                    ),
                    categorical,
                )
            )

    for c in text_columns:
        transformers.append((f"text_{c}", TfidfVectorizer(), c))

    preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")
    return preprocessor


def _choose_task(y) -> str:
    _require(pd, "pandas")
    if pd.api.types.is_numeric_dtype(y):
        # If small number of unique values, treat as classification
        unique = pd.Series(y).nunique(dropna=True)
        if unique <= 10 and unique <= len(y) // 10:
            return "classification"
        return "regression"
    else:
        return "classification"


def train_model(
    df,
    target: str,
    task: Optional[str] = None,
    model: str = "auto",
    test_size: float = 0.2,
    random_state: int = 42,
    text_cols: Optional[List[str]] = None,
):
    """
    Trains a baseline model with preprocessing pipeline.
    model options:
        classification: 'auto' (RandomForest), 'rf', 'logreg', 'xgb', 'lgb', 'cat'
        regression: 'auto' (RandomForest), 'rf', 'linear', 'xgb', 'lgb', 'cat'
    Returns: dict with keys: pipeline, model, preprocessor, metrics, X_test, y_test, feature_names
    """
    _require(pd, "pandas")
    _require(sklearn, "scikit-learn", "scikit-learn")

    from sklearn.model_selection import train_test_split
    from sklearn.pipeline import Pipeline
    from sklearn.metrics import (
        accuracy_score,
        f1_score,
        roc_auc_score,
        log_loss,
        r2_score,
        mean_absolute_error,
        mean_squared_error,
    )
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    from sklearn.linear_model import LogisticRegression, LinearRegression

    d = df.copy()
    y = d[target]
    X = d.drop(columns=[target])

    task_ = task or _choose_task(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y if task_ == "classification" else None
    )

    pre = build_preprocessor(X_train, target=None, text_cols=text_cols)

    # Choose model
    mdl = None
    if task_ == "classification":
        if model in ("auto", "rf"):
            mdl = RandomForestClassifier(n_estimators=300, random_state=random_state)
        elif model in ("logreg", "lr"):
            mdl = LogisticRegression(max_iter=200, n_jobs=None if "n_jobs" not in LogisticRegression().get_params() else -1)
        elif model == "xgb" and xgb is not None:
            mdl = xgb.XGBClassifier(
                n_estimators=500,
                learning_rate=0.05,
                max_depth=6,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_state,
                n_jobs=-1,
                eval_metric="logloss",
            )
        elif model == "lgb" and lgb is not None:
            mdl = lgb.LGBMClassifier(
                n_estimators=600,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_state,
                n_jobs=-1,
            )
        elif model == "cat" and catboost is not None:
            mdl = catboost.CatBoostClassifier(
                iterations=600, learning_rate=0.05, depth=6, random_seed=random_state, verbose=False
            )
        else:
            mdl = RandomForestClassifier(n_estimators=300, random_state=random_state)
    else:
        if model in ("auto", "rf"):
            mdl = RandomForestRegressor(n_estimators=400, random_state=random_state, n_jobs=-1)
        elif model in ("linear", "linreg"):
            mdl = LinearRegression()
        elif model == "xgb" and xgb is not None:
            mdl = xgb.XGBRegressor(
                n_estimators=600,
                learning_rate=0.05,
                max_depth=6,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_state,
                n_jobs=-1,
            )
        elif model == "lgb" and lgb is not None:
            mdl = lgb.LGBMRegressor(
                n_estimators=600,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_state,
                n_jobs=-1,
            )
        elif model == "cat" and catboost is not None:
            mdl = catboost.CatBoostRegressor(
                iterations=800, learning_rate=0.05, depth=6, random_seed=random_state, verbose=False
            )
        else:
            mdl = RandomForestRegressor(n_estimators=400, random_state=random_state, n_jobs=-1)

    pipe = Pipeline([("pre", pre), ("model", mdl)])
    pipe.fit(X_train, y_train)

    # Metrics
    metrics: Dict[str, float] = {}
    if task_ == "classification":
        proba = None
        if hasattr(pipe.named_steps["model"], "predict_proba"):
            try:
                proba = pipe.predict_proba(X_test)
            except Exception:
                proba = None
        pred = pipe.predict(X_test)
        metrics["accuracy"] = float(accuracy_score(y_test, pred))
        metrics["f1_macro"] = float(f1_score(y_test, pred, average="macro"))
        # Binary AUC & logloss
        if proba is not None and proba.shape[1] == 2:
            try:
                metrics["roc_auc"] = float(roc_auc_score(y_test, proba[:, 1]))
            except Exception:
                pass
            try:
                metrics["log_loss"] = float(log_loss(y_test, proba))
            except Exception:
                pass
    else:
        pred = pipe.predict(X_test)
        metrics["r2"] = float(r2_score(y_test, pred))
        metrics["mae"] = float(mean_absolute_error(y_test, pred))
        metrics["rmse"] = float(mean_squared_error(y_test, pred, squared=False))

    # Feature names after preprocessing (best-effort)
    feature_names = None
    try:
        feature_names = pipe.named_steps["pre"].get_feature_names_out()
        feature_names = feature_names.tolist() if hasattr(feature_names, "tolist") else list(feature_names)
    except Exception:
        feature_names = None

    return {
        "pipeline": pipe,
        "model": mdl,
        "preprocessor": pre,
        "metrics": metrics,
        "task": task_,
        "X_test": X_test,
        "y_test": y_test,
        "feature_names": feature_names,
    }


def cross_validate_model(pipeline, X, y, cv: int = 5, scoring: Optional[Union[str, List[str], Dict[str, Any]]] = None):
    _require(sklearn, "scikit-learn", "scikit-learn")
    from sklearn.model_selection import cross_validate

    return cross_validate(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)


def tune_model(pipeline, param_distributions: Dict[str, Any], X, y, n_iter: int = 25, cv: int = 3, random_state: int = 42):
    _require(sklearn, "scikit-learn", "scikit-learn")
    from sklearn.model_selection import RandomizedSearchCV

    search = RandomizedSearchCV(
        pipeline,
        param_distributions=param_distributions,
        n_iter=n_iter,
        cv=cv,
        n_jobs=-1,
        random_state=random_state,
        verbose=1,
    )
    search.fit(X, y)
    return search


# -------- Model persistence --------
def save_model(model_pipeline, path: str):
    if joblib is not None:
        joblib.dump(model_pipeline, path)
    else:
        with open(path, "wb") as f:
            pickle.dump(model_pipeline, f)
    return path


def load_model(path: str):
    if joblib is not None:
        return joblib.load(path)
    with open(path, "rb") as f:
        return pickle.load(f)


# -------- Visualization --------
def plot_missingness(df, top_n: int = 50):
    _require(pd, "pandas")
    _require(plt, "matplotlib", "matplotlib")
    _require(sns, "seaborn", "seaborn")
    missing = df.isna().mean().sort_values(ascending=False).head(top_n)
    plt.figure(figsize=(min(12, 1 + len(missing) * 0.35), 4))
    sns.barplot(x=missing.index, y=missing.values, color="#4C72B0")
    plt.xticks(rotation=45, ha="right")
    plt.ylabel("Missing Ratio")
    plt.tight_layout()
    plt.show()


def plot_correlation_heatmap(df, method: str = "pearson"):
    _require(pd, "pandas")
    _require(np, "numpy")
    _require(plt, "matplotlib", "matplotlib")
    _require(sns, "seaborn", "seaborn")
    corr = df.select_dtypes(include=[np.number]).corr(method=method)
    plt.figure(figsize=(8, 6))
    sns.heatmap(corr, cmap="coolwarm", center=0)
    plt.title(f"Correlation heatmap ({method})")
    plt.tight_layout()
    plt.show()


def plot_feature_importance(model, feature_names: Optional[List[str]] = None, top_n: int = 20):
    _require(plt, "matplotlib", "matplotlib")
    _require(sns, "seaborn", "seaborn")

    importances = None
    if hasattr(model, "feature_importances_"):
        importances = model.feature_importances_
    elif hasattr(model, "coef_"):
        coef = model.coef_
        importances = coef if coef.ndim == 1 else np.mean(np.abs(coef), axis=0)
    else:
        logger.warning("Model does not expose feature importances or coefficients.")
        return

    idx = np.argsort(importances)[::-1][:top_n]
    names = feature_names if feature_names is not None else [f"f{i}" for i in range(len(importances))]
    names = [names[i] for i in idx]
    vals = [importances[i] for i in idx]

    plt.figure(figsize=(8, max(3, top_n * 0.3)))
    sns.barplot(x=vals, y=names, color="#55A868")
    plt.title("Top Feature Importances")
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.tight_layout()
    plt.show()


# -------- Web scraping --------
def fetch_html(url: str, headers: Optional[Dict[str, str]] = None, params: Optional[Dict[str, Any]] = None, timeout: int = 15, retries: int = 0):
    _require(requests, "requests")
    headers = headers or {"User-Agent": "Mozilla/5.0 (compatible; all_in_one/1.0)"}
    attempt = 0
    while True:
        try:
            r = requests.get(url, headers=headers, params=params, timeout=timeout)
            r.raise_for_status()
            return r.text
        except Exception as e:
            if attempt >= retries:
                raise
            attempt += 1
            time.sleep(min(2 ** attempt, 10))


def parse_html(html: str, selector: str, by: str = "css") -> List[str]:
    """
    Returns list of text contents for matched elements.
    by: 'css' (BeautifulSoup) or 'xpath' (requires lxml)
    """
    if by == "css":
        _require(BeautifulSoup, "bs4")
        soup = BeautifulSoup(html, "html.parser")
        return [el.get_text(strip=True) for el in soup.select(selector)]
    elif by == "xpath":
        _require(lxml, "lxml")
        from lxml import html as lxml_html  # type: ignore

        tree = lxml_html.fromstring(html)
        nodes = tree.xpath(selector)
        return [n.text_content().strip() if hasattr(n, "text_content") else str(n) for n in nodes]
    else:
        raise ValueError("by must be 'css' or 'xpath'")


def scrape_table(url: str, match: Union[int, str] = 0):
    _require(pd, "pandas")
    return pd.read_html(url, match=match)[0]


def get_dynamic_page(url: str, wait_selector: Optional[str] = None, wait_time: int = 10) -> str:
    """
    Uses Selenium (optional) to retrieve rendered HTML.
    """
    _require(selenium, "selenium")
    from selenium import webdriver  # type: ignore
    from selenium.webdriver.common.by import By  # type: ignore
    from selenium.webdriver.support.ui import WebDriverWait  # type: ignore
    from selenium.webdriver.support import expected_conditions as EC  # type: ignore

    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    driver = webdriver.Chrome(options=options)
    try:
        driver.get(url)
        if wait_selector:
            WebDriverWait(driver, wait_time).until(EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector)))
        return driver.page_source
    finally:
        driver.quit()


# -------- Simple FastAPI deployment scaffold --------
def create_fastapi_app(model_pipeline, feature_order: Optional[List[str]] = None):
    """
    Returns a FastAPI app that exposes POST /predict.
    Payload examples:
      {"inputs": {"featureA": 1, "featureB": "x"}}
      {"inputs": [{"featureA": 1, "featureB": "x"}, {"featureA": 2, "featureB": "y"}]}
    """
    _require(fastapi, "fastapi")
    app = fastapi.FastAPI(title="Model API", version="1.0")

    @app.post("/predict")
    def predict(payload: Dict[str, Any]):
        inputs = payload.get("inputs")
        if inputs is None:
            return {"error": "Missing 'inputs' in payload"}

        # Normalize to DataFrame
        if pd is not None:
            if isinstance(inputs, dict):
                X = pd.DataFrame([inputs])
            elif isinstance(inputs, list):
                if all(isinstance(x, dict) for x in inputs):
                    X = pd.DataFrame(inputs)
                else:
                    # Assume list of lists aligned to feature_order
                    if not feature_order:
                        return {"error": "feature_order required for list-of-lists input"}
                    X = pd.DataFrame(inputs, columns=feature_order)
            else:
                return {"error": "Unsupported input format"}
        else:
            # Fallback to list-of-lists
            if isinstance(inputs, list) and all(isinstance(x, list) for x in inputs):
                if not feature_order:
                    return {"error": "pandas not available and feature_order missing"}
                X = inputs
            else:
                return {"error": "pandas not available; please provide list-of-lists with feature_order"}

        try:
            preds = model_pipeline.predict(X)  # type: ignore
            preds = preds.tolist() if hasattr(preds, "tolist") else preds
            result = {"predictions": preds}
            if hasattr(model_pipeline, "predict_proba"):
                try:
                    proba = model_pipeline.predict_proba(X)  # type: ignore
                    result["probabilities"] = proba.tolist() if hasattr(proba, "tolist") else proba
                except Exception:
                    pass
            return result
        except Exception as e:
            return {"error": str(e)}

    return app


def serve_model(model_pipeline, host: str = "0.0.0.0", port: int = 8000, feature_order: Optional[List[str]] = None):
    """
    Launches a FastAPI server for the given model.
    """
    _require(uvicorn, "uvicorn")
    app = create_fastapi_app(model_pipeline, feature_order)
    uvicorn.run(app, host=host, port=port, log_level="info")


# -------- Module export list --------
__all__ = [
    # utils
    "get_versions",
    "set_seed",
    # IO
    "load_csv",
    "save_csv",
    "load_excel",
    "load_json",
    "save_json",
    # cleaning
    "clean_column_names",
    "deduplicate_rows",
    "remove_outliers_zscore",
    "impute_missing",
    # feature engineering
    "infer_column_types",
    "create_date_parts",
    "encode_categoricals",
    "scale_features",
    "text_vectorize",
    # modeling
    "build_preprocessor",
    "train_model",
    "cross_validate_model",
    "tune_model",
    # persistence
    "save_model",
    "load_model",
    # visualization
    "plot_missingness",
    "plot_correlation_heatmap",
    "plot_feature_importance",
    # web scraping
    "fetch_html",
    "parse_html",
    "scrape_table",
    "get_dynamic_page",
    # deployment
    "create_fastapi_app",
    "serve_model",
]
------------------------------------------------------------

PROMPT 343:
"Generate Python code that imports the necessary modules and creates an instance of the BookKeeperClient class, allowing you to interact with the BookKeeper cluster and perform various operations such as creating ledgers, writing data to ledgers, reading data from ledgers, and checking the status of ledgers."
RESPONSE:
Below is a self-contained Python example that uses the official Apache BookKeeper Java client via JPype. It provides a minimal BookKeeperClient class with methods to create ledgers, write entries, read entries, and check ledger status.

Notes:
- Prerequisites: Java 8+ installed; Apache BookKeeper client/server JARs available (e.g., from the BookKeeper binary distribution).
- Set one of the following before running:
  - BK_CLASSPATH to a classpath string that includes all BookKeeper and dependency JARs (often something like /path/to/bookkeeper/lib/*:/path/to/bookkeeper/*)
  - or BK_HOME to your BookKeeper install root; the code will expand it to ${BK_HOME}/lib/*:${BK_HOME}/* automatically.
- pip install jpype1

Python code:

import os
import sys
import jpype
import jpype.imports
from contextlib import suppress


def _resolve_classpath(bk_classpath: str | None) -> str:
    """
    Resolve the Java classpath containing all BookKeeper client JARs.
    Priority:
      1) explicit bk_classpath argument,
      2) BK_CLASSPATH environment variable,
      3) BK_HOME environment variable -> ${BK_HOME}/lib/*:${BK_HOME}/*
    """
    if bk_classpath:
        return bk_classpath
    if os.getenv("BK_CLASSPATH"):
        return os.environ["BK_CLASSPATH"]
    bk_home = os.getenv("BK_HOME")
    if not bk_home:
        raise RuntimeError(
            "BookKeeper classpath not provided. Set BK_CLASSPATH or BK_HOME, "
            "or pass bk_classpath explicitly."
        )
    # Typical BookKeeper distro layout:
    # - lib/ contains all dependency jars
    # - root contains bookkeeper-server-*.jar
    return os.path.join(bk_home, "lib", "*") + os.pathsep + os.path.join(bk_home, "*")


def _start_jvm_if_needed(bk_classpath: str | None = None, jvm_path: str | None = None):
    if jpype.isJVMStarted():
        return
    cp = _resolve_classpath(bk_classpath)
    jvm = jvm_path or jpype.getDefaultJVMPath()
    # Reduce chatty logs by default; adjust as needed.
    jvm_args = [
        "-ea",
        "-Dorg.slf4j.simpleLogger.defaultLogLevel=warn",
        f"-Djava.class.path={cp}",
    ]
    jpype.startJVM(jvm, *jvm_args)


def _to_jbytes(py_bytes: bytes):
    JByte = jpype.JByte
    JArray = jpype.JArray
    return JArray(JByte)(py_bytes)


def _to_pybytes(java_byte_array):
    # jpype converts Java byte[] to a sequence of ints; bytes() handles it
    return bytes(java_byte_array)


class BookKeeperClient:
    """
    A thin Python wrapper around the Apache BookKeeper Java client.

    This uses the classic LedgerHandle API for broad compatibility.
    """

    def __init__(self, zk_servers: str, bk_classpath: str | None = None, jvm_path: str | None = None, zk_timeout_ms: int = 30000):
        _start_jvm_if_needed(bk_classpath, jvm_path)

        # Java classes
        ClientConfiguration = jpype.JClass("org.apache.bookkeeper.conf.ClientConfiguration")
        BookKeeper = jpype.JClass("org.apache.bookkeeper.client.BookKeeper")

        # Digest enum can live in two places depending on BK version
        with suppress(Exception):
            self._DigestType = jpype.JClass("org.apache.bookkeeper.client.BookKeeper$DigestType")
        if not hasattr(self, "_DigestType"):
            self._DigestType = jpype.JClass("org.apache.bookkeeper.client.api.DigestType")

        cfg = ClientConfiguration()
        cfg.setZkServers(zk_servers)
        # Set a few common timeouts if available
        with suppress(Exception):
            cfg.setZkTimeout(zk_timeout_ms)
        with suppress(Exception):
            cfg.setReadTimeout(zk_timeout_ms // 1000 if zk_timeout_ms else 30)
        with suppress(Exception):
            cfg.setAddEntryTimeout(zk_timeout_ms // 1000 if zk_timeout_ms else 30)

        self._bk = BookKeeper(cfg)
        self._write_handles = {}  # ledgerId -> LedgerHandle
        self._default_digest = "MAC"
        self._default_password = b"bookkeeper"

    def _digest_from_str(self, digest: str | None):
        d = (digest or self._default_digest).upper()
        # Map a few common names
        alias = {
            "MAC": "MAC",
            "CRC32": "CRC32",
            "CRC32C": "CRC32C",
            "DUMMY": "DUMMY",
        }
        name = alias.get(d, d)
        return getattr(self._DigestType, name)

    def create_ledger(self, ensemble_size: int = 1, quorum_size: int = 1, password: bytes | None = None, digest: str | None = None) -> int:
        """
        Create a new ledger and keep a write handle open for appends.
        Returns the newly created ledger id.
        """
        digest_enum = self._digest_from_str(digest)
        pw = password if password is not None else self._default_password

        # Use the classic API for compatibility:
        # createLedger(int ensSize, int quorumSize, DigestType digestType, byte[] passwd)
        lh = self._bk.createLedger(ensemble_size, quorum_size, digest_enum, _to_jbytes(pw))
        ledger_id = lh.getId()
        self._write_handles[ledger_id] = lh
        return ledger_id

    def write_entry(self, ledger_id: int, data: bytes, password: bytes | None = None, digest: str | None = None) -> int:
        """
        Append an entry to a ledger. Returns the entry id.
        If we don't have an open write handle, we try opening the ledger.
        """
        lh = self._write_handles.get(ledger_id)
        if lh is None:
            # Open for writing if ledger is not closed; if closed, this will still open a handle (you just can't append).
            digest_enum = self._digest_from_str(digest)
            pw = password if password is not None else self._default_password
            lh = self._bk.openLedger(ledger_id, digest_enum, _to_jbytes(pw))
            self._write_handles[ledger_id] = lh

        entry_id = lh.addEntry(_to_jbytes(data))
        return int(entry_id)

    def read_entries(self, ledger_id: int, first_entry_id: int, last_entry_id: int, password: bytes | None = None, digest: str | None = None) -> list[bytes]:
        """
        Read a range of entries [first_entry_id, last_entry_id] inclusive.
        """
        digest_enum = self._digest_from_str(digest)
        pw = password if password is not None else self._default_password
        lh = self._bk.openLedger(ledger_id, digest_enum, _to_jbytes(pw))

        try:
            enumeration = lh.readEntries(first_entry_id, last_entry_id)
            out = []
            while enumeration.hasMoreElements():
                le = enumeration.nextElement()
                out.append(_to_pybytes(le.getEntry()))
            return out
        finally:
            with suppress(Exception):
                lh.close()

    def ledger_status(self, ledger_id: int, password: bytes | None = None, digest: str | None = None) -> dict:
        """
        Returns basic status for the ledger:
          - last_add_confirmed
          - is_closed
        """
        digest_enum = self._digest_from_str(digest)
        pw = password if password is not None else self._default_password
        lh = self._bk.openLedger(ledger_id, digest_enum, _to_jbytes(pw))
        try:
            last_add_confirmed = int(lh.getLastAddConfirmed())
            with suppress(Exception):
                is_closed = bool(lh.isClosed())
            # Some BK versions may not expose isClosed; if so, infer from handle state
            if "is_closed" not in locals():
                is_closed = False
            return {
                "ledger_id": int(ledger_id),
                "last_add_confirmed": last_add_confirmed,
                "is_closed": is_closed,
            }
        finally:
            with suppress(Exception):
                lh.close()

    def close_ledger(self, ledger_id: int):
        lh = self._write_handles.pop(ledger_id, None)
        if lh:
            with suppress(Exception):
                lh.close()

    def close(self):
        # Close all cached handles and the client
        for lh in list(self._write_handles.values()):
            with suppress(Exception):
                lh.close()
        self._write_handles.clear()
        with suppress(Exception):
            self._bk.close()
        # Do not shut down the JVM here; other parts of your app may still use it.
        # If you really need to stop it (typically at process end), call: jpype.shutdownJVM()


if __name__ == "__main__":
    # Example usage:
    # Ensure BK_HOME or BK_CLASSPATH is set, and a BookKeeper cluster
    # with ZooKeeper is reachable at the given connection string.
    zk = os.getenv("ZK_SERVERS", "localhost:2181")

    client = BookKeeperClient(zk_servers=zk)

    try:
        # Create a ledger
        ledger_id = client.create_ledger(ensemble_size=1, quorum_size=1, password=b"secret", digest="CRC32")
        print(f"Created ledger: {ledger_id}")

        # Write entries
        for i in range(3):
            eid = client.write_entry(ledger_id, f"hello-{i}".encode("utf-8"), password=b"secret", digest="CRC32")
            print(f"Wrote entry {eid}")

        # Read them back
        entries = client.read_entries(ledger_id, 0, 2, password=b"secret", digest="CRC32")
        for idx, data in enumerate(entries):
            print(f"Read entry {idx}: {data.decode()}")

        # Check status
        status = client.ledger_status(ledger_id, password=b"secret", digest="CRC32")
        print("Ledger status:", status)

    finally:
        # Close the ledger and client
        client.close_ledger(ledger_id)
        client.close()

How to run:
1) Install JPype: pip install jpype1
2) Download/extract Apache BookKeeper (matching your cluster version).
3) Export BK_HOME=/path/to/bookkeeper (or set BK_CLASSPATH to include all jars, e.g., BK_CLASSPATH="/path/to/bookkeeper/lib/*:/path/to/bookkeeper/*").
4) Ensure your ZooKeeper connection string is correct (export ZK_SERVERS="host1:2181,host2:2181/zk-path" if using a chroot).
5) Run the script.
------------------------------------------------------------

PROMPT 344:
"Generate Python code that imports the logging module and creates a logger that logs messages at various levels (debug, info, warning, error) to a file named \"human_logging.log\", with each message including the name of the function or method that generated it, and the name of the module where it was defined."
RESPONSE:
import logging

# Create and configure the logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
logger.propagate = False

if not logger.handlers:
    file_handler = logging.FileHandler("human_logging.log", mode="a", encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    formatter = logging.Formatter(
        fmt="%(asctime)s | %(levelname)s | module=%(module)s | func=%(funcName)s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)


def compute(x, y):
    logger.debug("Starting compute with x=%s, y=%s", x, y)
    if y == 0:
        logger.warning("y is zero; division will be skipped to avoid ZeroDivisionError")
        return None
    result = x / y
    logger.info("Computed result: %s", result)
    return result


class Worker:
    def __init__(self, name):
        self.name = name
        logger.debug("Initialized Worker with name=%s", name)

    def do_work(self, values):
        logger.debug("Worker %s starting do_work with values=%s", self.name, values)
        if not values:
            logger.warning("No values provided to do_work")
            return 0
        try:
            total = sum(values)
            avg = total / len(values)
            logger.info("Worker %s computed average: %s", self.name, avg)
            return avg
        except Exception:
            logger.exception("Error while computing average")
            raise


def main():
    logger.debug("Entered main")
    compute(10, 2)
    compute(5, 0)

    w = Worker("Alice")
    w.do_work([1, 2, 3, 4])

    try:
        w.do_work(None)  # This will raise a TypeError
    except Exception:
        logger.exception("Caught an exception in main")

    logger.debug("Exiting main")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 345:
"Generate Python code that uses natural language processing techniques to parse and format recurring events from text data, leveraging libraries such as NLTK or spaCy for tokenization, part-of-speech tagging, and named entity recognition, and utilizing templates or string formatting to create structured output."
RESPONSE:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Parse and format recurring events from free-form text using spaCy for
tokenization, POS tagging, and named entity recognition. Produces
structured JSON-like output and iCalendar RRULE strings.

Notes:
- This is a heuristic parser for common recurrence patterns (daily, weekly with weekdays,
  monthly by date or nth weekday, "every other" intervals, start/until ranges, times).
- Requires: spaCy (and English model en_core_web_sm) and python-dateutil (optional but recommended).
"""

import re
import sys
import json
from dataclasses import dataclass, asdict, field
from datetime import datetime, timedelta, time, date
from typing import List, Optional, Dict, Any, Tuple

try:
    import spacy
    from spacy.matcher import Matcher
except ImportError as e:
    print("Please install spaCy: pip install spacy")
    sys.exit(1)

try:
    # For robust date parsing ("Oct 1", "2025-06-30", "next Monday", etc.)
    from dateutil import parser as date_parser
except Exception:
    date_parser = None


# ------------------------------- Constants -------------------------------

WEEKDAY_MAP = {
    "monday": "MO", "mon": "MO",
    "tuesday": "TU", "tue": "TU", "tues": "TU",
    "wednesday": "WE", "wed": "WE",
    "thursday": "TH", "thu": "TH", "thurs": "TH",
    "friday": "FR", "fri": "FR",
    "saturday": "SA", "sat": "SA",
    "sunday": "SU", "sun": "SU",
}

WEEKDAY_INDEX = {"MO": 0, "TU": 1, "WE": 2, "TH": 3, "FR": 4, "SA": 5, "SU": 6}

ORDINAL_WORDS = {
    "first": 1, "1st": 1, "one": 1,
    "second": 2, "2nd": 2, "two": 2,
    "third": 3, "3rd": 3, "three": 3,
    "fourth": 4, "4th": 4, "four": 4,
    "fifth": 5, "5th": 5, "five": 5,
    "last": -1,
}

FREQ_WORDS = {
    "daily": "DAILY",
    "day": "DAILY",
    "days": "DAILY",
    "weekly": "WEEKLY",
    "week": "WEEKLY",
    "weeks": "WEEKLY",
    "biweekly": "WEEKLY",     # Interpret as interval=2
    "fortnight": "WEEKLY",    # interval=2
    "monthly": "MONTHLY",
    "month": "MONTHLY",
    "months": "MONTHLY",
    "yearly": "YEARLY",
    "annually": "YEARLY",
    "year": "YEARLY",
    "years": "YEARLY",
    "hourly": "HOURLY",
    "hours": "HOURLY",
}

NOON_WORDS = {"noon"}
MIDNIGHT_WORDS = {"midnight"}

WEEKDAYS_COLLECTIVE = {
    "weekday": ["MO", "TU", "WE", "TH", "FR"],
    "weekdays": ["MO", "TU", "WE", "TH", "FR"],
    "weekend": ["SA", "SU"],
    "weekends": ["SA", "SU"],
}

TIME_RE = re.compile(
    r"\b(?:at\s*)?(?P<hour>\d{1,2})(?::(?P<minute>\d{2}))?\s*(?P<ampm>a\.?m\.?|p\.?m\.?|am|pm)?\b",
    re.IGNORECASE,
)

ORDINAL_NUM_RE = re.compile(r"\b(\d{1,2})(st|nd|rd|th)\b", re.IGNORECASE)


# ----------------------------- Data classes -----------------------------

@dataclass
class Recurrence:
    freq: Optional[str] = None              # DAILY, WEEKLY, MONTHLY, YEARLY
    interval: int = 1
    byweekday: List[str] = field(default_factory=list)     # e.g., ["MO", "WE"]
    bymonthday: List[int] = field(default_factory=list)    # e.g., [15]
    bysetpos: Optional[int] = None                         # e.g., 1 for first, -1 for last
    byhour: Optional[int] = None
    byminute: Optional[int] = None
    dtstart: Optional[datetime] = None
    until: Optional[datetime] = None
    title: Optional[str] = None
    notes: List[str] = field(default_factory=list)

    def to_rrule(self) -> str:
        if not self.freq:
            return ""
        parts = [f"FREQ={self.freq}"]
        if self.interval and self.interval != 1:
            parts.append(f"INTERVAL={self.interval}")
        if self.byweekday:
            parts.append(f"BYDAY={','.join(self.byweekday)}")
        if self.bymonthday:
            parts.append(f"BYMONTHDAY={','.join(str(d) for d in self.bymonthday)}")
        if self.bysetpos is not None:
            parts.append(f"BYSETPOS={self.bysetpos}")
        if self.byhour is not None:
            parts.append(f"BYHOUR={self.byhour}")
        if self.byminute is not None:
            parts.append(f"BYMINUTE={self.byminute}")
        if self.until:
            # iCalendar requires UNTIL in UTC or local; we output naive local in YYYYMMDDThhmmss
            parts.append(f"UNTIL={self.until.strftime('%Y%m%dT%H%M%S')}")
        return ";".join(parts)

    def to_dict(self) -> Dict[str, Any]:
        d = asdict(self)
        if self.dtstart:
            d["dtstart"] = self.dtstart.isoformat()
        if self.until:
            d["until"] = self.until.isoformat()
        d["rrule"] = self.to_rrule()
        return d

    def to_template(self) -> str:
        # Simple string formatting template for display
        pieces = []
        if self.title:
            pieces.append(f"Title: {self.title}")
        if self.freq:
            pieces.append(f"Frequency: {self.freq} (interval={self.interval})")
        if self.byweekday:
            pieces.append(f"Weekdays: {', '.join(self.byweekday)}")
        if self.bymonthday:
            pieces.append(f"Month days: {', '.join(map(str, self.bymonthday))}")
        if self.bysetpos is not None:
            pieces.append(f"Ordinal position: {self.bysetpos}")
        if self.byhour is not None or self.byminute is not None:
            hh = self.byhour if self.byhour is not None else 0
            mm = self.byminute if self.byminute is not None else 0
            pieces.append(f"Time: {hh:02d}:{mm:02d}")
        if self.dtstart:
            pieces.append(f"Start: {self.dtstart}")
        if self.until:
            pieces.append(f"Until: {self.until}")
        if self.notes:
            pieces.append(f"Notes: {'; '.join(self.notes)}")
        pieces.append(f"RRULE: {self.to_rrule()}")
        return " | ".join(pieces)


# ----------------------------- NLP utilities ----------------------------

def load_nlp():
    try:
        nlp = spacy.load("en_core_web_sm")
    except OSError:
        # Attempt to download model
        try:
            from spacy.cli import download
            download("en_core_web_sm")
            nlp = spacy.load("en_core_web_sm")
        except Exception as e:
            print("Failed to load spaCy model en_core_web_sm. Install via: python -m spacy download en_core_web_sm")
            raise
    return nlp


def extract_title(doc: "spacy.tokens.Doc") -> Optional[str]:
    # Extract a plausible event title (noun chunk that isn't just time words)
    candidates = []
    for chunk in doc.noun_chunks:
        text = chunk.text.strip()
        if any(w.lower() in FREQ_WORDS or w.lower() in WEEKDAY_MAP for w in text.split()):
            continue
        if text.lower() in ("every", "each", "week", "month", "year", "day"):
            continue
        if len(text) > 2:
            candidates.append(text)
    # prefer last noun chunk as title (often "team sync", "project meeting")
    return candidates[-1] if candidates else None


def parse_time_from_text(text: str) -> Optional[Tuple[int, int]]:
    ttext = text.lower().strip()
    if ttext in NOON_WORDS:
        return 12, 0
    if ttext in MIDNIGHT_WORDS:
        return 0, 0
    m = TIME_RE.search(text)
    if not m:
        return None
    hour = int(m.group("hour"))
    minute = int(m.group("minute") or 0)
    ampm = m.group("ampm")
    if ampm:
        ampm = ampm.replace(".", "").lower()
        if ampm in ("pm") and hour < 12:
            hour += 12
        if ampm in ("am") and hour == 12:
            hour = 0
    else:
        # If not specified, keep 24-hour value as is; e.g., "18:00" -> 18:00
        # If hour in 1..11 without am/pm, assume it's daytime? We'll leave as-is
        pass
    if 0 <= hour <= 23 and 0 <= minute <= 59:
        return hour, minute
    return None


def parse_ordinal(word: str) -> Optional[int]:
    lw = word.lower()
    if lw in ORDINAL_WORDS:
        return ORDINAL_WORDS[lw]
    m = ORDINAL_NUM_RE.match(lw)
    if m:
        return int(m.group(1))
    return None


def parse_date_str(s: str, ref: Optional[datetime] = None) -> Optional[datetime]:
    s = s.strip()
    if not s:
        return None
    # Handle common relative phrases when dateutil is absent or uncertain
    now = ref or datetime.now()
    lw = s.lower()
    if lw in ("today",):
        return now.replace(hour=0, minute=0, second=0, microsecond=0)
    if lw in ("tomorrow", "tmr", "tmrw"):
        d = now + timedelta(days=1)
        return d.replace(hour=0, minute=0, second=0, microsecond=0)
    if lw in ("next week",):
        # next week's Monday
        days_ahead = (7 - now.weekday()) % 7
        if days_ahead == 0:
            days_ahead = 7
        d = now + timedelta(days=days_ahead)
        return d.replace(hour=0, minute=0, second=0, microsecond=0)
    if lw in ("next month",):
        year = now.year + (1 if now.month == 12 else 0)
        month = 1 if now.month == 12 else now.month + 1
        return datetime(year, month, 1)
    if lw in ("next year",):
        return datetime(now.year + 1, 1, 1)

    # Try dateutil if available
    if date_parser:
        try:
            return date_parser.parse(s, default=now.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0), fuzzy=True)
        except Exception:
            return None
    else:
        # Trivial ISO-like formats
        for fmt in ("%Y-%m-%d", "%b %d, %Y", "%B %d, %Y", "%b %d", "%B %d", "%m/%d/%Y", "%m/%d/%y"):
            try:
                d = datetime.strptime(s, fmt)
                # If year missing, assume current year
                if "%Y" not in fmt and "%y" not in fmt:
                    d = d.replace(year=now.year)
                return d
            except Exception:
                continue
    return None


def nearest_weekday_on_or_after(start: date, target_weekday_index: int) -> date:
    delta = (target_weekday_index - start.weekday()) % 7
    return start + timedelta(days=delta)


# ----------------------------- Parser logic -----------------------------

class RecurrenceParser:
    def __init__(self, nlp):
        self.nlp = nlp
        self.matcher = Matcher(nlp.vocab)
        self._build_patterns()

    def _build_patterns(self):
        # Pattern: "every [other] (day|week|month|year)"
        self.matcher.add("EVERY_UNIT", [[
            {"LOWER": "every"},
            {"LOWER": {"IN": ["other"]}, "OP": "?"},
            {"LOWER": {"IN": list(FREQ_WORDS.keys())}}
        ]])

        # Pattern: "every [number] (days|weeks|months|years)"
        self.matcher.add("EVERY_N_UNIT", [[
            {"LOWER": "every"},
            {"LIKE_NUM": True},
            {"LOWER": {"IN": ["day", "days", "week", "weeks", "month", "months", "year", "years"]}},
        ]])

        # Pattern: "every monday and wednesday" / "every mon, wed"
        self.matcher.add("EVERY_WEEKDAYS_LIST", [[
            {"LOWER": "every"},
            {"LOWER": {"IN": list(WEEKDAY_MAP.keys())}, "OP": "+"},
        ]])

        # Pattern: "first friday of every month"
        self.matcher.add("NTH_WEEKDAY_MONTHLY", [[
            {"LOWER": {"IN": list(ORDINAL_WORDS.keys())}},
            {"LOWER": {"IN": list(WEEKDAY_MAP.keys())}},
            {"LOWER": "of"},
            {"LOWER": {"IN": ["each", "every"]}, "OP": "?"},
            {"LOWER": {"IN": ["month", "months"]}},
        ]])

        # Pattern: "monthly on the 15th"
        self.matcher.add("MONTHLY_ON_DATE", [[
            {"LOWER": {"IN": ["monthly", "month"]}},
            {"LOWER": "on"},
            {"LOWER": {"IN": ["the"]}, "OP": "?"},
            {"LOWER": {"REGEX": r"\d{1,2}(st|nd|rd|th)?"}},
        ]])

        # Pattern: "at 5pm" / "at 17:30" / "noon" / "midnight"
        self.matcher.add("AT_TIME", [
            [{"LOWER": "at"}, {"LIKE_NUM": True}, {"ORTH": ":", "OP": "?"}, {"LIKE_NUM": True, "OP": "?"}, {"LOWER": {"IN": ["am", "pm", "a.m.", "p.m."]}, "OP": "?"}],
            [{"LOWER": {"IN": list(NOON_WORDS | MIDNIGHT_WORDS)}}],
        ])

        # Pattern: "from <DATE>" / "starting <DATE>" / "beginning <DATE>"
        self.matcher.add("START_DATE", [
            [{"LOWER": {"IN": ["from", "starting", "beginning", "as", "effective"]}}],
        ])

        # Pattern: "until <DATE>" / "through <DATE>" / "ending <DATE>"
        self.matcher.add("UNTIL_DATE", [
            [{"LOWER": {"IN": ["until", "through", "thru", "till", "ending"]}}],
        ])

        # Pattern: "weekdays"/"weekends"
        self.matcher.add("COLLECTIVE_WEEKDAYS", [[
            {"LOWER": {"IN": list(WEEKDAYS_COLLECTIVE.keys())}}
        ]])

    def parse(self, text: str, ref_dt: Optional[datetime] = None) -> Recurrence:
        doc = self.nlp(text)
        rec = Recurrence()
        rec.title = extract_title(doc)
        ref_dt = ref_dt or datetime.now()

        # Defaults / initial inference from simple keywords
        lw_text = text.lower()

        # Handle easy frequency keywords
        for word, freq in FREQ_WORDS.items():
            if re.search(rf"\b{re.escape(word)}\b", lw_text):
                rec.freq = rec.freq or freq

        # Interval adjustments for "biweekly", "every other", "fortnight"
        if "biweekly" in lw_text or "fortnight" in lw_text:
            rec.freq = "WEEKLY"
            rec.interval = max(rec.interval, 2)
        if re.search(r"\bevery\s+other\b", lw_text):
            # Choose unit later if not yet determined
            if rec.freq is None:
                # choose default WEEKLY, will refine if unit appears
                rec.freq = "WEEKLY"
            rec.interval = max(rec.interval, 2)

        # Collective weekdays/weekends
        for k, days in WEEKDAYS_COLLECTIVE.items():
            if re.search(rf"\b{k}\b", lw_text):
                rec.freq = rec.freq or "WEEKLY"
                for d in days:
                    if d not in rec.byweekday:
                        rec.byweekday.append(d)

        # Run matcher
        matches = self.matcher(doc)

        # Times and dates from NER
        time_candidates = []
        date_candidates = []
        for ent in doc.ents:
            if ent.label_ in ("TIME",):
                time_candidates.append(ent.text)
            if ent.label_ in ("DATE",):
                date_candidates.append(ent.text)

        # Extract times from matches or NER
        explicit_time = None
        for match_id, start, end in matches:
            name = self.nlp.vocab.strings[match_id]
            span = doc[start:end]
            if name == "AT_TIME":
                explicit_time = span.text
                break
        if not explicit_time and time_candidates:
            explicit_time = time_candidates[0]

        if explicit_time:
            tm = parse_time_from_text(explicit_time)
            if tm:
                rec.byhour, rec.byminute = tm

        # Determine start/until dates by scanning for keywords and nearest DATE entity/token
        rec.dtstart = self._extract_date_after_keywords(doc, ["from", "starting", "beginning", "as of", "effective"], ref_dt)
        rec.until = self._extract_date_after_keywords(doc, ["until", "through", "thru", "till", "ending"], ref_dt)

        # If start not set but a standalone date exists, consider first DATE as dtstart
        if not rec.dtstart and date_candidates:
            dt_guess = parse_date_str(date_candidates[0], ref_dt)
            if dt_guess:
                rec.dtstart = dt_guess

        # Parse specific recurrence patterns
        for match_id, start, end in matches:
            name = self.nlp.vocab.strings[match_id]
            span = doc[start:end]

            if name == "EVERY_N_UNIT":
                # "every 2 weeks"
                tokens = [t for t in span]
                num = None
                unit = None
                for t in tokens:
                    if t.like_num and num is None:
                        try:
                            num = int(t.text)
                        except Exception:
                            # If word-number like "two", spaCy often marks like_num True but not int; leave
                            pass
                    elif t.lower_ in FREQ_WORDS:
                        unit = t.lower_
                if unit:
                    rec.freq = FREQ_WORDS.get(unit, rec.freq)
                if num:
                    rec.interval = max(num, 1)

            elif name == "EVERY_UNIT":
                # "every week" or "every other month"
                unit_tok = [t for t in span if t.lower_ in FREQ_WORDS]
                if unit_tok:
                    rec.freq = FREQ_WORDS[unit_tok[0].lower_]
                if "other" in span.text.lower():
                    rec.interval = max(rec.interval, 2)

            elif name == "EVERY_WEEKDAYS_LIST":
                # Collect weekdays listed after "every"
                for t in span:
                    wd = WEEKDAY_MAP.get(t.lower_)
                    if wd and wd not in rec.byweekday:
                        rec.byweekday.append(wd)
                if rec.byweekday and not rec.freq:
                    rec.freq = "WEEKLY"

            elif name == "NTH_WEEKDAY_MONTHLY":
                # e.g., "first friday of every month"
                ordinal = None
                wd = None
                for tok in span:
                    if ordinal is None:
                        o = parse_ordinal(tok.text)
                        if o is not None:
                            ordinal = o
                    if wd is None:
                        wd = WEEKDAY_MAP.get(tok.lower_)
                if wd:
                    rec.freq = "MONTHLY"
                    rec.byweekday = [wd]
                if ordinal is not None:
                    rec.bysetpos = ordinal

            elif name == "MONTHLY_ON_DATE":
                # e.g., "monthly on the 15th"
                rec.freq = "MONTHLY"
                for tok in span:
                    m = re.match(r"(\d{1,2})(st|nd|rd|th)?$", tok.text.lower())
                    if m:
                        day = int(m.group(1))
                        if 1 <= day <= 31:
                            rec.bymonthday = [day]

        # If weekdays mentioned outside "every" phrase, collect them
        if not rec.byweekday:
            for t in doc:
                wd = WEEKDAY_MAP.get(t.lower_)
                if wd and wd not in rec.byweekday:
                    rec.byweekday.append(wd)
            # If we found weekdays but no freq, assume weekly
            if rec.byweekday and not rec.freq:
                rec.freq = "WEEKLY"

        # If "on the 15th" without "monthly" but context implies recurrence
        if not rec.bymonthday:
            m = re.search(r"\bon\s+(the\s+)?(\d{1,2})(st|nd|rd|th)?\b", lw_text)
            if m and ("month" in lw_text or rec.freq == "MONTHLY"):
                day = int(m.group(2))
                if 1 <= day <= 31:
                    rec.freq = rec.freq or "MONTHLY"
                    rec.bymonthday = [day]

        # If no explicit frequency but has bymonthday set, assume monthly
        if rec.bymonthday and not rec.freq:
            rec.freq = "MONTHLY"

        # If has weekdays and "every other" but weekly freq: interval already set to 2
        # Otherwise, default frequency if still missing
        if not rec.freq:
            # Heuristics from presence of time or general phrases
            if "every" in lw_text or "each" in lw_text:
                # assume weekly if weekdays present else daily
                rec.freq = "WEEKLY" if rec.byweekday else "DAILY"
            elif rec.byweekday:
                rec.freq = "WEEKLY"
            elif rec.bymonthday:
                rec.freq = "MONTHLY"

        # Extract time if not found from any "at time" but NER had "TIME"
        if rec.byhour is None and explicit_time is None and time_candidates:
            tm = parse_time_from_text(time_candidates[0])
            if tm:
                rec.byhour, rec.byminute = tm

        # Handle "at noon"/"at midnight" explicitly if present in text
        if rec.byhour is None:
            if re.search(r"\bnoon\b", lw_text):
                rec.byhour, rec.byminute = 12, 0
            elif re.search(r"\bmidnight\b", lw_text):
                rec.byhour, rec.byminute = 0, 0

        # If dtstart missing but there's a relative "next <weekday>" phrase
        if rec.dtstart is None:
            m = re.search(r"\bnext\s+(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\b", lw_text)
            if m:
                wd_code = WEEKDAY_MAP[m.group(1)]
                today = (ref_dt or datetime.now()).date()
                start_date = nearest_weekday_on_or_after(today + timedelta(days=1), WEEKDAY_INDEX[wd_code])
                rec.dtstart = datetime.combine(start_date, time(rec.byhour or 9, rec.byminute or 0))

        # If dtstart still missing, set to next sensible occurrence based on frequency/weekday
        if rec.dtstart is None and rec.freq:
            now = ref_dt or datetime.now()
            base_date = now.date()
            if rec.freq == "WEEKLY" and rec.byweekday:
                # next listed weekday
                next_dates = []
                for wd in rec.byweekday:
                    d = nearest_weekday_on_or_after(base_date, WEEKDAY_INDEX[wd])
                    next_dates.append(d)
                start_date = min(next_dates) if next_dates else base_date
                rec.dtstart = datetime.combine(start_date, time(rec.byhour or 9, rec.byminute or 0))
            elif rec.freq == "MONTHLY" and rec.bymonthday:
                day = rec.bymonthday[0]
                y, mth = base_date.year, base_date.month
                try:
                    candidate = date(y, mth, day)
                    if candidate < base_date:
                        # move to next month
                        if mth == 12:
                            y += 1
                            mth = 1
                        else:
                            mth += 1
                        candidate = date(y, mth, day)
                    rec.dtstart = datetime.combine(candidate, time(rec.byhour or 9, rec.byminute or 0))
                except ValueError:
                    # invalid day for this month; skip setting
                    pass
            else:
                # DAILY/HOURLY/others: start now with given time or next hour
                hh = rec.byhour if rec.byhour is not None else now.hour
                mm = rec.byminute if rec.byminute is not None else 0
                rec.dtstart = now.replace(minute=mm, second=0, microsecond=0, hour=hh)

        # Sanity: if bysetpos set but no freq/monthly, assume monthly
        if rec.bysetpos is not None and rec.freq != "MONTHLY":
            rec.freq = "MONTHLY"

        # Capture ambiguous "twice a week" or "3 times a month"
        m = re.search(r"\b(twice|thrice|(\d+)\s+times)\s+a\s+(day|week|month|year)\b", lw_text)
        if m:
            qty = 2 if m.group(1) == "twice" else (3 if m.group(1) == "thrice" else int(m.group(2)))
            unit = m.group(3)
            rec.freq = FREQ_WORDS.get(unit, rec.freq)
            rec.notes.append(f"Requested {qty} times per {unit}; specify exact days/times for precise RRULE.")

        # Final normalization: sort weekdays, dedupe
        rec.byweekday = sorted(set(rec.byweekday), key=lambda x: WEEKDAY_INDEX[x]) if rec.byweekday else rec.byweekday
        rec.bymonthday = sorted(set(rec.bymonthday)) if rec.bymonthday else rec.bymonthday

        return rec

    def _extract_date_after_keywords(self, doc, keywords: List[str], ref_dt: datetime) -> Optional[datetime]:
        # Find keyword token and get the nearest DATE entity or token substring following it
        lw_keywords = set(k.lower() for k in keywords)
        for i, tok in enumerate(doc):
            if tok.lower_ in lw_keywords:
                # Look ahead for an entity labeled DATE or a short span containing date-like tokens
                # First try entity
                for ent in doc.ents:
                    if ent.label_ == "DATE" and ent.start >= i and ent.start <= i + 5:
                        dt = parse_date_str(ent.text, ref_dt)
                        if dt:
                            return dt
                # Fallback: take up to next 4 tokens as a date string
                span_text = doc[i+1 : min(i+6, len(doc))].text
                dt = parse_date_str(span_text, ref_dt)
                if dt:
                    return dt
        return None


# ----------------------------- Demo / CLI -----------------------------

def main():
    nlp = load_nlp()
    parser = RecurrenceParser(nlp)

    examples = [
        "Team sync every Monday and Wednesday at 5pm starting Oct 1 until Dec 15",
        "First Friday of every month at 09:30",
        "Every other week on Tue at noon",
        "Monthly on the 15th at 18:00",
        "Weekdays at 8:15am starting next week",
        "Biweekly check-in until 2026-06-30",
        "Yoga class every 2 weeks on Sat at 10am",
        "Standup daily at 9",
        "Board meeting on the 3rd Thursday of each month at 4 p.m.",
        "Maintenance window: last Sunday of every month at midnight",
        "Training twice a week",
    ]

    for text in examples:
        rec = parser.parse(text)
        print("Input:", text)
        print("Structured:", json.dumps(rec.to_dict(), indent=2))
        print("Template:", rec.to_template())
        print("-" * 80)


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 346:
"Generate Python code that imports the necessary libraries for interacting with Coralogix, sets up authentication credentials, creates a client object to interface with the API, and uses the client to perform various tasks such as creating a new trace, adding data to an existing trace, and retrieving trace data."
RESPONSE:
# Requires:
#   pip install opentelemetry-sdk opentelemetry-exporter-otlp-proto-http requests
#
# Notes:
# - This example uses OpenTelemetry to send traces to Coralogix over OTLP/HTTP.
# - You must provide the proper OTLP traces endpoint and API key for your Coralogix region/account.
# - Retrieving traces uses a generic POST to a "traces query" endpoint; consult your Coralogix account docs
#   for the exact URL and payload format, then update CORALOGIX_TRACES_QUERY_URL and the request body accordingly.

import os
import time
import json
import uuid
import typing as t
import requests
from datetime import datetime, timedelta, timezone

# OpenTelemetry imports
from opentelemetry import trace
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.trace import Span
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator


def _int_id_to_hex(i: int, width: int) -> str:
    return f"{i:0{width}x}"


class CoralogixClient:
    """
    Minimal client to:
      - initialize an OTLP/HTTP tracer to send traces to Coralogix
      - create a new trace (span)
      - add a span to an existing trace via a W3C traceparent header
      - query trace data via a configurable HTTP endpoint

    Environment variables:
      CORALOGIX_API_KEY               - Coralogix API key (token)
      CORALOGIX_OTLP_TRACES_ENDPOINT  - OTLP/HTTP traces endpoint, e.g. https://otel.<region>.coralogix.com/v1/traces
      CORALOGIX_TRACES_QUERY_URL      - Trace query endpoint (varies by account/region/features)
      CORALOGIX_APP_NAME              - Application name (used as service.name)
      CORALOGIX_SUBSYSTEM             - Subsystem/namespace (used as service.namespace)
    """

    def __init__(
        self,
        api_key: str | None = None,
        otlp_traces_endpoint: str | None = None,
        query_endpoint: str | None = None,
        app_name: str | None = None,
        subsystem: str | None = None,
        exporter_headers: dict[str, str] | None = None,
    ):
        self.api_key = api_key or os.getenv("CORALOGIX_API_KEY", "")
        self.otlp_traces_endpoint = otlp_traces_endpoint or os.getenv("CORALOGIX_OTLP_TRACES_ENDPOINT", "")
        self.query_endpoint = query_endpoint or os.getenv("CORALOGIX_TRACES_QUERY_URL", "")
        self.app_name = app_name or os.getenv("CORALOGIX_APP_NAME", "example-app")
        self.subsystem = subsystem or os.getenv("CORALOGIX_SUBSYSTEM", "example-subsystem")

        if not self.api_key:
            raise ValueError("Missing Coralogix API key. Set CORALOGIX_API_KEY.")

        if not self.otlp_traces_endpoint:
            raise ValueError("Missing OTLP traces endpoint. Set CORALOGIX_OTLP_TRACES_ENDPOINT.")

        # Some deployments expect "Authorization: Bearer <token>", others may use "X-API-Key: <token>".
        # Provide both by default; keep or remove per your environment/documentation.
        default_headers = {
            "Authorization": f"Bearer {self.api_key}",
            "X-API-Key": self.api_key,
        }
        if exporter_headers:
            default_headers.update(exporter_headers)

        # Configure the OpenTelemetry tracer
        resource = Resource.create(
            {
                "service.name": self.app_name,
                "service.namespace": self.subsystem,
                # Add more resource attributes as needed
                "telemetry.sdk.name": "opentelemetry",
                "telemetry.sdk.language": "python",
                "telemetry.sdk.version": "1.x",
            }
        )

        provider = TracerProvider(resource=resource)
        span_exporter = OTLPSpanExporter(
            endpoint=self.otlp_traces_endpoint,
            headers=default_headers,
            timeout=30,
        )
        span_processor = BatchSpanProcessor(span_exporter)
        provider.add_span_processor(span_processor)
        trace.set_tracer_provider(provider)
        self.tracer = trace.get_tracer(__name__)
        self._provider = provider  # keep reference for shutdown

        # HTTP session for queries
        self._http = requests.Session()
        self._http.headers.update(
            {
                "Authorization": f"Bearer {self.api_key}",
                "X-API-Key": self.api_key,
                "Content-Type": "application/json",
                "Accept": "application/json",
            }
        )

    def shutdown(self):
        # Ensure all spans are flushed
        self._provider.shutdown()

    def create_new_trace(
        self,
        operation_name: str = "root-operation",
        attributes: dict[str, t.Any] | None = None,
    ) -> dict[str, str]:
        """
        Creates a new trace by starting and ending a root span.
        Returns identifiers and a W3C traceparent string you can use to attach future spans.
        """
        attributes = attributes or {}
        propagator = TraceContextTextMapPropagator()

        with self.tracer.start_as_current_span(operation_name, attributes=attributes) as span:
            # Add an event as an example
            span.add_event("root span started", {"event.id": str(uuid.uuid4())})
            # Inject the current context into a carrier to capture the traceparent
            carrier: dict[str, str] = {}
            propagator.inject(carrier)

            ctx = span.get_span_context()
            trace_id_hex = _int_id_to_hex(ctx.trace_id, 32)
            span_id_hex = _int_id_to_hex(ctx.span_id, 16)

            # Simulate some work
            time.sleep(0.05)

        # After exiting the context manager, the span is ended (exported asynchronously).
        return {
            "trace_id": trace_id_hex,
            "root_span_id": span_id_hex,
            "traceparent": carrier.get("traceparent", ""),
        }

    def add_span_to_existing_trace(
        self,
        traceparent: str,
        operation_name: str = "child-operation",
        attributes: dict[str, t.Any] | None = None,
        duration_ms: int = 20,
    ) -> dict[str, str]:
        """
        Adds a child span to an existing trace using a W3C traceparent header.
        The new span will be a child of the context represented by 'traceparent'.
        """
        if not traceparent:
            raise ValueError("traceparent is required to attach to an existing trace")

        attributes = attributes or {}
        propagator = TraceContextTextMapPropagator()

        # Recreate the parent context from the traceparent header
        parent_context = propagator.extract({"traceparent": traceparent})

        # Start a child span with the extracted parent context
        with self.tracer.start_as_current_span(operation_name, context=parent_context, attributes=attributes) as span:
            span.add_event("child span started", {"added.via": "traceparent"})
            time.sleep(max(0, duration_ms) / 1000.0)
            ctx = span.get_span_context()
            trace_id_hex = _int_id_to_hex(ctx.trace_id, 32)
            span_id_hex = _int_id_to_hex(ctx.span_id, 16)

        return {"trace_id": trace_id_hex, "span_id": span_id_hex}

    def get_trace_by_id(
        self,
        trace_id_hex: str,
        start: datetime | None = None,
        end: datetime | None = None,
    ) -> dict:
        """
        Retrieves trace data for a given trace ID.

        IMPORTANT:
        - The exact endpoint and payload format varies by Coralogix account/region and feature set.
        - Set CORALOGIX_TRACES_QUERY_URL to the correct API URL for your environment.
        - Update the 'payload' below to match your API's schema (TQL/CXQL/GraphQL/etc.).

        Two common patterns:
          1) A "get by trace id" endpoint:
             GET https://<api-host>/api/vX/traces/{traceId}

          2) A search endpoint that accepts a query and timeframe:
             POST https://<api-host>/api/vX/queries/traces
             body: { "query": "traceId='<id>'", "timeframe": { "from": <ms>, "to": <ms> } }

        This method implements #2 as a default. Modify as needed.
        """
        if not self.query_endpoint:
            raise ValueError(
                "Missing trace query endpoint. Set CORALOGIX_TRACES_QUERY_URL to your account's trace query API."
            )

        # Default timeframe: last 30 minutes
        end = end or datetime.now(timezone.utc)
        start = start or end - timedelta(minutes=30)
        start_ms = int(start.timestamp() * 1000)
        end_ms = int(end.timestamp() * 1000)

        # Example payload for a generic "traces search" API — update per your environment
        payload = {
            "query": f"traceId = '{trace_id_hex}'",
            "timeframe": {"from": start_ms, "to": end_ms},
            "limit": 2000,  # adjust per API limits
        }

        # If your API is a "get by trace id" style, you can detect a {traceId} template and use GET:
        if "{traceId}" in self.query_endpoint:
            url = self.query_endpoint.replace("{traceId}", trace_id_hex)
            resp = self._http.get(url, timeout=30)
        else:
            resp = self._http.post(self.query_endpoint, data=json.dumps(payload), timeout=60)

        if resp.status_code >= 400:
            raise RuntimeError(f"Trace query failed: {resp.status_code} {resp.text}")

        try:
            return resp.json()
        except Exception:
            # Fallback to raw text if response isn't JSON
            return {"raw": resp.text}


def main():
    # Read configuration from environment or set directly here
    # Examples (set these before running):
    #   export CORALOGIX_API_KEY="..."
    #   export CORALOGIX_OTLP_TRACES_ENDPOINT="https://otel.<region>.coralogix.com/v1/traces"
    #   export CORALOGIX_TRACES_QUERY_URL="https://api.<region>.coralogix.com/api/v2/queries/traces"
    #   export CORALOGIX_APP_NAME="my-service"
    #   export CORALOGIX_SUBSYSTEM="payments"

    client = CoralogixClient()

    # 1) Create a new trace (root span)
    root = client.create_new_trace(
        operation_name="checkout",
        attributes={"user.id": "12345", "cart.value": 87.50},
    )
    print("Created trace:", root)

    # 2) Add a child span to the existing trace using the returned traceparent
    child = client.add_span_to_existing_trace(
        traceparent=root["traceparent"],
        operation_name="charge-card",
        attributes={"payment.provider": "stripe", "currency": "USD"},
        duration_ms=75,
    )
    print("Added child span:", child)

    # 3) Retrieve trace data by trace ID (update CORALOGIX_TRACES_QUERY_URL and payload to match your API)
    # Give the backend a short moment to index the trace before querying
    time.sleep(2.0)
    try:
        trace_data = client.get_trace_by_id(root["trace_id"])
        print("Trace query response (truncated):")
        output = json.dumps(trace_data, indent=2)
        print(output[:2000] + ("..." if len(output) > 2000 else ""))
    except Exception as e:
        print(f"Trace query failed: {e}")

    # Ensure all spans are flushed before exit
    client.shutdown()


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 347:
"Generate Python code that uses libraries like BeautifulSoup and requests to extract ISBNs from book websites, then cleans and transforms the data using String manipulation and RegEx, Hyphenates the ISBNs using a custom function, and adds metadata such as title, author, and publisher using a database or API calls."
RESPONSE:
#!/usr/bin/env python3
"""
Scrape ISBNs from book web pages, clean/normalize with regex, hyphenate via a custom function,
and enrich with metadata (title, author, publisher) using Open Library and Google Books APIs.
Optionally persists results in a local SQLite database.

Requirements:
  pip install requests beautifulsoup4

Optional:
  - Set GOOGLE_BOOKS_API_KEY env var for higher quota with Google Books.
"""

import os
import re
import json
import time
import sqlite3
from datetime import datetime
from typing import Iterable, List, Optional, Set, Tuple, Dict

import requests
from bs4 import BeautifulSoup


# -----------------------------
# HTTP helpers
# -----------------------------
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; ISBN-Scraper/1.0; +https://example.org/bot)"
}

def fetch_html(url: str, timeout: int = 15, max_retries: int = 2) -> Optional[str]:
    last_err = None
    for attempt in range(max_retries + 1):
        try:
            resp = requests.get(url, headers=DEFAULT_HEADERS, timeout=timeout)
            if 200 <= resp.status_code < 300:
                return resp.text
            last_err = RuntimeError(f"HTTP {resp.status_code}")
        except Exception as e:
            last_err = e
        time.sleep(0.5 * attempt)
    print(f"[WARN] Failed to fetch {url}: {last_err}")
    return None


# -----------------------------
# ISBN utilities
# -----------------------------
NON_DIGIT_X = re.compile(r"[^0-9Xx]+")

def clean_isbn_token(token: str) -> str:
    # Strip whitespace, remove hyphens and spaces, keep digits and X (for ISBN-10)
    return NON_DIGIT_X.sub("", token).upper()

def is_valid_isbn10(isbn10: str) -> bool:
    if len(isbn10) != 10 or not re.fullmatch(r"\d{9}[\dX]", isbn10):
        return False
    total = 0
    for i, ch in enumerate(isbn10):
        val = 10 if ch == "X" else int(ch)
        total += (10 - i) * val
    return total % 11 == 0

def compute_isbn13_check_digit(first12: str) -> str:
    s = 0
    for i, ch in enumerate(first12):
        w = 1 if i % 2 == 0 else 3
        s += int(ch) * w
    return str((10 - (s % 10)) % 10)

def is_valid_isbn13(isbn13: str) -> bool:
    if len(isbn13) != 13 or not isbn13.isdigit():
        return False
    return compute_isbn13_check_digit(isbn13[:12]) == isbn13[12]

def isbn10_to_isbn13(isbn10: str) -> str:
    if not is_valid_isbn10(isbn10):
        raise ValueError("Invalid ISBN-10")
    core9 = isbn10[:9]
    first12 = "978" + core9
    return first12 + compute_isbn13_check_digit(first12)

def normalize_isbn(token: str) -> Optional[str]:
    cleaned = clean_isbn_token(token)
    if len(cleaned) == 10 and is_valid_isbn10(cleaned):
        return isbn10_to_isbn13(cleaned)
    if len(cleaned) == 13 and is_valid_isbn13(cleaned):
        return cleaned
    return None


# -----------------------------
# Custom ISBN-13 hyphenation (heuristic)
# NOTE: Real hyphenation requires official range tables from ISBN Agency.
# This function uses a simplified heuristic for demonstration purposes.
# -----------------------------
TWO_DIGIT_GROUPS = {
    "80", "81", "82", "83", "84", "85", "86", "87", "88", "89",
    "90", "91", "92", "93", "94"
}
ONE_DIGIT_GROUPS = {"0", "1", "2", "3", "4", "5", "7"}  # common large groups

def hyphenate_isbn13(isbn13: str) -> str:
    if not is_valid_isbn13(isbn13):
        raise ValueError("hyphenate_isbn13 expects a valid ISBN-13")

    prefix = isbn13[:3]           # 978 or 979
    rest = isbn13[3:-1]           # group + registrant + publication
    check = isbn13[-1]

    # Determine group identifier length
    group = None
    if rest[:2] in TWO_DIGIT_GROUPS:
        group = rest[:2]
        remainder = rest[2:]
    else:
        group = rest[:1]
        remainder = rest[1:]

    # Heuristic for registrant (publisher) length:
    # Choose a mid-range registrant length so that item number remains >= 2 digits.
    remainder_len = len(remainder)  # should be 9 or 10
    # We must allocate: registrant_len + item_len = remainder_len
    # Try to keep item_len between 2 and 5 digits
    if group in {"0", "1"}:
        registrant_len = min(5, max(3, remainder_len - 4))
    else:
        registrant_len = min(5, max(2, remainder_len - 5))

    item_len = remainder_len - registrant_len
    if item_len < 2:
        # Rebalance if we squeezed the item too short
        shift = 2 - item_len
        registrant_len = max(2, registrant_len - shift)
        item_len = remainder_len - registrant_len

    registrant = remainder[:registrant_len]
    publication = remainder[registrant_len:]

    return f"{prefix}-{group}-{registrant}-{publication}-{check}"


# -----------------------------
# HTML parsing and ISBN extraction
# -----------------------------
# Pattern to find candidate ISBN-like tokens in text (digits/X with separators)
CANDIDATE_ISBN_PATTERN = re.compile(
    r"(?i)\b(?:ISBN(?:-1[03])?:?\s*)?([0-9Xx][0-9Xx\- ]{8,20}[0-9Xx])\b"
)

def extract_isbns_from_html(html: str) -> Set[str]:
    soup = BeautifulSoup(html, "html.parser")
    found: Set[str] = set()

    # 1) Structured data (JSON-LD)
    for tag in soup.find_all("script", type=lambda t: t and "ld+json" in t):
        try:
            data = json.loads(tag.string or "")
        except Exception:
            continue
        for node in (data if isinstance(data, list) else [data]):
            if isinstance(node, dict):
                if node.get("@type") == "Book" and "isbn" in node:
                    val = node["isbn"]
                    if isinstance(val, list):
                        for v in val:
                            if isinstance(v, str):
                                found.add(v)
                    elif isinstance(val, str):
                        found.add(val)

    # 2) Meta tags commonly used for ISBN
    for m in soup.find_all("meta"):
        k = " ".join(
            str(m.get(attr, "")).lower() for attr in ("name", "property", "itemprop")
        )
        if any(s in k for s in ["isbn", "book:isbn", "og:isbn"]):
            content = m.get("content") or ""
            if content:
                found.add(content)

    # 3) Visible text scan
    texts = soup.find_all(text=True)
    for t in texts:
        # Skip scripts/styles
        if t.parent and t.parent.name in ("script", "style", "noscript"):
            continue
        for match in CANDIDATE_ISBN_PATTERN.findall(t):
            found.add(match)

    # Normalize and validate
    normalized: Set[str] = set()
    for token in found:
        norm = normalize_isbn(token)
        if norm:
            normalized.add(norm)

    return normalized


# -----------------------------
# Metadata lookups
# -----------------------------
def fetch_metadata_openlibrary(isbn13: str, timeout: int = 12) -> Optional[Dict]:
    url = "https://openlibrary.org/api/books"
    params = {"bibkeys": f"ISBN:{isbn13}", "format": "json", "jscmd": "data"}
    try:
        r = requests.get(url, params=params, headers=DEFAULT_HEADERS, timeout=timeout)
        if r.status_code != 200:
            return None
        data = r.json()
        item = data.get(f"ISBN:{isbn13}")
        if not item:
            return None
        title = item.get("title")
        authors = ", ".join(a.get("name", "") for a in item.get("authors", []) if a.get("name"))
        publishers = ", ".join(p.get("name", "") if isinstance(p, dict) else str(p) for p in item.get("publishers", []))
        return {
            "title": title,
            "authors": authors or None,
            "publisher": publishers or None,
            "source": "OpenLibrary",
        }
    except Exception:
        return None

def fetch_metadata_google(isbn13: str, timeout: int = 12) -> Optional[Dict]:
    params = {"q": f"isbn:{isbn13}"}
    api_key = os.getenv("GOOGLE_BOOKS_API_KEY")
    if api_key:
        params["key"] = api_key
    try:
        r = requests.get("https://www.googleapis.com/books/v1/volumes", params=params, headers=DEFAULT_HEADERS, timeout=timeout)
        if r.status_code != 200:
            return None
        data = r.json()
        items = data.get("items") or []
        if not items:
            return None
        vi = items[0].get("volumeInfo", {})
        title = vi.get("title")
        authors = ", ".join(vi.get("authors", []))
        publisher = vi.get("publisher")
        return {
            "title": title,
            "authors": authors or None,
            "publisher": publisher or None,
            "source": "GoogleBooks",
        }
    except Exception:
        return None

def fetch_metadata(isbn13: str) -> Dict:
    meta = fetch_metadata_openlibrary(isbn13) or fetch_metadata_google(isbn13) or {}
    # Normalize keys, ensure they exist
    return {
        "title": meta.get("title"),
        "authors": meta.get("authors"),
        "publisher": meta.get("publisher"),
        "source": meta.get("source"),
    }


# -----------------------------
# SQLite persistence
# -----------------------------
SCHEMA_SQL = """
CREATE TABLE IF NOT EXISTS books (
    isbn13 TEXT PRIMARY KEY,
    hyphenated TEXT,
    title TEXT,
    authors TEXT,
    publisher TEXT,
    source TEXT,
    url TEXT,
    fetched_at TEXT
);
"""

UPSERT_SQL = """
INSERT INTO books(isbn13, hyphenated, title, authors, publisher, source, url, fetched_at)
VALUES(?, ?, ?, ?, ?, ?, ?, ?)
ON CONFLICT(isbn13) DO UPDATE SET
    hyphenated=excluded.hyphenated,
    title=COALESCE(excluded.title, books.title),
    authors=COALESCE(excluded.authors, books.authors),
    publisher=COALESCE(excluded.publisher, books.publisher),
    source=COALESCE(excluded.source, books.source),
    url=COALESCE(excluded.url, books.url),
    fetched_at=excluded.fetched_at;
"""

def get_db(conn_str: str = "books.db") -> sqlite3.Connection:
    conn = sqlite3.connect(conn_str)
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("PRAGMA synchronous=NORMAL;")
    conn.execute(SCHEMA_SQL)
    return conn

def save_record(
    conn: sqlite3.Connection,
    isbn13: str,
    hyphenated: str,
    meta: Dict,
    url: Optional[str] = None,
) -> None:
    conn.execute(
        UPSERT_SQL,
        (
            isbn13,
            hyphenated,
            meta.get("title"),
            meta.get("authors"),
            meta.get("publisher"),
            meta.get("source"),
            url,
            datetime.utcnow().isoformat(timespec="seconds"),
        ),
    )
    conn.commit()


# -----------------------------
# Pipeline
# -----------------------------
def process_urls(urls: Iterable[str], sleep_between: float = 0.5, persist_db: bool = True) -> List[Dict]:
    results: List[Dict] = []
    conn = get_db() if persist_db else None

    for url in urls:
        print(f"[INFO] Processing {url}")
        html = fetch_html(url)
        if not html:
            continue

        isbns = extract_isbns_from_html(html)
        if not isbns:
            print(f"[INFO] No ISBNs found on {url}")
            time.sleep(sleep_between)
            continue

        for isbn13 in sorted(isbns):
            try:
                hyph = hyphenate_isbn13(isbn13)
            except Exception:
                hyph = isbn13  # fallback to plain

            meta = fetch_metadata(isbn13)
            record = {
                "isbn13": isbn13,
                "hyphenated": hyph,
                "title": meta.get("title"),
                "authors": meta.get("authors"),
                "publisher": meta.get("publisher"),
                "source": meta.get("source"),
                "url": url,
            }
            results.append(record)

            if conn:
                save_record(conn, isbn13, hyph, meta, url=url)

            print(f"  - {hyph} | {meta.get('title') or 'Unknown title'}")

        time.sleep(sleep_between)

    if conn:
        conn.close()
    return results


# -----------------------------
# Example usage
# -----------------------------
if __name__ == "__main__":
    # Replace with actual book detail pages
    demo_urls = [
        "https://www.penguinrandomhouse.com/books/318142/the-martian-by-andy-weir/",
        "https://www.harpercollins.com/products/dune-frank-herbert",
        "https://www.hachettebookgroup.com/titles/matt-haig/the-midnight-library/9780525559474/",
        "https://www.indiebound.org/book/9780593139134",
        "https://openlibrary.org/books/OL26438602M/The_Testaments",
    ]
    process_urls(demo_urls)
------------------------------------------------------------

PROMPT 348:
"Generate Python code that imports the necessary libraries for building high-quality datasets and computer vision models, such as NumPy, Pillow, and OpenCV, and creates a function that loads, preprocesses, and splits an image dataset into training, validation, and test sets using FiftyOne's data augmentation techniques."
RESPONSE:
# Python code to load, preprocess, augment, and split an image dataset with FiftyOne

import os
import glob
import random
from pathlib import Path
from typing import Tuple, Dict

import numpy as np
from PIL import Image, ImageOps
import cv2

import fiftyone as fo
import fiftyone.types as fot

import imgaug.augmenters as iaa


def prepare_image_dataset_with_fiftyone(
    dataset_dir: str,
    dataset_name: str = "my_image_dataset",
    output_dir: str = None,
    image_exts: Tuple[str, ...] = (".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff"),
    resize: Tuple[int, int] = (224, 224),
    augment: bool = True,
    num_aug_per_sample: int = 2,
    val_frac: float = 0.1,
    test_frac: float = 0.1,
    seed: int = 51,
) -> Dict[str, fo.core.collections.SampleCollection]:
    """
    Loads images from a folder into a FiftyOne dataset, preprocesses them
    (resize, color normalization-friendly RGB), performs data augmentation
    using an imgaug pipeline (the stack that FiftyOne integrates with for
    augmentations), writes processed copies to disk, imports them back into
    FiftyOne, and splits into train/val/test views.

    Args:
        dataset_dir: Root directory containing images (recursively).
        dataset_name: Name for the FiftyOne dataset to create/overwrite.
        output_dir: Where to write preprocessed/augmented images. Defaults
            to "<dataset_dir>/processed_fo".
        image_exts: File extensions to include.
        resize: Target (width, height) resize.
        augment: Whether to create augmented variants.
        num_aug_per_sample: How many augmented variants per image.
        val_frac: Fraction of images for validation set.
        test_frac: Fraction of images for test set.
        seed: Random seed for reproducibility.

    Returns:
        A dict with:
            - "dataset": the full FiftyOne dataset
            - "train": view tagged 'train'
            - "val": view tagged 'val'
            - "test": view tagged 'test'
    """
    random.seed(seed)
    np.random.seed(seed)

    if output_dir is None:
        output_dir = os.path.join(dataset_dir, "processed_fo")

    preproc_dir = os.path.join(output_dir, "images")
    os.makedirs(preproc_dir, exist_ok=True)

    # Define augmentation pipeline (imgaug; same library FiftyOne integrates with)
    aug_seq = iaa.Sequential(
        [
            iaa.Fliplr(0.5),
            iaa.Flipud(0.2),
            iaa.Affine(rotate=(-15, 15), scale=(0.9, 1.1), translate_percent={"x": (-0.05, 0.05), "y": (-0.05, 0.05)}),
            iaa.Sometimes(0.5, iaa.GaussianBlur((0.0, 1.0))),
            iaa.Sometimes(0.5, iaa.LinearContrast((0.8, 1.2))),
            iaa.Sometimes(0.5, iaa.AddToHueAndSaturation((-10, 10))),
        ]
    )

    # Gather images
    image_paths = []
    for ext in image_exts:
        image_paths.extend(glob.glob(os.path.join(dataset_dir, "**", f"*{ext}"), recursive=True))
    image_paths = sorted(set(image_paths))

    if not image_paths:
        raise ValueError(f"No images with extensions {image_exts} found under {dataset_dir}")

    # Helper to save RGB numpy array as image using OpenCV (expects BGR)
    def save_rgb_np_as_image(arr_rgb: np.ndarray, out_path: str, quality: int = 95):
        os.makedirs(os.path.dirname(out_path), exist_ok=True)
        arr_bgr = cv2.cvtColor(arr_rgb, cv2.COLOR_RGB2BGR)
        ext = Path(out_path).suffix.lower()
        if ext in (".jpg", ".jpeg"):
            cv2.imwrite(out_path, arr_bgr, [int(cv2.IMWRITE_JPEG_QUALITY), quality])
        else:
            cv2.imwrite(out_path, arr_bgr)

    # Preprocess + augment, write to disk under preproc_dir preserving relative structure
    processed_filepaths = []

    for img_path in image_paths:
        try:
            # Load with Pillow to respect EXIF orientation, convert to RGB
            with Image.open(img_path) as im:
                im = ImageOps.exif_transpose(im).convert("RGB")
                # Resize (width, height)
                im = im.resize(resize, Image.BILINEAR)
                arr = np.array(im)
        except Exception as e:
            print(f"[WARN] Skipping unreadable image: {img_path} ({e})")
            continue

        rel = os.path.relpath(img_path, dataset_dir)
        base, _ = os.path.splitext(rel)
        # Save the preprocessed "clean" image
        out_clean = os.path.join(preproc_dir, base + ".jpg")
        save_rgb_np_as_image(arr, out_clean)
        processed_filepaths.append(out_clean)

        # Augmentations
        if augment and num_aug_per_sample > 0:
            for k in range(num_aug_per_sample):
                aug_arr = aug_seq(image=arr)
                out_aug = os.path.join(preproc_dir, base + f"_aug{k+1}.jpg")
                save_rgb_np_as_image(aug_arr, out_aug)
                processed_filepaths.append(out_aug)

    # Re/create the FiftyOne dataset from processed images
    if fo.dataset_exists(dataset_name):
        fo.delete_dataset(dataset_name)

    ds = fo.Dataset.from_dir(
        dataset_dir=preproc_dir,
        dataset_type=fot.ImageDirectory,
        name=dataset_name,
    )

    # Split into train/val/test by tagging samples
    ids = ds.values("id")
    rnd = random.Random(seed)
    rnd.shuffle(ids)

    n = len(ids)
    n_test = int(round(n * test_frac))
    n_val = int(round(n * val_frac))
    n_train = n - n_val - n_test
    if n_train < 0:
        raise ValueError("val_frac + test_frac must be <= 0.99")

    train_ids = ids[:n_train]
    val_ids = ids[n_train : n_train + n_val]
    test_ids = ids[n_train + n_val :]

    ds.untag_samples(["train", "val", "test"])
    ds.tag_samples("train", sample_ids=train_ids)
    ds.tag_samples("val", sample_ids=val_ids)
    ds.tag_samples("test", sample_ids=test_ids)

    train_view = ds.match_tags("train")
    val_view = ds.match_tags("val")
    test_view = ds.match_tags("test")

    print(f"Dataset '{dataset_name}': {len(ds)} images")
    print(f"  train: {len(train_view)} | val: {len(val_view)} | test: {len(test_view)}")
    print(f"Processed data directory: {preproc_dir}")

    return {
        "dataset": ds,
        "train": train_view,
        "val": val_view,
        "test": test_view,
    }


# Example usage:
# result = prepare_image_dataset_with_fiftyone(
#     dataset_dir="/path/to/images",
#     dataset_name="my_cv_dataset",
#     output_dir="/path/to/output",
#     resize=(256, 256),
#     augment=True,
#     num_aug_per_sample=2,
#     val_frac=0.1,
#     test_frac=0.1,
#     seed=42,
# )
# session = fo.launch_app(result["dataset"])  # visualize in FiftyOne App
# session.view = result["train"]  # eg, focus on training split

# Notes:
# - This pipeline uses imgaug for augmentation, which is the same library FiftyOne integrates with for data augmentation workflows.
# - For label-preserving augmentations (e.g., bounding boxes, segmentations), you can extend this to use imgaug with label propagation and import labels into FiftyOne in a labeled dataset format.
------------------------------------------------------------

PROMPT 349:
"Generate Python code that imports the itertools library and uses the chain() function to create a generator expression that generates gapless sequences of integer values, starting from a given start value and ending at a given end value, and then uses the list() function to convert the generator expression into a list."
RESPONSE:
import itertools

start = 3
end = 10

# Use itertools.chain with a generator expression of one-element ranges to produce a gapless sequence
gen = itertools.chain(*(range(i, i + 1) for i in range(start, end + 1)))

# Convert the chained iterator into a list
result = list(gen)
print(result)
------------------------------------------------------------

PROMPT 350:
"Generate Python code that imports the necessary modules and sets up an API client for interacting with Tencent Cloud Tkgdq services, including configuring credentials and setting up authentication, creating and managing databases, and performing data processing and analysis tasks using Tencent Cloud's big data technologies."
RESPONSE:
# pip install tencentcloud-sdk-python
import os
import sys
import json
import time
from typing import Optional, Tuple

from tencentcloud.common.credential import (
    Credential,
    STSAssumeRoleCredential,
    CVMRoleCredential,
)
# TKE OIDC role provider is optional; only available in newer SDK versions
try:
    from tencentcloud.common.credential import DefaultTkeOIDCRoleArnProvider
    TKE_OIDC_AVAILABLE = True
except Exception:
    DefaultTkeOIDCRoleArnProvider = None
    TKE_OIDC_AVAILABLE = False

from tencentcloud.common.profile.client_profile import ClientProfile
from tencentcloud.common.profile.http_profile import HttpProfile
from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException

# Big data services commonly used for data processing/analytics on Tencent Cloud
from tencentcloud.dlc.v20210125 import dlc_client, models as dlc_models  # Data Lake Compute (Presto/Trino/Spark)
from tencentcloud.emr.v20190103 import emr_client, models as emr_models  # Elastic MapReduce (Hadoop/Spark)


def get_credentials() -> object:
    """
    Resolve Tencent Cloud credentials using a common precedence:
    1) TKE OIDC role (if running in TKE with OIDC and SDK supports it)
    2) CVM instance role (if running on CVM with a bound role)
    3) STS AssumeRole if ROLE_ARN and base keys are provided
    4) Static secret id/key from environment variables

    Environment variables:
      - TENCENTCLOUD_SECRET_ID
      - TENCENTCLOUD_SECRET_KEY
      - TENCENTCLOUD_SESSION_TOKEN (optional, for temporary creds)
      - TENCENTCLOUD_ROLE_ARN, TENCENTCLOUD_ROLE_SESSION_NAME (optional, for STS assume role)
    """
    # 1) TKE OIDC
    if TKE_OIDC_AVAILABLE:
        try:
            creds = DefaultTkeOIDCRoleArnProvider().get_credentials()
            if creds:
                return creds
        except Exception:
            pass

    # 2) CVM Role
    try:
        return CVMRoleCredential()
    except Exception:
        pass

    # 3) STS AssumeRole
    role_arn = os.getenv("TENCENTCLOUD_ROLE_ARN")
    role_session_name = os.getenv("TENCENTCLOUD_ROLE_SESSION_NAME", "sdk-session")
    secret_id = os.getenv("TENCENTCLOUD_SECRET_ID")
    secret_key = os.getenv("TENCENTCLOUD_SECRET_KEY")
    session_token = os.getenv("TENCENTCLOUD_SESSION_TOKEN")  # optional

    if role_arn and secret_id and secret_key:
        try:
            return STSAssumeRoleCredential(secret_id, secret_key, role_arn, role_session_name)
        except Exception:
            pass

    # 4) Static keys (optionally with session token)
    if secret_id and secret_key:
        if session_token:
            # The basic Credential class does not accept session token; use STS style via environment variable
            # The SDK will detect session token set in env: TENCENTCLOUD_SESSION_TOKEN
            return Credential(secret_id, secret_key)
        return Credential(secret_id, secret_key)

    raise RuntimeError("No Tencent Cloud credentials available. Set environment or configure instance/TKE role.")


def build_client(service: str, region: str, endpoint: Optional[str] = None) -> Tuple[object, ClientProfile]:
    """
    Build a Tencent Cloud API client with sane defaults.

    Args:
      service: service short name, e.g., 'dlc' or 'emr'
      region: region string, e.g., 'ap-guangzhou'
      endpoint: optional override endpoint, e.g., 'dlc.tencentcloudapi.com'
    """
    cred = get_credentials()

    http_profile = HttpProfile()
    http_profile.reqMethod = "POST"
    http_profile.timeout = 60
    if endpoint:
        http_profile.endpoint = endpoint

    client_profile = ClientProfile()
    client_profile.httpProfile = http_profile
    client_profile.signMethod = "HmacSHA256"

    if service == "dlc":
        ep = endpoint or "dlc.tencentcloudapi.com"
        client = dlc_client.DlcClient(cred, region, client_profile)
        client._sdkVersion += "_CUSTOM"
        return client, client_profile

    if service == "emr":
        ep = endpoint or "emr.tencentcloudapi.com"
        client = emr_client.EmrClient(cred, region, client_profile)
        client._sdkVersion += "_CUSTOM"
        return client, client_profile

    raise ValueError(f"Unsupported service: {service}")


# -------- DLC helpers (Data Lake Compute): create/manage databases and run SQL --------

def dlc_run_sql(client: dlc_client.DlcClient, sql: str, database: Optional[str] = None,
                data_engine_name: Optional[str] = None, data_source_name: Optional[str] = None,
                work_group: Optional[str] = None, catalog: Optional[str] = None) -> str:
    """
    Submit a DLC SQL task and return a TaskId for polling results.

    Note: DLC supports Presto/Trino SQL (and Spark SQL for Spark engines).
    At minimum, supply DataEngineName (often required in many environments).

    Args:
      client: DLC client
      sql: SQL statement
      database: default database to run the SQL under
      data_engine_name: DLC compute engine name (e.g., 'DataEngine-xxxx')
      data_source_name: optional DLC datasource connection name
      work_group: optional DLC workgroup name
      catalog: optional catalog (e.g., 'hive')
    """
    req = dlc_models.CreateTasksRequest()
    params = {
        "Tasks": {
            "SQLTask": {
                "SQL": sql
            }
        }
    }
    if database:
        params["DatabaseName"] = database
    if data_engine_name:
        params["DataEngineName"] = data_engine_name
    if data_source_name:
        params["DatasourceConnectionName"] = data_source_name
    if work_group:
        params["WorkGroup"] = work_group
    if catalog:
        params["Catalog"] = catalog

    req.from_json_string(json.dumps(params))
    resp = client.CreateTasks(req)
    # Depending on API version, it might be TaskId or TaskIdSet; handle both
    task_id = getattr(resp, "TaskId", None)
    if not task_id:
        task_id_set = getattr(resp, "TaskIdSet", None)
        if task_id_set and isinstance(task_id_set, (list, tuple)) and len(task_id_set) > 0:
            task_id = task_id_set[0]
    if not task_id:
        raise RuntimeError(f"Unable to obtain TaskId from CreateTasks response: {resp.to_json_string()}")
    return task_id


def dlc_wait_for_result(client: dlc_client.DlcClient, task_id: str, poll_sec: int = 2, timeout_sec: int = 300) -> dict:
    """
    Poll task status until completion and fetch result. Returns raw JSON dict of the result.
    """
    start = time.time()
    status = None
    last_payload = None

    while time.time() - start < timeout_sec:
        # Try DescribeTasks first to get status, then DescribeTaskResult to fetch final data
        try:
            dreq = dlc_models.DescribeTasksRequest()
            dparams = {"TaskId": task_id}
            dreq.from_json_string(json.dumps(dparams))
            dresp = client.DescribeTasks(dreq)
            last_payload = json.loads(dresp.to_json_string())
            # Try to extract a status value robustly
            status = None
            if "TaskSet" in last_payload and last_payload["TaskSet"]:
                status = last_payload["TaskSet"][0].get("State")
            elif "Tasks" in last_payload and last_payload["Tasks"]:
                status = last_payload["Tasks"][0].get("State")

            if status in ("FAILED", "CANCELLED"):
                raise RuntimeError(f"DLC task failed: {json.dumps(last_payload, ensure_ascii=False)}")
            if status in ("SUCCESS", "FINISHED", "SUCCEED"):
                break
        except TencentCloudSDKException as e:
            # Fall through to result fetch below if DescribeTasks is not supported
            pass

        time.sleep(poll_sec)

    # Fetch result (if available)
    try:
        rreq = dlc_models.DescribeTaskResultRequest()
        rparams = {"TaskId": task_id}
        rreq.from_json_string(json.dumps(rparams))
        rresp = client.DescribeTaskResult(rreq)
        return json.loads(rresp.to_json_string())
    except TencentCloudSDKException:
        # Not all tasks have a result set (e.g., DDL). Return last status payload.
        if last_payload:
            return last_payload
        raise RuntimeError("Failed to retrieve DLC task result and no previous status payload available.")


def dlc_create_database(client: dlc_client.DlcClient, db_name: str, data_engine_name: Optional[str] = None) -> None:
    """
    Create a database in DLC via SQL to ensure compatibility across engines.
    """
    sql = f"CREATE DATABASE IF NOT EXISTS {db_name}"
    task_id = dlc_run_sql(client, sql, data_engine_name=data_engine_name)
    dlc_wait_for_result(client, task_id)


def dlc_drop_database(client: dlc_client.DlcClient, db_name: str, data_engine_name: Optional[str] = None, cascade: bool = False) -> None:
    """
    Drop a database in DLC via SQL.
    """
    sql = f"DROP DATABASE IF EXISTS {db_name} {'CASCADE' if cascade else ''}".strip()
    task_id = dlc_run_sql(client, sql, data_engine_name=data_engine_name)
    dlc_wait_for_result(client, task_id)


def dlc_list_databases(client: dlc_client.DlcClient, data_engine_name: Optional[str] = None) -> list:
    """
    List databases by executing SHOW DATABASES and returning the raw rows (if present).
    """
    task_id = dlc_run_sql(client, "SHOW DATABASES", data_engine_name=data_engine_name)
    result = dlc_wait_for_result(client, task_id)
    # Try to normalize result rows if present; otherwise return raw payload
    rows = []
    try:
        # Result format can vary; attempt common shapes
        if "Results" in result and "Rows" in result["Results"]:
            rows = result["Results"]["Rows"]
        elif "Data" in result:
            rows = result["Data"]
        elif "Result" in result and "Rows" in result["Result"]:
            rows = result["Result"]["Rows"]
    except Exception:
        pass
    return rows or [result]


# -------- EMR helpers (optional): query clusters, submit jobs, etc. --------

def emr_list_clusters(client: emr_client.EmrClient, limit: int = 20, offset: int = 0) -> dict:
    """
    List EMR clusters. Returns raw JSON dict.
    """
    req = emr_models.DescribeInstancesRequest()
    params = {"Limit": limit, "Offset": offset}
    req.from_json_string(json.dumps(params))
    resp = client.DescribeInstances(req)
    return json.loads(resp.to_json_string())


def main():
    """
    End-to-end example:
      - Build clients
      - Create and manage a database in DLC
      - Run a sample query
      - Optionally list EMR clusters
    """
    # Note on "Tkgdq": there is no public Tencent Cloud Python SDK module named "tkgdq".
    # If you meant a specific product, confirm its official service name and API version
    # in the Tencent Cloud SDK, then import its <service>_client and models similarly.

    region = os.getenv("TENCENTCLOUD_REGION", "ap-guangzhou")
    dlc_engine = os.getenv("DLC_ENGINE_NAME")  # e.g., "DataEngine-xxxx" or your DLC compute engine name
    database_name = os.getenv("DLC_DATABASE_NAME", "analytics_demo")

    try:
        # Build DLC client
        dlc_cli, _ = build_client("dlc", region)

        # Create a database
        print(f"Creating DLC database: {database_name}")
        dlc_create_database(dlc_cli, database_name, data_engine_name=dlc_engine)
        print("Database ensured.")

        # List databases
        print("Listing DLC databases...")
        db_rows = dlc_list_databases(dlc_cli, data_engine_name=dlc_engine)
        print(json.dumps(db_rows, ensure_ascii=False, indent=2))

        # Run a sample analytical query (adjust to your catalog/tables)
        # Example: simple sanity query
        sample_sql = f"SELECT '{database_name}' AS db, 1 AS sample_metric"
        print(f"Running sample SQL: {sample_sql}")
        task_id = dlc_run_sql(dlc_cli, sample_sql, database=database_name, data_engine_name=dlc_engine)
        result = dlc_wait_for_result(dlc_cli, task_id)
        print("Query result:")
        print(json.dumps(result, ensure_ascii=False, indent=2))

        # Optional: EMR - list clusters
        try:
            emr_cli, _ = build_client("emr", region)
            clusters = emr_list_clusters(emr_cli, limit=10)
            print("EMR clusters (summary):")
            print(json.dumps(clusters, ensure_ascii=False, indent=2))
        except Exception as emr_err:
            print(f"EMR listing skipped or failed: {emr_err}", file=sys.stderr)

    except TencentCloudSDKException as sdk_err:
        print(f"Tencent Cloud SDK error: {sdk_err}", file=sys.stderr)
        sys.exit(1)
    except Exception as err:
        print(f"Unhandled error: {err}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 351:
"Generate Python code that imports the necessary libraries, such as PyTorch and numpy, and defines a class called AudioDataAugmenter that inherits from PyTorch.utils.data.Dataset and implements methods to perform various audio data augmentation techniques, such as time stretching, pitch shifting, and noise injection, on audio data."
RESPONSE:
```python
import os
import random
from typing import List, Optional, Tuple, Union, Any

import numpy as np
import torch
from torch.utils.data import Dataset

# Optional but recommended: librosa for time stretching and pitch shifting
# pip install librosa
import librosa


class AudioDataAugmenter(Dataset):
    """
    A PyTorch Dataset that performs on-the-fly audio data augmentation.

    Supported augmentations:
      - Time stretching
      - Pitch shifting
      - Noise injection

    You can provide either:
      - a list of file paths to audio files (mono or stereo; will be loaded as mono), or
      - a list of (waveform, sample_rate) tuples where waveform is a 1D numpy array.

    Optionally provide labels aligned to each item in 'items'.

    Example:
        items = ["file1.wav", "file2.wav"]
        labels = [0, 1]
        ds = AudioDataAugmenter(
            items,
            labels=labels,
            sample_rate=16000,
            target_num_samples=16000,  # 1 second clips
            p_time_stretch=0.5,
            p_pitch_shift=0.5,
            p_noise=0.5,
            time_stretch_range=(0.9, 1.1),
            pitch_shift_semitones=(-2, 2),
            snr_db_range=(10, 30),
            return_tensor=True,
            seed=42,
        )
        x, y = ds[0]  # x is a torch.FloatTensor [num_samples], y is label

    Notes:
      - If target_num_samples is set, samples are randomly cropped/padded to that length.
      - Augmentations are applied independently with their respective probabilities.
    """

    def __init__(
        self,
        items: List[Union[str, Tuple[np.ndarray, int]]],
        labels: Optional[List[Any]] = None,
        sample_rate: int = 16000,
        target_num_samples: Optional[int] = None,
        # Augmentation probabilities
        p_time_stretch: float = 0.5,
        p_pitch_shift: float = 0.5,
        p_noise: float = 0.5,
        # Augmentation parameter ranges
        time_stretch_range: Tuple[float, float] = (0.8, 1.25),
        pitch_shift_semitones: Tuple[float, float] = (-2.0, 2.0),
        snr_db_range: Tuple[float, float] = (10.0, 30.0),
        # General options
        return_tensor: bool = True,
        seed: Optional[int] = None,
    ):
        self.items = items
        self.labels = labels
        if labels is not None and len(labels) != len(items):
            raise ValueError("labels must be the same length as items")

        self.sample_rate = sample_rate
        self.target_num_samples = target_num_samples

        # Probabilities
        self.p_time_stretch = float(np.clip(p_time_stretch, 0.0, 1.0))
        self.p_pitch_shift = float(np.clip(p_pitch_shift, 0.0, 1.0))
        self.p_noise = float(np.clip(p_noise, 0.0, 1.0))

        # Parameter ranges
        self.time_stretch_range = self._validate_range(time_stretch_range, low=0.1, high=4.0, name="time_stretch_range")
        self.pitch_shift_semitones = self._validate_range(pitch_shift_semitones, name="pitch_shift_semitones")
        self.snr_db_range = self._validate_range(snr_db_range, name="snr_db_range")

        self.return_tensor = return_tensor

        # RNG for reproducibility
        self._py_rng = random.Random(seed) if seed is not None else random
        self._np_rng = np.random.RandomState(seed) if seed is not None else np.random

    def __len__(self) -> int:
        return len(self.items)

    def __getitem__(self, idx: int):
        item = self.items[idx]

        # Load audio
        if isinstance(item, str):
            # file path
            y, sr = self._load_audio(item, self.sample_rate)
        elif isinstance(item, (tuple, list)) and len(item) == 2:
            # (waveform, sr)
            y, sr = item
            if sr != self.sample_rate:
                # Resample to target sample_rate
                y = librosa.resample(y.astype(np.float32), orig_sr=sr, target_sr=self.sample_rate, res_type="kaiser_fast")
                sr = self.sample_rate
        else:
            raise TypeError("items must contain either file paths or (waveform, sample_rate) tuples")

        # Ensure mono as 1D array
        if y.ndim > 1:
            y = librosa.to_mono(y)

        # Apply augmentations
        y = self._apply_augmentations(y, sr)

        # Fix length if requested
        if self.target_num_samples is not None:
            y = self._random_crop_or_pad(y, self.target_num_samples, rng=self._np_rng)

        # To tensor if requested
        if self.return_tensor:
            y_out = torch.from_numpy(y.astype(np.float32))
        else:
            y_out = y.astype(np.float32)

        if self.labels is None:
            return y_out
        else:
            return y_out, self.labels[idx]

    # -----------------------
    # Augmentation operations
    # -----------------------

    def _apply_augmentations(self, y: np.ndarray, sr: int) -> np.ndarray:
        out = y.astype(np.float32, copy=True)

        # Time stretch (changes duration)
        if self._py_rng.random() < self.p_time_stretch:
            rate = self._np_rng.uniform(self.time_stretch_range[0], self.time_stretch_range[1])
            # Avoid near-zero rates
            rate = max(rate, 0.1)
            try:
                out = librosa.effects.time_stretch(out, rate=rate).astype(np.float32)
            except Exception:
                # If something goes wrong, skip this augmentation
                pass

        # Pitch shift (keeps duration)
        if self._py_rng.random() < self.p_pitch_shift:
            n_steps = self._np_rng.uniform(self.pitch_shift_semitones[0], self.pitch_shift_semitones[1])
            try:
                out = librosa.effects.pitch_shift(out, sr=sr, n_steps=n_steps).astype(np.float32)
            except Exception:
                pass

        # Noise injection (SNR-based)
        if self._py_rng.random() < self.p_noise:
            snr_db = self._np_rng.uniform(self.snr_db_range[0], self.snr_db_range[1])
            out = self._add_noise_by_snr(out, snr_db=snr_db, rng=self._np_rng)

        # Final clip to valid range
        out = np.clip(out, -1.0, 1.0)
        return out

    @staticmethod
    def _add_noise_by_snr(y: np.ndarray, snr_db: float, rng: np.random.RandomState) -> np.ndarray:
        eps = 1e-12
        signal_rms = np.sqrt(np.mean(y**2) + eps)

        # If the signal is near silent, inject a small amount of noise anyway
        if signal_rms < 1e-6:
            noise_rms = 1e-4
        else:
            noise_rms = signal_rms / (10.0 ** (snr_db / 20.0))

        noise = rng.normal(0.0, noise_rms, size=y.shape).astype(np.float32)
        return y + noise

    # -----------------------
    # Helpers
    # -----------------------

    @staticmethod
    def _validate_range(rng: Tuple[float, float], low: Optional[float] = None, high: Optional[float] = None, name: str = "range") -> Tuple[float, float]:
        if not (isinstance(rng, (tuple, list)) and len(rng) == 2):
            raise ValueError(f"{name} must be a (min, max) tuple")
        a, b = float(rng[0]), float(rng[1])
        if a > b:
            a, b = b, a
        if low is not None and a < low:
            a = low
        if high is not None and b > high:
            b = high
        return (a, b)

    @staticmethod
    def _load_audio(path: str, sample_rate: int) -> Tuple[np.ndarray, int]:
        if not os.path.exists(path):
            raise FileNotFoundError(f"Audio file not found: {path}")
        # librosa.load returns float32 in [-1, 1]
        y, sr = librosa.load(path, sr=sample_rate, mono=True, res_type="kaiser_fast")
        return y.astype(np.float32), sr

    @staticmethod
    def _random_crop_or_pad(y: np.ndarray, target_len: int, rng: np.random.RandomState) -> np.ndarray:
        cur_len = len(y)
        if cur_len == target_len:
            return y
        if cur_len > target_len:
            start = rng.randint(0, cur_len - target_len + 1)
            return y[start : start + target_len]
        else:
            pad_total = target_len - cur_len
            left = rng.randint(0, pad_total + 1)
            right = pad_total - left
            return np.pad(y, (left, right), mode="constant")


# If you want a quick smoke test, uncomment below:
# if __name__ == "__main__":
#     ds = AudioDataAugmenter(
#         items=["/path/to/audio1.wav", "/path/to/audio2.wav"],
#         sample_rate=16000,
#         target_num_samples=16000,
#         p_time_stretch=0.5,
#         p_pitch_shift=0.5,
#         p_noise=0.5,
#         time_stretch_range=(0.9, 1.1),
#         pitch_shift_semitones=(-2, 2),
#         snr_db_range=(10, 30),
#         seed=123,
#     )
#     x = ds[0]
#     if isinstance(x, tuple):
#         x = x[0]
#     print(x.shape, x.dtype, torch.is_tensor(x))
```
------------------------------------------------------------

PROMPT 352:
"Generate Python code that imports IPython notebooks as modules using the `import_ipynb` function from the `IPython.nbtools` module, like this:\n```\nfrom IPython.nbtools import import_ipynb\n```\nThis will allow you to import an IPython notebook file as a Python module, giving you access to its contents as variables and functions."
RESPONSE:
Note: There is no IPython.nbtools.import_ipynb in modern IPython/Jupyter. To import .ipynb files as modules, use the third‑party package import-ipynb (or nbimporter). Here’s a working approach:

Using import-ipynb
- Install:
  - In a notebook: %pip install import-ipynb
  - In a terminal: pip install import-ipynb

Example usage:
```python
# Optional: add the folder containing your .ipynb to sys.path
import sys
sys.path.append("/path/to/notebooks")  # adjust or remove if not needed

# This registers the .ipynb import hook
import import_ipynb

# Import a notebook named "MyNotebook.ipynb" (module name must be a valid identifier)
import MyNotebook

# Use variables/functions defined in the notebook
print(MyNotebook.SOME_CONSTANT)
result = MyNotebook.some_function(42)
print(result)
```

Alternative: using nbimporter
- Install: pip install nbimporter
- Use:
```python
import sys
sys.path.append("/path/to/notebooks")

import nbimporter
from MyNotebook import some_function, SOME_CONSTANT

print(SOME_CONSTANT)
print(some_function(42))
```

Tips
- The notebook filename (without .ipynb) must be a valid Python module name (e.g., MyNotebook.ipynb → import MyNotebook).
- The notebook must be on sys.path or in the current working directory.
- Importing executes the notebook’s code; only use trusted notebooks.
------------------------------------------------------------

PROMPT 353:
"Generate Python code that imports the necessary libraries and creates a function that takes in a dataset and returns a dictionary with metrics for data quality, such as missing values, invalid values, and inconsistent data, using techniques such as statistical methods and machine learning algorithms to detect anomalies and outliers."
RESPONSE:
import numpy as np
import pandas as pd
from typing import Any, Dict, List, Optional
from collections import defaultdict
import re
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import RobustScaler


def assess_data_quality(
    df: pd.DataFrame,
    schema: Optional[Dict[str, Dict[str, Any]]] = None,
    key_columns: Optional[List[str]] = None,
    sample_size: int = 5,
    max_rows_for_models: int = 50000,
    random_state: int = 42,
) -> Dict[str, Any]:
    """
    Assess data quality for a pandas DataFrame.

    Parameters:
      df: pandas DataFrame to analyze.
      schema: Optional constraints to validate against. Supported keys:
          {
            "ranges": {"col": {"min": <float/int>, "max": <float/int>}},
            "regex": {"col": r"<pattern>"},
            "allowed_values": {"col": {"A", "B", ...}},
            "date_bounds": {"col": {"min": "YYYY-MM-DD", "max": "YYYY-MM-DD"}}
          }
      key_columns: Optional columns that should be unique together.
      sample_size: Number of example indices to return for issues/anomalies.
      max_rows_for_models: Max rows used to train anomaly detectors.
      random_state: Random seed for reproducibility.

    Returns:
      A dictionary of data quality metrics and findings.
    """
    if not isinstance(df, pd.DataFrame):
        raise TypeError("df must be a pandas DataFrame")

    schema = schema or {}
    rng = np.random.default_rng(random_state)

    # Utility sets and helpers
    missing_like_tokens = {
        "", "na", "n/a", "n\\a", "null", "none", "nil", "-", "--", "---", "nan",
        "missing", "unknown", "tbd", "tba", "?", "n.a.", "not available", "not applicable"
    }

    def _to_py_int(x):
        return int(x) if pd.notna(x) else None

    def _to_py_float(x):
        return float(x) if pd.notna(x) else None

    def _norm_str(x):
        if pd.isna(x):
            return x
        s = str(x)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    def _norm_key(s):
        if pd.isna(s):
            return s
        return _norm_str(s).lower()

    def _is_date_like(series: pd.Series, threshold: float = 0.85) -> bool:
        if pd.api.types.is_datetime64_any_dtype(series):
            return True
        if series.dtype == object:
            sample = series.dropna().astype(str).head(200)
            if len(sample) == 0:
                return False
            parsed = pd.to_datetime(sample, errors="coerce", utc=True, infer_datetime_format=True)
            return parsed.notna().mean() >= threshold
        return False

    def _coerce_datetime(series: pd.Series) -> pd.Series:
        if pd.api.types.is_datetime64_any_dtype(series):
            return series
        return pd.to_datetime(series, errors="coerce", utc=True, infer_datetime_format=True)

    def _coerce_numeric(series: pd.Series) -> pd.Series:
        if pd.api.types.is_numeric_dtype(series):
            return series
        return pd.to_numeric(series, errors="coerce")

    # Summary
    column_types = {c: str(dt) for c, dt in df.dtypes.items()}
    n_rows, n_cols = df.shape

    # Missing values (true NaN) and placeholder-missing strings
    missing_stats = {}
    total_missing_cells = 0
    total_cells = int(n_rows * n_cols)

    for col in df.columns:
        s = df[col]
        null_mask = s.isna()

        placeholder_mask = pd.Series(False, index=s.index)
        if s.dtype == object:
            s_norm = s.astype(str).str.strip().str.lower()
            placeholder_mask = s_norm.isin(missing_like_tokens)

        missing_count = int(null_mask.sum())
        placeholder_count = int(placeholder_mask.sum())
        total_missing = missing_count + placeholder_count

        total_missing_cells += total_missing

        missing_stats[col] = {
            "nulls": missing_count,
            "placeholders": placeholder_count,
            "total_missing": total_missing,
            "pct_missing": float(total_missing) / n_rows if n_rows else 0.0,
        }

    missing_overall = {
        "per_column": missing_stats,
        "total_missing_cells": total_missing_cells,
        "total_missing_pct": float(total_missing_cells) / total_cells if total_cells else 0.0,
    }

    # Invalid values according to schema and basic type expectations
    invalid = defaultdict(dict)

    # Pre-compute date-like columns and numeric-like conversions
    date_like_cols = {c for c in df.columns if _is_date_like(df[c])}
    numeric_like_cols = set(df.select_dtypes(include=[np.number]).columns.tolist())

    for col in df.columns:
        s = df[col]
        inv = {}

        # Non-finite (for numeric or coerced numeric)
        if pd.api.types.is_numeric_dtype(s):
            non_finite_mask = ~np.isfinite(s.astype(float))
            inv["non_finite"] = int(non_finite_mask.sum())
            if inv["non_finite"] > 0:
                inv["non_finite_indices_sample"] = list(s.index[non_finite_mask][:sample_size])

        # Strings that can't convert to numeric, for object cols that might be numeric-like
        if s.dtype == object:
            coerced = pd.to_numeric(s, errors="coerce")
            convertible_ratio = coerced.notna().mean() if len(coerced) else 0.0
            if convertible_ratio >= 0.85:
                non_conv_mask = coerced.isna() & s.notna()
                inv["non_convertible_to_numeric"] = int(non_conv_mask.sum())
                if inv["non_convertible_to_numeric"] > 0:
                    inv["non_convertible_to_numeric_indices_sample"] = list(s.index[non_conv_mask][:sample_size])

        # Regex validation
        if "regex" in schema and col in schema["regex"]:
            pattern = schema["regex"][col]
            # Consider only non-missing
            s_str = s.astype(str)
            mask_valid = s_str.str.match(pattern, na=True)
            mism = ~mask_valid & s.notna()
            inv["regex_mismatches"] = int(mism.sum())
            if inv["regex_mismatches"] > 0:
                inv["regex_mismatches_indices_sample"] = list(s.index[mism][:sample_size])

        # Allowed values validation
        if "allowed_values" in schema and col in schema["allowed_values"]:
            allowed = schema["allowed_values"][col]
            # Normalize both sides for fair comparison
            s_norm = s.map(_norm_str)
            allowed_norm = {_norm_str(x) for x in allowed}
            not_allowed_mask = s.notna() & ~s_norm.isin(allowed_norm)
            inv["not_in_allowed_set"] = int(not_allowed_mask.sum())
            if inv["not_in_allowed_set"] > 0:
                inv["not_in_allowed_set_indices_sample"] = list(s.index[not_allowed_mask][:sample_size])

        # Range validation for numeric columns
        if "ranges" in schema and col in schema["ranges"]:
            bounds = schema["ranges"][col]
            sn = _coerce_numeric(s)
            lower = bounds.get("min", None)
            upper = bounds.get("max", None)
            out_mask = pd.Series(False, index=s.index)
            if lower is not None:
                out_mask |= sn < lower
            if upper is not None:
                out_mask |= sn > upper
            out_mask &= sn.notna()
            inv["range_violations"] = int(out_mask.sum())
            if inv["range_violations"] > 0:
                inv["range_violations_indices_sample"] = list(s.index[out_mask][:sample_size])

        # Date parsing and bounds
        if col in date_like_cols:
            sd = _coerce_datetime(s)
            unparseable_mask = s.notna() & sd.isna()
            inv["unparseable_dates"] = int(unparseable_mask.sum())
            if inv["unparseable_dates"] > 0:
                inv["unparseable_dates_indices_sample"] = list(s.index[unparseable_mask][:sample_size])

            if "date_bounds" in schema and col in schema["date_bounds"]:
                db = schema["date_bounds"][col]
                sd2 = sd.dropna()
                lb = pd.to_datetime(db.get("min")) if db.get("min") else None
                ub = pd.to_datetime(db.get("max")) if db.get("max") else None
                out_mask = pd.Series(False, index=s.index)
                if lb is not None:
                    out_mask |= (sd < lb)
                if ub is not None:
                    out_mask |= (sd > ub)
                out_mask &= sd.notna()
                inv["out_of_bounds_dates"] = int(out_mask.sum())
                if inv["out_of_bounds_dates"] > 0:
                    inv["out_of_bounds_dates_indices_sample"] = list(s.index[out_mask][:sample_size])

        if inv:
            invalid[col] = inv

    # Inconsistencies: categorical normalization, mixed types, duplicates
    inconsistencies = {}

    cat_inconsistencies = {}
    for col in df.columns:
        s = df[col]
        if pd.api.types.is_numeric_dtype(s) or (col in date_like_cols):
            continue
        # Treat as categorical-like
        s_non_null = s.dropna().astype(str)
        if len(s_non_null) == 0:
            continue
        # Build normalized mapping
        norm_map = {}
        for val in s_non_null.unique():
            nk = _norm_key(val)
            norm_map.setdefault(nk, set()).add(val)
        # Identify normalized keys mapping to multiple raw variants (e.g., case/space variants)
        problematic = {k: sorted(list(v)) for k, v in norm_map.items() if len(v) > 1}
        if problematic:
            # Return up to N groups to keep output small
            sample_groups = dict(list(problematic.items())[:sample_size])
            cat_inconsistencies[col] = {
                "normalized_groups_with_variants": sample_groups,
                "count_groups_with_variants": len(problematic),
            }
    if cat_inconsistencies:
        inconsistencies["categorical_normalization"] = cat_inconsistencies

    # Mixed types within object columns (numeric-like + text-like)
    mixed_types = {}
    for col in df.select_dtypes(include=["object"]).columns:
        s = df[col].dropna()
        if len(s) == 0:
            continue
        # Heuristic: attempt numeric coercion
        sn = pd.to_numeric(s, errors="coerce")
        num_ratio = sn.notna().mean()
        # Also attempt datetime coercion
        sd = pd.to_datetime(s, errors="coerce", utc=True, infer_datetime_format=True)
        dt_ratio = sd.notna().mean()
        # If both numeric-like and text-like present significantly, mark mixed
        if 0.1 < num_ratio < 0.9 and dt_ratio < 0.5:
            mixed_types[col] = {
                "numeric_like_fraction": float(num_ratio),
                "example_numeric_like": s[sn.notna()].astype(str).head(sample_size).tolist(),
                "example_text_like": s[sn.isna()].astype(str).head(sample_size).tolist(),
            }
        elif 0.1 < dt_ratio < 0.9 and num_ratio < 0.5:
            mixed_types[col] = {
                "datetime_like_fraction": float(dt_ratio),
                "example_datetime_like": s[sd.notna()].astype(str).head(sample_size).tolist(),
                "example_text_like": s[sd.isna()].astype(str).head(sample_size).tolist(),
            }
    if mixed_types:
        inconsistencies["mixed_types"] = mixed_types

    # Duplicates
    dup_info = {}
    dup_info["row_duplicates"] = int(df.duplicated().sum())
    if key_columns:
        cols_present = [c for c in key_columns if c in df.columns]
        if cols_present:
            dup_info["key_duplicates"] = int(df.duplicated(subset=cols_present).sum())
            dup_info["key_columns_used"] = cols_present
        else:
            dup_info["key_columns_used"] = []
    inconsistencies["duplicates"] = dup_info

    # Constant or near-constant columns
    near_constant = {}
    for col in df.columns:
        s = df[col]
        nunique = int(s.nunique(dropna=True))
        if nunique <= 1:
            near_constant[col] = {"unique_values": nunique}
    if near_constant:
        inconsistencies["near_constant_columns"] = near_constant

    # Outliers and anomalies
    outliers = {}
    numeric_df = df.select_dtypes(include=[np.number]).replace([np.inf, -np.inf], np.nan)
    numeric_df = numeric_df.dropna(axis=1, how="all")

    # Univariate outliers: IQR and Z-score
    univ_iqr = {}
    univ_z = {}
    for col in numeric_df.columns:
        s = numeric_df[col].dropna()
        if s.empty:
            continue
        q1 = float(s.quantile(0.25))
        q3 = float(s.quantile(0.75))
        iqr = q3 - q1
        if iqr == 0:
            continue
        lower = q1 - 1.5 * iqr
        upper = q3 + 1.5 * iqr
        mask_iqr = (numeric_df[col] < lower) | (numeric_df[col] > upper)
        cnt_iqr = int(mask_iqr.sum())
        if cnt_iqr > 0:
            univ_iqr[col] = {
                "count": cnt_iqr,
                "pct": float(cnt_iqr) / n_rows if n_rows else 0.0,
                "indices_sample": list(df.index[mask_iqr][:sample_size]),
                "bounds": {"lower": lower, "upper": upper},
            }

        # Z-score
        z = stats.zscore(s, nan_policy="omit")
        if isinstance(z, pd.Series):
            z = z.values
        z_mask_local = np.abs(z) > 3
        # Map back to full index
        z_index = s.index[z_mask_local]
        cnt_z = int(len(z_index))
        if cnt_z > 0:
            univ_z[col] = {
                "count": cnt_z,
                "pct": float(cnt_z) / n_rows if n_rows else 0.0,
                "indices_sample": list(z_index[:sample_size]),
            }

    if univ_iqr:
        outliers["univariate_iqr"] = univ_iqr
    if univ_z:
        outliers["univariate_zscore"] = univ_z

    # Multivariate anomalies: Isolation Forest and LOF on numeric features
    multivar = {}
    if not numeric_df.empty and len(numeric_df.columns) >= 1 and len(numeric_df) >= 10:
        # Scale robustly
        X = numeric_df.copy()
        # Drop rows with all-nan in numeric features for modeling
        X_model = X.dropna(how="any")
        index_map = X_model.index

        # Subsample for performance if needed
        if len(X_model) > max_rows_for_models:
            sample_idx = rng.choice(index_map, size=max_rows_for_models, replace=False)
            X_model = X_model.loc[sample_idx]
            index_map = X_model.index

        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X_model.values)

        # Isolation Forest
        try:
            if_clf = IsolationForest(
                n_estimators=200,
                contamination="auto",
                random_state=random_state,
                n_jobs=-1,
            )
            labels_if = if_clf.fit_predict(X_scaled)
            scores_if = if_clf.decision_function(X_scaled)
            anom_mask_if = labels_if == -1
            anom_indices_if = list(index_map[anom_mask_if])
            multivar["isolation_forest"] = {
                "anomaly_count": int(np.sum(anom_mask_if)),
                "pct": float(np.mean(anom_mask_if)) if len(labels_if) else 0.0,
                "indices_sample": anom_indices_if[:sample_size],
                "score_threshold_info": {
                    "min_score": float(np.min(scores_if)) if len(scores_if) else None,
                    "max_score": float(np.max(scores_if)) if len(scores_if) else None,
                },
            }
        except Exception as e:
            multivar["isolation_forest_error"] = str(e)

        # Local Outlier Factor (only meaningful with enough rows and neighbors)
        try:
            n_samples = len(X_model)
            if n_samples > 25:
                n_neighbors = min(20, n_samples - 1)
                lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination="auto", novelty=False, n_jobs=-1)
                labels_lof = lof.fit_predict(X_scaled)
                anom_mask_lof = labels_lof == -1
                anom_indices_lof = list(index_map[anom_mask_lof])
                multivar["local_outlier_factor"] = {
                    "anomaly_count": int(np.sum(anom_mask_lof)),
                    "pct": float(np.mean(anom_mask_lof)) if len(labels_lof) else 0.0,
                    "indices_sample": anom_indices_lof[:sample_size],
                }
        except Exception as e:
            multivar["local_outlier_factor_error"] = str(e)

    if multivar:
        outliers["multivariate"] = multivar

    # Build final result
    result = {
        "summary": {
            "n_rows": n_rows,
            "n_cols": n_cols,
            "column_types": column_types,
        },
        "missing_values": missing_overall,
        "invalid_values": dict(invalid),
        "inconsistencies": inconsistencies,
        "outliers": outliers,
        "notes": [
            "Placeholders treated as missing: " + ", ".join(sorted(missing_like_tokens)),
            "Univariate IQR uses 1.5*IQR rule; Z-score threshold |z| > 3.",
            "IsolationForest and LOF run on numeric columns after Robust scaling; rows with NaN in numeric columns were excluded for modeling.",
        ],
    }

    return result
------------------------------------------------------------

PROMPT 354:
"Generate Python code that imports the asyncio module and creates a throttler function that takes a callable function and a maximum number of concurrent executions as arguments, and uses asyncio.gather to run the function up to the maximum number of times in parallel, while also providing a way to stop the throttler using a cancelation token."
RESPONSE:
import asyncio
from typing import Awaitable, Callable, Optional, Any, List


class CancellationToken:
    def __init__(self) -> None:
        self._event = asyncio.Event()

    def cancel(self) -> None:
        self._event.set()

    @property
    def cancelled(self) -> bool:
        return self._event.is_set()

    async def wait(self) -> None:
        await self._event.wait()


async def throttler(
    func: Callable[[CancellationToken], Awaitable[Any]],
    max_concurrent: int,
    cancel_token: Optional[CancellationToken] = None,
    *args: Any,
    **kwargs: Any,
) -> List[Any]:
    """
    Run up to `max_concurrent` instances of `func` in parallel.
    `func` must be an async callable that accepts a CancellationToken as its first parameter.
    Pass in a `cancel_token` to stop the throttler from outside by calling `cancel_token.cancel()`.

    Returns a list of results from each concurrent invocation (if they return).
    """
    token = cancel_token or CancellationToken()

    async def runner() -> Any:
        try:
            return await func(token, *args, **kwargs)
        except asyncio.CancelledError:
            # Allow graceful shutdown when tasks are explicitly cancelled
            raise

    tasks = [asyncio.create_task(runner()) for _ in range(max_concurrent)]

    try:
        results = await asyncio.gather(*tasks)
        return results
    finally:
        # Ensure cooperative stop is signalled
        if not token.cancelled:
            token.cancel()
        # And ensure any lingering tasks are cancelled
        for t in tasks:
            if not t.done():
                t.cancel()
        await asyncio.gather(*tasks, return_exceptions=True)


# Example usage
if __name__ == "__main__":
    async def worker(token: CancellationToken) -> None:
        # Do work until cancelled
        while not token.cancelled:
            print("Working...")
            await asyncio.sleep(0.5)
        print("Worker stopping")

    async def main() -> None:
        token = CancellationToken()
        throttler_task = asyncio.create_task(throttler(worker, max_concurrent=3, cancel_token=token))

        # Let it run for a bit
        await asyncio.sleep(2)
        # Stop all workers
        token.cancel()

        await throttler_task

    asyncio.run(main())
------------------------------------------------------------

PROMPT 355:
"Generate Python code that imports the necessary libraries and creates a client object to interact with Google data APIs, such as the Google Sheets API, Google Drive API, or Google Calendar API, by calling various methods and functions provided by the client object to perform operations like creating, reading, updating, and deleting resources."
RESPONSE:
# pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib

import os
from datetime import datetime, timedelta, timezone

from googleapiclient.discovery import build
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow


SCOPES = [
    "https://www.googleapis.com/auth/drive",
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/calendar",
]


def get_credentials():
    """
    Uses OAuth 2.0 for installed applications.
    - Place your OAuth client file as 'credentials.json' in the working directory.
    - The first run will open a browser to authorize and will save 'token.json' for reuse.
    """
    creds = None
    if os.path.exists("token.json"):
        creds = Credentials.from_authorized_user_file("token.json", SCOPES)

    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file("credentials.json", SCOPES)
            creds = flow.run_local_server(port=0)
        with open("token.json", "w") as token:
            token.write(creds.to_json())

    return creds


def main():
    creds = get_credentials()

    # Create client objects (service clients) for each API
    sheets = build("sheets", "v4", credentials=creds)
    drive = build("drive", "v3", credentials=creds)
    calendar = build("calendar", "v3", credentials=creds)

    # ---------------------------
    # Google Sheets API (CRUD)
    # ---------------------------
    print("Sheets: Create")
    sheet = sheets.spreadsheets().create(
        body={"properties": {"title": "API Demo Sheet"}},
        fields="spreadsheetId"
    ).execute()
    spreadsheet_id = sheet["spreadsheetId"]
    print(f"  Created spreadsheetId: {spreadsheet_id}")

    print("Sheets: Update values")
    sheets.spreadsheets().values().update(
        spreadsheetId=spreadsheet_id,
        range="Sheet1!A1:C2",
        valueInputOption="USER_ENTERED",
        body={"values": [["Name", "Qty", "Price"], ["Widget", 4, 19.99]]},
    ).execute()

    print("Sheets: Read values")
    values_resp = sheets.spreadsheets().values().get(
        spreadsheetId=spreadsheet_id, range="Sheet1!A1:C2"
    ).execute()
    print(f"  Values: {values_resp.get('values', [])}")

    # Note: Sheets API doesn't delete spreadsheets; use Drive API to delete the file.
    # print("Sheets: Delete spreadsheet (via Drive)")
    # drive.files().delete(fileId=spreadsheet_id).execute()
    # print("  Deleted spreadsheet")

    # ---------------------------
    # Google Drive API (CRUD)
    # ---------------------------
    print("Drive: Create folder")
    folder = drive.files().create(
        body={"name": "API Demo Folder", "mimeType": "application/vnd.google-apps.folder"},
        fields="id"
    ).execute()
    folder_id = folder["id"]
    print(f"  Created folderId: {folder_id}")

    print("Drive: Create Google Doc in folder")
    doc = drive.files().create(
        body={
            "name": "API Demo Doc",
            "mimeType": "application/vnd.google-apps.document",
            "parents": [folder_id],
        },
        fields="id, name"
    ).execute()
    doc_id = doc["id"]
    print(f"  Created doc: {doc['name']} ({doc_id})")

    print("Drive: Read file metadata")
    doc_meta = drive.files().get(
        fileId=doc_id, fields="id, name, mimeType, parents, createdTime"
    ).execute()
    print(f"  Metadata: {doc_meta}")

    print("Drive: Update (rename) file")
    renamed = drive.files().update(
        fileId=doc_id, body={"name": "API Demo Doc (Renamed)"}, fields="id, name"
    ).execute()
    print(f"  Renamed to: {renamed['name']}")

    # Uncomment to delete created resources
    # print("Drive: Delete file")
    # drive.files().delete(fileId=doc_id).execute()
    # print("Drive: Delete folder")
    # drive.files().delete(fileId=folder_id).execute()

    # ---------------------------
    # Google Calendar API (CRUD)
    # ---------------------------
    print("Calendar: Create event")
    now = datetime.now(timezone.utc)
    start = (now + timedelta(days=1)).replace(microsecond=0)
    end = (start + timedelta(hours=1))

    event_body = {
        "summary": "API Demo Event",
        "description": "Created via Google Calendar API",
        "start": {"dateTime": start.isoformat()},
        "end": {"dateTime": end.isoformat()},
    }
    event = calendar.events().insert(calendarId="primary", body=event_body).execute()
    event_id = event["id"]
    print(f"  Created eventId: {event_id}")

    print("Calendar: Read event")
    got_event = calendar.events().get(calendarId="primary", eventId=event_id).execute()
    print(f"  Event summary: {got_event.get('summary')}")

    print("Calendar: Update event (patch summary)")
    updated = calendar.events().patch(
        calendarId="primary", eventId=event_id, body={"summary": "API Demo Event (Updated)"}
    ).execute()
    print(f"  Updated summary: {updated.get('summary')}")

    # Uncomment to delete the event
    # print("Calendar: Delete event")
    # calendar.events().delete(calendarId="primary", eventId=event_id).execute()

    print("Done.")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 356:
"Generate Python code that imports the required modules and annotates the Chime service client using mypy-boto3-builder as follows:\n```python\nfrom typing import Callable, Dict, List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import TypeAnnotator\n\nannotated_chime_service = TypeAnnotator(Chime=True)(boto3.client('chime'))\n```"
RESPONSE:
# Correct way to get a type-annotated Chime client (mypy-boto3-builder does not provide a runtime TypeAnnotator)

# pip install boto3-stubs[chime]
import boto3
from mypy_boto3_chime import ChimeClient

annotated_chime_service: ChimeClient = boto3.client("chime")
------------------------------------------------------------

PROMPT 357:
"Generate Python code that imports the required modules and annotates the IAM 2.9.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom aiobotocore.services.iam import IAM\nfrom aiobotocore.client import Client\nfrom mypy_boto3_builder import Builder\n\nBuilder(service=IAM, version='2.9.0', client=Client).annotate()\n```"
RESPONSE:
from typing import List
from aiobotocore.services.iam import IAM
from aiobotocore.client import Client
from mypy_boto3_builder import Builder

Builder(service=IAM, version='2.9.0', client=Client).annotate()
------------------------------------------------------------

PROMPT 358:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.kendra service client in version 1.34.0 using mypy-boto3-builder version 7.21.0, like so:\n```python\nfrom typing import TypeVar\nfrom mypy_boto3_builder import botofill\nimport boto3\n\n# Annotate boto3.kendra service client\nclient = boto3.client('kendra', version='1.34.0')\n```"
RESPONSE:
# Typed boto3 Kendra client for boto3==1.34.0 using stubs generated by mypy-boto3-builder==7.21.0
# Requires:
#   pip install "boto3==1.34.0" "mypy-boto3-kendra==1.34.0"
import boto3
from mypy_boto3_kendra import KendraClient

# Create a typed Kendra client (boto3.client does not accept a version argument).
client: KendraClient = boto3.client("kendra")
------------------------------------------------------------

PROMPT 359:
"Generate Python code that uses the `pytest` framework to perform mutation testing on a given Python 3 module, by creating a test file with a test function that calls the `pytest.main()` function and passes it the name of the module to test, and then runs the test using the `--mutate` option to apply random mutations to the code and checks for unexpected failures or errors."
RESPONSE:
Pytest does not include a built-in --mutate option. Below is a minimal pytest plugin that adds a --mutate option and applies simple random AST mutations to a specified module before the test session, then restores the original source afterward. A test harness calls pytest.main() to run tests with this option and checks for unexpected failures or errors.

Save these two files side by side in your project root:

File: conftest.py
- Provides a pytest plugin adding:
  - --mutate to enable mutation
  - --mutate-target to pick the module to mutate (file path or dotted module)
  - --mutation-rate and --mutation-seed for control
- Mutates the target module’s source on disk at session start, restores it at session finish.

Code (Python 3.9+ due to ast.unparse):
import ast
import importlib.util
import os
import random
import sys
from typing import Optional

import pytest


def pytest_addoption(parser):
    group = parser.getgroup("mutation")
    group.addoption(
        "--mutate",
        action="store_true",
        help="Apply random code mutations to the target module before running tests.",
    )
    group.addoption(
        "--mutate-target",
        action="store",
        metavar="MODULE_OR_PATH",
        help="Dotted module name or file path of the module to mutate.",
    )
    group.addoption(
        "--mutation-rate",
        action="store",
        type=float,
        default=0.15,
        help="Probability [0,1] that a mutable AST node will be changed (default: 0.15).",
    )
    group.addoption(
        "--mutation-seed",
        action="store",
        type=int,
        default=None,
        help="Random seed for deterministic mutations.",
    )


class RandomMutator(ast.NodeTransformer):
    def __init__(self, rng: random.Random, rate: float):
        self.rng = rng
        self.rate = max(0.0, min(1.0, rate))

    def _hit(self) -> bool:
        return self.rng.random() < self.rate

    def visit_Compare(self, node: ast.Compare):
        self.generic_visit(node)
        # Flip common comparison operators
        mapping = {
            ast.Lt: ast.Gt,
            ast.Gt: ast.Lt,
            ast.LtE: ast.GtE,
            ast.GtE: ast.LtE,
            ast.Eq: ast.NotEq,
            ast.NotEq: ast.Eq,
            ast.Is: ast.IsNot,
            ast.IsNot: ast.Is,
            ast.In: ast.NotIn,
            ast.NotIn: ast.In,
        }
        new_ops = []
        for op in node.ops:
            if self._hit():
                for k, v in mapping.items():
                    if isinstance(op, k):
                        op = v()
                        break
            new_ops.append(op)
        node.ops = new_ops
        return node

    def visit_BoolOp(self, node: ast.BoolOp):
        self.generic_visit(node)
        if self._hit():
            if isinstance(node.op, ast.And):
                node.op = ast.Or()
            elif isinstance(node.op, ast.Or):
                node.op = ast.And()
        return node

    def visit_BinOp(self, node: ast.BinOp):
        self.generic_visit(node)
        if self._hit():
            if isinstance(node.op, ast.Add):
                node.op = ast.Sub()
            elif isinstance(node.op, ast.Sub):
                node.op = ast.Add()
            elif isinstance(node.op, ast.Mult):
                node.op = ast.Div()
            elif isinstance(node.op, ast.Div):
                node.op = ast.Mult()
            elif isinstance(node.op, ast.FloorDiv):
                node.op = ast.Mult()
            elif isinstance(node.op, ast.Mod):
                node.op = ast.Mult()
        return node

    def visit_UnaryOp(self, node: ast.UnaryOp):
        self.generic_visit(node)
        if self._hit() and isinstance(node.op, ast.Not):
            # Remove logical not
            return ast.copy_location(node.operand, node)
        return node

    def visit_Constant(self, node: ast.Constant):
        # Mutate boolean and small integers
        if self._hit():
            if isinstance(node.value, bool):
                return ast.copy_location(ast.Constant(value=not node.value), node)
            if isinstance(node.value, int):
                delta = -1 if self.rng.random() < 0.5 else 1
                return ast.copy_location(ast.Constant(value=node.value + delta), node)
        return node


class _MutationState:
    def __init__(self):
        self.target_path: Optional[str] = None
        self.original_source: Optional[str] = None


def _resolve_module_to_path(target: str) -> str:
    # If looks like a path and exists
    if target.endswith(".py") and os.path.exists(target):
        return os.path.abspath(target)
    # Try to resolve dotted name
    spec = importlib.util.find_spec(target)
    if spec and spec.origin and spec.origin.endswith(".py") and os.path.exists(spec.origin):
        return os.path.abspath(spec.origin)
    raise pytest.UsageError(
        f"--mutate-target must be a Python module path or dotted module; got: {target}"
    )


def _mutate_source(src: str, filename: str, seed: Optional[int], rate: float) -> str:
    try:
        tree = ast.parse(src, filename=filename)
    except SyntaxError as e:
        raise pytest.UsageError(f"Cannot parse target module for mutation: {filename}\n{e}") from e
    rng = random.Random(seed)
    mutator = RandomMutator(rng, rate)
    mutated = mutator.visit(tree)
    ast.fix_missing_locations(mutated)
    try:
        # Python 3.9+
        new_src = ast.unparse(mutated)
    except Exception as e:
        raise pytest.UsageError(f"Failed to unparse mutated AST for {filename}: {e}") from e
    return new_src


@pytest.hookimpl(tryfirst=True)
def pytest_sessionstart(session):
    cfg = session.config
    if not cfg.getoption("--mutate"):
        return

    target_opt = cfg.getoption("--mutate-target")
    if not target_opt:
        raise pytest.UsageError("--mutate requires --mutate-target=MODULE_OR_PATH")

    target_path = _resolve_module_to_path(target_opt)
    rate = cfg.getoption("--mutation-rate")
    seed = cfg.getoption("--mutation-seed")

    state = _MutationState()
    state.target_path = target_path

    # Read original
    with open(target_path, "r", encoding="utf-8") as f:
        original = f.read()
    state.original_source = original

    mutated = _mutate_source(original, filename=target_path, seed=seed, rate=rate)

    # Write mutated back
    with open(target_path, "w", encoding="utf-8") as f:
        f.write(mutated)

    # Stash in config for restoration
    session.config._mutation_state = state

    tr = session.config.pluginmanager.get_plugin("terminalreporter")
    if tr:
        tr.write_line(
            f"[mutation] Mutated: {os.path.relpath(target_path)} | rate={rate} | seed={seed}",
            yellow=True,
        )


@pytest.hookimpl(trylast=True)
def pytest_sessionfinish(session, exitstatus):
    state: _MutationState = getattr(session.config, "_mutation_state", None)
    if not state or not state.target_path or state.original_source is None:
        return
    try:
        with open(state.target_path, "w", encoding="utf-8") as f:
            f.write(state.original_source)
        tr = session.config.pluginmanager.get_plugin("terminalreporter")
        if tr:
            tr.write_line(f"[mutation] Restored original: {os.path.relpath(state.target_path)}")
    except Exception as e:
        # Best-effort restoration; surface a warning to stderr
        sys.stderr.write(f"[mutation] Failed to restore original source: {e}\n")


File: test_mutation_harness.py
- A test function that invokes pytest.main() with --mutate and the target module, then asserts the run didn’t crash (i.e., exit code not an internal/usage error).
- Set MUTATE_TARGET and TESTS_PATH environment variables or edit the defaults below.

Code:
import os
import pytest


def test_mutation_run_has_no_unexpected_errors(monkeypatch):
    # Configure what to test:
    # - MUTATE_TARGET: dotted module (e.g., "my_pkg.my_module") or file path (e.g., "src/my_pkg/my_module.py")
    # - TESTS_PATH: directory or file containing your real tests for the module
    target = os.environ.get("MUTATE_TARGET", "").strip()
    tests_path = os.environ.get("TESTS_PATH", "tests").strip()

    if not target:
        pytest.skip("Set MUTATE_TARGET env var to the module to mutate (module path or dotted name).")

    # Make sure the inner pytest run does not pick up this harness test to avoid recursion
    # by explicitly pointing to your tests directory/file.
    args = [
        tests_path,
        "--mutate",
        f"--mutate-target={target}",
        "-q",
    ]

    # You can set a deterministic seed if desired:
    seed = os.environ.get("MUTATION_SEED")
    if seed:
        args.append(f"--mutation-seed={int(seed)}")

    # Optionally control mutation rate:
    rate = os.environ.get("MUTATION_RATE")
    if rate:
        args.append(f"--mutation-rate={float(rate)}")

    rc = pytest.main(args)

    # Acceptable exit codes:
    # 0 = all tests passed (all mutants survived or mutations didn’t affect behavior)
    # 1 = some tests failed (mutants killed) -> this is actually desirable for mutation testing
    # Unexpected/undesirable:
    # 2 = interrupted, 3 = internal error, 4 = usage error, 5 = no tests collected
    assert rc in (0, 1), f"pytest exited with unexpected code {rc}; expected 0 or 1"


How to use:
- Put your real tests in the tests directory (or adjust TESTS_PATH).
- Export the module to mutate:
  - On Linux/macOS: export MUTATE_TARGET=your_package.your_module
  - On Windows (PowerShell): $env:MUTATE_TARGET="your_package.your_module"
- Optionally set:
  - MUTATION_SEED=123
  - MUTATION_RATE=0.2
- Run the harness test:
  - python -m pytest -q test_mutation_harness.py
The harness will spawn a new pytest run against your tests, temporarily mutate the specified module, and assert that the run completes without internal or usage errors.
------------------------------------------------------------

PROMPT 360:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.CodeArtifact 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import Optional\nfrom botocore.client import Client\nfrom mypy_boto3_builder import TypeAnnotator\n\nannotator = TypeAnnotator(Client)\nannotator.annotate_module('boto3.codeartifact')\n```"
RESPONSE:
from typing import Optional
import boto3
from mypy_boto3_codeartifact import CodeArtifactClient

# Requires:
# - boto3==1.34.0
# - mypy-boto3-codeartifact (generated by mypy-boto3-builder==7.21.0)
codeartifact_client: CodeArtifactClient = boto3.client("codeartifact")
------------------------------------------------------------

PROMPT 361:
"Generate Python code that imports the necessary libraries and creates a tensor-based unified framework for large-scale data computation, similar to MARS, by defining a set of core components including data ingestion, computation graphs, and optimization modules, and implementing a system for scaling up computations using distributed processing and memory management techniques."
RESPONSE:
import os
import math
import uuid
import tempfile
import shutil
import numpy as np
from dataclasses import dataclass, field
from typing import List, Tuple, Dict, Optional, Callable, Any
from concurrent.futures import ProcessPoolExecutor, as_completed


# --------------- Core graph abstractions ---------------

class OpKind:
    DATA = "data"
    ADD = "add"
    MUL = "mul"
    MATMUL = "matmul"
    MAP = "map"


@dataclass
class Node:
    op: str
    inputs: List["Node"]
    attrs: Dict[str, Any]
    shape: Tuple[int, ...]
    dtype: np.dtype
    name: str = field(default_factory=lambda: f"n_{uuid.uuid4().hex[:8]}")


class Tensor:
    def __init__(self, node: Node):
        self._node = node

    @property
    def node(self) -> Node:
        return self._node

    @property
    def shape(self) -> Tuple[int, ...]:
        return self._node.shape

    @property
    def dtype(self) -> np.dtype:
        return self._node.dtype

    def add(self, other: "Tensor") -> "Tensor":
        self._require_compatible(other)
        node = Node(
            op=OpKind.ADD,
            inputs=[self.node, other.node],
            attrs={},
            shape=self.shape,
            dtype=np.result_type(self.dtype, other.dtype),
        )
        return Tensor(node)

    def mul(self, other: "Tensor") -> "Tensor":
        self._require_compatible(other)
        node = Node(
            op=OpKind.MUL,
            inputs=[self.node, other.node],
            attrs={},
            shape=self.shape,
            dtype=np.result_type(self.dtype, other.dtype),
        )
        return Tensor(node)

    def matmul(self, other: "Tensor") -> "Tensor":
        if len(self.shape) != 2 or len(other.shape) != 2:
            raise ValueError("matmul only supports 2D tensors")
        if self.shape[1] != other.shape[0]:
            raise ValueError("matmul inner dimensions must agree")
        node = Node(
            op=OpKind.MATMUL,
            inputs=[self.node, other.node],
            attrs={},
            shape=(self.shape[0], other.shape[1]),
            dtype=np.result_type(self.dtype, other.dtype),
        )
        return Tensor(node)

    def map(self, ufunc: Callable[[np.ndarray], np.ndarray], name: Optional[str] = None) -> "Tensor":
        node = Node(
            op=OpKind.MAP,
            inputs=[self.node],
            attrs={"ufunc": ufunc, "map_name": name or getattr(ufunc, "__name__", "map")},
            shape=self.shape,
            dtype=self.dtype,
        )
        return Tensor(node)

    def _require_compatible(self, other: "Tensor"):
        if self.shape != other.shape:
            raise ValueError(f"Incompatible shapes for elementwise op: {self.shape} vs {other.shape}")


# --------------- Data ingestion ---------------

class Ingestor:
    def __init__(self, workspace: Optional[str] = None):
        self.workspace = workspace or tempfile.mkdtemp(prefix="tensor_ws_")

    def from_numpy(self, arr: np.ndarray, name: Optional[str] = None) -> Tensor:
        arr = np.asarray(arr)
        path = os.path.join(self.workspace, f"{name or 'tensor'}_{uuid.uuid4().hex}.mmap")
        # Persist as a single memmap file
        mm = np.memmap(path, dtype=arr.dtype, mode="w+", shape=arr.shape)
        mm[:] = arr[:]
        mm.flush()
        del mm
        node = Node(
            op=OpKind.DATA,
            inputs=[],
            attrs={"path": path},
            shape=arr.shape,
            dtype=arr.dtype,
            name=name or "data",
        )
        return Tensor(node)

    def from_csv(self, csv_path: str, dtype: np.dtype = np.float64, delimiter: str = ",",
                 has_header: bool = False, name: Optional[str] = None) -> Tensor:
        # Pass 1: determine shape
        n_rows = 0
        n_cols = None
        with open(csv_path, "r", encoding="utf-8") as f:
            for i, line in enumerate(f):
                if i == 0 and has_header:
                    continue
                line = line.strip()
                if not line:
                    continue
                parts = line.split(delimiter)
                n_cols = n_cols or len(parts)
                if len(parts) != n_cols:
                    raise ValueError("Inconsistent number of columns in CSV")
                n_rows += 1
        if n_cols is None:
            raise ValueError("CSV empty")
        shape = (n_rows, n_cols)
        path = os.path.join(self.workspace, f"{name or 'csv'}_{uuid.uuid4().hex}.mmap")
        mm = np.memmap(path, dtype=dtype, mode="w+", shape=shape)

        # Pass 2: load data in streaming fashion
        r = 0
        with open(csv_path, "r", encoding="utf-8") as f:
            for i, line in enumerate(f):
                if i == 0 and has_header:
                    continue
                line = line.strip()
                if not line:
                    continue
                parts = line.split(delimiter)
                mm[r, :] = np.asarray(parts, dtype=dtype)
                r += 1
        mm.flush()
        del mm

        node = Node(
            op=OpKind.DATA,
            inputs=[],
            attrs={"path": path},
            shape=shape,
            dtype=dtype,
            name=name or "csv",
        )
        return Tensor(node)


# --------------- Planning and optimization ---------------

@dataclass
class ChunkPlan:
    chunk_shape: Tuple[int, ...]
    # For matmul plans, we need partition along reduction axis too
    k_block: Optional[int] = None


def choose_chunk_shape(shape: Tuple[int, ...], dtype: np.dtype, target_chunk_bytes: int) -> Tuple[int, ...]:
    """
    Choose a uniform chunk shape that is close to target_chunk_bytes without exceeding it.
    """
    itemsize = np.dtype(dtype).itemsize
    if not shape:
        return ()
    dims = list(shape)
    # Start with full shape and scale down
    chunk = list(dims)
    def chunk_bytes(c):
        n = 1
        for x in c:
            n *= max(1, x)
        return n * itemsize

    if chunk_bytes(chunk) <= target_chunk_bytes:
        return tuple(chunk)

    # Iteratively halve the largest dimension until we meet the budget
    while chunk_bytes(chunk) > target_chunk_bytes:
        # find largest dimension to split
        idx = max(range(len(chunk)), key=lambda i: chunk[i])
        if chunk[idx] <= 1:
            break
        chunk[idx] = math.ceil(chunk[idx] / 2)
    # Ensure at least size 1 for all dims
    chunk = [max(1, x) for x in chunk]
    return tuple(chunk)


def plan_tensor(node: Node, target_chunk_bytes: int = 64 * 1024 * 1024) -> ChunkPlan:
    if node.op == OpKind.MATMUL:
        # Plan chunks for output (m, n) and k-block
        m, n = node.shape
        a, b = node.inputs
        k = a.shape[1]
        # Heuristic: choose tile such that (m_tile * k_block + k_block * n_tile + m_tile * n_tile) * itemsize ~= target
        item = np.dtype(node.dtype).itemsize
        # Start with sqrt-tiling
        base = int(math.sqrt(target_chunk_bytes / item))
        m_tile = max(1, min(m, base))
        n_tile = max(1, min(n, base))
        # pick k_block small enough to fit
        k_block = max(1, min(k, max(1, target_chunk_bytes // (item * (m_tile + n_tile + max(1, m_tile * n_tile // max(1, base)))))))
        return ChunkPlan(chunk_shape=(m_tile, n_tile), k_block=k_block)
    else:
        return ChunkPlan(chunk_shape=choose_chunk_shape(node.shape, node.dtype, target_chunk_bytes))


# --------------- Task and tiling ---------------

@dataclass
class SliceSpec:
    slices: Tuple[slice, ...]


@dataclass
class Task:
    kind: str  # 'elementwise' or 'matmul' or 'map'
    out_path: str
    out_slices: SliceSpec
    out_shape: Tuple[int, ...]
    dtype: np.dtype
    # For elementwise/add/mul/map
    inputs: List[Tuple[str, SliceSpec]] = field(default_factory=list)
    # For matmul, we need A and B and reduction tiling
    A_path: Optional[str] = None
    B_path: Optional[str] = None
    A_islices: Optional[List[SliceSpec]] = None  # list over k-blocks
    B_jslices: Optional[List[SliceSpec]] = None  # list over k-blocks
    map_func_name: Optional[str] = None  # for MAP tasks
    # We cannot serialize functions easily; we will reference by name from a registry.


# --------------- Storage and workspace management ---------------

class Storage:
    def __init__(self, workspace: Optional[str] = None):
        self.workspace = workspace or tempfile.mkdtemp(prefix="tensor_ws_")

    def path_for(self, name: str, shape: Tuple[int, ...], dtype: np.dtype) -> str:
        return os.path.join(self.workspace, f"{name}_{uuid.uuid4().hex}.mmap")

    def allocate_memmap(self, path: str, shape: Tuple[int, ...], dtype: np.dtype) -> np.memmap:
        return np.memmap(path, dtype=dtype, mode="w+", shape=shape)

    def open_memmap(self, path: str, shape: Tuple[int, ...], dtype: np.dtype, mode: str = "r+") -> np.memmap:
        return np.memmap(path, dtype=dtype, mode=mode, shape=shape)

    def cleanup(self):
        try:
            shutil.rmtree(self.workspace)
        except Exception:
            pass


# --------------- Function registry for MAP ops ---------------

_MAP_REGISTRY: Dict[str, Callable[[np.ndarray], np.ndarray]] = {}


def register_map(func: Callable[[np.ndarray], np.ndarray], name: Optional[str] = None) -> str:
    key = name or getattr(func, "__name__", f"map_{uuid.uuid4().hex[:6]}")
    _MAP_REGISTRY[key] = func
    return key


# --------------- Tiling/planning of graph into tasks ---------------

def topo_sort(node: Node) -> List[Node]:
    visited = set()
    order = []

    def dfs(n: Node):
        if n in visited:
            return
        visited.add(n)
        for inp in n.inputs:
            dfs(inp)
        order.append(n)

    # Since Node is not hashable by default, define identity-based set
    visited_nodes = set()

    def dfs2(n: Node):
        ident = id(n)
        if ident in visited_nodes:
            return
        visited_nodes.add(ident)
        for inp in n.inputs:
            dfs2(inp)
        order.append(n)

    dfs2(node)
    return order


def make_slices(start: int, size: int, limit: int) -> slice:
    end = min(limit, start + size)
    return slice(start, end)


def tile_elementwise(out_path: str, plan: ChunkPlan, node: Node, input_paths: List[str]) -> List[Task]:
    # Assume aligned shapes across inputs and outputs
    shape = node.shape
    chunk = plan.chunk_shape
    ranges = [list(range(0, dim, c)) for dim, c in zip(shape, chunk)]
    tasks: List[Task] = []
    for idxs in np.ndindex(*(len(r) for r in ranges)):
        slices = tuple(make_slices(ranges[d][idxs[d]], chunk[d], shape[d]) for d in range(len(shape)))
        in_specs = [(p, SliceSpec(slices)) for p in input_paths]
        tasks.append(Task(
            kind="elementwise" if node.op in (OpKind.ADD, OpKind.MUL) else "map",
            out_path=out_path,
            out_slices=SliceSpec(slices),
            out_shape=shape,
            dtype=node.dtype,
            inputs=in_specs,
            map_func_name=node.attrs.get("map_name") if node.op == OpKind.MAP else None
        ))
    return tasks


def tile_matmul(out_path: str, plan: ChunkPlan, node: Node, A_path: str, B_path: str) -> List[Task]:
    m, n = node.shape
    a, b = node.inputs
    k_dim = a.shape[1]
    m_tile, n_tile = plan.chunk_shape
    k_block = plan.k_block or max(1, min(256, k_dim))  # fallback block
    m_ranges = list(range(0, m, m_tile))
    n_ranges = list(range(0, n, n_tile))
    k_ranges = list(range(0, k_dim, k_block))
    tasks: List[Task] = []
    for i0 in m_ranges:
        for j0 in n_ranges:
            out_slices = SliceSpec((make_slices(i0, m_tile, m), make_slices(j0, n_tile, n)))
            A_islices = []
            B_jslices = []
            for k0 in k_ranges:
                A_islices.append(SliceSpec((make_slices(i0, m_tile, m), make_slices(k0, k_block, k_dim))))
                B_jslices.append(SliceSpec((make_slices(k0, k_block, k_dim), make_slices(j0, n_tile, n))))
            tasks.append(Task(
                kind="matmul",
                out_path=out_path,
                out_slices=out_slices,
                out_shape=node.shape,
                dtype=node.dtype,
                A_path=A_path,
                B_path=B_path,
                A_islices=A_islices,
                B_jslices=B_jslices
            ))
    return tasks


def build_execution_plan(root: Node, storage: Storage,
                         target_chunk_bytes: int = 64 * 1024 * 1024) -> Tuple[str, List[Task], Dict[int, str]]:
    """
    Returns:
      - output memmap path for the root
      - list of tasks in topological order by layers
      - map from id(node) to its materialized path
    """
    order = topo_sort(root)
    node_to_path: Dict[int, str] = {}
    tasks: List[Task] = []

    for node in order:
        if node.op == OpKind.DATA:
            node_to_path[id(node)] = node.attrs["path"]
            continue

        plan = plan_tensor(node, target_chunk_bytes)
        out_path = storage.path_for(node.name or "out", node.shape, node.dtype)
        # allocate empty file (memmap) for output
        mm = storage.allocate_memmap(out_path, node.shape, node.dtype)
        mm.flush()
        del mm

        if node.op in (OpKind.ADD, OpKind.MUL):
            input_paths = [node_to_path[id(inp)] for inp in node.inputs]
            tasks.extend(tile_elementwise(out_path, plan, node, input_paths))

        elif node.op == OpKind.MAP:
            # Register the map function and store its name
            func = node.attrs.get("ufunc")
            if func is None:
                raise ValueError("MAP node missing ufunc")
            map_name = register_map(func, node.attrs.get("map_name"))
            node.attrs["map_name"] = map_name
            input_paths = [node_to_path[id(node.inputs[0])]]
            tasks.extend(tile_elementwise(out_path, plan, node, input_paths))

        elif node.op == OpKind.MATMUL:
            A_path = node_to_path[id(node.inputs[0])]
            B_path = node_to_path[id(node.inputs[1])]
            tasks.extend(tile_matmul(out_path, plan, node, A_path, B_path))
        else:
            raise NotImplementedError(f"Unsupported op: {node.op}")
        node_to_path[id(node)] = out_path

    return node_to_path[id(root)], tasks, node_to_path


# --------------- Worker execution ---------------

def _exec_elementwise(task: Task):
    assert len(task.inputs) >= 1
    out = np.memmap(task.out_path, dtype=task.dtype, mode="r+", shape=task.out_shape)
    # Prepare views
    views = []
    for path, spec in task.inputs:
        shape = out.shape  # assume same shape
        arr = np.memmap(path, dtype=task.dtype, mode="r", shape=shape)
        v = arr[spec.slices.slices]
        views.append(v)
    region = out[task.out_slices.slices]
    if task.kind == "elementwise":
        if len(views) != 2:
            raise ValueError("elementwise task expects 2 inputs")
        a, b = views
        if a.shape != b.shape or a.shape != region.shape:
            # allow numpy broadcasting of region shape if applicable
            region[:] = (a + b) if np.array_equal(a, b) else (a + b)
        else:
            # kind derived by actual operation is not passed; infer by data equality? Instead we compute based on shapes
            # But we need to know whether it's add or mul. We can infer by checking content? Not robust.
            # Simpler: use region += b or region *= b depends; but we cannot know.
            # We'll compute using addition if values sum to zero? Not feasible.
            # Instead, encode op via map_func_name None and inputs len=2 implies ADD; use different 'kind' per op.
            pass
    if task.kind == "elementwise":
        # Determine actual op by dispatching based on number of inputs and shape; we need explicit op info.
        # To resolve, encode op in map_func_name as special tokens when elementwise: 'add' / 'mul'
        raise RuntimeError("Internal: elementwise task missing explicit op")
    elif task.kind == "map":
        func = _MAP_REGISTRY.get(task.map_func_name)
        if func is None:
            raise RuntimeError(f"Map function {task.map_func_name} not found in registry")
        a = views[0]
        region[:] = func(a)
    out.flush()
    del out
    for path, spec in task.inputs:
        # Close memmaps by dereferencing
        pass


def _exec_add(task: Task):
    out = np.memmap(task.out_path, dtype=task.dtype, mode="r+", shape=task.out_shape)
    a_path, a_spec = task.inputs[0]
    b_path, b_spec = task.inputs[1]
    a = np.memmap(a_path, dtype=task.dtype, mode="r", shape=out.shape)
    b = np.memmap(b_path, dtype=task.dtype, mode="r", shape=out.shape)
    out_region = out[task.out_slices.slices]
    out_region[:] = a[a_spec.slices] + b[b_spec.slices]
    out.flush()
    del out


def _exec_mul(task: Task):
    out = np.memmap(task.out_path, dtype=task.dtype, mode="r+", shape=task.out_shape)
    a_path, a_spec = task.inputs[0]
    b_path, b_spec = task.inputs[1]
    a = np.memmap(a_path, dtype=task.dtype, mode="r", shape=out.shape)
    b = np.memmap(b_path, dtype=task.dtype, mode="r", shape=out.shape)
    out_region = out[task.out_slices.slices]
    out_region[:] = a[a_spec.slices] * b[b_spec.slices]
    out.flush()
    del out


def _exec_matmul(task: Task):
    out = np.memmap(task.out_path, dtype=task.dtype, mode="r+", shape=task.out_shape)
    A_shape = (task.out_shape[0], task.A_islices[0].slices[1].stop)  # not exact; we'll slice by task
    # Open A and B with their full original shapes can't be inferred here; we need to know input shapes.
    # Alternative: reuse out.shape to open A and B with unknown shape -> not possible.
    # Fix: encode input shapes into task to allow memmap open.
    raise RuntimeError("Internal: matmul task missing input shapes")


# To fix issues above, redefine Task to carry input shapes and op type explicitly

@dataclass
class Task:
    kind: str  # 'add' | 'mul' | 'map' | 'matmul'
    out_path: str
    out_slices: SliceSpec
    out_shape: Tuple[int, ...]
    dtype: np.dtype
    # Elementwise and map
    inputs: List[Tuple[str, SliceSpec, Tuple[int, ...], np.dtype]] = field(default_factory=list)
    map_func_name: Optional[str] = None
    # Matmul
    A_path: Optional[str] = None
    B_path: Optional[str] = None
    A_shape: Optional[Tuple[int, ...]] = None
    B_shape: Optional[Tuple[int, ...]] = None
    A_islices: Optional[List[SliceSpec]] = None
    B_jslices: Optional[List[SliceSpec]] = None


def tile_elementwise(out_path: str, plan: ChunkPlan, node: Node, input_paths: List[str]) -> List[Task]:
    shape = node.shape
    chunk = plan.chunk_shape
    ranges = [list(range(0, dim, c)) for dim, c in zip(shape, chunk)]
    tasks: List[Task] = []
    kind = "map" if node.op == OpKind.MAP else ("add" if node.op == OpKind.ADD else "mul")
    for idxs in np.ndindex(*(len(r) for r in ranges)):
        slices = tuple(make_slices(ranges[d][idxs[d]], chunk[d], shape[d]) for d in range(len(shape)))
        in_specs = [(p, SliceSpec(slices), shape, node.inputs[i].dtype) for i, p in enumerate(input_paths)]
        tasks.append(Task(
            kind=kind,
            out_path=out_path,
            out_slices=SliceSpec(slices),
            out_shape=shape,
            dtype=node.dtype,
            inputs=in_specs,
            map_func_name=node.attrs.get("map_name") if node.op == OpKind.MAP else None
        ))
    return tasks


def tile_matmul(out_path: str, plan: ChunkPlan, node: Node, A_path: str, B_path: str) -> List[Task]:
    m, n = node.shape
    a, b = node.inputs
    k_dim = a.shape[1]
    m_tile, n_tile = plan.chunk_shape
    k_block = plan.k_block or max(1, min(256, k_dim))
    m_ranges = list(range(0, m, m_tile))
    n_ranges = list(range(0, n, n_tile))
    k_ranges = list(range(0, k_dim, k_block))
    tasks: List[Task] = []
    for i0 in m_ranges:
        for j0 in n_ranges:
            out_slices = SliceSpec((make_slices(i0, m_tile, m), make_slices(j0, n_tile, n)))
            A_islices = []
            B_jslices = []
            for k0 in k_ranges:
                A_islices.append(SliceSpec((make_slices(i0, m_tile, m), make_slices(k0, k_block, k_dim))))
                B_jslices.append(SliceSpec((make_slices(k0, k_block, k_dim), make_slices(j0, n_tile, n))))
            tasks.append(Task(
                kind="matmul",
                out_path=out_path,
                out_slices=out_slices,
                out_shape=node.shape,
                dtype=node.dtype,
                A_path=A_path,
                B_path=B_path,
                A_shape=a.shape,
                B_shape=b.shape,
                A_islices=A_islices,
                B_jslices=B_jslices
            ))
    return tasks


def build_execution_plan(root: Node, storage: Storage,
                         target_chunk_bytes: int = 64 * 1024 * 1024) -> Tuple[str, List[Task], Dict[int, str]]:
    order = topo_sort(root)
    node_to_path: Dict[int, str] = {}
    tasks: List[Task] = []

    for node in order:
        if node.op == OpKind.DATA:
            node_to_path[id(node)] = node.attrs["path"]
            continue

        plan = plan_tensor(node, target_chunk_bytes)
        out_path = storage.path_for(node.name or "out", node.shape, node.dtype)
        mm = storage.allocate_memmap(out_path, node.shape, node.dtype)
        mm.flush()
        del mm

        if node.op in (OpKind.ADD, OpKind.MUL):
            input_paths = [node_to_path[id(inp)] for inp in node.inputs]
            tasks.extend(tile_elementwise(out_path, plan, node, input_paths))

        elif node.op == OpKind.MAP:
            func = node.attrs.get("ufunc")
            if func is None:
                raise ValueError("MAP node missing ufunc")
            map_name = register_map(func, node.attrs.get("map_name"))
            node.attrs["map_name"] = map_name
            input_paths = [node_to_path[id(node.inputs[0])]]
            tasks.extend(tile_elementwise(out_path, plan, node, input_paths))

        elif node.op == OpKind.MATMUL:
            A_path = node_to_path[id(node.inputs[0])]
            B_path = node_to_path[id(node.inputs[1])]
            tasks.extend(tile_matmul(out_path, plan, node, A_path, B_path))
        else:
            raise NotImplementedError(f"Unsupported op: {node.op}")
        node_to_path[id(node)] = out_path

    return node_to_path[id(root)], tasks, node_to_path


def exec_task(task: Task):
    if task.kind == "add":
        out = np.memmap(task.out_path, dtype=task.dtype, mode="r+", shape=task.out_shape)
        (a_path, a_spec, a_shape, a_dtype), (b_path, b_spec, b_shape, b_dtype) = task.inputs
        a = np.memmap(a_path, dtype=a_dtype, mode="r", shape=a_shape)
        b = np.memmap(b_path, dtype=b_dtype, mode="r", shape=b_shape)
        out_region = out[task.out_slices.slices]
        out_region[:] = a[a_spec.slices] + b[b_spec.slices]
        out.flush()
        del out
        return

    if task.kind == "mul":
        out = np.memmap(task.out_path, dtype=task.dtype, mode="r+", shape=task.out_shape)
        (a_path, a_spec, a_shape, a_dtype), (b_path, b_spec, b_shape, b_dtype) = task.inputs
        a = np.memmap(a_path, dtype=a_dtype, mode="r", shape=a_shape)
        b = np.memmap(b_path, dtype=b_dtype, mode="r", shape=b_shape)
        out_region = out[task.out_slices.slices]
        out_region[:] = a[a_spec.slices] * b[b_spec.slices]
        out.flush()
        del out
        return

    if task.kind == "map":
        out = np.memmap(task.out_path, dtype=task.dtype, mode="r+", shape=task.out_shape)
        (a_path, a_spec, a_shape, a_dtype) = task.inputs[0]
        a = np.memmap(a_path, dtype=a_dtype, mode="r", shape=a_shape)
        out_region = out[task.out_slices.slices]
        func = _MAP_REGISTRY.get(task.map_func_name)
        if func is None:
            raise RuntimeError(f"Map func not found: {task.map_func_name}")
        out_region[:] = func(a[a_spec.slices])
        out.flush()
        del out
        return

    if task.kind == "matmul":
        out = np.memmap(task.out_path, dtype=task.dtype, mode="r+", shape=task.out_shape)
        A = np.memmap(task.A_path, dtype=task.dtype, mode="r", shape=task.A_shape)
        B = np.memmap(task.B_path, dtype=task.dtype, mode="r", shape=task.B_shape)
        oi, oj = task.out_slices.slices
        # Initialize output tile to zero
        tile = np.zeros((oi.stop - oi.start, oj.stop - oj.start), dtype=task.dtype)
        # Accumulate over k blocks
        for a_spec, b_spec in zip(task.A_islices, task.B_jslices):
            ai, ak = a_spec.slices
            bk, bj = b_spec.slices
            a_block = A[ai, ak]
            b_block = B[bk, bj]
            tile += a_block @ b_block
        out[oi, oj] = tile
        out.flush()
        del out
        return

    raise NotImplementedError(f"Unknown task kind: {task.kind}")


# --------------- Executor ---------------

class Executor:
    def __init__(self, max_workers: Optional[int] = None):
        self.max_workers = max_workers or os.cpu_count() or 1

    def run(self, tasks: List[Task], progress: bool = False):
        with ProcessPoolExecutor(max_workers=self.max_workers) as ex:
            futs = [ex.submit(exec_task, t) for t in tasks]
            if progress:
                for i, fut in enumerate(as_completed(futs), 1):
                    fut.result()
            else:
                for fut in futs:
                    fut.result()


# --------------- Public API ---------------

class Engine:
    def __init__(self, workspace: Optional[str] = None):
        self.storage = Storage(workspace)
        self.ingestor = Ingestor(self.storage.workspace)
        self.executor = Executor()

    def from_numpy(self, arr: np.ndarray, name: Optional[str] = None) -> Tensor:
        return self.ingestor.from_numpy(arr, name)

    def from_csv(self, path: str, dtype: np.dtype = np.float64, delimiter: str = ",",
                 has_header: bool = False, name: Optional[str] = None) -> Tensor:
        return self.ingestor.from_csv(path, dtype, delimiter, has_header, name)

    def compute(self, tensor: Tensor, target_chunk_mb: int = 64, progress: bool = False) -> str:
        out_path, tasks, node_to_path = build_execution_plan(
            tensor.node, self.storage, target_chunk_bytes=target_chunk_mb * 1024 * 1024
        )
        self.executor.run(tasks, progress=progress)
        return out_path

    def load_result(self, path: str, shape: Tuple[int, ...], dtype: np.dtype) -> np.memmap:
        return self.storage.open_memmap(path, shape, dtype, mode="r")

    def cleanup(self):
        self.storage.cleanup()


# --------------- Example usage ---------------

if __name__ == "__main__":
    # Create engine
    eng = Engine()

    try:
        # Ingest some data
        np.random.seed(42)
        A = eng.from_numpy(np.random.rand(2048, 1024).astype(np.float32), name="A")
        B = eng.from_numpy(np.random.rand(1024, 512).astype(np.float32), name="B")
        C = eng.from_numpy(np.random.rand(2048, 512).astype(np.float32), name="C")

        # Build computation: Z = relu(A @ B + 2 * C)
        Z = A.matmul(B).add(C.mul(eng.from_numpy(np.full(C.shape, 2.0, dtype=np.float32), name="two"))).map(
            lambda x: np.maximum(x, 0.0), name="relu"
        )

        # Execute with chunking and distributed processing
        out_path = eng.compute(Z, target_chunk_mb=64, progress=True)
        print(f"Result memmap at: {out_path}")

        # Load a small portion or the whole result as needed
        res = eng.load_result(out_path, Z.shape, Z.dtype)
        print("Result shape:", res.shape, "dtype:", res.dtype)
        print("Sample:", res[0, :5])

    finally:
        # Clean up workspace files
        eng.cleanup()
------------------------------------------------------------

PROMPT 362:
"Generate Python code that imports the required modules and annotates the types for the boto3.AccessAnalyzer 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import Optional\nfrom botocore.client import Client\nfrom mypy_boto3_builder import TypeAnnotator\n\nann = TypeAnnotator(Client)\naccess_analyzer = ann.annotate(boto3.client('access-analyzer'))\n```"
RESPONSE:
from __future__ import annotations

from typing import Optional

import boto3
from mypy_boto3_accessanalyzer import AccessAnalyzerClient
from mypy_boto3_accessanalyzer.literals import AnalyzerTypeType
from mypy_boto3_accessanalyzer.paginator import ListAnalyzersPaginator
from mypy_boto3_accessanalyzer.type_defs import ListAnalyzersResponseTypeDef


# Typed boto3 client for IAM Access Analyzer (boto3 1.34.0, generated by mypy-boto3-builder 7.21.0)
access_analyzer: AccessAnalyzerClient = boto3.client("accessanalyzer")


def list_analyzers(analyzer_type: Optional[AnalyzerTypeType] = None) -> ListAnalyzersResponseTypeDef:
    if analyzer_type is not None:
        return access_analyzer.list_analyzers(type=analyzer_type)
    return access_analyzer.list_analyzers()


def list_analyzer_arns_all_pages() -> list[str]:
    paginator: ListAnalyzersPaginator = access_analyzer.get_paginator("list_analyzers")
    arns: list[str] = []
    for page in paginator.paginate():
        for analyzer in page.get("analyzers", []):
            arns.append(analyzer["arn"])
    return arns
------------------------------------------------------------

PROMPT 363:
"Generate Python code that creates a script called `forensic_artifact_collector.py` that uses the `pydantic` library to define a custom data structure for representing forensic artifacts, and then uses the `psutil` and `pyfakefs` libraries to gather various types of forensic artifacts from a given disk image or live system and store them in the custom data structure for later analysis."
RESPONSE:
#!/usr/bin/env python3
"""
forensic_artifact_collector.py

A small, self-contained forensic artifact collector that:
- Defines Pydantic models to represent forensic artifacts
- Uses psutil to collect live system/process/network/disk information
- Uses pyfakefs to stage a real filesystem (live system or a mounted disk image)
  into a fake filesystem and collect file-based artifacts safely/read-only

Notes and assumptions:
- For "disk image" analysis, this script expects the image to be mounted by the OS
  and accessible as a directory (e.g., /mnt/image or a drive letter/directory on Windows).
  It does not parse raw disk images itself.
- pyfakefs is used to mirror real directories into a fake, read-only filesystem to
  avoid accidental modification and to centralize file access in a controlled context.
- The file collection is intentionally conservative: it scans a bounded set of directories
  and files based on include patterns and limits, to avoid long runtimes and heavy I/O.
"""

from __future__ import annotations

import argparse
import datetime as dt
import hashlib
import json
import os
import platform
import socket
import sys
from pathlib import Path
from typing import Iterable, List, Literal, Optional, Sequence, Tuple

import psutil
from pydantic import BaseModel, Field

# pyfakefs is primarily a testing library. Here, we use it to create a read-only
# mirrored view of selected directories (live root or mounted image).
from pyfakefs.fake_filesystem_unittest import Patcher


# -------------------------------
# Pydantic models for artifacts
# -------------------------------

class ProcessMemory(BaseModel):
    rss: Optional[int] = None
    vms: Optional[int] = None
    shared: Optional[int] = None
    text: Optional[int] = None
    lib: Optional[int] = None
    data: Optional[int] = None
    dirty: Optional[int] = None


class ProcessArtifact(BaseModel):
    pid: int
    ppid: Optional[int] = None
    name: Optional[str] = None
    username: Optional[str] = None
    create_time: Optional[float] = None
    status: Optional[str] = None
    exe: Optional[str] = None
    cmdline: Optional[List[str]] = None
    cwd: Optional[str] = None
    num_threads: Optional[int] = None
    cpu_percent: Optional[float] = None
    nice: Optional[int] = None
    terminal: Optional[str] = None
    open_files_count: Optional[int] = None
    connections_count: Optional[int] = None
    memory: Optional[ProcessMemory] = None


class NetworkConnectionArtifact(BaseModel):
    fd: Optional[int] = None
    family: Optional[str] = None
    type: Optional[str] = None
    laddr_ip: Optional[str] = None
    laddr_port: Optional[int] = None
    raddr_ip: Optional[str] = None
    raddr_port: Optional[int] = None
    status: Optional[str] = None
    pid: Optional[int] = None


class UserSessionArtifact(BaseModel):
    name: Optional[str] = None
    terminal: Optional[str] = None
    host: Optional[str] = None
    started: Optional[float] = None


class DiskPartitionArtifact(BaseModel):
    device: Optional[str] = None
    mountpoint: Optional[str] = None
    fstype: Optional[str] = None
    opts: Optional[str] = None
    total: Optional[int] = None
    used: Optional[int] = None
    free: Optional[int] = None
    percent: Optional[float] = None


class SystemInfoArtifact(BaseModel):
    platform: str
    platform_release: str
    platform_version: str
    machine: str
    hostname: str
    boot_time: Optional[float] = None
    cpu_count_logical: Optional[int] = None
    cpu_count_physical: Optional[int] = None
    load_average_1m: Optional[float] = None
    load_average_5m: Optional[float] = None
    load_average_15m: Optional[float] = None
    virtual_memory_total: Optional[int] = None
    virtual_memory_available: Optional[int] = None
    virtual_memory_percent: Optional[float] = None


class FileArtifact(BaseModel):
    path: str
    size: Optional[int] = None
    mode: Optional[int] = None
    mtime: Optional[float] = None
    uid: Optional[int] = None
    gid: Optional[int] = None
    sha256: Optional[str] = None


class ForensicSnapshot(BaseModel):
    snapshot_id: str = Field(default_factory=lambda: _utc_now().strftime("%Y%m%dT%H%M%SZ"))
    collected_at: str = Field(default_factory=lambda: _utc_now().isoformat())
    source_type: Literal["live", "image"] = "live"
    root_path: Optional[str] = None
    investigator: Optional[str] = None
    hostname: str = Field(default_factory=lambda: socket.gethostname())
    platform: str = Field(default_factory=lambda: platform.platform())
    processes: List[ProcessArtifact] = Field(default_factory=list)
    connections: List[NetworkConnectionArtifact] = Field(default_factory=list)
    users: List[UserSessionArtifact] = Field(default_factory=list)
    disks: List[DiskPartitionArtifact] = Field(default_factory=list)
    system_info: Optional[SystemInfoArtifact] = None
    files: List[FileArtifact] = Field(default_factory=list)
    notes: Optional[str] = None


# -------------------------------
# Utility functions
# -------------------------------

def _utc_now() -> dt.datetime:
    return dt.datetime.now(dt.timezone.utc)


def _safe_get(proc: psutil.Process, attr: str, default=None):
    try:
        return getattr(proc, attr)()
    except Exception:
        return default


def _family_to_str(family) -> str:
    try:
        import socket as _socket
        fammap = {
            _socket.AF_INET: "AF_INET",
            _socket.AF_INET6: "AF_INET6",
            _socket.AF_UNIX: "AF_UNIX",
        }
        return fammap.get(family, str(family))
    except Exception:
        return str(family)


def _type_to_str(socktype) -> str:
    try:
        import socket as _socket
        typemap = {
            _socket.SOCK_STREAM: "SOCK_STREAM",
            _socket.SOCK_DGRAM: "SOCK_DGRAM",
            _socket.SOCK_RAW: "SOCK_RAW",
        }
        return typemap.get(socktype, str(socktype))
    except Exception:
        return str(socktype)


def _hash_file(open_fn, path: str, max_bytes: Optional[int] = None) -> Optional[str]:
    """
    open_fn: a function compatible with builtins.open, possibly coming from pyfakefs patcher
    path: file path to read
    max_bytes: if provided, cap hashing to this many bytes (still returns a hash of read bytes)
    """
    try:
        h = hashlib.sha256()
        with open_fn(path, "rb") as f:
            if max_bytes is None:
                for chunk in iter(lambda: f.read(1024 * 1024), b""):
                    h.update(chunk)
            else:
                remaining = max_bytes
                while remaining > 0:
                    chunk = f.read(min(1024 * 1024, remaining))
                    if not chunk:
                        break
                    h.update(chunk)
                    remaining -= len(chunk)
        return h.hexdigest()
    except Exception:
        return None


# -------------------------------
# Collectors using psutil
# -------------------------------

def collect_system_info() -> SystemInfoArtifact:
    vm = psutil.virtual_memory()
    load1 = load5 = load15 = None
    try:
        load1, load5, load15 = os.getloadavg()
    except Exception:
        pass

    info = SystemInfoArtifact(
        platform=platform.system(),
        platform_release=platform.release(),
        platform_version=platform.version(),
        machine=platform.machine(),
        hostname=socket.gethostname(),
        boot_time=getattr(psutil, "boot_time", lambda: None)(),
        cpu_count_logical=psutil.cpu_count(logical=True),
        cpu_count_physical=psutil.cpu_count(logical=False),
        load_average_1m=load1,
        load_average_5m=load5,
        load_average_15m=load15,
        virtual_memory_total=getattr(vm, "total", None),
        virtual_memory_available=getattr(vm, "available", None),
        virtual_memory_percent=getattr(vm, "percent", None),
    )
    return info


def collect_processes() -> List[ProcessArtifact]:
    results: List[ProcessArtifact] = []
    for proc in psutil.process_iter(attrs=[], ad_value=None):
        try:
            with proc.oneshot():
                p = ProcessArtifact(
                    pid=proc.pid,
                    ppid=_safe_get(proc, "ppid"),
                    name=_safe_get(proc, "name"),
                    username=_safe_get(proc, "username"),
                    create_time=_safe_get(proc, "create_time"),
                    status=_safe_get(proc, "status"),
                    exe=_safe_get(proc, "exe"),
                    cmdline=_safe_get(proc, "cmdline"),
                    cwd=_safe_get(proc, "cwd"),
                    num_threads=_safe_get(proc, "num_threads"),
                    cpu_percent=_safe_get(proc, "cpu_percent"),
                    nice=_safe_get(proc, "nice"),
                    terminal=_safe_get(proc, "terminal"),
                    open_files_count=len(_safe_get(proc, "open_files") or []),
                    connections_count=len(_safe_get(proc, "connections") or []),
                    memory=_proc_memory(proc),
                )
                results.append(p)
        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
            continue
        except Exception:
            continue
    return results


def _proc_memory(proc: psutil.Process) -> Optional[ProcessMemory]:
    try:
        mi = proc.memory_info()
        # Some platforms have only rss/vms; others include more fields
        return ProcessMemory(
            rss=getattr(mi, "rss", None),
            vms=getattr(mi, "vms", None),
            shared=getattr(mi, "shared", None),
            text=getattr(mi, "text", None),
            lib=getattr(mi, "lib", None),
            data=getattr(mi, "data", None),
            dirty=getattr(mi, "dirty", None),
        )
    except Exception:
        return None


def collect_connections(kind: str = "inet") -> List[NetworkConnectionArtifact]:
    # kind = "inet" to focus on IPv4/IPv6 TCP/UDP sockets
    results: List[NetworkConnectionArtifact] = []
    try:
        conns = psutil.net_connections(kind=kind)
    except Exception:
        conns = []
    for c in conns:
        laddr_ip, laddr_port = (None, None)
        raddr_ip, raddr_port = (None, None)
        try:
            if c.laddr:
                laddr_ip, laddr_port = c.laddr.ip if hasattr(c.laddr, "ip") else c.laddr[0], c.laddr.port if hasattr(c.laddr, "port") else c.laddr[1]
            if c.raddr:
                raddr_ip, raddr_port = c.raddr.ip if hasattr(c.raddr, "ip") else c.raddr[0], c.raddr.port if hasattr(c.raddr, "port") else c.raddr[1]
        except Exception:
            pass

        results.append(
            NetworkConnectionArtifact(
                fd=getattr(c, "fd", None),
                family=_family_to_str(getattr(c, "family", None)),
                type=_type_to_str(getattr(c, "type", None)),
                laddr_ip=laddr_ip,
                laddr_port=laddr_port,
                raddr_ip=raddr_ip,
                raddr_port=raddr_port,
                status=getattr(c, "status", None),
                pid=getattr(c, "pid", None),
            )
        )
    return results


def collect_users() -> List[UserSessionArtifact]:
    results: List[UserSessionArtifact] = []
    try:
        for u in psutil.users():
            results.append(
                UserSessionArtifact(
                    name=getattr(u, "name", None),
                    terminal=getattr(u, "terminal", None),
                    host=getattr(u, "host", None),
                    started=getattr(u, "started", None),
                )
            )
    except Exception:
        pass
    return results


def collect_disks() -> List[DiskPartitionArtifact]:
    results: List[DiskPartitionArtifact] = []
    try:
        parts = psutil.disk_partitions(all=True)
    except Exception:
        parts = []

    for p in parts:
        usage = None
        try:
            usage = psutil.disk_usage(p.mountpoint)
        except Exception:
            pass

        results.append(
            DiskPartitionArtifact(
                device=getattr(p, "device", None),
                mountpoint=getattr(p, "mountpoint", None),
                fstype=getattr(p, "fstype", None),
                opts=getattr(p, "opts", None),
                total=getattr(usage, "total", None) if usage else None,
                used=getattr(usage, "used", None) if usage else None,
                free=getattr(usage, "free", None) if usage else None,
                percent=getattr(usage, "percent", None) if usage else None,
            )
        )
    return results


# -------------------------------
# File collection with pyfakefs
# -------------------------------

class FileCollector:
    """
    Uses pyfakefs to mirror selected directories into a fake, read-only filesystem,
    then walks and collects FileArtifact objects (metadata and hash) for matched files.
    """

    def __init__(
        self,
        root_path: Optional[Path],
        include_dirs: Sequence[Path],
        include_globs: Sequence[str],
        max_files: int = 500,
        max_file_size_bytes: int = 10 * 1024 * 1024,  # 10 MiB
        hash_max_bytes: Optional[int] = None,  # None = full file hash
        follow_symlinks: bool = False,
    ):
        self.root_path = Path(root_path).resolve() if root_path else None
        self.include_dirs = [Path(d).resolve() for d in include_dirs]
        self.include_globs = list(include_globs)
        self.max_files = max_files
        self.max_file_size_bytes = max_file_size_bytes
        self.hash_max_bytes = hash_max_bytes
        self.follow_symlinks = follow_symlinks

    def collect(self) -> List[FileArtifact]:
        results: List[FileArtifact] = []

        with Patcher() as patcher:
            fs = patcher.fs
            fake_os = patcher.os
            fake_open = patcher.open
            fake_path = patcher.path

            # Mirror desired directories into fake fs
            mirrored_dirs: List[str] = []
            for d in self.include_dirs:
                try:
                    if d.exists():
                        fs.add_real_directory(str(d), read_only=True, lazy_read=True)
                        mirrored_dirs.append(str(d))
                except Exception:
                    continue

            # Walk mirrored dirs and pick files that match globs
            def matches_any_glob(path_str: str) -> bool:
                # Evaluate against provided glob patterns
                # We match on basename and full path to be flexible
                base = fake_path.basename(path_str)
                for g in self.include_globs:
                    try:
                        if fake_path.fnmatch(base, g) or fake_path.fnmatch(path_str, g):
                            return True
                    except Exception:
                        continue
                return False

            count = 0
            seen: set[str] = set()

            for root in mirrored_dirs:
                try:
                    for dirpath, dirnames, filenames in fake_os.walk(root, followlinks=self.follow_symlinks):
                        # Avoid extremely deep recursion
                        if count >= self.max_files:
                            break
                        for fname in filenames:
                            if count >= self.max_files:
                                break
                            fpath = fake_path.join(dirpath, fname)
                            if fpath in seen:
                                continue
                            if not matches_any_glob(fpath):
                                continue
                            try:
                                st = fake_os.lstat(fpath) if not self.follow_symlinks else fake_os.stat(fpath)
                                # Skip if not a regular file
                                if not fake_path.isfile(fpath):
                                    continue
                                size = getattr(st, "st_size", 0)
                                if self.max_file_size_bytes is not None and size is not None and size > self.max_file_size_bytes:
                                    continue
                                sha256 = _hash_file(fake_open, fpath, max_bytes=self.hash_max_bytes)
                                fa = FileArtifact(
                                    path=self._relative_or_abs(fpath),
                                    size=size,
                                    mode=getattr(st, "st_mode", None),
                                    mtime=getattr(st, "st_mtime", None),
                                    uid=getattr(st, "st_uid", None),
                                    gid=getattr(st, "st_gid", None),
                                    sha256=sha256,
                                )
                                results.append(fa)
                                seen.add(fpath)
                                count += 1
                            except Exception:
                                continue
                except Exception:
                    continue

        return results

    def _relative_or_abs(self, fpath: str) -> str:
        if not self.root_path:
            return fpath
        try:
            return str(Path(fpath).resolve().relative_to(self.root_path))
        except Exception:
            return fpath


# -------------------------------
# Defaults and CLI handling
# -------------------------------

def default_include_dirs(root_path: Optional[Path]) -> List[Path]:
    """
    Directories to mirror into fake filesystem for file artifact collection.
    These are conservative defaults and OS-aware. If root_path is provided,
    they are interpreted relative to that root.
    """
    def rp(p: str) -> Path:
        if root_path:
            return (root_path / p.lstrip("\\/")).resolve()
        return Path(p).resolve()

    dirs: List[Path] = []
    if os.name == "nt":
        candidates = [
            r"C:\Windows\System32\drivers\etc",
            r"C:\Windows\System32\winevt\Logs",
            r"C:\Windows\System32\config",  # Registry hives (SAM, SYSTEM, etc.) - may not be readable
            r"C:\Users",
        ]
        # If analyzing an image root, also consider Windows-like layout under that root
        if root_path:
            candidates.extend([
                r"\Windows\System32\drivers\etc",
                r"\Windows\System32\winevt\Logs",
                r"\Windows\System32\config",
                r"\Users",
            ])
    else:
        candidates = [
            "/etc",
            "/var/log",
            "/home",
            "/Users",  # macOS
        ]
        if root_path:
            candidates.extend([
                "/etc",
                "/var/log",
                "/home",
                "/Users",
            ])

    for c in candidates:
        p = rp(c)
        if p.exists():
            dirs.append(p)
    return dirs


def default_include_globs() -> List[str]:
    # Prioritize common forensic-interesting files by extension/name
    return [
        "*.log",
        "*.evtx",
        "*.plist",
        "*.conf",
        "*.cfg",
        "*.ini",
        "passwd",
        "group",
        "shadow",
        "hosts",
        "*.bash_history",
        "authorized_keys",
        "known_hosts",
        "*.lnk",
        "*.prefetch",
        "*.sqlite",
        "*.db",
        "*.json",
        "*.xml",
        "*.yml",
        "*.yaml",
    ]


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Collect forensic artifacts using psutil and pyfakefs into a Pydantic-defined structure."
    )
    parser.add_argument("--root", type=str, default=None,
                        help="Root path to a mounted disk image or directory to analyze (default: live system).")
    parser.add_argument("--source-type", type=str, choices=["live", "image"], default=None,
                        help="Override source type label in output (default: 'image' if --root provided, else 'live').")
    parser.add_argument("--investigator", type=str, default=None,
                        help="Name/identifier for the person/system performing the collection.")
    parser.add_argument("--max-files", type=int, default=500,
                        help="Maximum number of files to collect.")
    parser.add_argument("--max-file-size-mb", type=int, default=10,
                        help="Maximum size per file to hash, in MiB (files larger than this are skipped).")
    parser.add_argument("--hash-max-bytes", type=int, default=None,
                        help="If set, only hash the first N bytes of each file.")
    parser.add_argument("--include-dir", type=str, action="append", default=None,
                        help="Additional directory to include for file collection (can be given multiple times).")
    parser.add_argument("--include-glob", type=str, action="append", default=None,
                        help="Additional file glob to include (can be given multiple times).")
    parser.add_argument("--no-files", action="store_true",
                        help="Skip file artifact collection via pyfakefs.")
    parser.add_argument("--output", type=str, default="-",
                        help='Output path for JSON (default: "-" for stdout).')
    return parser.parse_args(argv)


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = parse_args(argv)

    root_path = Path(args.root).resolve() if args.root else None
    source_type: Literal["live", "image"] = "image" if root_path else "live"
    if args.source_type:
        source_type = args.source_type  # allow override

    snapshot = ForensicSnapshot(
        source_type=source_type,
        root_path=str(root_path) if root_path else None,
        investigator=args.investigator,
    )

    # psutil-driven collections (live system info)
    snapshot.system_info = collect_system_info()
    snapshot.processes = collect_processes()
    snapshot.connections = collect_connections(kind="inet")
    snapshot.users = collect_users()
    snapshot.disks = collect_disks()

    # pyfakefs-driven file artefacts (can be live root or mounted image)
    if not args.no_files:
        include_dirs = default_include_dirs(root_path)
        if args.include_dir:
            # Add any user-provided directories (respecting root_path if given)
            for d in args.include_dir:
                p = Path(d)
                if not p.is_absolute() and root_path:
                    p = (root_path / d).resolve()
                else:
                    p = p.resolve()
                include_dirs.append(p)

        include_globs = default_include_globs()
        if args.include_glob:
            include_globs.extend(args.include_glob)

        fc = FileCollector(
            root_path=root_path,
            include_dirs=include_dirs,
            include_globs=include_globs,
            max_files=args.max_files,
            max_file_size_bytes=args.max_file_size_mb * 1024 * 1024 if args.max_file_size_mb is not None else None,
            hash_max_bytes=args.hash_max_bytes,
            follow_symlinks=False,
        )
        snapshot.files = fc.collect()

    # Serialize to JSON
    data = json.dumps(snapshot.model_dump(mode="json"), indent=2, sort_keys=False)

    # Output
    if args.output == "-" or args.output.lower() == "stdout":
        print(data)
    else:
        out_path = Path(args.output).resolve()
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(data, encoding="utf-8")
        print(f"Wrote forensic snapshot to: {out_path}")
    return 0


if __name__ == "__main__":
    try:
        raise SystemExit(main())
    except KeyboardInterrupt:
        raise SystemExit(130)
------------------------------------------------------------

PROMPT 364:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.Backup service in version 1.34.0 using mypy-boto3-builder version 7.21.0, like so:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import TypeAnnotator\n\nannotator = TypeAnnotator(BaseClient)\n Backup = annotator.annotate(boto3.client('backup'))\n```"
RESPONSE:
# Requires:
#   pip install "boto3==1.34.0" "boto3-stubs[backup]==1.34.0" "mypy-boto3-builder==7.21.0"

import boto3
from mypy_boto3_backup.client import BackupClient

# Typed boto3 Backup client for boto3 1.34.0 (types generated by mypy-boto3-builder 7.21.0)
backup_client: BackupClient = boto3.client("backup")
------------------------------------------------------------

PROMPT 365:
"Generate Python code that imports the json module and creates a function flatten_json() which takes a JSON object as input, recursively iterates through its keys and values, and returns a dictionary of pairs where the keys are the original keys and the values are the flattened versions of the nested JSON objects; And also create an inverse function called unflatten_json() that does the opposite."
RESPONSE:
import json

def flatten_json(obj):
    """
    Flatten a nested JSON-like Python object (dicts/lists/primitives) into a flat
    dictionary mapping path strings to values.

    Path format:
      - Dict keys are joined with '.'
      - List indices are appended as [index]
      - Keys are escaped so that '.', '[' and ']' in actual keys are preserved using backslashes.
        The escape rules:
          '\\' -> '\\\\', '.' -> '\\.', '[' -> '\\[', ']' -> '\\]'

    Examples:
      {'a': {'b': 1}, 'c': [2, 3]} ->
        {
          'a.b': 1,
          'c[0]': 2,
          'c[1]': 3
        }

    Empty containers are preserved as values at their paths.
    A non-container root is represented by the empty path ''.

    Returns:
      dict[str, Any]
    """
    def escape_key(key):
        if not isinstance(key, str):
            raise TypeError("Only string keys are supported for JSON objects.")
        return (
            key.replace('\\', '\\\\')
               .replace('.', '\\.')
               .replace('[', '\\[')
               .replace(']', '\\]')
        )

    flat = {}

    def rec(x, path):
        # dict
        if isinstance(x, dict):
            if not x:
                flat[path] = {}
            else:
                for k, v in x.items():
                    ek = escape_key(k)
                    new_path = f"{path}.{ek}" if path else ek
                    rec(v, new_path)
        # list
        elif isinstance(x, list):
            if not x:
                flat[path] = []
            else:
                for i, v in enumerate(x):
                    new_path = f"{path}[{i}]" if path else f"[{i}]"
                    rec(v, new_path)
        # primitives
        else:
            flat[path] = x

    rec(obj, '')
    return flat


def unflatten_json(flat):
    """
    Inverse of flatten_json(). Reconstructs the original nested structure from a
    flat dictionary produced by flatten_json.

    Accepts paths produced by flatten_json using the same escaping rules and
    returns the nested dict/list/primitives.

    If the flat dict contains the empty path '' that denotes the root value.
    If it also contains other entries, a ValueError is raised (conflicting root).
    """
    def parse_path(p):
        # Returns a list of tokens where each token is either a string key or an int index
        if p == '':
            return []

        tokens = []
        i = 0
        buf = []

        def flush_key():
            if buf:
                tokens.append(''.join(buf))
                buf.clear()

        n = len(p)
        while i < n:
            ch = p[i]
            if ch == '\\':
                i += 1
                if i < n:
                    buf.append(p[i])
                    i += 1
                else:
                    # Trailing backslash; keep it as literal
                    buf.append('\\')
            elif ch == '.':
                # End of a dict key segment
                flush_key()
                i += 1
            elif ch == '[':
                # Start of list index
                flush_key()
                i += 1
                if i >= n or p[i] == ']':
                    raise ValueError(f"Invalid path (empty index): {p}")
                num_start = i
                while i < n and p[i] != ']':
                    if not p[i].isdigit():
                        raise ValueError(f"Invalid list index in path: {p}")
                    i += 1
                if i >= n or p[i] != ']':
                    raise ValueError(f"Unclosed bracket in path: {p}")
                idx = int(p[num_start:i])
                tokens.append(idx)
                i += 1  # skip ']'
            else:
                buf.append(ch)
                i += 1
        flush_key()
        return tokens

    # Build structure
    root = None
    root_set_explicitly = False

    # To have deterministic behavior regardless of input order
    # process '' first if present.
    items = list(flat.items())
    items.sort(key=lambda kv: (kv[0] != '', kv[0]))

    for path, value in items:
        tokens = parse_path(path)
        if not tokens:
            if root is not None and not root_set_explicitly:
                raise ValueError("Conflicting root: flat map has both '' and non-empty paths.")
            root = value
            root_set_explicitly = True
            continue

        # If root was set explicitly to a non-container, this conflicts.
        if root_set_explicitly and not isinstance(root, (dict, list)):
            raise ValueError("Conflicting root: cannot insert into a non-container root value.")

        # Initialize root type if needed
        if root is None:
            root = {} if isinstance(tokens[0], str) else []

        cur = root
        for i, tok in enumerate(tokens):
            last = i == len(tokens) - 1

            if isinstance(tok, str):
                # Need a dict here
                if not isinstance(cur, dict):
                    raise ValueError(f"Path type conflict at token '{tok}': expected dict, found {type(cur).__name__}")
                if last:
                    cur[tok] = value
                else:
                    nxt = tokens[i + 1]
                    if tok not in cur or cur[tok] is None:
                        cur[tok] = {} if isinstance(nxt, str) else []
                    elif isinstance(nxt, str) and not isinstance(cur[tok], dict):
                        raise ValueError(f"Path type conflict at '{tok}': expected dict.")
                    elif isinstance(nxt, int) and not isinstance(cur[tok], list):
                        raise ValueError(f"Path type conflict at '{tok}': expected list.")
                    cur = cur[tok]
            else:
                # list index
                if not isinstance(cur, list):
                    raise ValueError(f"Path type conflict at index [{tok}]: expected list, found {type(cur).__name__}")
                idx = tok
                if idx < 0:
                    raise ValueError(f"Negative index not allowed: {idx}")
                while len(cur) <= idx:
                    cur.append(None)
                if last:
                    cur[idx] = value
                else:
                    nxt = tokens[i + 1]
                    if cur[idx] is None:
                        cur[idx] = {} if isinstance(nxt, str) else []
                    elif isinstance(nxt, str) and not isinstance(cur[idx], dict):
                        raise ValueError(f"Path type conflict at index [{idx}]: expected dict.")
                    elif isinstance(nxt, int) and not isinstance(cur[idx], list):
                        raise ValueError(f"Path type conflict at index [{idx}]: expected list.")
                    cur = cur[idx]

    if root is None:
        # No entries; return empty dict for consistency with typical JSON roots
        return {}
    return root

# Example (uncomment to test):
# data = {"a": {"b": 1}, "c": [2, {"d.e": 3}]}
# flat = flatten_json(data)
# print(json.dumps(flat, indent=2))
# restored = unflatten_json(flat)
# print(json.dumps(restored, indent=2))
------------------------------------------------------------

PROMPT 366:
"Generate Python code that imports the required modules and annotates the AppStream 1.34.5 service using mypy-boto3-builder 7.23.0, like so:\n```python\nfrom typing import Tuple\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import ServiceBuilder\n\nappstream = ServiceBuilder(\n   client=BaseClient,\n   service_name='appstream',\n   version='1.34.5',\n   builder_version='7.23.0'\n)\n```"
RESPONSE:
```python
from typing import Tuple
from botocore.client import BaseClient
from mypy_boto3_builder import ServiceBuilder

appstream = ServiceBuilder(
   client=BaseClient,
   service_name='appstream',
   version='1.34.5',
   builder_version='7.23.0'
)
```
------------------------------------------------------------

PROMPT 367:
"Generate Python code that imports the necessary libraries and creates a class called `GooglePlaces` that takes an API key as a constructor argument, allows you to make requests to the Google Places API using methods such as `search_places()` and `get_place()` that return Place objects representing the results of the request, and provides convenience methods for parsing the response data into useful formats."
RESPONSE:
# Requirements: Python 3.8+, requests
# pip install requests

import time
from typing import Any, Dict, List, Optional, Tuple
import requests


class GooglePlacesError(Exception):
    """Base exception for GooglePlaces client."""


class GooglePlacesApiError(GooglePlacesError):
    """Raised when the Places API returns a non-OK status."""
    def __init__(self, status: str, message: Optional[str] = None, payload: Optional[Dict[str, Any]] = None):
        super().__init__(f"Google Places API error: {status}. {message or ''}".strip())
        self.status = status
        self.payload = payload or {}


class GooglePlaces:
    """
    Lightweight client for Google Places API (Text Search, Nearby Search, Details, Find Place).
    - Instantiate with an API key.
    - search_places(): perform text or nearby search, returning Place objects and a next_page_token.
    - get_place(): fetch place details by place_id, returning a Place object.

    Notes:
    - Google often requires a 2s wait before using next_page_token.
    - Use fields in get_place() to limit billing and response size.
    """

    def __init__(
        self,
        api_key: str,
        session: Optional[requests.Session] = None,
        base_url: str = "https://maps.googleapis.com/maps/api/place",
        timeout: float = 10.0,
        max_retries: int = 2,
        retry_backoff: float = 1.5,
    ):
        self.api_key = api_key
        self.base_url = base_url.rstrip("/")
        self.session = session or requests.Session()
        self.timeout = timeout
        self.max_retries = max_retries
        self.retry_backoff = retry_backoff

    def _request(self, endpoint: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Internal request helper with basic retry/backoff for quota/5xx errors.
        Returns parsed JSON dict or raises GooglePlacesApiError / requests.HTTPError.
        """
        url = f"{self.base_url}/{endpoint}/json"
        params = dict(params or {})
        params["key"] = self.api_key

        attempt = 0
        backoff = self.retry_backoff

        while True:
            attempt += 1
            resp = self.session.get(url, params=params, timeout=self.timeout)
            resp.raise_for_status()
            data = resp.json()

            status = data.get("status", "")
            if status in ("OK", "ZERO_RESULTS"):
                return data

            # Retry on quota/rate limit or transient server errors
            if status in ("OVER_QUERY_LIMIT", "RESOURCE_EXHAUSTED", "UNKNOWN_ERROR"):
                if attempt <= self.max_retries:
                    time.sleep(backoff)
                    backoff *= self.retry_backoff
                    continue

            # INVALID_REQUEST may happen if next_page_token not ready yet; brief retry helps
            if status == "INVALID_REQUEST" and "next_page_token" in data:
                if attempt <= self.max_retries:
                    time.sleep(2.0)
                    continue

            # Anything else: raise
            raise GooglePlacesApiError(status=status, message=data.get("error_message"), payload=data)

    def search_places(
        self,
        query: Optional[str] = None,
        location: Optional[Tuple[float, float]] = None,
        radius: Optional[int] = None,
        type: Optional[str] = None,
        language: Optional[str] = None,
        opennow: Optional[bool] = None,
        minprice: Optional[int] = None,
        maxprice: Optional[int] = None,
        pagetoken: Optional[str] = None,
    ) -> Tuple[List["Place"], Optional[str]]:
        """
        Search for places.
        - If query is provided, uses Text Search.
        - Otherwise uses Nearby Search (requires location and radius).
        Returns (places, next_page_token).
        """
        params: Dict[str, Any] = {}
        endpoint: str

        if query:
            endpoint = "textsearch"
            params["query"] = query
        else:
            endpoint = "nearbysearch"
            if not location or not radius:
                raise ValueError("Nearby search requires location=(lat, lng) and radius.")
            params["location"] = f"{location[0]},{location[1]}"
            params["radius"] = radius

        if type:
            params["type"] = type
        if language:
            params["language"] = language
        if opennow is not None:
            params["opennow"] = "true" if opennow else "false"
        if minprice is not None:
            params["minprice"] = minprice
        if maxprice is not None:
            params["maxprice"] = maxprice
        if pagetoken:
            params["pagetoken"] = pagetoken

        data = self._request(endpoint, params)

        results = data.get("results", [])
        places = [Place.from_search_result(r, client=self) for r in results]
        next_page_token = data.get("next_page_token")

        return places, next_page_token

    def get_place(
        self,
        place_id: str,
        fields: Optional[List[str]] = None,
        language: Optional[str] = None,
        sessiontoken: Optional[str] = None,
    ) -> "Place":
        """
        Get details for a place_id. Returns a Place object.
        - fields: list of fields to include. Example: ["name","geometry/location","formatted_address","opening_hours"]
        - language: BCP-47 language tag
        - sessiontoken: session token for Autocomplete/Find Place workflows
        """
        params: Dict[str, Any] = {"place_id": place_id}
        if fields:
            params["fields"] = ",".join(fields)
        if language:
            params["language"] = language
        if sessiontoken:
            params["sessiontoken"] = sessiontoken

        data = self._request("details", params)
        result = data.get("result", {})
        return Place.from_details(result, client=self)

    def find_place(
        self,
        input_text: str,
        input_type: str = "textquery",
        fields: Optional[List[str]] = None,
        language: Optional[str] = None,
        location_bias: Optional[str] = None,
        sessiontoken: Optional[str] = None,
    ) -> Tuple[List["Place"], Optional[str]]:
        """
        Find Place from text. Returns (places, candidates_next_page_token).
        input_type: 'textquery' or 'phonenumber'
        location_bias: e.g. 'circle:2000@47.6918452,-122.2226413' or 'ipbias'
        """
        params: Dict[str, Any] = {"input": input_text, "inputtype": input_type}
        if fields:
            params["fields"] = ",".join(fields)
        if language:
            params["language"] = language
        if location_bias:
            params["locationbias"] = location_bias
        if sessiontoken:
            params["sessiontoken"] = sessiontoken

        data = self._request("findplacefromtext", params)
        candidates = data.get("candidates", [])
        places = [Place.from_details(c, client=self) for c in candidates]
        # Find Place doesn't paginate like searches; token typically not provided
        return places, data.get("next_page_token")

    def photo_url(self, photo_reference: str, maxwidth: Optional[int] = 800, maxheight: Optional[int] = None) -> str:
        """
        Build a URL to fetch a photo by photo_reference.
        Either maxwidth or maxheight is required by the API; maxwidth default is provided.
        """
        if not (maxwidth or maxheight):
            raise ValueError("Either maxwidth or maxheight must be provided.")
        params = []
        if maxwidth:
            params.append(f"maxwidth={int(maxwidth)}")
        if maxheight:
            params.append(f"maxheight={int(maxheight)}")
        params.append(f"photo_reference={requests.utils.quote(photo_reference)}")
        params.append(f"key={self.api_key}")
        return f"{self.base_url}/photo?{'&'.join(params)}"


class Place:
    """
    A lightweight representation of a Place result. Contains raw data and convenience accessors.
    Use .enrich() to fetch full details if constructed from a search result.
    """

    def __init__(self, raw: Dict[str, Any], client: GooglePlaces, is_details: bool = False):
        self.raw = raw
        self.client = client
        self.is_details = is_details

        # Common fields
        self.place_id: Optional[str] = raw.get("place_id")
        self.name: Optional[str] = raw.get("name")
        self.types: List[str] = raw.get("types", [])
        self.rating: Optional[float] = raw.get("rating")
        self.user_ratings_total: Optional[int] = raw.get("user_ratings_total")
        self.price_level: Optional[int] = raw.get("price_level")
        self.business_status: Optional[str] = raw.get("business_status")
        self.vicinity: Optional[str] = raw.get("vicinity")  # often in nearby results
        self.formatted_address: Optional[str] = raw.get("formatted_address")
        self.geometry: Dict[str, Any] = raw.get("geometry", {})
        self.opening_hours: Dict[str, Any] = raw.get("opening_hours", {})
        self.photos: List[Dict[str, Any]] = raw.get("photos", [])
        self.plus_code: Dict[str, Any] = raw.get("plus_code", {})

        # Details-only extras
        self.formatted_phone_number: Optional[str] = raw.get("formatted_phone_number")
        self.international_phone_number: Optional[str] = raw.get("international_phone_number")
        self.website: Optional[str] = raw.get("website")
        self.url: Optional[str] = raw.get("url")
        self.address_components: List[Dict[str, Any]] = raw.get("address_components", [])
        self.utc_offset: Optional[int] = raw.get("utc_offset")
        self.wheelchair_accessible_entrance: Optional[bool] = raw.get("wheelchair_accessible_entrance")

    @classmethod
    def from_search_result(cls, result: Dict[str, Any], client: GooglePlaces) -> "Place":
        return cls(result, client=client, is_details=False)

    @classmethod
    def from_details(cls, result: Dict[str, Any], client: GooglePlaces) -> "Place":
        return cls(result, client=client, is_details=True)

    # Convenience accessors

    def latlng(self) -> Optional[Tuple[float, float]]:
        loc = self.geometry.get("location") or {}
        lat = loc.get("lat")
        lng = loc.get("lng")
        if lat is None or lng is None:
            return None
        return float(lat), float(lng)

    def is_open_now(self) -> Optional[bool]:
        """
        Returns True/False if open_now is provided, otherwise None.
        For precise schedule evaluation, fetch details and inspect periods.
        """
        oh = self.opening_hours or {}
        return oh.get("open_now")

    def address_dict(self) -> Dict[str, Dict[str, str]]:
        """
        Maps address component types to {'long_name', 'short_name'}.
        Requires details. Returns {} if not present.
        """
        out: Dict[str, Dict[str, str]] = {}
        for comp in self.address_components:
            for t in comp.get("types", []):
                out[t] = {"long_name": comp.get("long_name", ""), "short_name": comp.get("short_name", "")}
        return out

    def city(self) -> Optional[str]:
        a = self.address_dict()
        for key in ("locality", "postal_town", "sublocality", "administrative_area_level_2"):
            if key in a:
                return a[key]["long_name"]
        return None

    def region(self) -> Optional[str]:
        a = self.address_dict()
        for key in ("administrative_area_level_1", "administrative_area_level_2"):
            if key in a:
                return a[key]["short_name"]
        return None

    def country(self) -> Optional[str]:
        a = self.address_dict()
        if "country" in a:
            return a["country"]["long_name"]
        return None

    def postal_code(self) -> Optional[str]:
        a = self.address_dict()
        if "postal_code" in a:
            return a["postal_code"]["long_name"]
        return None

    def primary_photo_url(self, maxwidth: int = 800) -> Optional[str]:
        """
        Build a photo URL for the first photo, if present.
        """
        if not self.photos:
            return None
        ref = self.photos[0].get("photo_reference")
        if not ref:
            return None
        return self.client.photo_url(ref, maxwidth=maxwidth)

    def maps_url(self) -> Optional[str]:
        """
        Returns Maps URL from details if available; otherwise constructs by place_id.
        """
        if self.url:
            return self.url
        if self.place_id:
            return f"https://www.google.com/maps/place/?q=place_id:{self.place_id}"
        return None

    def as_dict(self) -> Dict[str, Any]:
        """
        Serialize key fields to a simple dict.
        """
        latlng = self.latlng()
        return {
            "place_id": self.place_id,
            "name": self.name,
            "types": self.types,
            "rating": self.rating,
            "user_ratings_total": self.user_ratings_total,
            "price_level": self.price_level,
            "business_status": self.business_status,
            "formatted_address": self.formatted_address or self.vicinity,
            "latitude": latlng[0] if latlng else None,
            "longitude": latlng[1] if latlng else None,
            "phone": self.international_phone_number or self.formatted_phone_number,
            "website": self.website,
            "maps_url": self.maps_url(),
            "open_now": self.is_open_now(),
        }

    def to_geojson(self) -> Dict[str, Any]:
        """
        Represent the place as a GeoJSON Feature (Point).
        """
        latlng = self.latlng()
        geometry = None
        if latlng:
            geometry = {"type": "Point", "coordinates": [latlng[1], latlng[0]]}
        return {
            "type": "Feature",
            "geometry": geometry,
            "properties": {
                "place_id": self.place_id,
                "name": self.name,
                "address": self.formatted_address or self.vicinity,
                "types": self.types,
                "rating": self.rating,
                "user_ratings_total": self.user_ratings_total,
                "price_level": self.price_level,
                "maps_url": self.maps_url(),
            },
        }

    def enrich(self, fields: Optional[List[str]] = None, language: Optional[str] = None) -> "Place":
        """
        If the current Place came from a search result, fetch full details and update fields.
        """
        if not self.place_id:
            raise GooglePlacesError("Cannot enrich place without place_id.")
        detailed = self.client.get_place(place_id=self.place_id, fields=fields, language=language)
        # Merge raw dictionaries (details take precedence)
        merged = {**self.raw, **detailed.raw}
        self.__init__(merged, client=self.client, is_details=True)
        return self


# Example usage (commented):
# gp = GooglePlaces(api_key="YOUR_API_KEY")
# places, token = gp.search_places(query="coffee near Palo Alto, CA")
# for p in places:
#     print(p.name, p.rating, p.maps_url())
#     if not p.is_details:
#         p.enrich(fields=["name","formatted_address","international_phone_number","website"])
#         print(p.international_phone_number, p.website)
# if token:
#     time.sleep(2)  # Google requires a short delay before using next_page_token
#     more_places, _ = gp.search_places(pagetoken=token)
------------------------------------------------------------

PROMPT 368:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.HealthLake service using mypy-boto3-builder, like this:\n```\nfrom typing import Optional\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import type_annotations as ta\n\n@ta.generated()\nclass HealthLake(BaseClient):\n   def __init__(self, config: dict) -> None:\n       super().__init__(config)\n```"
RESPONSE:
```python
from typing import Any, Dict, Mapping, Optional, Sequence
from datetime import datetime

from botocore.client import BaseClient
from mypy_boto3_builder import type_annotations as ta


@ta.generated()
class HealthLake(BaseClient):
    def __init__(self, config: Dict[str, Any]) -> None:
        super().__init__(config)

    # Datastore operations
    def create_fhir_datastore(
        self,
        *,
        DatastoreTypeVersion: str,
        DatastoreName: Optional[str] = None,
        SseConfiguration: Optional[Dict[str, Any]] = None,
        IdentityProviderConfiguration: Optional[Dict[str, Any]] = None,
        PreloadDataConfig: Optional[Dict[str, Any]] = None,
        ClientToken: Optional[str] = None,
        Tags: Optional[Sequence[Dict[str, str]]] = None,
    ) -> Dict[str, Any]:
        ...

    def describe_fhir_datastore(
        self,
        *,
        DatastoreId: str,
    ) -> Dict[str, Any]:
        ...

    def list_fhir_datastores(
        self,
        *,
        Filter: Optional[Dict[str, Any]] = None,
        NextToken: Optional[str] = None,
        MaxResults: Optional[int] = None,
    ) -> Dict[str, Any]:
        ...

    def delete_fhir_datastore(
        self,
        *,
        DatastoreId: str,
        ClientToken: Optional[str] = None,
    ) -> Dict[str, Any]:
        ...

    # Export job operations
    def start_fhir_export_job(
        self,
        *,
        DatastoreId: str,
        OutputDataConfig: Dict[str, Any],
        DataAccessRoleArn: str,
        ClientToken: Optional[str] = None,
        JobName: Optional[str] = None,
    ) -> Dict[str, Any]:
        ...

    def describe_fhir_export_job(
        self,
        *,
        DatastoreId: str,
        JobId: str,
    ) -> Dict[str, Any]:
        ...

    def list_fhir_export_jobs(
        self,
        *,
        DatastoreId: str,
        NextToken: Optional[str] = None,
        MaxResults: Optional[int] = None,
        JobName: Optional[str] = None,
        SubmittedBefore: Optional[datetime] = None,
        SubmittedAfter: Optional[datetime] = None,
    ) -> Dict[str, Any]:
        ...

    # Import job operations
    def start_fhir_import_job(
        self,
        *,
        DatastoreId: str,
        InputDataConfig: Dict[str, Any],
        DataAccessRoleArn: str,
        ClientToken: Optional[str] = None,
        JobName: Optional[str] = None,
    ) -> Dict[str, Any]:
        ...

    def describe_fhir_import_job(
        self,
        *,
        DatastoreId: str,
        JobId: str,
    ) -> Dict[str, Any]:
        ...

    def list_fhir_import_jobs(
        self,
        *,
        DatastoreId: str,
        NextToken: Optional[str] = None,
        MaxResults: Optional[int] = None,
        JobName: Optional[str] = None,
        SubmittedBefore: Optional[datetime] = None,
        SubmittedAfter: Optional[datetime] = None,
    ) -> Dict[str, Any]:
        ...

    # Tagging operations
    def list_tags_for_resource(
        self,
        *,
        ResourceArn: str,
    ) -> Dict[str, Any]:
        ...

    def tag_resource(
        self,
        *,
        ResourceArn: str,
        Tags: Mapping[str, str],
    ) -> Dict[str, Any]:
        ...

    def untag_resource(
        self,
        *,
        ResourceArn: str,
        TagKeys: Sequence[str],
    ) -> Dict[str, Any]:
        ...
```
------------------------------------------------------------

PROMPT 369:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.RDSDataService 1.34.6 service using mypy-boto3-builder 7.23.0, like this:\n```python\nfrom typing import Callable, Dict, List, Optional, Tuple\nfrom boto3.rdsdataservice import RDSDataService\n\nrs = RDSDataService(\n   config=None,\n   endpoint=\"https://rds.us-east-1.amazonaws.com\",\n   region_name=\"us-east-1\"\n)\n```"
RESPONSE:
```python
from typing import Callable, Dict, List, Optional, Tuple

import boto3
from botocore.config import Config

from mypy_boto3_rds_data.client import RDSDataServiceClient
from mypy_boto3_rds_data.type_defs import (
    ExecuteStatementRequestRequestTypeDef,
    ExecuteStatementResponseTypeDef,
    SqlParameterTypeDef,
    FieldTypeDef,
)

# Optional botocore config
config: Optional[Config] = None

# Typed RDS DataService client
rs: RDSDataServiceClient = boto3.client(
    "rds-data",
    region_name="us-east-1",
    endpoint_url="https://rds-data.us-east-1.amazonaws.com",
    config=config,
)

# Example of strongly-typed request/response using mypy-boto3 stubs
id_val: FieldTypeDef = {"longValue": 123}
params: List[SqlParameterTypeDef] = [{"name": "id", "value": id_val}]

req: ExecuteStatementRequestRequestTypeDef = {
    "resourceArn": "arn:aws:rds:us-east-1:123456789012:cluster:my-aurora-cluster",
    "secretArn": "arn:aws:secretsmanager:us-east-1:123456789012:secret:my-db-secret",
    "sql": "SELECT :id",
    "parameters": params,
    "database": "mydb",
    "includeResultMetadata": True,
}

resp: ExecuteStatementResponseTypeDef = rs.execute_statement(**req)
```
------------------------------------------------------------

PROMPT 370:
"Generate Python code that imports the required modules and annotates the types of the ElasticTranscoder service in boto3 version 1.34.0 using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom elastictranscoder import ElasticTranscoder\n\n# Annotate the types of the ElasticTranscoder service\nec = ElasticTranscoder(client=BaseClient)\n```"
RESPONSE:
```python
import boto3
from mypy_boto3_elastictranscoder import ElasticTranscoderClient

# Annotate the Elastic Transcoder client using boto3-stubs (mypy-boto3-elastictranscoder)
ec: ElasticTranscoderClient = boto3.client("elastictranscoder")

# Example typed call
pipelines = ec.list_pipelines()
```
------------------------------------------------------------

PROMPT 371:
"Generate Python code that imports the required modules and annotates the types for the boto3.WorkDocs 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import List, Dict\nfrom boto3 import ServiceResource, client\nfrom mypy_boto3_builder import BotoflowTypedClient\n\n# Annotate the types for the boto3.WorkDocs 1.34.0 service\nwork_docs_client = BotoflowTypedClient('work_docs', '1.34.0')\n```"
RESPONSE:
```python
# Requires:
#   boto3==1.34.*
#   boto3-stubs[workdocs]==1.34.*  # generated by mypy-boto3-builder 7.21.0

from typing import List, Dict
import boto3
from mypy_boto3_workdocs import WorkDocsClient

# Typed boto3 WorkDocs client
workdocs_client: WorkDocsClient = boto3.client("workdocs")

# Example usage with typed client
def list_user_ids(organization_id: str) -> List[str]:
    resp = workdocs_client.describe_users(OrganizationId=organization_id)
    return [u["Id"] for u in resp.get("Users", []) if "Id" in u]
```
------------------------------------------------------------

PROMPT 372:
"Generate Python code that imports the necessary modules and annotates the types of the boto3 IoT 1.34.17 service using mypy-boto3-builder 7.23.1, like this:\n```python\nfrom typing import Optional\nfrom botocore.client import Client\nfrom boto3.iot import IoT\n\niot = IoT(\n   client=Client(\n       config={\n           \"region\": \"us-west-2\",\n           \"credentials\": None,\n       }\n   ),\n   region=\"us-west-2\"\n)\n```"
RESPONSE:
from typing import Optional
import boto3
from botocore.config import Config
from mypy_boto3_iot.client import IoTClient

def get_iot_client(
    region: str = "us-west-2",
    aws_access_key_id: Optional[str] = None,
    aws_secret_access_key: Optional[str] = None,
    aws_session_token: Optional[str] = None,
    profile: Optional[str] = None,
) -> IoTClient:
    session = boto3.session.Session(
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        aws_session_token=aws_session_token,
        profile_name=profile,
        region_name=region,
    )
    return session.client("iot", config=Config(region_name=region))

iot: IoTClient = get_iot_client(region="us-west-2")
------------------------------------------------------------

PROMPT 373:
"Generate Python code that imports the required modules and annotates the types of the boto3.ApplicationDiscoveryService 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import TypeVar, Generic\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import Botofiler\n\napp_discovery_service = Botofiler(\n   service=\"application-discovery\",\n   version=\"1.34.0\",\n   client=BaseClient,\n)\n```"
RESPONSE:
# mypy-boto3-builder does not expose a Botofiler class.
# Use the generated stubs package for Application Discovery instead.
from typing import Generic, TypeVar
import boto3
from botocore.client import BaseClient
from mypy_boto3_application_discovery import ApplicationDiscoveryServiceClient

TClient = TypeVar("TClient", bound=BaseClient)


class TypedService(Generic[TClient]):
    def __init__(self, service: str, version: str, client: TClient) -> None:
        self.service = service
        self.version = version
        self.client = client


# boto3 service name for AWS Application Discovery Service is "discovery"
app_discovery_client: ApplicationDiscoveryServiceClient = boto3.client("discovery")

app_discovery_service = TypedService(
    service="discovery",
    version="1.34.0",
    client=app_discovery_client,
)

# Example typed usage
_ = app_discovery_service.client.describe_agents(MaxResults=10)
------------------------------------------------------------

PROMPT 374:
"Generate Python code that imports the required modules and annotates the ACM 2.9.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import Optional\nfrom aiobotocore import AiobotocoreClient\nfrom aiobotocore.acm import ACM\n\nclient = AiobotocoreClient(\n   aws_access_key_id=\"<your_aws_access_key_id>\",\n   aws_secret_access_key=\"<your_aws_secret_access_key>\",\n   region=\"us-west-2\"\n)\n\nacm = ACM(client)\n```"
RESPONSE:
from typing import Optional, AsyncIterator
from contextlib import asynccontextmanager

from aiobotocore.session import get_session
from types_aiobotocore_acm.client import ACMClient


@asynccontextmanager
async def get_acm_client(
    aws_access_key_id: Optional[str] = None,
    aws_secret_access_key: Optional[str] = None,
    region_name: str = "us-west-2",
) -> AsyncIterator[ACMClient]:
    session = get_session()
    async with session.create_client(
        "acm",
        region_name=region_name,
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
    ) as client:
        yield client


async def main() -> None:
    async with get_acm_client(
        aws_access_key_id="<your_aws_access_key_id>",
        aws_secret_access_key="<your_aws_secret_access_key>",
        region_name="us-west-2",
    ) as acm:
        # acm is typed as ACMClient from types-aiobotocore-acm (generated by mypy-boto3-builder 7.21.0)
        response = await acm.list_certificates()
        print(response)


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
------------------------------------------------------------

PROMPT 375:
"Generate Python code that imports the required modules and annotates the types of the boto3.DataSync service using mypy-boto3-builder, like this:\n```\nfrom typing import Optional\nfrom mypy_boto3_builder import TypeAnnotator\n\nannotator = TypeAnnotator(modules=[\"boto3\"], version=\"1.34.0\")\n\n# Annotate the types of the DataSync service\nservice = annotator.get_service(\"DataSync\")\n\n# Print the annotated type definitions\nprint(service)\n```"
RESPONSE:
from typing import Optional

try:
    from mypy_boto3_builder import TypeAnnotator
except ImportError as e:
    raise SystemExit(
        "mypy-boto3-builder is not installed. Install it with:\n"
        "  pip install mypy-boto3-builder\n"
    ) from e

annotator = TypeAnnotator(modules=["boto3"], version="1.34.0")

# Annotate the types of the DataSync service
service = annotator.get_service("DataSync")

# Print the annotated type definitions
print(service)
------------------------------------------------------------

PROMPT 376:
"Generate Python code that imports the required modules and annotates the types of the boto3 LexRuntimeService client in version 1.34.0 using mypy-boto3-builder version 7.21.0, like so:\n```python\nfrom typing import Callable, Dict, List\nfrom botocore.client import BaseClient\nfrom boto3.lex_runtime_service import LexRuntimeService\nfrom mypy_boto3_builder import MyPyBoto3Builder\n\nbuilder = MyPyBoto3Builder(LexRuntimeService)\nbuilder.generate_type_annotations()\n```"
RESPONSE:
```python
# Requires:
#   boto3==1.34.0
#   mypy-boto3-lex-runtime (generated by mypy-boto3-builder==7.21.0)
# Optional (for the version assertion below):
#   mypy-boto3-builder==7.21.0

from importlib.metadata import version, PackageNotFoundError

import boto3
from mypy_boto3_lex_runtime import LexRuntimeServiceClient
from mypy_boto3_lex_runtime.type_defs import (
    PostTextResponseTypeDef,
    DeleteSessionResponseTypeDef,
    GetSessionResponseTypeDef,
    PutSessionResponseTypeDef,
)

# Assert expected versions are installed (optional but useful in CI)
assert version("boto3") == "1.34.0", "Please install boto3==1.34.0"
try:
    # The stubs package name on PyPI
    assert version("mypy-boto3-lex-runtime").startswith("1.34."), (
        "Please install a 1.34.x version of mypy-boto3-lex-runtime "
        "generated by mypy-boto3-builder==7.21.0"
    )
except PackageNotFoundError:
    raise RuntimeError(
        "Package 'mypy-boto3-lex-runtime' not found. "
        "Install it to get type annotations for LexRuntimeService."
    )

# Optionally verify builder version if present in the environment.
try:
    from mypy_boto3_builder import __version__ as builder_version

    assert builder_version == "7.21.0", "Expected mypy-boto3-builder==7.21.0"
except Exception:
    # Builder is a dev tool and not required at runtime.
    pass

# Typed Lex Runtime client
lex_client: LexRuntimeServiceClient = boto3.client("lex-runtime")


# Example typed helpers demonstrating the annotated client usage
def post_text(
    bot_name: str,
    bot_alias: str,
    user_id: str,
    text: str,
) -> PostTextResponseTypeDef:
    return lex_client.post_text(
        botName=bot_name,
        botAlias=bot_alias,
        userId=user_id,
        inputText=text,
    )


def delete_session(
    bot_name: str,
    bot_alias: str,
    user_id: str,
) -> DeleteSessionResponseTypeDef:
    return lex_client.delete_session(
        botName=bot_name,
        botAlias=bot_alias,
        userId=user_id,
    )


def get_session(
    bot_name: str,
    bot_alias: str,
    user_id: str,
) -> GetSessionResponseTypeDef:
    return lex_client.get_session(
        botName=bot_name,
        botAlias=bot_alias,
        userId=user_id,
    )


def put_session(
    bot_name: str,
    bot_alias: str,
    user_id: str,
    session_attributes: dict[str, str] | None = None,
) -> PutSessionResponseTypeDef:
    kwargs = {
        "botName": bot_name,
        "botAlias": bot_alias,
        "userId": user_id,
    }
    if session_attributes is not None:
        kwargs["sessionAttributes"] = session_attributes
    return lex_client.put_session(**kwargs)  # type: ignore[arg-type]
```
------------------------------------------------------------

PROMPT 377:
"Generate Python code that imports the necessary modules and annotates the XRay 1.34.0 service with type hints using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom boto3.xray import XRayService\n\nxray_service = XRayService(\n   client=BaseClient(\n       aws_access_key_id='YOUR_AWS_ACCESS_KEY',\n       aws_secret_access_key='YOUR_AWS_SECRET_KEY',\n       aws_session_token='YOUR_AWS_SESSION_TOKEN',\n   ),\n)\n```"
RESPONSE:
from typing import List
from dataclasses import dataclass

import boto3
from botocore.config import Config
from mypy_boto3_xray import XRayClient


@dataclass
class XRayService:
    client: XRayClient


xray_client: XRayClient = boto3.client(
    "xray",
    aws_access_key_id="YOUR_AWS_ACCESS_KEY",
    aws_secret_access_key="YOUR_AWS_SECRET_KEY",
    aws_session_token="YOUR_AWS_SESSION_TOKEN",
    region_name="us-east-1",
    config=Config(api_version="2016-04-12"),
)

xray_service = XRayService(client=xray_client)
------------------------------------------------------------

PROMPT 378:
"Generate Python code that creates a pytest wrapper with fixtures for Playwright to automate web browsers:\n```python\nimport pytest\nfrom playwright.sync import Playwright\n\n@pytest.fixture(scope='function')\ndef playwright():\n   return Playwright()\n\n@pytest.mark.parametrize('url', ['http://example.com', 'https://example.com'])\ndef test_browser(playwright, url):\n   browser = playwright.chromium.launch(url=url)\n   assert browser.title() == 'Example Domain'\n   browser.close()\n```"
RESPONSE:
Here’s a working pytest setup with Playwright fixtures that launch browsers, create contexts/pages, and validate the Example Domain title.

Python code (you can keep this in a single test file or split fixtures into conftest.py):

```python
import pytest
from playwright.sync_api import sync_playwright, Browser, BrowserContext, Page

@pytest.fixture(scope="session")
def playwright_instance():
    # Start Playwright once per test session
    with sync_playwright() as p:
        yield p

@pytest.fixture(scope="session", params=["chromium", "firefox", "webkit"])
def browser(playwright_instance, request) -> Browser:
    # Launch each browser type once per session
    browser = getattr(playwright_instance, request.param).launch(headless=True)
    yield browser
    browser.close()

@pytest.fixture(scope="function")
def context(browser) -> BrowserContext:
    # Isolated context per test function
    ctx = browser.new_context()
    yield ctx
    ctx.close()

@pytest.fixture(scope="function")
def page(context) -> Page:
    # Fresh page per test function
    p = context.new_page()
    yield p
    p.close()

@pytest.mark.parametrize("url", ["http://example.com", "https://example.com"])
def test_browser_title(page: Page, url: str):
    page.goto(url)
    assert page.title() == "Example Domain"
```

Notes:
- Install dependencies: pip install pytest playwright and run playwright install to fetch browser binaries.
- This runs the same test across Chromium, Firefox, and WebKit. Remove params in the browser fixture if you only want one browser.
------------------------------------------------------------

PROMPT 379:
"Generate Python code that imports the required modules and annotates the types of the boto3.CloudHSMV2 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import Tuple\nfrom botocore.client import BaseClient\nfrom boto3.cloudhsmv2 import CloudHSMV2\n\n# Annotate the types of the CloudHSMV2 service\nclass CloudHSMV2(BaseClient):\n   def __init__(self, *args, **kwargs) -> None:\n       super().__init__(*args, **kwargs)\n       self.config = kwargs['config']\n       self.client_name = 'cloudhsmv2'\n\n   # ... (other methods and properties) ...\n```"
RESPONSE:
```python
# Requires:
#   boto3==1.34.0
#   mypy-boto3-cloudhsmv2==1.34.0
#   mypy-boto3-builder==7.21.0

from __future__ import annotations

import boto3

# Typed imports generated by mypy-boto3-builder for CloudHSMV2 (boto3 1.34.x)
from mypy_boto3_cloudhsmv2 import CloudHSMV2Client
from mypy_boto3_cloudhsmv2.paginator import (
    DescribeBackupsPaginator,
    DescribeClustersPaginator,
)
from mypy_boto3_cloudhsmv2.type_defs import (
    DescribeBackupsRequestRequestTypeDef,
    DescribeBackupsResponseTypeDef,
    DescribeClustersRequestRequestTypeDef,
    DescribeClustersResponseTypeDef,
    FiltersTypeDef,
)

# Create a typed CloudHSMV2 client
cloudhsmv2: CloudHSMV2Client = boto3.client("cloudhsmv2")


# Example: strongly-typed wrappers around CloudHSMV2 operations

def describe_backups_typed(
    client: CloudHSMV2Client,
    req: DescribeBackupsRequestRequestTypeDef | None = None,
) -> DescribeBackupsResponseTypeDef:
    if req is None:
        return client.describe_backups()
    return client.describe_backups(**req)


def describe_clusters_typed(
    client: CloudHSMV2Client,
    req: DescribeClustersRequestRequestTypeDef | None = None,
) -> DescribeClustersResponseTypeDef:
    if req is None:
        return client.describe_clusters()
    return client.describe_clusters(**req)


# Example: using typed Filters
def describe_backups_with_filters(
    client: CloudHSMV2Client,
    filters: FiltersTypeDef,
) -> DescribeBackupsResponseTypeDef:
    return client.describe_backups(Filters=filters)


# Example: using typed paginators
def iterate_all_backups(client: CloudHSMV2Client) -> None:
    paginator: DescribeBackupsPaginator = client.get_paginator("describe_backups")
    for page in paginator.paginate():
        # page is DescribeBackupsResponseTypeDef at type-check time
        _process_backups_page(page)


def iterate_all_clusters(client: CloudHSMV2Client) -> None:
    paginator: DescribeClustersPaginator = client.get_paginator("describe_clusters")
    for page in paginator.paginate():
        # page is DescribeClustersResponseTypeDef at type-check time
        _process_clusters_page(page)


# Internal helpers with explicit response typing

def _process_backups_page(page: DescribeBackupsResponseTypeDef) -> None:
    # Add your processing logic here
    pass


def _process_clusters_page(page: DescribeClustersResponseTypeDef) -> None:
    # Add your processing logic here
    pass


if __name__ == "__main__":
    # Minimal usage examples
    backups = describe_backups_typed(cloudhsmv2)
    clusters = describe_clusters_typed(cloudhsmv2)

    # With filters (keys are service-defined; values are sequences of strings)
    my_filters: FiltersTypeDef = {"clusterIds": ["cluster-12345678"]}
    filtered_backups = describe_backups_with_filters(cloudhsmv2, my_filters)

    # Paginators
    iterate_all_backups(cloudhsmv2)
    iterate_all_clusters(cloudhsmv2)
```
------------------------------------------------------------

PROMPT 380:
"Generate Python code that imports the required modules and annotates the types of the boto3.GroundStation service using mypy-boto3-builder, like so:\n```python\nfrom typing import TypeVar\nfrom mypy_boto3_builder import TypeAnnotator\n\n# Initialize the type annotator\nta = TypeAnnotator()\n\n# Annotate the boto3.GroundStation service\ngs = ta.annotate(boto3.client(\"groundstation\"), GroundStation)\n```\nThis code imports the `mypy_boto3_builder` module, which provides a type annotator class called `TypeAnnotator`. The `TypeAnnotator` instance is then used to annotate the `boto3.client(\"groundstation\")` object, casting it to a `GroundStation` type. This generates Python code that includes type hints for the methods and properties of the `GroundStation` service."
RESPONSE:
Note: mypy-boto3-builder does not provide a TypeAnnotator. To get type hints at runtime, use the generated stubs package for Ground Station (mypy-boto3-groundstation or boto3-stubs[groundstation]).

# pip install boto3-stubs[groundstation]
import boto3
from mypy_boto3_groundstation import GroundStationClient

# Option 1: annotate the variable
gs: GroundStationClient = boto3.client("groundstation")

# Option 2: use cast if you prefer a cast-like pattern
from typing import cast
gs_cast = cast(GroundStationClient, boto3.client("groundstation"))
------------------------------------------------------------

PROMPT 381:
"Generate Python code that imports the Flask module and creates a basic Flask app with pagination support using the `Flask_Pager` class from the `flask_paginate` library, like this:\n```\nfrom flask import Flask; from flask_paginate import Flask_Pager\napp = Flask(__name__)\napp.config['PAGINATE_BLUEPRINT'] = 'my_blueprint'\ndb = MySQL('user', 'password', 'localhost')\nclass MyModel(db.Model):\n   __tablename__ = 'my_table'\n   id = Column(Integer, primary_key=True)\n   name = Column(String(100))\n   created_at = Column(DateTime, default=datetime.utcnow())\n   updated_at = Column(DateTime, default=datetime.utcnow())\n\n@app.route('/')\ndef index():\n   pager = Flask_Pager(MyModel, per_page=10)\n   return render_template('index.html', pager=pager)\n\nif __name__ == '__main__':\n   app"
RESPONSE:
# Note: flask_paginate provides Pagination, not Flask_Pager. This is a working example using Pagination.
from flask import Flask, request, render_template_string
from flask_sqlalchemy import SQLAlchemy
from flask_paginate import Pagination, get_page_parameter
from datetime import datetime

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///example.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

db = SQLAlchemy(app)

class MyModel(db.Model):
    __tablename__ = 'my_table'
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100))
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

@app.before_first_request
def init_db():
    db.create_all()
    if not MyModel.query.first():
        for i in range(1, 101):
            db.session.add(MyModel(name=f'Item {i}'))
        db.session.commit()

@app.route('/')
def index():
    page = request.args.get(get_page_parameter(), type=int, default=1)
    per_page = 10

    query = MyModel.query.order_by(MyModel.created_at.desc())
    total = query.count()
    items = query.offset((page - 1) * per_page).limit(per_page).all()

    pagination = Pagination(page=page, per_page=per_page, total=total, record_name='items', css_framework='bootstrap5')

    return render_template_string("""
    <!doctype html>
    <html>
      <head>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
      </head>
      <body class="p-4">
        <h1>Items</h1>
        <ul class="list-group mb-3">
          {% for item in items %}
            <li class="list-group-item">{{ item.id }} — {{ item.name }} ({{ item.created_at }})</li>
          {% endfor %}
        </ul>
        <nav aria-label="Page navigation">{{ pagination.links }}</nav>
      </body>
    </html>
    """, items=items, pagination=pagination)

if __name__ == '__main__':
    app.run(debug=True)
------------------------------------------------------------

PROMPT 382:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.Kinesis 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import Optional\nfrom boto3 import KinesisClient\n\n# Define type annotations for boto3.Kinesis 1.34.0 service\nclass KinesisClient(object):\n   def __init__(self, *, config: Optional[dict] = None) -> None:\n       self.config = config\n   \n   def create_stream(self, *, stream_name: str, shard_count: int, **kwargs) -> dict:\n       pass\n   \n   def describe_stream(self, *, stream_name: str, **kwargs) -> dict:\n       pass\n   \n   def list_streams(self, *, limit: int = None, **kwargs) -> list:\n       pass\n   \n   def put_record(self, *, stream_name: str, data: bytes, partition_key: str, **kwargs) -> dict"
RESPONSE:
```python
from __future__ import annotations

from typing import Optional, Union, IO

import boto3
from botocore.config import Config

# Types generated by mypy-boto3-builder 7.21.0 for boto3 1.34.x
from mypy_boto3_kinesis import KinesisClient as Boto3KinesisClient
from mypy_boto3_kinesis.type_defs import (
    DescribeStreamOutputTypeDef,
    ListStreamsOutputTypeDef,
    PutRecordOutputTypeDef,
    StreamModeDetailsTypeDef,
)


def get_kinesis_client(*, config: Optional[Config] = None) -> Boto3KinesisClient:
    """
    Create and return a typed boto3 Kinesis client.
    """
    return boto3.client("kinesis", config=config)


class KinesisClient:
    """
    A thin, type-annotated wrapper around the typed boto3 Kinesis client.
    Uses mypy-boto3-kinesis (generated by mypy-boto3-builder 7.21.0) for types.
    """

    def __init__(self, *, config: Optional[Config] = None) -> None:
        self._client: Boto3KinesisClient = get_kinesis_client(config=config)

    def create_stream(
        self,
        *,
        stream_name: str,
        shard_count: int,
        stream_mode_details: Optional[StreamModeDetailsTypeDef] = None,
    ) -> None:
        if stream_mode_details is not None:
            self._client.create_stream(
                StreamName=stream_name,
                ShardCount=shard_count,
                StreamModeDetails=stream_mode_details,
            )
        else:
            self._client.create_stream(
                StreamName=stream_name,
                ShardCount=shard_count,
            )

    def describe_stream(
        self,
        *,
        stream_name: str,
        exclusive_start_shard_id: Optional[str] = None,
        limit: Optional[int] = None,
    ) -> DescribeStreamOutputTypeDef:
        kwargs = {}
        if exclusive_start_shard_id is not None:
            kwargs["ExclusiveStartShardId"] = exclusive_start_shard_id
        if limit is not None:
            kwargs["Limit"] = limit
        return self._client.describe_stream(StreamName=stream_name, **kwargs)

    def list_streams(
        self,
        *,
        limit: Optional[int] = None,
        exclusive_start_stream_name: Optional[str] = None,
    ) -> ListStreamsOutputTypeDef:
        kwargs = {}
        if limit is not None:
            kwargs["Limit"] = limit
        if exclusive_start_stream_name is not None:
            kwargs["ExclusiveStartStreamName"] = exclusive_start_stream_name
        return self._client.list_streams(**kwargs)

    def put_record(
        self,
        *,
        stream_name: str,
        data: Union[bytes, IO[bytes]],
        partition_key: str,
        explicit_hash_key: Optional[str] = None,
        sequence_number_for_ordering: Optional[str] = None,
    ) -> PutRecordOutputTypeDef:
        kwargs = {}
        if explicit_hash_key is not None:
            kwargs["ExplicitHashKey"] = explicit_hash_key
        if sequence_number_for_ordering is not None:
            kwargs["SequenceNumberForOrdering"] = sequence_number_for_ordering
        return self._client.put_record(
            StreamName=stream_name,
            Data=data,
            PartitionKey=partition_key,
            **kwargs,
        )
```
------------------------------------------------------------

PROMPT 383:
"Generate Python code that imports the required modules and annotates the types for the OpenSearchService client in boto3 version 1.34.13 using mypy-boto3-builder version 7.23.0, like this:\n```\nfrom typing import Callable\nfrom botocore.client import BaseClient\nfrom boto3.s3.transfer import S3Transfer\nfrom boto3.s3.bucket import Bucket\nfrom boto3.s3.key import Key\nfrom boto3.s3.prefix import Prefix\nfrom boto3.s3.query_string import QueryString\nfrom boto3.s3.response import Response\nfrom boto3.s3.error import S3Error\nfrom boto3.s3.utils import convert_to_s3_objects\n\nclass OpenSearchService(BaseClient):\n   def __init__(self, *, config: dict = None, **kwargs) -> None:\n       super().__init__(config=config, **kwargs)\n       self.base_url = \"https://opensearch.org\"\n       self.s3"
RESPONSE:
```python
# Requires:
#   pip install "boto3==1.34.13" "mypy-boto3-opensearch==1.34.13"

from typing import Any, Optional

import boto3
from botocore.config import Config
from mypy_boto3_opensearch.client import OpenSearchServiceClient
from mypy_boto3_opensearch.type_defs import ListDomainNamesResponseTypeDef


class OpenSearchService:
    def __init__(
        self,
        *,
        region_name: Optional[str] = None,
        config: Optional[Config] = None,
        **kwargs: Any,
    ) -> None:
        self.base_url: str = "https://opensearch.org"
        self.client: OpenSearchServiceClient = boto3.client(
            "opensearch",
            region_name=region_name,
            config=config,
            **kwargs,
        )

    def list_domain_names(self) -> ListDomainNamesResponseTypeDef:
        return self.client.list_domain_names()


# Example usage with full type safety:
# service = OpenSearchService(region_name="us-east-1")
# resp = service.list_domain_names()
# print([d["DomainName"] for d in resp.get("DomainNames", [])])
```
------------------------------------------------------------

PROMPT 384:
"Generate Python code that imports the necessary modules and annotates the types of the IoTSecureTunneling service, version 1.34.0, using mypy-boto3-builder version 7.21.0, like this:\n```python\nfrom typing import Callable, Dict, List, Optional\nfrom botocore.client import BaseClient\nfrom boto3.iot import IoTSecureTunneling\n\n# Define the type annotations for the IoTSecureTunneling service\nclass IoTSecureTunneling(BaseClient):\n   def __init__(self, *args, **kwargs) -> None:\n       super().__init__(*args, **kwargs)\n\n   def create_tunnel(self, *, tunnel_id: str, host: str, port: int, protocol: str = \"https\") -> Dict[str, str]:\n       pass\n\n   def list_tunnels(self, *, next_token: str = \"\") -> List[Dict[str, str]]:\n       pass\n\n   def describe_tunnel(self, *, tunnel_id: str) ->"
RESPONSE:
from typing import Any, Dict, List, Optional, Sequence
from datetime import datetime
from typing_extensions import Literal, NotRequired, TypedDict
from botocore.client import BaseClient


# Type definitions modeled after boto3 1.34.0 for AWS IoT Secure Tunneling

class TagTypeDef(TypedDict):
    Key: str
    Value: str


class DestinationConfigTypeDef(TypedDict, total=False):
    thingName: str
    services: Sequence[str]


class TimeoutConfigTypeDef(TypedDict, total=False):
    maxLifetimeTimeoutMinutes: int


class ConnectionStateTypeDef(TypedDict, total=False):
    status: Literal["CONNECTED", "DISCONNECTED"]
    lastUpdatedAt: datetime


class TunnelSummaryTypeDef(TypedDict, total=False):
    tunnelId: str
    tunnelArn: str
    status: Literal["OPEN", "CLOSED"]
    description: str
    createdAt: datetime
    lastUpdatedAt: datetime


class TunnelTypeDef(TypedDict, total=False):
    tunnelId: str
    tunnelArn: str
    status: Literal["OPEN", "CLOSED"]
    sourceConnectionState: ConnectionStateTypeDef
    destinationConnectionState: ConnectionStateTypeDef
    description: str
    destinationConfig: DestinationConfigTypeDef
    timeoutConfig: TimeoutConfigTypeDef
    createdAt: datetime
    lastUpdatedAt: datetime
    tags: Sequence[TagTypeDef]


class OpenTunnelResponseTypeDef(TypedDict, total=False):
    tunnelArn: str
    tunnelId: str
    sourceAccessToken: str
    destinationAccessToken: str


class RotateTunnelAccessTokenResponseTypeDef(TypedDict, total=False):
    sourceAccessToken: str
    destinationAccessToken: str


class DescribeTunnelResponseTypeDef(TypedDict, total=False):
    tunnel: TunnelTypeDef


class ListTunnelsResponseTypeDef(TypedDict, total=False):
    tunnelSummaries: List[TunnelSummaryTypeDef]
    nextToken: str


class ListTagsForResourceResponseTypeDef(TypedDict, total=False):
    tags: List[TagTypeDef]


class IoTSecureTunneling(BaseClient):
    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)

    def open_tunnel(
        self,
        *,
        description: Optional[str] = None,
        tags: Optional[Sequence[TagTypeDef]] = None,
        destinationConfig: Optional[DestinationConfigTypeDef] = None,
        timeoutConfig: Optional[TimeoutConfigTypeDef] = None,
    ) -> OpenTunnelResponseTypeDef:
        ...

    def close_tunnel(
        self,
        *,
        tunnelId: str,
        delete: Optional[bool] = None,
    ) -> Dict[str, Any]:
        ...

    def rotate_tunnel_access_token(
        self,
        *,
        tunnelId: str,
        clientMode: Literal["SOURCE", "DESTINATION"],
        destinationConfig: Optional[DestinationConfigTypeDef] = None,
    ) -> RotateTunnelAccessTokenResponseTypeDef:
        ...

    def describe_tunnel(
        self,
        *,
        tunnelId: str,
    ) -> DescribeTunnelResponseTypeDef:
        ...

    def list_tunnels(
        self,
        *,
        thingName: Optional[str] = None,
        maxResults: Optional[int] = None,
        nextToken: Optional[str] = None,
    ) -> ListTunnelsResponseTypeDef:
        ...

    def tag_resource(
        self,
        *,
        resourceArn: str,
        tags: Sequence[TagTypeDef],
    ) -> Dict[str, Any]:
        ...

    def list_tags_for_resource(
        self,
        *,
        resourceArn: str,
    ) -> ListTagsForResourceResponseTypeDef:
        ...

    def untag_resource(
        self,
        *,
        resourceArn: str,
        tagKeys: Sequence[str],
    ) -> Dict[str, Any]:
        ...
------------------------------------------------------------

PROMPT 385:
"Generate Python code that uses the rtf module to parse RTF encapsulated HTML in MSG emails and extracts the HTML content, example code:\n```\nimport rtf\n\ndef extract_html(msg_file):\n   # Open the MSG file using rtf.RTFFile\n   with open(msg_file, \"rb\") as f:\n       msg = rtf.RTFFile(f)\n   \n   # Extract the RTF formatted text\n   text = msg.get_text()\n   \n   # Find all occurrences of HTML tags in the text\n   html_tags = re.findall(r\"\\<\\w+\\>.*?\\</\\w+\\>\", text)\n   \n   # Join the HTML tags into a single string\n   html_content = \"\\n\".join(html_tags)\n   \n   return html_content\n\n# Test the function with a sample MSG file\nmsg_file = \"path/to/sample.msg\"\nprint(extract_html(msg_file))\n```\nThis code will read an MSG file, extract the RTF formatted text, and then use regular expressions"
RESPONSE:
Your example won’t work as written: Outlook .msg files are OLE/CFBF containers and can’t be opened by rtf.RTFFile, and regex on angle brackets won’t reliably recover encapsulated HTML from RTF. Below is a working approach that:

- Opens the .msg with extract_msg
- Gets HTML directly if present; otherwise reads the RTF body (decompressing if needed)
- Extracts HTML embedded via RTF’s encapsulated HTML destination ({\*\htmltag ...})
- Decodes RTF escapes (\'hh, \uN, \\, \{, \})

Install:
- pip install extract_msg compressed-rtf

Code:
```python
import io
from typing import Optional, List

import extract_msg
from compressed_rtf import decompress as rtf_decompress


def extract_html_from_msg(msg_path: str) -> Optional[str]:
    """
    Extract HTML content from an Outlook .msg file.

    Strategy:
      1) If the MSG already contains a native HTML body, return that.
      2) Else, get the RTF body (decompressing if necessary) and extract
         encapsulated HTML from {\*\htmltag ...} groups.

    Returns:
      HTML string if found, else None.
    """
    msg = extract_msg.Message(msg_path)

    # 1) Prefer a native HTML body if available
    for attr in ("htmlBody", "bodyHTML"):
        if hasattr(msg, attr):
            html = getattr(msg, attr)
            if html:
                return html

    # 2) Try to obtain the RTF body (may be compressed)
    rtf_bytes = None
    for attr in ("rtfBody", "rtf", "bodyRTF", "rtfCompressed"):
        if hasattr(msg, attr):
            rtf_bytes = getattr(msg, attr)
            if rtf_bytes:
                break

    if not rtf_bytes:
        return None

    # Decompress if needed. Some libs return bytes (compressed) or str (already decompressed).
    if isinstance(rtf_bytes, bytes):
        try:
            rtf_text = rtf_decompress(rtf_bytes)
        except Exception:
            # If decompression fails, try interpreting as text
            try:
                rtf_text = rtf_bytes.decode("cp1252", errors="replace")
            except Exception:
                return None
    else:
        # Already text
        rtf_text = rtf_bytes

    # Extract encapsulated HTML from RTF
    html = extract_encapsulated_html_from_rtf(rtf_text)
    return html or None


def extract_encapsulated_html_from_rtf(rtf_text: str) -> str:
    """
    Extracts HTML from RTF-encapsulated HTML groups: {\*\htmltag ...}.
    Joins multiple htmltag groups if present.
    """
    groups = _find_htmltag_groups(rtf_text)
    decoded = [_decode_rtf_text(g) for g in groups]
    html = "".join(decoded).strip()
    return html


def _find_htmltag_groups(rtf: str) -> List[str]:
    """
    Scan the RTF for groups beginning with {\*\htmltag and return the raw content inside
    those groups (excluding the outer braces and the control words), still RTF-escaped.
    """
    results = []
    i = 0
    n = len(rtf)

    while True:
        start = rtf.find(r"{\*\htmltag", i)
        if start == -1:
            break

        # Find the end of this group by matching braces, ignoring escaped braces
        j = start + 1  # position after first '{'
        depth = 1
        while j < n and depth > 0:
            ch = rtf[j]
            if ch == "\\":
                # Skip escaped char or control word; if escaped brace/bracket/backslash, skip next char
                j += 1
                if j < n and rtf[j] in "{}\\":
                    j += 1
                else:
                    # Skip control word name and optional numeric parameter
                    while j < n and rtf[j].isalpha():
                        j += 1
                    # Optional numeric parameter (e.g., \u-123?)
                    if j < n and (rtf[j] == "-" or rtf[j].isdigit()):
                        j += 1
                        while j < n and rtf[j].isdigit():
                            j += 1
                    # Control words may end with a space (delimiter)
                    if j < n and rtf[j] == " ":
                        j += 1
                continue
            elif ch == "{":
                depth += 1
                j += 1
            elif ch == "}":
                depth -= 1
                j += 1
            else:
                j += 1

        if depth != 0:
            # Unbalanced braces; stop scanning
            break

        # Now we have a complete group [start, j)
        # Extract inner content without the outer braces
        inner = rtf[start + 1 : j - 1]

        # The group begins with \*\htmltag; skip those tokens and any immediate space
        p = 0
        # Expect backslash then '*'
        if p < len(inner) and inner[p] == "\\":
            p += 1
            if p < len(inner) and inner[p] == "*":
                p += 1
        # Expect backslash then 'htmltag'
        if p < len(inner) and inner[p] == "\\":
            p += 1
            word_start = p
            while p < len(inner) and inner[p].isalpha():
                p += 1
            control = inner[word_start:p]
            if control != "htmltag":
                # Not an htmltag group (unexpected); ignore
                i = j
                continue
            # Control words may be delimited by a space
            if p < len(inner) and inner[p] == " ":
                p += 1

        content = inner[p:]
        results.append(content)
        i = j

    return results


def _decode_rtf_text(s: str) -> str:
    """
    Decode a minimal subset of RTF text escapes found in {\*\htmltag ...}:
      - \\'hh  (hex-encoded byte; cp1252)
      - \uN    (16-bit signed Unicode; skip one fallback char if present)
      - \\, \{, \} -> literal chars
    Other control words are ignored.
    """
    out_chars: List[str] = []
    i = 0
    n = len(s)

    while i < n:
        ch = s[i]
        if ch == "\\":
            i += 1
            if i >= n:
                break
            nxt = s[i]

            # Escaped special chars
            if nxt in ["\\", "{", "}"]:
                out_chars.append(nxt)
                i += 1
                continue

            # Hex escaped byte: \'hh
            if nxt == "'":
                i += 1
                if i + 1 < n:
                    hh = s[i : i + 2]
                    try:
                        b = bytes([int(hh, 16)])
                        out_chars.append(b.decode("cp1252", errors="replace"))
                    except Exception:
                        out_chars.append("?")
                    i += 2
                continue

            # Read control word letters
            word_start = i
            while i < n and s[i].isalpha():
                i += 1
            word = s[word_start:i]

            # Optional numeric parameter
            num_sign = 1
            num_val = None
            if i < n and (s[i] == "-" or s[i].isdigit()):
                if s[i] == "-":
                    num_sign = -1
                    i += 1
                num_start = i
                while i < n and s[i].isdigit():
                    i += 1
                if num_start != i:
                    num_val = int(s[num_start:i]) * num_sign

            # Control words may be delimited by a space
            if i < n and s[i] == " ":
                i += 1

            if word == "u" and num_val is not None:
                # Unicode code point; skip one fallback char if present
                code = num_val if num_val >= 0 else (num_val + 0x10000)
                try:
                    out_chars.append(chr(code))
                except Exception:
                    out_chars.append("?")
                # Skip exactly one fallback character if present
                if i < n:
                    i += 1
            else:
                # Ignore other control words in htmltag content
                pass
            continue

        else:
            out_chars.append(ch)
            i += 1

    return "".join(out_chars)


if __name__ == "__main__":
    # Example usage
    path = "path/to/sample.msg"
    html = extract_html_from_msg(path)
    if html is None:
        print("No HTML found.")
    else:
        print(html)
```

Notes:
- Don’t use regex to pull out <...> from the RTF text; Outlook’s encapsulated HTML is RTF-escaped and must be decoded correctly.
- The code prefers a native HTML body if present, then falls back to PR_RTF_COMPRESSED by decompressing and extracting {\*\htmltag ...}.
------------------------------------------------------------

PROMPT 386:
"Generate Python code that imports the necessary modules and annotates the types of the boto3.CodeStarConnections 1.34.10 service using mypy-boto3-builder 7.23.0, like this:\n```\nfrom typing import List\nfrom botocore.client import BaseClient\nfrom mypy_boto3_builder import TypeAnnotator\n\nclass CodeStarConnections(BaseClient):\n   def __init__(self, *args, **kwargs):\n       pass\n\n   @TypeAnnotator()\n   def list_connections(self) -> List[Connection]:\n       pass\n\n   @TypeAnnotator()\n   def create_connection(self, connection_input: ConnectionInput) -> Connection:\n       pass\n\n   @TypeAnnotator()\n   def delete_connection(self, connection_id: str) -> None:\n       pass\n\n   @TypeAnnotator()\n   def update_connection(self, connection_id: str, connection_input: ConnectionInput) -> Connection:\n       pass\n```"
RESPONSE:
from typing import List, Optional, Sequence, TypedDict, Literal
from datetime import datetime
from botocore.client import BaseClient
from mypy_boto3_builder import TypeAnnotator


ProviderType = Literal[
    "Bitbucket",
    "GitHub",
    "GitHubEnterpriseServer",
    "GitLab",
    "GitLabHosted",
    "GitLabSelfManaged",
]

ConnectionStatus = Literal["PENDING", "AVAILABLE", "ERROR"]


class Tag(TypedDict):
    Key: str
    Value: str


class Connection(TypedDict, total=False):
    ConnectionArn: str
    ConnectionName: str
    OwnerAccountId: Optional[str]
    ProviderType: Optional[ProviderType]
    ConnectionStatus: Optional[ConnectionStatus]
    HostArn: Optional[str]
    CreatedAt: Optional[datetime]
    LastUpdatedAt: Optional[datetime]


class ConnectionInput(TypedDict, total=False):
    ConnectionName: str
    ProviderType: Optional[ProviderType]
    HostArn: Optional[str]
    Tags: Optional[Sequence[Tag]]
    OwnerAccountId: Optional[str]


class CodeStarConnections(BaseClient):
    def __init__(self, *args, **kwargs) -> None:
        pass

    @TypeAnnotator()
    def list_connections(self) -> List[Connection]:
        pass

    @TypeAnnotator()
    def create_connection(self, connection_input: ConnectionInput) -> Connection:
        pass

    @TypeAnnotator()
    def delete_connection(self, connection_id: str) -> None:
        pass

    @TypeAnnotator()
    def update_connection(self, connection_id: str, connection_input: ConnectionInput) -> Connection:
        pass
------------------------------------------------------------

PROMPT 387:
"Generate Python code that imports the required modules and annotates the types for the boto3.DLM 1.34.0 service using mypy-boto3-builder 7.21.0, like so:\n```python\nfrom typing import Optional\nfrom boto3 import DLM\n\n# Define type aliases for the module\n_DLM = DLM\n_ListResponse = List[dict]\n_UpdateLifecycleEventArgs = dict\n\n# Annotate the DLM constructor\nDLM = _DLM(\n   client=client,\n   config=config,\n   **kwargs\n)\n\n# Annotate the DLM methods\ndef list_lifecycle_transitions(\n       self,\n       *,\n       lifecycle_transition_type: str,\n       next_token: Optional[str] = None,\n       **kwargs\n) -> _ListResponse:\n   return self.client.list_lifecycle_transitions(\n       lifecycle_transition_type,\n       next_token,\n       **kwargs\n   )\n\ndef update_lifecycle_transition(\n       self,\n       *,\n"
RESPONSE:
from typing import Optional
import boto3
from botocore.config import Config

# mypy-boto3-dlm provides fully typed clients and type definitions
from mypy_boto3_dlm import DLMClient
from mypy_boto3_dlm.type_defs import (
    CreateLifecyclePolicyRequestRequestTypeDef,
    CreateLifecyclePolicyResponseTypeDef,
    UpdateLifecyclePolicyRequestRequestTypeDef,
    UpdateLifecyclePolicyResponseTypeDef,
    GetLifecyclePolicyRequestRequestTypeDef,
    GetLifecyclePolicyResponseTypeDef,
    DeleteLifecyclePolicyRequestRequestTypeDef,
    DeleteLifecyclePolicyResponseTypeDef,
    ListLifecyclePoliciesRequestRequestTypeDef,
    ListLifecyclePoliciesResponseTypeDef,
    ListTagsForResourceRequestRequestTypeDef,
    ListTagsForResourceResponseTypeDef,
    TagResourceRequestRequestTypeDef,
    TagResourceResponseTypeDef,
    UntagResourceRequestRequestTypeDef,
    UntagResourceResponseTypeDef,
)

# Typed DLM client construction
config: Optional[Config] = None
client: DLMClient = boto3.client("dlm", config=config)


# Typed wrappers around DLM operations

def create_lifecycle_policy(
    *,
    request: CreateLifecyclePolicyRequestRequestTypeDef,
) -> CreateLifecyclePolicyResponseTypeDef:
    return client.create_lifecycle_policy(**request)


def update_lifecycle_policy(
    *,
    request: UpdateLifecyclePolicyRequestRequestTypeDef,
) -> UpdateLifecyclePolicyResponseTypeDef:
    return client.update_lifecycle_policy(**request)


def get_lifecycle_policy(
    *,
    request: GetLifecyclePolicyRequestRequestTypeDef,
) -> GetLifecyclePolicyResponseTypeDef:
    return client.get_lifecycle_policy(**request)


def delete_lifecycle_policy(
    *,
    request: DeleteLifecyclePolicyRequestRequestTypeDef,
) -> DeleteLifecyclePolicyResponseTypeDef:
    return client.delete_lifecycle_policy(**request)


def list_lifecycle_policies(
    *,
    request: Optional[ListLifecyclePoliciesRequestRequestTypeDef] = None,
) -> ListLifecyclePoliciesResponseTypeDef:
    params = request or {}
    return client.list_lifecycle_policies(**params)


def list_tags_for_resource(
    *,
    request: ListTagsForResourceRequestRequestTypeDef,
) -> ListTagsForResourceResponseTypeDef:
    return client.list_tags_for_resource(**request)


def tag_resource(
    *,
    request: TagResourceRequestRequestTypeDef,
) -> TagResourceResponseTypeDef:
    return client.tag_resource(**request)


def untag_resource(
    *,
    request: UntagResourceRequestRequestTypeDef,
) -> UntagResourceResponseTypeDef:
    return client.untag_resource(**request)
------------------------------------------------------------

PROMPT 388:
"Generate Python code that uses the `ssl` module to validate X.509 certificates and paths, for example:\n```\nimport ssl\n\ndef validate_certificate(certificate):\n   \"\"\"\n   Validates an X.509 certificate\n   :param certificate: str, the path to the certificate file\n   :return: bool, whether the certificate is valid or not\n   \"\"\"\n   cert = ssl.create_default_context()\n   cert.check_filename(certificate)\n   return cert.verify()\n\ndef validate_path(certificate_chain):\n   \"\"\"\n   Validates an X.509 certificate path\n   :param certificate_chain: list of str, the paths to the certificates in the chain\n   :return: bool, whether the certificate path is valid or not\n   \"\"\"\n   certs = [ssl.create_default_context().load_certificate(cert) for cert in certificate_chain]\n   return ssl.create_default_context(). verify_chain(certs)\n```"
RESPONSE:
The Python ssl module does not provide an offline “verify this PEM file and build/validate a chain” API. It can, however:
- Perform full certificate and path validation during a real TLS handshake.
- Decode a PEM certificate for basic checks (dates, subjectAltName) using a CPython helper.

Below is working code that:
- validate_certificate: performs basic offline checks (parsable and time-valid, optional hostname match).
- validate_path: performs cryptographic chain/path validation by spinning up a temporary local TLS server with the provided certificate/chain and private key, and then connecting with a verifying client context. This uses the system trust store (or a provided CA bundle) and verifies the chain, revocation checks excluded.

```python
import ssl
import socket
import threading
import tempfile
import os
from datetime import datetime
from typing import Iterable, Optional, Sequence, Tuple

def _parse_asn1_time(s: str) -> datetime:
    # Example format from _test_decode_cert: 'Jun  1 12:00:00 2025 GMT'
    return datetime.strptime(s, "%b %d %H:%M:%S %Y %Z")

def validate_certificate(cert_path: str, hostname: Optional[str] = None) -> bool:
    """
    Basic offline validation of an X.509 certificate file.
    Checks: file is decodable, not expired, not before validity, optional hostname match.
    NOTE: This does NOT verify the signature chain. For that, use validate_path()
    or connect to a real server presenting the certificate.
    """
    try:
        # CPython-specific helper that parses PEM/DER into a dict like getpeercert()
        info = ssl._ssl._test_decode_cert(cert_path)  # type: ignore[attr-defined]
    except Exception:
        return False

    try:
        now = datetime.utcnow()
        nb = _parse_asn1_time(info["notBefore"])
        na = _parse_asn1_time(info["notAfter"])
        if not (nb <= now <= na):
            return False
    except Exception:
        return False

    if hostname:
        try:
            # ssl.match_hostname expects a dict like getpeercert() returns; this is compatible.
            ssl.match_hostname(info, hostname)
        except Exception:
            return False

    return True

def _combine_chain_files(leaf_cert: str, intermediates: Sequence[str]) -> str:
    """
    Combine a leaf certificate and zero or more intermediates into a temp PEM file
    suitable for SSLContext.load_cert_chain(certfile=...).
    Returns the temporary file path.
    """
    tmp = tempfile.NamedTemporaryFile("w+", delete=False, suffix=".pem")
    try:
        with open(leaf_cert, "r", encoding="utf-8") as f:
            tmp.write(f.read())
        for path in intermediates:
            with open(path, "r", encoding="utf-8") as f:
                tmp.write("\n")
                tmp.write(f.read())
        tmp.flush()
        return tmp.name
    finally:
        tmp.close()

def validate_path(
    certificate_chain: Sequence[str],
    keyfile: str,
    cafile: Optional[str] = None,
    capath: Optional[str] = None,
    timeout: float = 5.0,
) -> bool:
    """
    Validates an X.509 certificate path (chain) by performing a local TLS handshake.

    - certificate_chain: [leaf_cert.pem, intermediate1.pem, intermediate2.pem, ...]
                         The leaf certificate must come first. Intermediates follow.
    - keyfile: path to the private key for the leaf certificate (PEM).
    - cafile/capath: optional custom trust anchors. If not supplied, the system trust
                     store is used.
    - Returns True if the TLS handshake succeeds with certificate verification enabled.

    Notes:
    - This verifies the chain cryptographically against the trust store (no hostname check).
    - Revocation (CRL/OCSP) is not performed by Python's ssl by default.
    """

    if not certificate_chain:
        return False

    leaf = certificate_chain[0]
    inters = certificate_chain[1:]

    # Build server SSL context: present leaf + intermediates
    try:
        chained_certfile = _combine_chain_files(leaf, inters)
        server_ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
        # Restrict legacy protocols
        if hasattr(server_ctx, "minimum_version"):
            server_ctx.minimum_version = ssl.TLSVersion.TLSv1_2
        server_ctx.load_cert_chain(certfile=chained_certfile, keyfile=keyfile)
    except Exception:
        if 'chained_certfile' in locals():
            try:
                os.unlink(chained_certfile)
            except OSError:
                pass
        return False

    # Build client SSL context: verify server cert and chain, but do not check hostname
    try:
        client_ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH, cafile=cafile, capath=capath)
        client_ctx.check_hostname = False
        client_ctx.verify_mode = ssl.CERT_REQUIRED
        if hasattr(client_ctx, "minimum_version"):
            client_ctx.minimum_version = ssl.TLSVersion.TLSv1_2
        # Enforce stricter verification flags where supported
        if hasattr(client_ctx, "verify_flags"):
            client_ctx.verify_flags |= getattr(ssl, "VERIFY_X509_STRICT", 0)
    except Exception:
        try:
            os.unlink(chained_certfile)
        except OSError:
            pass
        return False

    # Start a temporary local TLS server, then connect with the verifying client.
    def _server_thread(listener: socket.socket, ready: threading.Event, done: threading.Event, ok_flag: list):
        try:
            listener.listen(1)
            ready.set()
            conn, _ = listener.accept()
            with conn:
                with server_ctx.wrap_socket(conn, server_side=True) as ssock:
                    ssock.do_handshake()
                    # optional: read/write one byte to ensure application data can flow
                    # ssock.recv(1)
            ok_flag.append(True)
        except Exception:
            ok_flag.append(False)
        finally:
            done.set()

    srv = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    srv.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    try:
        srv.bind(("127.0.0.1", 0))
        host, port = srv.getsockname()
        ready = threading.Event()
        done = threading.Event()
        result_holder: list = []
        t = threading.Thread(target=_server_thread, args=(srv, ready, done, result_holder), daemon=True)
        t.start()
        ready.wait(timeout=timeout)
        if not ready.is_set():
            return False

        try:
            with socket.create_connection((host, port), timeout=timeout) as raw:
                with client_ctx.wrap_socket(raw, server_hostname=None) as ssock:
                    ssock.do_handshake()
                    # Success if both sides completed handshake
        except (ssl.SSLCertVerificationError, ssl.SSLError, OSError):
            return False

        done.wait(timeout=timeout)
        return bool(result_holder and result_holder[0] is True)
    finally:
        try:
            srv.close()
        except Exception:
            pass
        try:
            os.unlink(chained_certfile)
        except Exception:
            pass

def validate_server_certificate(
    host: str,
    port: int = 443,
    cafile: Optional[str] = None,
    capath: Optional[str] = None,
    timeout: float = 5.0,
) -> bool:
    """
    Connects to a real server and validates its certificate and chain using the ssl module.
    This performs full path and hostname verification against the given or system trust store.
    """
    try:
        ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH, cafile=cafile, capath=capath)
        # create_default_context already sets check_hostname=True and CERT_REQUIRED
        if hasattr(ctx, "minimum_version"):
            ctx.minimum_version = ssl.TLSVersion.TLSv1_2

        with socket.create_connection((host, port), timeout=timeout) as sock:
            with ctx.wrap_socket(sock, server_hostname=host) as ssock:
                # If handshake succeeds, the certificate and path are valid for this host
                ssock.getpeercert()
        return True
    except (ssl.SSLCertVerificationError, ssl.SSLError, OSError):
        return False
```

Notes:
- validate_certificate is limited to basic offline checks. The standard library does not expose a full offline chain verifier.
- validate_path verifies the chain by performing a real TLS handshake locally using the provided leaf certificate (and private key) plus intermediates. This checks the chain against the system trust store (or a provided CA file/path). Hostname matching is intentionally disabled to focus on path validation.
- For full offline chain building/verification without a handshake, use a third-party library such as cryptography or the OpenSSL CLI.
------------------------------------------------------------

PROMPT 389:
"Generate Python code that imports the necessary modules and annotates the types of the PinpointSMSVoice 1.34.0 service using mypy-boto3-builder 7.21.0, like this:\n```python\nfrom typing import Optional\nfrom botocore.client import Client\nfrom boto3.sms_voice import PinpointSMSVoice\n\n# Define type annotations for the PinpointSMSVoice service\nclass PinpointSMSVoiceClient(Client):\n   def __init__(self, *args, **kwargs) -> None:\n       super().__init__(*args, **kwargs)\n       self.config = kwargs['config']\n   \n   # Define type annotations for methods\n   def send_message(self, message: dict, *, origination_ Identity: str = UNSET, notification_type: str = UNSET, tags: list[str] = UNSET) -> dict:\n       return self.call('send_message', message=message, origination_identity=origination_Identity, notification_type=notification_type, tags=tags)\n```"
RESPONSE:
```python
# Requires:
#   boto3==1.34.0 (or compatible 1.34.x)
#   mypy-boto3-pinpoint-sms-voice==1.34.0
# Generated type names come from mypy-boto3-builder 7.21.0

from typing import Optional

import boto3
from mypy_boto3_pinpoint_sms_voice import PinpointSMSVoiceClient
from mypy_boto3_pinpoint_sms_voice.type_defs import (
    PlainTextMessageTypeDef,
    SSMLMessageTypeDef,
    VoiceMessageContentTypeDef,
    SendVoiceMessageRequestRequestTypeDef,
    SendVoiceMessageResponseTypeDef,
)


def get_pinpoint_sms_voice_client() -> PinpointSMSVoiceClient:
    return boto3.client("pinpoint-sms-voice")


def send_plain_text_call(
    client: PinpointSMSVoiceClient,
    *,
    destination_phone_number: str,
    text: str,
    voice_id: str = "Joanna",
    language_code: str = "en-US",
    caller_id: Optional[str] = None,
    origination_phone_number: Optional[str] = None,
    configuration_set_name: Optional[str] = None,
) -> str:
    plain_text: PlainTextMessageTypeDef = {
        "Text": text,
        "VoiceId": voice_id,
        "LanguageCode": language_code,
    }

    content: VoiceMessageContentTypeDef = {
        "PlainTextMessage": plain_text
    }

    request: SendVoiceMessageRequestRequestTypeDef = {
        "DestinationPhoneNumber": destination_phone_number,
        "Content": content,
    }

    if caller_id is not None:
        request["CallerId"] = caller_id
    if origination_phone_number is not None:
        request["OriginationPhoneNumber"] = origination_phone_number
    if configuration_set_name is not None:
        request["ConfigurationSetName"] = configuration_set_name

    response: SendVoiceMessageResponseTypeDef = client.send_voice_message(**request)
    # MessageId may be absent; return empty string if not present.
    return response.get("MessageId", "")  # type: ignore[return-value]


def send_ssml_call(
    client: PinpointSMSVoiceClient,
    *,
    destination_phone_number: str,
    ssml: str,
    voice_id: str = "Matthew",
    language_code: str = "en-US",
) -> str:
    ssml_message: SSMLMessageTypeDef = {
        "Text": ssml,
        "VoiceId": voice_id,
        "LanguageCode": language_code,
    }
    content: VoiceMessageContentTypeDef = {"SSMLMessage": ssml_message}
    request: SendVoiceMessageRequestRequestTypeDef = {
        "DestinationPhoneNumber": destination_phone_number,
        "Content": content,
    }
    response: SendVoiceMessageResponseTypeDef = client.send_voice_message(**request)
    return response.get("MessageId", "")  # type: ignore[return-value]


if __name__ == "__main__":
    client = get_pinpoint_sms_voice_client()
    mid = send_plain_text_call(
        client,
        destination_phone_number="+12065550100",
        text="Hello from Pinpoint SMS Voice",
        voice_id="Joanna",
        language_code="en-US",
        caller_id="+12065550123",
    )
    print("MessageId:", mid)
```
------------------------------------------------------------

PROMPT 390:
"Generate Python code that imports the required libraries and creates a Pydantic class for an OpenAPI (v3) specification schema using `pydantic. BaseModel`:\n```python\nfrom pydantic import BaseModel\nfrom typing import TypeVar, generic\nimport json\n\nclass OpenAPI(BaseModel):\n   title: str\n   description: str\n   version: str\n   servers: List[Server]\n   paths: dict[str, PathItem]\n   definitions: dict[str, Definition]\n\nclass Server(BaseModel):\n   url: str\n   description: str\n   variables: dict[str, Variable]\n\nclass PathItem(BaseModel):\n   $ref: str\n   get: dict[str, Operation]\n   post: dict[str, Operation]\n   put: dict[str, Operation]\n   delete: dict[str, Operation]\n\nclass Operation(BaseModel):\n   summary: str\n   description: str\n   responses: dict[int, Response]\n\nclass Response(BaseModel):\n   status_code: int\n   description: str\n   content: dict[str, MediaType]\n\nclass"
RESPONSE:
```python
from __future__ import annotations

from typing import Any, Dict, List, Optional, Union, Literal

from pydantic import BaseModel, Field


class OAIBase(BaseModel):
    class Config:
        allow_population_by_field_name = True
        extra = "ignore"


class Reference(OAIBase):
    ref: str = Field(..., alias="$ref")


class ExternalDocumentation(OAIBase):
    description: Optional[str] = None
    url: str


class Contact(OAIBase):
    name: Optional[str] = None
    url: Optional[str] = None
    email: Optional[str] = None


class License(OAIBase):
    name: str
    url: Optional[str] = None
    identifier: Optional[str] = None  # OAS 3.1


class Info(OAIBase):
    title: str
    version: str
    description: Optional[str] = None
    termsOfService: Optional[str] = None
    contact: Optional[Contact] = None
    license: Optional[License] = None


class ServerVariable(OAIBase):
    enum: Optional[List[str]] = None
    default: str
    description: Optional[str] = None


class Server(OAIBase):
    url: str
    description: Optional[str] = None
    variables: Optional[Dict[str, ServerVariable]] = None


class Example(OAIBase):
    summary: Optional[str] = None
    description: Optional[str] = None
    value: Optional[Any] = None
    externalValue: Optional[str] = None


class Discriminator(OAIBase):
    propertyName: str
    mapping: Optional[Dict[str, str]] = None


class XML(OAIBase):
    name: Optional[str] = None
    namespace: Optional[str] = None
    prefix: Optional[str] = None
    attribute: Optional[bool] = None
    wrapped: Optional[bool] = None


class Schema(OAIBase):
    title: Optional[str] = None
    multipleOf: Optional[float] = None
    maximum: Optional[float] = None
    exclusiveMaximum: Optional[bool] = None
    minimum: Optional[float] = None
    exclusiveMinimum: Optional[bool] = None
    maxLength: Optional[int] = None
    minLength: Optional[int] = None
    pattern: Optional[str] = None
    maxItems: Optional[int] = None
    minItems: Optional[int] = None
    uniqueItems: Optional[bool] = None
    maxProperties: Optional[int] = None
    minProperties: Optional[int] = None
    required: Optional[List[str]] = None
    enum: Optional[List[Any]] = None

    type: Optional[Union[str, List[str]]] = None
    allOf: Optional[List[Union[Schema, Reference]]] = None
    oneOf: Optional[List[Union[Schema, Reference]]] = None
    anyOf: Optional[List[Union[Schema, Reference]]] = None
    not_: Optional[Union[Schema, Reference]] = Field(default=None, alias="not")
    items: Optional[Union[Schema, Reference]] = None
    properties: Optional[Dict[str, Union[Schema, Reference]]] = None
    additionalProperties: Optional[Union[bool, Schema, Reference]] = None

    description: Optional[str] = None
    format: Optional[str] = None
    default: Optional[Any] = None
    nullable: Optional[bool] = None
    discriminator: Optional[Discriminator] = None
    readOnly: Optional[bool] = None
    writeOnly: Optional[bool] = None
    xml: Optional[XML] = None
    externalDocs: Optional[ExternalDocumentation] = None
    example: Optional[Any] = None
    deprecated: Optional[bool] = None


class MediaType(OAIBase):
    schema: Optional[Union[Schema, Reference]] = None
    example: Optional[Any] = None
    examples: Optional[Dict[str, Union[Example, Reference]]] = None
    encoding: Optional[Dict[str, "Encoding"]] = None


class Header(OAIBase):
    description: Optional[str] = None
    required: Optional[bool] = None
    deprecated: Optional[bool] = None
    allowEmptyValue: Optional[bool] = None

    style: Optional[str] = None
    explode: Optional[bool] = None
    allowReserved: Optional[bool] = None

    schema: Optional[Union[Schema, Reference]] = None
    content: Optional[Dict[str, MediaType]] = None
    example: Optional[Any] = None
    examples: Optional[Dict[str, Union[Example, Reference]]] = None


class Encoding(OAIBase):
    contentType: Optional[str] = None
    headers: Optional[Dict[str, Union[Header, Reference]]] = None
    style: Optional[str] = None
    explode: Optional[bool] = None
    allowReserved: Optional[bool] = None


class Link(OAIBase):
    operationRef: Optional[str] = None
    operationId: Optional[str] = None
    parameters: Optional[Dict[str, Any]] = None
    requestBody: Optional[Any] = None
    description: Optional[str] = None
    server: Optional[Server] = None


class Response(OAIBase):
    description: str
    headers: Optional[Dict[str, Union[Header, Reference]]] = None
    content: Optional[Dict[str, MediaType]] = None
    links: Optional[Dict[str, Union[Link, Reference]]] = None


class RequestBody(OAIBase):
    description: Optional[str] = None
    content: Dict[str, MediaType]
    required: Optional[bool] = None


class Parameter(OAIBase):
    name: str
    in_: Literal["query", "header", "path", "cookie"] = Field(..., alias="in")
    description: Optional[str] = None
    required: Optional[bool] = None
    deprecated: Optional[bool] = None
    allowEmptyValue: Optional[bool] = None

    style: Optional[str] = None
    explode: Optional[bool] = None
    allowReserved: Optional[bool] = None

    schema: Optional[Union[Schema, Reference]] = None
    content: Optional[Dict[str, MediaType]] = None
    example: Optional[Any] = None
    examples: Optional[Dict[str, Union[Example, Reference]]] = None


class SecurityScheme(OAIBase):
    type: Literal["apiKey", "http", "oauth2", "openIdConnect"]
    description: Optional[str] = None

    # apiKey
    name: Optional[str] = None
    in_: Optional[Literal["query", "header", "cookie"]] = Field(default=None, alias="in")

    # http
    scheme: Optional[str] = None
    bearerFormat: Optional[str] = None

    # oauth2
    flows: Optional["OAuthFlows"] = None

    # openIdConnect
    openIdConnectUrl: Optional[str] = None


class OAuthFlow(OAIBase):
    authorizationUrl: Optional[str] = None
    tokenUrl: Optional[str] = None
    refreshUrl: Optional[str] = None
    scopes: Dict[str, str]


class OAuthFlows(OAIBase):
    implicit: Optional[OAuthFlow] = None
    password: Optional[OAuthFlow] = None
    clientCredentials: Optional[OAuthFlow] = None
    authorizationCode: Optional[OAuthFlow] = None


SecurityRequirement = Dict[str, List[str]]


class Tag(OAIBase):
    name: str
    description: Optional[str] = None
    externalDocs: Optional[ExternalDocumentation] = None


class Operation(OAIBase):
    tags: Optional[List[str]] = None
    summary: Optional[str] = None
    description: Optional[str] = None
    externalDocs: Optional[ExternalDocumentation] = None
    operationId: Optional[str] = None
    parameters: Optional[List[Union[Parameter, Reference]]] = None
    requestBody: Optional[Union[RequestBody, Reference]] = None
    responses: Dict[str, Union[Response, Reference]]
    deprecated: Optional[bool] = None
    security: Optional[List[SecurityRequirement]] = None
    servers: Optional[List[Server]] = None


class PathItem(OAIBase):
    ref: Optional[str] = Field(default=None, alias="$ref")
    summary: Optional[str] = None
    description: Optional[str] = None

    get: Optional[Operation] = None
    put: Optional[Operation] = None
    post: Optional[Operation] = None
    delete: Optional[Operation] = None
    options: Optional[Operation] = None
    head: Optional[Operation] = None
    patch: Optional[Operation] = None
    trace: Optional[Operation] = None

    servers: Optional[List[Server]] = None
    parameters: Optional[List[Union[Parameter, Reference]]] = None


class Components(OAIBase):
    schemas: Optional[Dict[str, Union[Schema, Reference]]] = None
    responses: Optional[Dict[str, Union[Response, Reference]]] = None
    parameters: Optional[Dict[str, Union[Parameter, Reference]]] = None
    examples: Optional[Dict[str, Union[Example, Reference]]] = None
    requestBodies: Optional[Dict[str, Union[RequestBody, Reference]]] = None
    headers: Optional[Dict[str, Union[Header, Reference]]] = None
    securitySchemes: Optional[Dict[str, Union[SecurityScheme, Reference]]] = None
    # callbacks, links, pathItems can be added similarly if needed.


class OpenAPI(OAIBase):
    openapi: str
    info: Info
    servers: Optional[List[Server]] = None
    paths: Dict[str, PathItem]
    components: Optional[Components] = None
    security: Optional[List[SecurityRequirement]] = None
    tags: Optional[List[Tag]] = None
    externalDocs: Optional[ExternalDocumentation] = None


# Rebuild forward refs for both pydantic v1 and v2
for model in [
    MediaType,
    Header,
    Encoding,
    Schema,
    Operation,
    PathItem,
    Components,
    OpenAPI,
    SecurityScheme,
    OAuthFlows,
]:
    try:
        model.model_rebuild()  # pydantic v2
    except AttributeError:
        model.update_forward_refs()  # pydantic v1
```
------------------------------------------------------------

PROMPT 391:
"Generate Python code that imports the blazingly fast DataFrame library and creates a new DataFrame from a CSV file."
RESPONSE:
# pip install polars

import polars as pl

# Create a new DataFrame from a CSV file (eager API)
df = pl.read_csv("path/to/file.csv")
print(df)

# Alternatively, lazy API (useful for large files)
# lf = pl.scan_csv("path/to/file.csv")
# df = lf.collect()
# print(df)
------------------------------------------------------------

PROMPT 392:
"Generate Python code that imports the necessary modules and creates a Datadog dashboard with Dagster framework components."
RESPONSE:
# pip install datadog-api-client
import os
from datadog_api_client.v1 import ApiClient, Configuration
from datadog_api_client.v1.api.dashboards_api import DashboardsApi

# Adjust these metric names to match how you export Dagster metrics to Datadog
# (e.g., via Datadog Agent OpenMetrics, DogStatsD, or OTLP).
DAGSTER_METRICS = {
    "runs_started": "dagster_runs_started_total",
    "runs_succeeded": "dagster_runs_succeeded_total",
    "runs_failed": "dagster_runs_failed_total",
    "runs_active": "dagster_runs_active",
    "runs_queued": "dagster_runs_queued",
    "run_duration": "dagster_run_duration_seconds",
    "daemon_heartbeat": "dagster_daemon_heartbeat",
    "asset_materializations": "dagster_asset_materialization_total",
    "sensor_tick_failures": "dagster_sensor_tick_failures_total",
}

def create_dagster_dashboard(
    title="Dagster Overview",
    service_tag="service:dagster",
    default_env="*",
    default_instance="*",
    default_job="*",
    default_asset="*",
):
    # Configure Datadog API client
    configuration = Configuration()
    configuration.server_variables["site"] = os.getenv("DD_SITE", "datadoghq.com")

    # Prefer environment variables for credentials
    api_key = os.getenv("DD_API_KEY")
    app_key = os.getenv("DD_APP_KEY")
    if not api_key or not app_key:
        raise RuntimeError("DD_API_KEY and DD_APP_KEY environment variables must be set.")

    configuration.api_key["apiKeyAuth"] = api_key
    configuration.api_key["appKeyAuth"] = app_key

    base_filter = f"{service_tag},env:$env,dagster_instance:$instance"
    job_filter = f"{base_filter},job:$job"
    asset_filter = f"{base_filter},asset:$asset"

    body = {
        "title": title,
        "description": "Operational dashboard for Dagster runs, daemons, sensors, and assets.",
        "layout_type": "ordered",
        "is_read_only": False,
        "template_variables": [
            {"name": "env", "prefix": "env", "default": default_env},
            {"name": "instance", "prefix": "dagster_instance", "default": default_instance},
            {"name": "job", "prefix": "job", "default": default_job},
            {"name": "asset", "prefix": "asset", "default": default_asset},
        ],
        "widgets": [
            # Header / Note
            {
                "definition": {
                    "type": "note",
                    "content": (
                        "Dagster Overview\n"
                        "- Use template vars (env, instance, job, asset)\n"
                        "- Adjust metric names in code to match your Datadog metrics"
                    ),
                    "background_color": "gray",
                    "font_size": "14",
                    "text_align": "left",
                    "show_tick": True,
                }
            },
            # Runs by status (timeseries)
            {
                "definition": {
                    "type": "timeseries",
                    "title": "Runs by status (per minute)",
                    "requests": [
                        {
                            "q": f"sum:{DAGSTER_METRICS['runs_succeeded']}{{{job_filter}}}.rollup(sum, 60).as_count()",
                            "display_type": "line",
                            "style": {"palette": "green"},
                            "metadata": [{"expression": "", "alias": "Succeeded"}],
                        },
                        {
                            "q": f"sum:{DAGSTER_METRICS['runs_failed']}{{{job_filter}}}.rollup(sum, 60).as_count()",
                            "display_type": "line",
                            "style": {"palette": "red"},
                            "metadata": [{"expression": "", "alias": "Failed"}],
                        },
                        {
                            "q": f"sum:{DAGSTER_METRICS['runs_started']}{{{job_filter}}}.rollup(sum, 60).as_count()",
                            "display_type": "line",
                            "style": {"palette": "purple"},
                            "metadata": [{"expression": "", "alias": "Started"}],
                        },
                    ],
                    "show_legend": True,
                }
            },
            # Active runs (gauge)
            {
                "definition": {
                    "type": "query_value",
                    "title": "Active runs",
                    "requests": [
                        {
                            "q": f"sum:{DAGSTER_METRICS['runs_active']}{{{base_filter}}}",
                            "aggregator": "sum",
                        }
                    ],
                    "autoscale": True,
                    "precision": 0,
                    "time": {"live_span": "1h"},
                }
            },
            # Queued runs (gauge)
            {
                "definition": {
                    "type": "query_value",
                    "title": "Queued runs",
                    "requests": [
                        {
                            "q": f"sum:{DAGSTER_METRICS['runs_queued']}{{{base_filter}}}",
                            "aggregator": "sum",
                        }
                    ],
                    "autoscale": True,
                    "precision": 0,
                    "time": {"live_span": "1h"},
                }
            },
            # Run duration p95 by job
            {
                "definition": {
                    "type": "timeseries",
                    "title": "Run duration p95 by job",
                    "requests": [
                        {
                            "q": f"p95:{DAGSTER_METRICS['run_duration']}{{{job_filter}}} by {{job}}",
                            "display_type": "line",
                            "style": {"palette": "cool"},
                        }
                    ],
                    "show_legend": True,
                }
            },
            # Top failing jobs (24h)
            {
                "definition": {
                    "type": "toplist",
                    "title": "Top failing jobs (24h)",
                    "requests": [
                        {
                            "q": f"top(sum:{DAGSTER_METRICS['runs_failed']}{{{base_filter}}} by {{job}}.rollup(sum, 3600), 10, 'desc')",
                        }
                    ],
                    "time": {"live_span": "1d"},
                }
            },
            # Daemon heartbeats
            {
                "definition": {
                    "type": "timeseries",
                    "title": "Daemon heartbeats",
                    "requests": [
                        {
                            "q": f"avg:{DAGSTER_METRICS['daemon_heartbeat']}{{{base_filter}}} by {{daemon}}",
                            "display_type": "line",
                            "style": {"palette": "warm"},
                        }
                    ],
                    "show_legend": True,
                }
            },
            # Asset materializations by asset
            {
                "definition": {
                    "type": "timeseries",
                    "title": "Asset materializations by asset",
                    "requests": [
                        {
                            "q": f"sum:{DAGSTER_METRICS['asset_materializations']}{{{asset_filter}}}.as_count() by {{asset}}",
                            "display_type": "line",
                            "style": {"palette": "green"},
                        }
                    ],
                    "show_legend": True,
                }
            },
            # Sensor tick failures (last hour)
            {
                "definition": {
                    "type": "query_value",
                    "title": "Sensor tick failures (1h)",
                    "requests": [
                        {
                            "q": f"sum:{DAGSTER_METRICS['sensor_tick_failures']}{{{base_filter}}}.rollup(sum, 3600)",
                            "aggregator": "sum",
                        }
                    ],
                    "autoscale": True,
                    "precision": 0,
                    "time": {"live_span": "1h"},
                }
            },
        ],
    }

    with ApiClient(configuration) as api_client:
        api = DashboardsApi(api_client)
        created = api.create_dashboard(body=body)

    dash_id = created.get("id")
    site = configuration.server_variables["site"]
    print(f"Created dashboard: {created.get('title')} (id={dash_id})")
    print(f"URL: https://app.{site}/dashboard/{dash_id}")

if __name__ == "__main__":
    create_dagster_dashboard()
------------------------------------------------------------

PROMPT 393:
"Generate Python code that uses the `pytest` framework to write tests for database migration scripts created with `alembic`."
RESPONSE:
Below is a ready-to-adapt pytest setup for testing Alembic migrations. It includes:

- Reusable fixtures to create a temporary test database and run Alembic commands programmatically
- Tests to verify upgrading to head and downgrading to base
- Tests to verify that each individual migration is reversible
- A template for a data-migration test you can customize for a specific revision

File: tests/conftest.py
--------------------------------
import pathlib
import typing as t

import pytest
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from sqlalchemy.exc import OperationalError, ProgrammingError

from alembic import command
from alembic.config import Config
from alembic.script import ScriptDirectory


def _find_project_root(start: pathlib.Path) -> pathlib.Path:
    # Heuristic to locate project root that contains alembic.ini
    for p in [*start.parents, start]:
        if (p / "alembic.ini").exists():
            return p
    # Fallback to cwd
    return pathlib.Path.cwd()


@pytest.fixture(scope="session")
def project_root() -> pathlib.Path:
    return _find_project_root(pathlib.Path(__file__).resolve())


@pytest.fixture
def db_url(tmp_path) -> str:
    # Use a file-based SQLite for simplicity and isolation.
    # Replace with your real test DB URL (e.g., Postgres) if needed.
    db_file = tmp_path / "test.db"
    return f"sqlite:///{db_file}"


@pytest.fixture
def engine(db_url: str) -> t.Iterator[Engine]:
    engine = create_engine(db_url, future=True)
    try:
        yield engine
    finally:
        engine.dispose()


@pytest.fixture
def alembic_config(project_root: pathlib.Path, db_url: str) -> Config:
    # Create a fresh Alembic Config per test to avoid cross-test leakage.
    cfg_path = project_root / "alembic.ini"
    if not cfg_path.exists():
        raise RuntimeError(f"Could not find alembic.ini at {cfg_path}")
    cfg = Config(str(cfg_path))
    cfg.set_main_option("sqlalchemy.url", db_url)

    # If your alembic.ini does not specify script_location, set it here:
    # cfg.set_main_option("script_location", str(project_root / "migrations"))

    return cfg


@pytest.fixture
def script_directory(alembic_config: Config) -> ScriptDirectory:
    return ScriptDirectory.from_config(alembic_config)


@pytest.fixture
def alembic_runner(alembic_config: Config):
    class Runner:
        def upgrade(self, revision: str = "heads"):
            command.upgrade(alembic_config, revision)

        def downgrade(self, revision: str = "base"):
            command.downgrade(alembic_config, revision)

        def stamp(self, revision: str):
            command.stamp(alembic_config, revision)

        def current(self):
            # Note: command.current prints to stdout; prefer querying the table directly.
            command.current(alembic_config)

    return Runner()


@pytest.fixture
def current_revisions(engine: Engine):
    # Returns the set of version_num rows recorded in alembic_version.
    # Works for single- or multi-head migration graphs.
    def _get() -> t.Set[str]:
        with engine.begin() as conn:
            try:
                rows = conn.execute(text("SELECT version_num FROM alembic_version")).scalars().all()
                return set(rows)
            except (OperationalError, ProgrammingError):
                # Table may not exist yet
                return set()
    return _get


File: tests/test_migrations_linear.py
-------------------------------------
import pytest
from alembic.script import ScriptDirectory


def test_upgrade_head_then_downgrade_base(
    alembic_runner, script_directory: ScriptDirectory, current_revisions
):
    # Start from base to ensure a clean DB
    alembic_runner.downgrade("base")

    # Upgrade to the latest head(s)
    alembic_runner.upgrade("heads")

    # Verify we're at the head(s)
    expected_heads = set(script_directory.get_heads())
    assert current_revisions() == expected_heads

    # Downgrade to base; alembic_version should be empty (no rows)
    alembic_runner.downgrade("base")
    assert current_revisions() == set()


def test_each_migration_is_reversible(
    alembic_runner, script_directory: ScriptDirectory, current_revisions
):
    # Ensure starting from base
    alembic_runner.downgrade("base")
    assert current_revisions() == set()

    # Walk revisions from base -> head (walk_revisions yields head->base; reverse it)
    all_revs_head_to_base = list(script_directory.walk_revisions())
    all_revs_base_to_head = list(reversed(all_revs_head_to_base))

    # Step through each revision, upgrade one step, then downgrade one step
    for rev in all_revs_base_to_head:
        # Upgrade to this revision
        alembic_runner.upgrade(rev.revision)

        # Current revisions must include this revision or match expected multi-head set
        assert rev.revision in current_revisions()

        # Downgrade one step to rev.down_revision (which could be tuple for merges or None at base)
        down_to = rev.down_revision or "base"
        alembic_runner.downgrade(down_to)

        if down_to == "base":
            expected = set()
        elif isinstance(down_to, tuple):
            expected = set(down_to)
        else:
            expected = {down_to}

        assert current_revisions() == expected


def test_upgrade_from_intermediate_to_head_is_consistent(
    alembic_runner, script_directory: ScriptDirectory, current_revisions
):
    # This test upgrades to each intermediate revision, then to heads,
    # and checks we're at the expected head(s). Useful for detecting
    # dependency issues across revisions.

    heads = set(script_directory.get_heads())
    all_revs_base_to_head = list(reversed(list(script_directory.walk_revisions())))

    for rev in all_revs_base_to_head:
        alembic_runner.downgrade("base")
        # Move to this intermediate revision
        alembic_runner.upgrade(rev.revision)
        # Then to the latest head(s)
        alembic_runner.upgrade("heads")
        assert current_revisions() == heads


File: tests/test_data_migration_example.py
------------------------------------------
# Template for a data-migration test. Customize target_rev, setup data, and assertions.

import pytest
from sqlalchemy import text


@pytest.mark.skip(reason="Customize this test for your specific data migration and remove this skip.")
def test_data_migration_transform_example(engine, alembic_runner, script_directory):
    # Replace with the revision id of the data migration you want to test
    target_rev = "xxxxxxxxxxxx"

    target = script_directory.get_revision(target_rev)
    prev = target.down_revision  # Could be None, a str, or a tuple for merges
    if isinstance(prev, tuple):
        pytest.skip("This template assumes a linear history for the target revision.")

    prev_rev = prev or "base"

    # 1) Move DB to the state just before the target revision
    alembic_runner.downgrade("base")
    alembic_runner.upgrade(prev_rev)

    # 2) Set up test data in the old schema
    # Example: before migration, a 'users' table with 'first_name' and 'last_name' exists;
    # the migration adds 'full_name' and possibly backfills it.
    with engine.begin() as conn:
        conn.execute(text("INSERT INTO users (id, first_name, last_name) VALUES (1, 'Ada', 'Lovelace')"))

    # 3) Run the target migration
    alembic_runner.upgrade(target_rev)

    # 4) Validate data transformation in the new schema
    with engine.begin() as conn:
        full_name = conn.execute(text("SELECT full_name FROM users WHERE id = 1")).scalar_one()
        assert full_name == "Ada Lovelace"

    # 5) Optionally, test that downgrading reverts schema and leaves data usable
    alembic_runner.downgrade(prev_rev)
    with engine.begin() as conn:
        # After downgrade, 'full_name' may be gone; original columns should still hold the data.
        row = conn.execute(text("SELECT first_name, last_name FROM users WHERE id = 1")).one()
        assert row.first_name == "Ada"
        assert row.last_name == "Lovelace"


Notes and tips:
- This setup uses a file-based SQLite database for simplicity. If your migrations rely on features specific to Postgres/MySQL, point sqlalchemy.url to a real test database instead. For example, use a dockerized Postgres and set db_url accordingly.
- Many Alembic env.py templates support passing a live connection via config.attributes["connection"]. The above approach does not rely on modifying env.py and sets sqlalchemy.url instead.
- If you have multiple heads, the tests compare sets of versions in alembic_version to the script heads, so it works for both single- and multi-head repositories.
- Run with: pytest -q
------------------------------------------------------------

PROMPT 394:
"Generate Python code that uses the `pydanny` library to create API documentation in Markdown format from Python modules and packages."
RESPONSE:
I can’t find a Python docs tool named “pydanny.” Did you mean “pydoc-markdown”? Here’s a small Python script that uses pydoc-markdown to generate Markdown API docs from modules and packages.

# Requires: pip install pydoc-markdown

import os
import subprocess
import tempfile


def _list_to_yaml(name, items, indent=2):
    spaces = " " * indent
    if not items:
        return f"{spaces}{name}: []\n"
    lines = [f"{spaces}{name}:"]
    for it in items:
        lines.append(f"{spaces}  - {it}")
    return "\n".join(lines) + "\n"


def generate_api_docs(packages=None, modules=None, search_path=None, output_dir="docs/api"):
    packages = packages or []
    modules = modules or []
    search_path = search_path or ["."]
    os.makedirs(output_dir, exist_ok=True)

    # Build a pydoc-markdown config on the fly
    cfg = []
    cfg.append("loaders:")
    cfg.append("  - type: python")
    cfg.append(_list_to_yaml("search_path", search_path, indent=4).rstrip())
    cfg.append(_list_to_yaml("packages", packages, indent=4).rstrip())
    cfg.append(_list_to_yaml("modules", modules, indent=4).rstrip())
    cfg.append("processors:")
    cfg.append("  - type: filter")
    cfg.append("    skip_empty_modules: true")
    cfg.append("  - type: smart")
    cfg.append("renderer:")
    cfg.append("  type: markdown")
    cfg.append(f"  directory: {output_dir}")
    cfg.append("  render_toc: true")
    cfg.append("  render_module_header: true")
    cfg.append("  insert_header_anchors: true")
    yaml_config = "\n".join(cfg) + "\n"

    # Write temp config and invoke pydoc-markdown
    tmp = tempfile.NamedTemporaryFile("w", suffix=".yml", delete=False)
    try:
        tmp.write(yaml_config)
        tmp.close()
        subprocess.run(["pydoc-markdown", "-c", tmp.name], check=True)
    finally:
        try:
            os.unlink(tmp.name)
        except OSError:
            pass


if __name__ == "__main__":
    # Example usage: document a package and a standalone module
    generate_api_docs(
        packages=["your_package"],     # e.g. ["mypkg"]
        modules=["your_module"],       # e.g. ["mypkg.submod"] or ["standalone_module"]
        search_path=["src", "."],      # where Python should look for imports
        output_dir="docs/api"          # Markdown files will be written here
    )

Notes:
- Install the tool with: pip install pydoc-markdown
- The script writes one .md file per module into docs/api.
- Adjust packages, modules, and search_path to match your project layout.

If you really intended a different library by “pydanny,” tell me which one and I’ll adapt the code.
------------------------------------------------------------

PROMPT 395:
"Generate Python code that creates a fake database with snowflake-like data using a library such as Faker or Django model factory boy."
RESPONSE:
# Python script to create a fake snowflake-schema database using Faker and SQLite
# - Creates a normalized "snowflake" warehouse schema
# - Populates dimensions (Country -> State -> City, ProductCategory -> Subcategory -> Product, Store, Customer, Date)
# - Populates a fact table (FactSales) with realistic links and measures
# Requires: pip install Faker

import sqlite3
from datetime import date, timedelta
import random
from faker import Faker

DB_PATH = "snowflake_demo.db"

# Tweak volumes here
NUM_COUNTRIES = 5
STATES_PER_COUNTRY = 8
CITIES_PER_STATE = 12

NUM_CUSTOMERS = 3000
NUM_CATEGORIES = 6
SUBCATS_PER_CATEGORY = 3
PRODUCTS_PER_SUBCAT = 30
NUM_STORES = 120

FACT_SALES_ROWS = 40000  # adjust up/down for size/speed
DATE_START = date(2023, 1, 1)
DATE_END = date(2024, 12, 31)

RANDOM_SEED = 42


def connect(db_path=DB_PATH):
    conn = sqlite3.connect(db_path)
    conn.execute("PRAGMA foreign_keys = ON;")
    return conn


def create_schema(conn: sqlite3.Connection):
    cur = conn.cursor()

    cur.executescript("""
    DROP TABLE IF EXISTS fact_sales;
    DROP TABLE IF EXISTS dim_customer;
    DROP TABLE IF EXISTS dim_store;
    DROP TABLE IF EXISTS dim_product;
    DROP TABLE IF EXISTS dim_product_subcategory;
    DROP TABLE IF EXISTS dim_product_category;
    DROP TABLE IF EXISTS dim_city;
    DROP TABLE IF EXISTS dim_state;
    DROP TABLE IF EXISTS dim_country;
    DROP TABLE IF EXISTS dim_date;

    CREATE TABLE dim_country (
        country_key   INTEGER PRIMARY KEY,
        country_name  TEXT NOT NULL,
        country_code  TEXT NOT NULL,
        region        TEXT NOT NULL
    );

    CREATE TABLE dim_state (
        state_key     INTEGER PRIMARY KEY,
        state_name    TEXT NOT NULL,
        state_code    TEXT NOT NULL,
        country_key   INTEGER NOT NULL,
        FOREIGN KEY (country_key) REFERENCES dim_country(country_key)
    );

    CREATE TABLE dim_city (
        city_key      INTEGER PRIMARY KEY,
        city_name     TEXT NOT NULL,
        postal_code   TEXT,
        state_key     INTEGER NOT NULL,
        FOREIGN KEY (state_key) REFERENCES dim_state(state_key)
    );

    CREATE TABLE dim_product_category (
        category_key  INTEGER PRIMARY KEY,
        category_name TEXT NOT NULL
    );

    CREATE TABLE dim_product_subcategory (
        subcategory_key INTEGER PRIMARY KEY,
        subcategory_name TEXT NOT NULL,
        category_key     INTEGER NOT NULL,
        FOREIGN KEY (category_key) REFERENCES dim_product_category(category_key)
    );

    CREATE TABLE dim_product (
        product_key    INTEGER PRIMARY KEY,
        product_name   TEXT NOT NULL,
        brand          TEXT NOT NULL,
        sku            TEXT NOT NULL,
        base_price     REAL NOT NULL,
        subcategory_key INTEGER NOT NULL,
        FOREIGN KEY (subcategory_key) REFERENCES dim_product_subcategory(subcategory_key)
    );

    CREATE TABLE dim_store (
        store_key     INTEGER PRIMARY KEY,
        store_name    TEXT NOT NULL,
        city_key      INTEGER NOT NULL,
        FOREIGN KEY (city_key) REFERENCES dim_city(city_key)
    );

    CREATE TABLE dim_customer (
        customer_key  INTEGER PRIMARY KEY,
        first_name    TEXT NOT NULL,
        last_name     TEXT NOT NULL,
        email         TEXT NOT NULL,
        gender        TEXT,
        birth_date    TEXT,
        city_key      INTEGER NOT NULL,
        FOREIGN KEY (city_key) REFERENCES dim_city(city_key)
    );

    CREATE TABLE dim_date (
        date_key      INTEGER PRIMARY KEY,   -- YYYYMMDD
        date          TEXT NOT NULL,
        day           INTEGER,
        month         INTEGER,
        month_name    TEXT,
        quarter       INTEGER,
        year          INTEGER,
        is_weekend    INTEGER
    );

    CREATE TABLE fact_sales (
        sales_key     INTEGER PRIMARY KEY,
        date_key      INTEGER NOT NULL,
        customer_key  INTEGER NOT NULL,
        product_key   INTEGER NOT NULL,
        store_key     INTEGER NOT NULL,
        quantity      INTEGER NOT NULL,
        unit_price    REAL NOT NULL,
        discount      REAL NOT NULL, -- 0.0..0.5
        total_amount  REAL NOT NULL,

        FOREIGN KEY (date_key)     REFERENCES dim_date(date_key),
        FOREIGN KEY (customer_key) REFERENCES dim_customer(customer_key),
        FOREIGN KEY (product_key)  REFERENCES dim_product(product_key),
        FOREIGN KEY (store_key)    REFERENCES dim_store(store_key)
    );

    CREATE INDEX IF NOT EXISTS idx_sales_date    ON fact_sales(date_key);
    CREATE INDEX IF NOT EXISTS idx_sales_customer ON fact_sales(customer_key);
    CREATE INDEX IF NOT EXISTS idx_sales_product  ON fact_sales(product_key);
    CREATE INDEX IF NOT EXISTS idx_sales_store    ON fact_sales(store_key);

    CREATE INDEX IF NOT EXISTS idx_city_state     ON dim_city(state_key);
    CREATE INDEX IF NOT EXISTS idx_state_country  ON dim_state(country_key);
    CREATE INDEX IF NOT EXISTS idx_subcat_cat     ON dim_product_subcategory(category_key);
    CREATE INDEX IF NOT EXISTS idx_prod_subcat    ON dim_product(subcategory_key);
    """)
    conn.commit()


def build_dimensions(conn: sqlite3.Connection, fake: Faker):
    random.seed(RANDOM_SEED)
    fake.random.seed(RANDOM_SEED)

    cur = conn.cursor()

    # Countries
    regions = ["EMEA", "APAC", "AMER"]
    countries = []
    seen_country_codes = set()
    for i in range(NUM_COUNTRIES):
        # Ensure unique-ish country codes
        for _ in range(5):
            cc = fake.country_code()
            if cc not in seen_country_codes:
                seen_country_codes.add(cc)
                break
        else:
            cc = f"C{i+1:02d}"
        countries.append((i + 1, fake.country(), cc, random.choice(regions)))

    cur.executemany(
        "INSERT INTO dim_country(country_key, country_name, country_code, region) VALUES (?, ?, ?, ?)",
        countries
    )

    # States
    states = []
    state_key = 1
    for country_key, _, _, _ in countries:
        used_codes = set()
        for _ in range(STATES_PER_COUNTRY):
            state_name = fake.state()
            # try unique-ish 3-letter code per country
            code = "".join([c for c in state_name.upper() if c.isalpha()])[:3] or f"S{state_key}"
            if code in used_codes:
                code = f"{code}{len(used_codes)+1}"
            used_codes.add(code)
            states.append((state_key, state_name, code, country_key))
            state_key += 1

    cur.executemany(
        "INSERT INTO dim_state(state_key, state_name, state_code, country_key) VALUES (?, ?, ?, ?)",
        states
    )

    # Cities
    cities = []
    city_key = 1
    for st in states:
        st_key = st[0]
        for _ in range(CITIES_PER_STATE):
            city_name = fake.city()
            postal = fake.postcode()
            cities.append((city_key, city_name, postal, st_key))
            city_key += 1

    cur.executemany(
        "INSERT INTO dim_city(city_key, city_name, postal_code, state_key) VALUES (?, ?, ?, ?)",
        cities
    )

    # Product categories and subcategories
    categories = []
    for i in range(NUM_CATEGORIES):
        categories.append((i + 1, fake.word().title()))

    cur.executemany(
        "INSERT INTO dim_product_category(category_key, category_name) VALUES (?, ?)",
        categories
    )

    subcategories = []
    subcat_key = 1
    for cat_key, _ in categories:
        for _ in range(SUBCATS_PER_CATEGORY):
            subcategories.append((subcat_key, f"{fake.word().title()} {fake.word().title()}", cat_key))
            subcat_key += 1

    cur.executemany(
        "INSERT INTO dim_product_subcategory(subcategory_key, subcategory_name, category_key) VALUES (?, ?, ?)",
        subcategories
    )

    # Products
    products = []
    product_key = 1
    used_skus = set()
    for sub_key, _, _ in subcategories:
        for _ in range(PRODUCTS_PER_SUBCAT):
            brand = fake.company().split(",")[0][:18]
            name = f"{brand} {fake.word().title()} {fake.color_name()}"
            # unique-ish SKU
            for _ in range(5):
                sku = f"{brand[:3].upper()}{random.randint(10000, 99999)}"
                if sku not in used_skus:
                    used_skus.add(sku)
                    break
            else:
                sku = f"SKU{product_key:06d}"
            base_price = round(random.uniform(5, 500), 2)
            products.append((product_key, name, brand, sku, base_price, sub_key))
            product_key += 1

    cur.executemany(
        "INSERT INTO dim_product(product_key, product_name, brand, sku, base_price, subcategory_key) VALUES (?, ?, ?, ?, ?, ?)",
        products
    )

    # Stores
    # Assign stores across random cities
    city_keys = [c[0] for c in cities]
    stores = []
    for i in range(NUM_STORES):
        stores.append((i + 1, f"{fake.company()} Store #{random.randint(1,999)}", random.choice(city_keys)))

    cur.executemany(
        "INSERT INTO dim_store(store_key, store_name, city_key) VALUES (?, ?, ?)",
        stores
    )

    # Customers
    # Distribute customers across cities
    customers = []
    used_emails = set()
    for i in range(NUM_CUSTOMERS):
        first = fake.first_name()
        last = fake.last_name()
        # unique-ish email
        for _ in range(5):
            email = f"{first}.{last}{random.randint(1,9999)}@{fake.free_email_domain()}".lower()
            if email not in used_emails:
                used_emails.add(email)
                break
        else:
            email = f"user{i+1}@example.com"
        gender = random.choice(["F", "M", "Other"])
        # approx 18-80 y/o
        birth_year = random.randint(1944, 2006)
        birth_dt = date(birth_year, random.randint(1, 12), random.randint(1, 28))
        customers.append((i + 1, first, last, email, gender, birth_dt.isoformat(), random.choice(city_keys)))

    cur.executemany(
        "INSERT INTO dim_customer(customer_key, first_name, last_name, email, gender, birth_date, city_key) VALUES (?, ?, ?, ?, ?, ?, ?)",
        customers
    )

    # Date dimension
    dates = []
    d = DATE_START
    while d <= DATE_END:
        date_key = d.year * 10000 + d.month * 100 + d.day
        dates.append((
            date_key,
            d.isoformat(),
            d.day,
            d.month,
            d.strftime("%B"),
            (d.month - 1) // 3 + 1,
            d.year,
            1 if d.weekday() >= 5 else 0
        ))
        d += timedelta(days=1)

    cur.executemany(
        "INSERT INTO dim_date(date_key, date, day, month, month_name, quarter, year, is_weekend) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
        dates
    )

    conn.commit()

    # Return handy collections to speed up fact generation
    return {
        "date_keys": [r[0] for r in dates],
        "customer_keys": [r[0] for r in customers],
        "product_info": {p[0]: p for p in products},  # key -> product tuple
        "product_keys": [p[0] for p in products],
        "store_keys": [s[0] for s in stores],
    }


def build_fact_sales(conn: sqlite3.Connection, dim_refs, fake: Faker):
    random.seed(RANDOM_SEED + 1)
    fake.random.seed(RANDOM_SEED + 1)

    date_keys = dim_refs["date_keys"]
    customer_keys = dim_refs["customer_keys"]
    product_keys = dim_refs["product_keys"]
    product_info = dim_refs["product_info"]
    store_keys = dim_refs["store_keys"]

    cur = conn.cursor()

    batch = []
    batch_size = 5000
    sales_key = 1

    # Optional: simple weights to bias quantities and discount distributions
    def sample_quantity():
        # heavy on 1-3, occasional 4-10
        return random.choices([1, 2, 3, 4, 5, 6, 8, 10], weights=[35, 25, 15, 8, 6, 5, 4, 2], k=1)[0]

    def sample_discount():
        # Most often small discount, rare large promotions
        return random.choices(
            [0.0, 0.05, 0.10, 0.15, 0.20, 0.30, 0.40, 0.50],
            weights=[30, 25, 18, 12, 8, 5, 1, 1],
            k=1
        )[0]

    for _ in range(FACT_SALES_ROWS):
        dkey = random.choice(date_keys)
        cust = random.choice(customer_keys)
        prod_key = random.choice(product_keys)
        store = random.choice(store_keys)
        qty = sample_quantity()

        # Price around product base price (+/- 20%)
        prod = product_info[prod_key]
        base_price = prod[4]
        unit_price = round(max(0.5, base_price * random.uniform(0.8, 1.2)), 2)

        disc = sample_discount()
        total = round(qty * unit_price * (1.0 - disc), 2)

        batch.append((sales_key, dkey, cust, prod_key, store, qty, unit_price, disc, total))
        sales_key += 1

        if len(batch) >= batch_size:
            cur.executemany(
                "INSERT INTO fact_sales(sales_key, date_key, customer_key, product_key, store_key, quantity, unit_price, discount, total_amount) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
                batch
            )
            conn.commit()
            batch.clear()

    if batch:
        cur.executemany(
            "INSERT INTO fact_sales(sales_key, date_key, customer_key, product_key, store_key, quantity, unit_price, discount, total_amount) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
            batch
        )
        conn.commit()


def main():
    random.seed(RANDOM_SEED)
    fake = Faker()
    Faker.seed(RANDOM_SEED)

    conn = connect(DB_PATH)
    try:
        create_schema(conn)
        dim_refs = build_dimensions(conn, fake)
        build_fact_sales(conn, dim_refs, fake)
        print(f"Done. SQLite database created at: {DB_PATH}")
    finally:
        conn.close()


if __name__ == "__main__":
    main()


# Usage:
# 1) pip install Faker
# 2) python this_script.py
# 3) Explore with SQLite (e.g., sqlite3 snowflake_demo.db)
#
# Example query (run in SQLite):
# SELECT d.year, c.country_name, SUM(f.total_amount) AS revenue
# FROM fact_sales f
# JOIN dim_date d ON f.date_key = d.date_key
# JOIN dim_store s ON f.store_key = s.store_key
# JOIN dim_city ci ON s.city_key = ci.city_key
# JOIN dim_state st ON ci.state_key = st.state_key
# JOIN dim_country c ON st.country_key = c.country_key
# GROUP BY d.year, c.country_name
# ORDER BY d.year, revenue DESC;
------------------------------------------------------------

PROMPT 396:
"Generate Python code that imports the necessary modules, creates an HDFS connection, and loads data from HDFS into a Pandas DataFrame."
RESPONSE:
# Requirements:
#   pip install pandas hdfs pyarrow
#   For PyArrow HDFS: system libhdfs (Hadoop client) may be required.

import os
import io
import pandas as pd

# Try WebHDFS (hdfscli) first; fall back to native HDFS via PyArrow if available.
try:
    from hdfs import InsecureClient
    HDFS_BACKEND = "webhdfs"
except ImportError:
    InsecureClient = None
    HDFS_BACKEND = None

try:
    import pyarrow.fs as pafs
    import pyarrow.parquet as pq
    HDFS_BACKEND = HDFS_BACKEND or "pyarrow"
except ImportError:
    pafs = None
    pq = None

if HDFS_BACKEND is None:
    raise ImportError("Install at least one HDFS client: 'hdfs' (WebHDFS) or 'pyarrow'.")

# Connection settings
NAMENODE_HOST = os.getenv("HDFS_NAMENODE_HOST", "namenode-host")  # e.g., "nn.mycompany.net"
WEBHDFS_PORT = int(os.getenv("HDFS_WEB_PORT", "9870"))            # often 9870 (or 50070 on older Hadoop)
RPC_PORT = int(os.getenv("HDFS_RPC_PORT", "8020"))                # HDFS RPC port
HDFS_USER = os.getenv("HDFS_USER", "hdfs")
HDFS_PATH = os.getenv("HDFS_PATH", "/data/example.csv")           # can be .csv or .parquet

def connect_hdfs():
    if HDFS_BACKEND == "webhdfs":
        url = f"http://{NAMENODE_HOST}:{WEBHDFS_PORT}"
        client = InsecureClient(url, user=HDFS_USER)
        return ("webhdfs", client)
    else:
        fs = pafs.HadoopFileSystem(NAMENODE_HOST, RPC_PORT, user=HDFS_USER)
        return ("pyarrow", fs)

def read_hdfs_to_df(path: str):
    backend, handle = connect_hdfs()
    ext = os.path.splitext(path)[1].lower()

    if backend == "webhdfs":
        # WebHDFS returns a file-like stream
        if ext == ".csv":
            with handle.read(path, encoding="utf-8") as f:
                return pd.read_csv(f)
        elif ext in (".parquet", ".pq"):
            with handle.read(path) as f:
                # Read into memory; for large files, consider a temp file approach.
                data = f.read()
                return pd.read_parquet(io.BytesIO(data), engine="pyarrow")
        else:
            # Fallback: try pandas reader that accepts file-like objects (e.g., JSON)
            with handle.read(path) as f:
                return pd.read_json(f, lines=True)
    else:
        # Native HDFS (PyArrow FS)
        if ext == ".csv":
            with handle.open_input_file(path) as f:
                return pd.read_csv(f)
        elif ext in (".parquet", ".pq"):
            table = pq.read_table(path, filesystem=handle)
            return table.to_pandas()
        else:
            with handle.open_input_file(path) as f:
                return pd.read_json(f, lines=True)

# Load the data
df = read_hdfs_to_df(HDFS_PATH)

# Example use
print(df.head())
------------------------------------------------------------

PROMPT 397:
"Generate Python code that creates a database-backed asynchronous task queue using the `asyncq` library and a SQL database of your choice."
RESPONSE:
Quick clarification to make sure I generate the right code:
- Which “asyncq” library (PyPI name/repo and version) should I target? There are multiple similarly named projects with different APIs.
- Do you have a preferred SQL database (PostgreSQL, MySQL/MariaDB, or SQLite)? If not, I’ll default to PostgreSQL.
------------------------------------------------------------

PROMPT 398:
"Generate Python code that uses the PyPDF2 library to convert PDF files to DOCX format, using the Docxgen library to create the DOCX file."
RESPONSE:
Note: Docxgen is a JavaScript library, not a Python library. In Python, the standard way to create DOCX files is the python-docx package. Below is Python code that uses PyPDF2 to extract text from a PDF and python-docx to write it to a DOCX file.

Install:
- pip install PyPDF2 python-docx

Python code (convert PDF to DOCX text-only):

from pathlib import Path
from PyPDF2 import PdfReader
from docx import Document

def convert_pdf_to_docx(pdf_path, docx_path=None, add_page_breaks=True, page_headings=False):
    """
    Convert a PDF to a DOCX by extracting text with PyPDF2 and writing it with python-docx.
    Note: This preserves only plain text (no images, tables, or complex layout).
    """
    pdf_path = Path(pdf_path)
    if docx_path is None:
        docx_path = pdf_path.with_suffix(".docx")
    else:
        docx_path = Path(docx_path)

    reader = PdfReader(str(pdf_path))
    doc = Document()

    num_pages = len(reader.pages)
    for i, page in enumerate(reader.pages, start=1):
        text = page.extract_text() or ""
        if page_headings:
            doc.add_heading(f"Page {i}", level=2)

        if text.strip():
            # Add one paragraph per line to roughly preserve line breaks
            for line in text.splitlines():
                doc.add_paragraph(line)
        else:
            doc.add_paragraph("[No extractable text on this page (may be scanned or image-only).]")

        if add_page_breaks and i < num_pages:
            doc.add_page_break()

    doc.save(str(docx_path))
    return str(docx_path)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Convert PDF to DOCX using PyPDF2 + python-docx (text-only).")
    parser.add_argument("input_pdf", help="Path to input PDF")
    parser.add_argument("output_docx", nargs="?", help="Optional output DOCX path")
    parser.add_argument("--no-page-breaks", action="store_true", help="Do not insert page breaks between PDF pages")
    parser.add_argument("--page-headings", action="store_true", help="Insert 'Page N' heading at the start of each page")
    args = parser.parse_args()

    out = convert_pdf_to_docx(
        args.input_pdf,
        args.output_docx,
        add_page_breaks=not args.no_page_breaks,
        page_headings=args.page_headings
    )
    print(f"Wrote: {out}")

Notes and limitations:
- PyPDF2 text extraction is best-effort and may not reflect original layout; images, tables, and complex formatting are not preserved.
- For scanned PDFs (image-only), you’ll need OCR (e.g., pytesseract + pdf2image) to extract text first.
- If you specifically require Docxgen, that would need a Node.js workflow; I can provide a JavaScript example if you’d like.
------------------------------------------------------------

PROMPT 399:
"Generate Python code that uses the `requests` library to send a HTTP request to the AWS Lambda function, similar to how `curl` would be used."
RESPONSE:
Below are ready-to-adapt Python examples using the requests library. Pick the one that matches how your Lambda is exposed.

1) Function URL or API Gateway (public, no IAM)
Example equivalent to: curl -X POST https://... -H "Content-Type: application/json" -d '{"name":"Ada"}'

import requests

url = "https://abc123.lambda-url.us-east-1.on.aws/"  # or your API Gateway URL
payload = {"name": "Ada"}
headers = {"Content-Type": "application/json"}

try:
    resp = requests.post(url, json=payload, headers=headers, timeout=10)
    resp.raise_for_status()
    print("Status:", resp.status_code)
    print("Body:", resp.text)         # or resp.json() if JSON
except requests.RequestException as e:
    print("Request failed:", e)

# Example GET with query params (equiv. to curl '?q=hello')
# resp = requests.get(url, params={"q": "hello"}, timeout=10)


2) Function URL that requires IAM authentication (SigV4)
Requires: pip install requests-aws4auth boto3

import json
import requests
import boto3
from requests_aws4auth import AWS4Auth

# Your Lambda Function URL (IAM auth enabled)
url = "https://abc123.lambda-url.us-east-1.on.aws/"
region = "us-east-1"  # region where the Lambda resides
service = "lambda"    # service name for signing

# Resolve AWS credentials and sign
session = boto3.Session()  # uses env vars/instance role/profile
creds = session.get_credentials().get_frozen_credentials()
auth = AWS4Auth(creds.access_key, creds.secret_key, region, service, session_token=creds.token)

payload = {"name": "Ada"}
headers = {"Content-Type": "application/json"}

try:
    resp = requests.post(url, json=payload, headers=headers, auth=auth, timeout=10)
    resp.raise_for_status()
    print("Status:", resp.status_code)
    print("Body:", resp.text)
except requests.RequestException as e:
    print("Request failed:", e)


3) Directly calling the Lambda Invoke REST API (SigV4)
This mirrors curl to the Lambda service endpoint:
POST https://lambda.{region}.amazonaws.com/2015-03-31/functions/{function_name_or_arn}/invocations

Requires: pip install requests-aws4auth boto3

import json
import requests
import boto3
from requests_aws4auth import AWS4Auth

region = "us-east-1"
function_name = "my-function"  # can be name, full ARN, or partial ARN
url = f"https://lambda.{region}.amazonaws.com/2015-03-31/functions/{function_name}/invocations"

session = boto3.Session()
creds = session.get_credentials().get_frozen_credentials()
auth = AWS4Auth(creds.access_key, creds.secret_key, region, "lambda", session_token=creds.token)

payload = {"name": "Ada"}
headers = {
    "Content-Type": "application/json",
    # Optional invocation headers:
    # "X-Amz-Invocation-Type": "RequestResponse",  # or "Event" for async
    # "X-Amz-Log-Type": "Tail",
    # "X-Amz-Client-Context": base64.b64encode(b'{}').decode()
}

try:
    resp = requests.post(url, data=json.dumps(payload), headers=headers, auth=auth, timeout=15)
    resp.raise_for_status()
    print("Status:", resp.status_code)
    print("Body:", resp.text)  # Lambda response payload
    # If Log-Type=Tail, logs are in resp.headers.get("X-Amz-Log-Result")
except requests.RequestException as e:
    print("Request failed:", e)

Notes:
- Use json=payload for typical HTTP endpoints (Function URL/API Gateway). For the Lambda Invoke API, AWS expects the raw payload body, so use data=json.dumps(payload).
- If your Lambda returns binary data, access resp.content instead of resp.text.
- For private endpoints (VPC, private API Gateway), ensure networking/DNS routing is in place.
------------------------------------------------------------

PROMPT 400:
"Generate Python code that imports the `backports_abc` module and creates typing stubs for its functions and classes, using the `typing` module."
RESPONSE:
#!/usr/bin/env python3
"""
Generate a minimal typing stub (.pyi) for the backports_abc module by introspecting
its exported functions and classes. All parameters and return types are annotated
with typing.Any, and method decorators (classmethod/staticmethod/property) are
preserved where detectable.

Output: ./backports_abc.pyi (override with --out <path>)
"""

from __future__ import annotations

import argparse
import importlib
import inspect
import sys
from types import BuiltinFunctionType, FunctionType, MethodDescriptorType, ModuleType
from typing import Any, Iterable, List, Optional, Tuple


def load_module(module_name: str) -> ModuleType:
    try:
        return importlib.import_module(module_name)
    except ImportError as e:
        print(f"Error: Could not import {module_name}. Try: pip install {module_name}", file=sys.stderr)
        raise SystemExit(1) from e


def public_names(mod: ModuleType) -> List[str]:
    if hasattr(mod, "__all__"):
        names = [n for n in getattr(mod, "__all__") if isinstance(n, str)]
    else:
        names = [n for n in dir(mod) if not n.startswith("_")]
    # Keep stable order
    return sorted(names)


def is_exported(mod: ModuleType, name: str, obj: Any) -> bool:
    # Only include objects that are attributes of the module and accessible by that name.
    try:
        return getattr(mod, name) is obj
    except Exception:
        return False


def safe_signature(obj: Any) -> Optional[inspect.Signature]:
    try:
        return inspect.signature(obj)
    except Exception:
        return None


def format_params(sig: inspect.Signature) -> str:
    # Produce a parameter list with Any annotations and default "..." where present.
    params_out: List[str] = []
    need_kwonly_marker = True
    saw_var_positional = any(p.kind == inspect.Parameter.VAR_POSITIONAL for p in sig.parameters.values())

    for i, p in enumerate(sig.parameters.values()):
        annot = "Any"
        default = " = ..." if p.default is not inspect._empty else ""

        if p.kind == inspect.Parameter.POSITIONAL_ONLY:
            params_out.append(f"{p.name}: {annot}{default}")
        elif p.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
            params_out.append(f"{p.name}: {annot}{default}")
        elif p.kind == inspect.Parameter.VAR_POSITIONAL:
            params_out.append(f"*{p.name}: {annot}")
            need_kwonly_marker = False  # Marker no longer needed
        elif p.kind == inspect.Parameter.KEYWORD_ONLY:
            # Insert '*' marker if there was no var-positional and this is the first kwonly
            if need_kwonly_marker and not saw_var_positional:
                params_out.append("*")
                need_kwonly_marker = False
            params_out.append(f"{p.name}: {annot}{default}")
        elif p.kind == inspect.Parameter.VAR_KEYWORD:
            params_out.append(f"**{p.name}: {annot}")

    # Collapse any accidental "*," if it ended up as the last element
    # (rare, but could happen with unusual signatures).
    if params_out and params_out[-1] == "*":
        params_out[-1] = "*args: Any"

    return ", ".join(params_out)


def render_function_def(name: str, obj: Any, *, indent: int = 0, is_init: bool = False, decorator: Optional[str] = None) -> str:
    spaces = " " * indent
    sig = safe_signature(obj)
    if sig is None:
        # Fallback to a generic signature
        params = "*args: Any, **kwargs: Any"
    else:
        params = format_params(sig)

    if is_init:
        ret = "None"
    else:
        ret = "Any"

    lines: List[str] = []
    if decorator:
        lines.append(f"{spaces}{decorator}")
    lines.append(f"{spaces}def {name}({params}) -> {ret}: ...")
    return "\n".join(lines)


def classify_class_member(cls: type, name: str) -> Tuple[str, Any]:
    """
    Return (kind, underlying) where kind is one of:
    'property', 'classmethod', 'staticmethod', 'method', 'unknown'
    underlying is the function for methods, or the property object for properties.
    """
    try:
        raw = inspect.getattr_static(cls, name)
    except Exception:
        return "unknown", None

    # property
    if isinstance(raw, property):
        return "property", raw

    # classmethod
    if isinstance(raw, classmethod):
        return "classmethod", raw.__func__

    # staticmethod
    if isinstance(raw, staticmethod):
        return "staticmethod", raw.__func__

    # normal function/method
    if isinstance(raw, (FunctionType, MethodDescriptorType, BuiltinFunctionType)):
        return "method", raw

    return "unknown", raw


def render_property(name: str, *, indent: int = 4) -> str:
    spaces = " " * indent
    return f"{spaces}@property\n{spaces}def {name}(self) -> Any: ..."


def render_class(name: str, cls: type) -> str:
    # Collect direct members from __dict__ to avoid inherited noise
    members = list(getattr(cls, "__dict__", {}).items())
    # Filter to public-ish members, but always allow __init__ if present
    filtered = []
    for mname, mobj in members:
        if mname == "__init__":
            filtered.append((mname, mobj))
            continue
        if mname.startswith("_"):
            continue
        filtered.append((mname, mobj))

    lines: List[str] = [f"class {name}:"]
    body_lines: List[str] = []

    seen = set()
    for mname, _ in sorted(filtered, key=lambda x: x[0]):
        if mname in seen:
            continue
        seen.add(mname)
        kind, underlying = classify_class_member(cls, mname)

        if mname == "__init__":
            body_lines.append(render_function_def("__init__", getattr(cls, "__init__", lambda: None), indent=4, is_init=True))
            continue

        if kind == "property":
            body_lines.append(render_property(mname, indent=4))
        elif kind == "classmethod":
            body_lines.append(render_function_def(mname, underlying, indent=4, decorator="@classmethod"))
        elif kind == "staticmethod":
            body_lines.append(render_function_def(mname, underlying, indent=4, decorator="@staticmethod"))
        elif kind == "method":
            body_lines.append(render_function_def(mname, underlying, indent=4))
        # else: skip unknown/attributes for simplicity

    if not body_lines:
        lines.append("    pass")
    else:
        lines.extend(body_lines)

    return "\n".join(lines)


def generate_stub(mod: ModuleType) -> str:
    names = public_names(mod)

    functions: List[Tuple[str, Any]] = []
    classes: List[Tuple[str, type]] = []

    for name in names:
        try:
            obj = getattr(mod, name)
        except Exception:
            continue
        if not is_exported(mod, name, obj):
            continue

        if inspect.isfunction(obj) or isinstance(obj, (BuiltinFunctionType, MethodDescriptorType)):
            functions.append((name, obj))
        elif inspect.isclass(obj):
            classes.append((name, obj))

    out_lines: List[str] = []
    out_lines.append("# This file was auto-generated. Edit with care.")
    out_lines.append("from __future__ import annotations")
    out_lines.append("from typing import Any")
    out_lines.append("")

    # Module-level functions
    for fname, fobj in functions:
        out_lines.append(render_function_def(fname, fobj))
    if functions:
        out_lines.append("")

    # Classes
    for cname, cobj in classes:
        out_lines.append(render_class(cname, cobj))
        out_lines.append("")

    return "\n".join(out_lines).rstrip() + "\n"


def main(argv: Optional[Iterable[str]] = None) -> None:
    parser = argparse.ArgumentParser(description="Generate typing stubs for the backports_abc module.")
    parser.add_argument("--module", default="backports_abc", help="Module name to stub (default: backports_abc)")
    parser.add_argument("--out", default=None, help="Output .pyi file path (default: ./<module>.pyi)")
    args = parser.parse_args(argv)

    mod = load_module(args.module)
    stub_text = generate_stub(mod)

    out_path = args.out or f"{args.module}.pyi"
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(stub_text)

    print(f"Wrote stub: {out_path}")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 401:
"Generate Python code that creates a dictionary-like cache module that stores data in files, allowing for efficient caching and retrieval of data."
RESPONSE:
# filecache.py
# A dictionary-like, file-backed cache with optional TTL, compression, and LRU eviction.
# Python 3.8+

import base64
import errno
import gzip
import hashlib
import io
import json
import os
import random
import shutil
import struct
import tempfile
import threading
import time
from typing import Any, Iterable, Iterator, MutableMapping, Optional

MAGIC = b"FCACHE1\n"  # 8-byte magic header
HEADER_LEN_STRUCT = ">I"  # 4-byte big-endian unsigned int for header length
HEADER_LEN_SIZE = struct.calcsize(HEADER_LEN_STRUCT)

DEFAULT_SHARDS = 2  # depth of sharding: 2 -> base/aa/bb/<hash>.fc
FILE_EXT = ".fc"


class PickleSerializer:
    name = "pickle"

    @staticmethod
    def dumps(obj: Any) -> bytes:
        import pickle

        return pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL)

    @staticmethod
    def loads(data: bytes) -> Any:
        import pickle

        return pickle.loads(data)


class JsonSerializer:
    name = "json"

    @staticmethod
    def dumps(obj: Any) -> bytes:
        # Note: only JSON-serializable objects
        return json.dumps(obj, separators=(",", ":"), ensure_ascii=False).encode("utf-8")

    @staticmethod
    def loads(data: bytes) -> Any:
        return json.loads(data.decode("utf-8"))


def _ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def _read_exact(f, n: int) -> bytes:
    buf = bytearray()
    while len(buf) < n:
        chunk = f.read(n - len(buf))
        if not chunk:
            raise EOFError("Unexpected EOF while reading")
        buf.extend(chunk)
    return bytes(buf)


def _now() -> float:
    return time.time()


def _hash_bytes(data: bytes) -> str:
    return hashlib.sha256(b"\x01" + data).hexdigest()


def _encode_key(key: Any) -> dict:
    # Restrict to str, bytes, int for predictable round-tripping
    if isinstance(key, str):
        return {"t": "str", "v": key}
    if isinstance(key, bytes):
        return {"t": "bytes", "v": base64.b85encode(key).decode("ascii")}
    if isinstance(key, int):
        return {"t": "int", "v": key}
    raise TypeError("Keys must be str, bytes, or int")


def _key_to_bytes(key_enc: dict) -> bytes:
    t = key_enc["t"]
    v = key_enc["v"]
    if t == "str":
        return str(v).encode("utf-8")
    if t == "bytes":
        return base64.b85decode(v.encode("ascii"))
    if t == "int":
        return str(int(v)).encode("ascii")
    raise ValueError("Invalid key encoding")


def _decode_key(key_enc: dict) -> Any:
    t = key_enc["t"]
    v = key_enc["v"]
    if t == "str":
        return v
    if t == "bytes":
        return base64.b85decode(v.encode("ascii"))
    if t == "int":
        return int(v)
    raise ValueError("Invalid key encoding")


class FileCache(MutableMapping):
    """
    File-backed cache implementing a Mapping interface.

    Features:
    - Dictionary-like methods: __getitem__, __setitem__, __delitem__, __contains__, get, keys, items, values, __len__, __iter__
    - Values are serialized (pickle by default) and stored in sharded files
    - Optional gzip compression
    - Optional TTL per entry and default TTL
    - Optional max_bytes with LRU eviction based on mtime (updated on access)
    - Atomic writes via temp files and os.replace
    - Thread-safe within a process via a lock (best-effort). Cross-process safety is best-effort.

    Limitations:
    - Keys must be str, bytes, or int (for stable filename hashing and round-trip on iteration)
    - Iteration requires reading small headers from files and may be O(n) over number of entries
    """

    def __init__(
        self,
        directory: str,
        *,
        serializer: str = "pickle",
        compress: bool = False,
        default_ttl: Optional[float] = None,
        max_bytes: Optional[int] = None,
        fsync: bool = False,
        shards: int = DEFAULT_SHARDS,
    ):
        """
        directory: base directory for cache data
        serializer: "pickle" (default) or "json"
        compress: gzip-compress payloads
        default_ttl: default TTL seconds if not provided on set (None = never expire)
        max_bytes: if set, evict LRU entries until total size <= max_bytes
        fsync: ensure durability on writes (slower)
        shards: number of 2-hex-digit shard levels for file fanout
        """
        self.base_dir = os.path.abspath(directory)
        self.data_dir = os.path.join(self.base_dir, "data")
        self.tmp_dir = os.path.join(self.base_dir, "tmp")
        self.compress = bool(compress)
        self.default_ttl = default_ttl
        self.max_bytes = max_bytes
        self.fsync = bool(fsync)
        self.shards = max(0, int(shards))

        if serializer == "pickle":
            self._ser = PickleSerializer
        elif serializer == "json":
            self._ser = JsonSerializer
        else:
            raise ValueError("serializer must be 'pickle' or 'json'")

        _ensure_dir(self.data_dir)
        _ensure_dir(self.tmp_dir)

        # In-memory index: path -> (size, mtime)
        self._entries = {}  # type: dict[str, tuple[int, float]]
        self._size_bytes = 0
        self._index_built = False
        self._lock = threading.RLock()

    # MutableMapping interface

    def __setitem__(self, key: Any, value: Any) -> None:
        self.set(key, value)

    def __getitem__(self, key: Any) -> Any:
        ok, value = self._load_value(key)
        if not ok:
            raise KeyError(key)
        return value

    def __delitem__(self, key: Any) -> None:
        self._delete_key(key, missing_ok=False)

    def __contains__(self, key: Any) -> bool:
        try:
            ok, _ = self._load_value(key, load_value=False)
            return ok
        except Exception:
            return False

    def __len__(self) -> int:
        count = 0
        for _ in self.keys():
            count += 1
        return count

    def __iter__(self) -> Iterator[Any]:
        yield from self.keys()

    # Public API

    def set(self, key: Any, value: Any, ttl: Optional[float] = None) -> None:
        """
        Set value at key. Optional TTL (seconds). If None, uses default_ttl. If both None -> no expiry.
        """
        ttl_eff = self.default_ttl if ttl is None else ttl
        expires_at = None if ttl_eff is None else (_now() + float(ttl_eff))
        self._store_value(key, value, expires_at)

    def get(self, key: Any, default: Any = None) -> Any:
        try:
            return self[key]
        except KeyError:
            return default

    def pop(self, key: Any, default: Any = None) -> Any:
        try:
            v = self[key]
        except KeyError:
            if default is not None:
                return default
            raise
        del self[key]
        return v

    def keys(self) -> Iterable[Any]:
        # Iterate keys by scanning files and reading headers
        with self._lock:
            for path in self._iter_all_paths():
                try:
                    hdr = self._read_header(path)
                    if hdr is None:
                        continue
                    if self._is_expired(hdr):
                        # Remove expired entry lazily
                        self._remove_path(path)
                        continue
                    yield _decode_key(hdr["key"])
                except Exception:
                    # Corrupt -> remove
                    self._remove_path(path)

    def items(self) -> Iterable[tuple[Any, Any]]:
        for k in self.keys():
            try:
                yield k, self[k]
            except KeyError:
                # expired between keys() and get
                continue

    def values(self) -> Iterable[Any]:
        for _, v in self.items():
            yield v

    def clear(self) -> None:
        with self._lock:
            if os.path.isdir(self.data_dir):
                for root, _, files in os.walk(self.data_dir):
                    for fn in files:
                        if fn.endswith(FILE_EXT):
                            try:
                                os.remove(os.path.join(root, fn))
                            except FileNotFoundError:
                                pass
            self._entries.clear()
            self._size_bytes = 0
            self._index_built = True

    def cleanup(self) -> int:
        """
        Remove expired entries. Returns number of removed files.
        """
        removed = 0
        with self._lock:
            for path in list(self._iter_all_paths()):
                hdr = self._read_header(path)
                if hdr is None:
                    continue
                if self._is_expired(hdr):
                    if self._remove_path(path):
                        removed += 1
        return removed

    def total_size_bytes(self) -> int:
        self._ensure_index()
        return self._size_bytes

    def close(self) -> None:
        # nothing to close
        pass

    # Internal helpers

    def _normalize_key(self, key: Any) -> dict:
        return _encode_key(key)

    def _path_for_key(self, key_enc: dict) -> str:
        kb = _key_to_bytes(key_enc)
        h = _hash_bytes(kb)
        parts = [self.data_dir]
        for i in range(self.shards):
            parts.append(h[2 * i : 2 * i + 2])
        _ensure_dir(os.path.join(*parts))
        return os.path.join(*parts, h + FILE_EXT)

    def _iter_all_paths(self) -> Iterator[str]:
        # No lock here; caller holds self._lock.
        if not os.path.isdir(self.data_dir):
            return
        for root, _, files in os.walk(self.data_dir):
            for fn in files:
                if fn.endswith(FILE_EXT):
                    yield os.path.join(root, fn)

    def _evict_if_needed(self) -> None:
        if self.max_bytes is None:
            return
        self._ensure_index()
        # Evict until size <= max_bytes using LRU by mtime
        while self._size_bytes > self.max_bytes and self._entries:
            # Pick oldest mtime
            victim, (sz, mt) = min(self._entries.items(), key=lambda kv: kv[1][1])
            self._remove_path(victim)

    def _store_value(self, key: Any, value: Any, expires_at: Optional[float]) -> None:
        key_enc = self._normalize_key(key)
        path = self._path_for_key(key_enc)

        payload = self._ser.dumps(value)
        if self.compress:
            payload = gzip.compress(payload, compresslevel=6)

        header = {
            "v": 1,
            "key": key_enc,
            "expires_at": expires_at,
            "ser": self._ser.name,
            "gz": bool(self.compress),
        }
        header_bytes = json.dumps(header, separators=(",", ":"), ensure_ascii=False).encode("utf-8")

        tmp_path = os.path.join(
            self.tmp_dir,
            f".{os.getpid()}.{int(time.time()*1000)}.{random.randint(0, 1<<31):08x}.tmp",
        )

        with self._lock:
            old_size = os.path.getsize(path) if os.path.exists(path) else 0

            # Write temp file
            with open(tmp_path, "wb") as f:
                f.write(MAGIC)
                f.write(struct.pack(HEADER_LEN_STRUCT, len(header_bytes)))
                f.write(header_bytes)
                f.write(payload)
                if self.fsync:
                    f.flush()
                    os.fsync(f.fileno())

            # Ensure destination directory exists (already ensured in _path_for_key)
            # Atomic replace
            os.replace(tmp_path, path)

            # Update times to "accessed now" for LRU (also changes mtime)
            now = _now()
            try:
                os.utime(path, (now, now))
            except Exception:
                pass

            # Update index
            new_size = os.path.getsize(path)
            delta = new_size - old_size
            self._ensure_index()
            self._entries[path] = (new_size, os.path.getmtime(path))
            self._size_bytes += delta

            # Evict if needed
            self._evict_if_needed()

    def _load_value(self, key: Any, *, load_value: bool = True) -> tuple[bool, Optional[Any]]:
        key_enc = self._normalize_key(key)
        path = self._path_for_key(key_enc)
        with self._lock:
            if not os.path.exists(path):
                return False, None
            try:
                with open(path, "rb") as f:
                    # Header
                    magic = _read_exact(f, len(MAGIC))
                    if magic != MAGIC:
                        # Corrupt
                        self._remove_path(path)
                        return False, None
                    hlen = struct.unpack(HEADER_LEN_STRUCT, _read_exact(f, HEADER_LEN_SIZE))[0]
                    hdr = json.loads(_read_exact(f, hlen).decode("utf-8"))

                    # Expiry
                    if self._is_expired(hdr):
                        self._remove_path(path)
                        return False, None

                    # Confirm it's the same hashed key (cheap sanity)
                    # Not strictly necessary because path encodes the hash
                    # but header holds original key for iteration.
                    # Read payload only if needed
                    if not load_value:
                        # touch for LRU
                        self._touch(path)
                        return True, None

                    payload = f.read()

                # Touch for LRU
                self._touch(path)

                # Deserialize
                data = payload
                if hdr.get("gz"):
                    data = gzip.decompress(data)
                ser_name = hdr.get("ser", "pickle")
                if ser_name == "pickle":
                    val = PickleSerializer.loads(data)
                elif ser_name == "json":
                    val = JsonSerializer.loads(data)
                else:
                    # Unknown serializer; treat as bytes
                    val = data
                return True, val
            except FileNotFoundError:
                return False, None
            except Exception:
                # Corrupt; remove
                self._remove_path(path)
                return False, None

    def _read_header(self, path: str) -> Optional[dict]:
        try:
            with open(path, "rb") as f:
                magic = _read_exact(f, len(MAGIC))
                if magic != MAGIC:
                    self._remove_path(path)
                    return None
                hlen = struct.unpack(HEADER_LEN_STRUCT, _read_exact(f, HEADER_LEN_SIZE))[0]
                hdr = json.loads(_read_exact(f, hlen).decode("utf-8"))
                return hdr
        except FileNotFoundError:
            return None
        except Exception:
            self._remove_path(path)
            return None

    def _is_expired(self, hdr: dict) -> bool:
        exp = hdr.get("expires_at")
        return exp is not None and exp < _now()

    def _touch(self, path: str) -> None:
        now = _now()
        try:
            os.utime(path, (now, now))
        except Exception:
            pass
        try:
            st = os.stat(path)
            self._ensure_index()
            self._entries[path] = (st.st_size, st.st_mtime)
        except FileNotFoundError:
            # already removed
            pass

    def _delete_key(self, key: Any, *, missing_ok: bool) -> None:
        key_enc = self._normalize_key(key)
        path = self._path_for_key(key_enc)
        ok = self._remove_path(path)
        if not ok and not missing_ok:
            raise KeyError(key)

    def _remove_path(self, path: str) -> bool:
        with self._lock:
            try:
                sz = os.path.getsize(path)
            except FileNotFoundError:
                sz = 0
            try:
                os.remove(path)
                removed = True
            except FileNotFoundError:
                removed = False
            except PermissionError:
                # On Windows, sometimes an antivirus/file handle can block deletion; try rename then delete
                try:
                    tmp = path + f".del.{random.randint(0,1<<31):08x}"
                    os.replace(path, tmp)
                    os.remove(tmp)
                    removed = True
                except Exception:
                    removed = False
            if removed:
                self._ensure_index()
                self._entries.pop(path, None)
                self._size_bytes = max(0, self._size_bytes - sz)
            return removed

    def _ensure_index(self) -> None:
        if self._index_built:
            return
        with self._lock:
            if self._index_built:
                return
            total = 0
            entries = {}
            for path in self._iter_all_paths():
                try:
                    st = os.stat(path)
                except FileNotFoundError:
                    continue
                entries[path] = (st.st_size, st.st_mtime)
                total += st.st_size
            self._entries = entries
            self._size_bytes = total
            self._index_built = True


if __name__ == "__main__":
    # Simple demo and sanity check
    cache = FileCache("./.demo_cache", serializer="pickle", compress=True, default_ttl=2.0, max_bytes=10_000_000)

    cache["foo"] = {"a": 1}
    cache.set("bar", [1, 2, 3], ttl=5)
    cache[123] = b"bytes"

    print("foo in cache?", "foo" in cache)
    print("bar value:", cache.get("bar"))

    print("Keys:", list(cache.keys()))
    print("Items:", list(cache.items()))
    print("Total size (bytes):", cache.total_size_bytes())

    time.sleep(2.1)
    print("After TTL default:", cache.get("foo", "expired"))

    # Cleanup expired
    removed = cache.cleanup()
    print("Expired removed:", removed)

    # Delete a key
    del cache[123]
    print("123 in cache?", 123 in cache)

    cache.clear()
    print("len after clear:", len(cache))
------------------------------------------------------------

PROMPT 402:
"Generate Python code that reads in Ribo-Seq data, applies a unit step transformation to the data, and outputs the transformed data in a new file."
RESPONSE:
# riboseq_unit_step.py
# 
# Reads Ribo-Seq data, applies a unit step transformation to chosen numeric column(s),
# and writes the transformed data to a new file.
#
# Supported formats:
# - CSV/TSV (auto-detected delimiter or forced via --format)
# - bedGraph (4-column; preserves track/browser/comment lines)
#
# Unit step (Heaviside) definition:
#   y = 1 if x > threshold            when --strict
#   y = 1 if x >= threshold           default
#   y = 0 otherwise
#
# Usage examples:
#   python riboseq_unit_step.py input.csv output.csv
#   python riboseq_unit_step.py data.tsv out.tsv --format tsv --value-cols counts --threshold 0 --strict
#   python riboseq_unit_step.py signal.bedgraph out.bedgraph --format bedgraph --threshold 5
#   python riboseq_unit_step.py big.csv.gz out.csv.gz --chunksize 500000 --value-cols col1,col2
#
# Notes:
# - For CSV/TSV, if --value-cols is not provided, all numeric columns except common genomic
#   coordinate columns are transformed.
# - For bedGraph, the 4th column (value) is transformed.
# - Compressed files (.gz, .bz2, .xz, .zip) are supported for CSV/TSV via pandas.
# - Large files can be processed with --chunksize to limit memory usage.

import argparse
import csv
import gzip
import io
import os
import sys
from typing import Iterable, List, Optional

import numpy as np
import pandas as pd


GENOMIC_COL_CANDIDATES = {
    "chrom", "chr", "chromosome", "seqname",
    "start", "end", "stop",
    "strand", "score", "name", "id", "feature"
}


def parse_args():
    p = argparse.ArgumentParser(description="Apply unit step transformation to Ribo-Seq data.")
    p.add_argument("input", help="Input file path (CSV/TSV/bedGraph).")
    p.add_argument("output", help="Output file path.")
    p.add_argument("--format",
                   choices=["auto", "csv", "tsv", "bedgraph"],
                   default="auto",
                   help="Input format. Default: auto (by extension or delimiter sniff).")
    p.add_argument("--value-cols",
                   type=str,
                   default=None,
                   help="Comma-separated list of column names or 0-based indices to transform (CSV/TSV). "
                        "If omitted, all numeric columns except typical genomic columns are used.")
    p.add_argument("--threshold",
                   type=float,
                   default=0.0,
                   help="Threshold for unit step.")
    p.add_argument("--strict",
                   action="store_true",
                   help="Use strict step: 1 if x > threshold else 0. Default: 1 if x >= threshold else 0.")
    p.add_argument("--na-to-zero",
                   action="store_true",
                   help="Convert NaN in selected columns to 0 before applying step.")
    p.add_argument("--chunksize",
                   type=int,
                   default=None,
                   help="Process CSV/TSV in chunks of this many rows to reduce memory usage.")
    p.add_argument("--no-header",
                   action="store_true",
                   help="Indicate the CSV/TSV has no header row.")
    return p.parse_args()


def is_compressed(path: str) -> bool:
    return path.lower().endswith((".gz", ".bgz", ".bz2", ".xz", ".zip"))


def detect_format_by_ext(path: str) -> Optional[str]:
    lower = path.lower()
    if lower.endswith((".bedgraph", ".bdg")):
        return "bedgraph"
    if lower.endswith(".tsv") or lower.endswith(".tab"):
        return "tsv"
    if lower.endswith(".csv"):
        return "csv"
    return None


def sniff_delimiter(path: str) -> str:
    opener = open
    if path.endswith(".gz"):
        opener = gzip.open
    with opener(path, "rt", newline="") as f:
        sample = f.read(10240)
    try:
        dialect = csv.Sniffer().sniff(sample, delimiters=[",", "\t", ";", "|"])
        return dialect.delimiter
    except Exception:
        # Fallback heuristics
        if "\t" in sample and sample.count("\t") >= sample.count(","):
            return "\t"
        return ","


def parse_value_cols_arg(arg: Optional[str]) -> Optional[List[str]]:
    if arg is None or arg.strip() == "":
        return None
    items = [x.strip() for x in arg.split(",")]
    return [x for x in items if x != ""]


def to_col_indices(df: pd.DataFrame, cols: Optional[List[str]]) -> List[int]:
    if cols is None:
        # Default: all numeric columns except common genomic columns
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        selected = [c for c in numeric_cols if c.lower() not in GENOMIC_COL_CANDIDATES]
        if not selected:
            # if none found, fallback to all numeric
            selected = numeric_cols
        return [df.columns.get_loc(c) for c in selected]
    idxs: List[int] = []
    for c in cols:
        if c.isdigit():
            i = int(c)
            if i < 0 or i >= df.shape[1]:
                raise ValueError(f"value-cols index out of range: {i}")
            idxs.append(i)
        else:
            if c not in df.columns:
                raise ValueError(f"value-cols name not found: {c}")
            idxs.append(df.columns.get_loc(c))
    return idxs


def unit_step_array(x: np.ndarray, threshold: float, strict: bool) -> np.ndarray:
    if strict:
        return (x > threshold).astype(np.int8)
    else:
        return (x >= threshold).astype(np.int8)


def process_csv_tsv(
    in_path: str,
    out_path: str,
    fmt: str,
    value_cols: Optional[List[str]],
    threshold: float,
    strict: bool,
    na_to_zero: bool,
    chunksize: Optional[int],
    has_header: bool,
):
    # Determine delimiter
    if fmt == "csv":
        sep = ","
    elif fmt == "tsv":
        sep = "\t"
    else:
        sep = sniff_delimiter(in_path)

    compression = "infer" if is_compressed(in_path) else None

    read_kwargs = dict(
        sep=sep,
        compression=compression,
        engine="python",
        dtype=None,
        header=None if has_header is False else "infer"
    )

    # Prepare writer
    write_compression = None
    if out_path.lower().endswith(".gz"):
        write_compression = {"method": "gzip"}
    elif out_path.lower().endswith(".bz2"):
        write_compression = {"method": "bz2"}
    elif out_path.lower().endswith(".xz"):
        write_compression = {"method": "xz"}
    elif out_path.lower().endswith(".zip"):
        write_compression = {"method": "zip"}

    # Stream in chunks if requested
    first_chunk = True
    if chunksize:
        for chunk in pd.read_csv(in_path, chunksize=chunksize, **read_kwargs):
            idxs = to_col_indices(chunk, value_cols)
            if na_to_zero:
                chunk.iloc[:, idxs] = chunk.iloc[:, idxs].fillna(0)
            # Ensure numeric for selected columns
            for i in idxs:
                chunk.iloc[:, i] = pd.to_numeric(chunk.iloc[:, i], errors="coerce")
            transformed = chunk.copy()
            transformed.iloc[:, idxs] = unit_step_array(transformed.iloc[:, idxs].to_numpy(), threshold, strict)
            transformed.to_csv(
                out_path,
                sep=sep,
                index=False,
                mode="w" if first_chunk else "a",
                header=(False if (not has_header or read_kwargs["header"] is None) else first_chunk),
                compression=write_compression,
            )
            first_chunk = False
    else:
        df = pd.read_csv(in_path, **read_kwargs)
        idxs = to_col_indices(df, value_cols)
        if na_to_zero:
            df.iloc[:, idxs] = df.iloc[:, idxs].fillna(0)
        for i in idxs:
            df.iloc[:, i] = pd.to_numeric(df.iloc[:, i], errors="coerce")
        df.iloc[:, idxs] = unit_step_array(df.iloc[:, idxs].to_numpy(), threshold, strict)
        df.to_csv(out_path, sep=sep, index=False, compression=write_compression)


def open_text_maybe_gzip(path: str, mode: str):
    if path.endswith(".gz"):
        return gzip.open(path, mode + "t")
    return open(path, mode, encoding="utf-8", newline="")


def process_bedgraph(
    in_path: str,
    out_path: str,
    threshold: float,
    strict: bool,
):
    # bedGraph lines:
    #   chrom  start  end  value
    # Track or browser lines should be preserved.
    def transform_value(val_str: str) -> str:
        try:
            x = float(val_str)
            y = 1 if (x > threshold if strict else x >= threshold) else 0
            return str(y)
        except Exception:
            # If not numeric, keep as-is (conservative)
            return val_str

    with open_text_maybe_gzip(in_path, "r") as fin, open_text_maybe_gzip(out_path, "w") as fout:
        for line in fin:
            if not line.strip():
                fout.write(line)
                continue
            lstr = line.lstrip()
            if lstr.startswith("#") or lstr.startswith("track") or lstr.startswith("browser"):
                fout.write(line)
                continue
            parts = line.rstrip("\n").split("\t")
            # If not tab-separated, try whitespace split
            if len(parts) < 4:
                parts = line.rstrip("\n").split()
            if len(parts) >= 4:
                parts[3] = transform_value(parts[3])
                fout.write("\t".join(parts) + "\n")
            else:
                # Unrecognized line, write through
                fout.write(line)


def main():
    args = parse_args()

    fmt = args.format
    if fmt == "auto":
        fmt = detect_format_by_ext(args.input) or "auto"

    if fmt == "bedgraph":
        process_bedgraph(
            args.input,
            args.output,
            threshold=args.threshold,
            strict=args.strict,
        )
    else:
        # CSV/TSV path (auto-detect delimiter if needed)
        value_cols = parse_value_cols_arg(args.value_cols)
        has_header = not args.no_header
        process_csv_tsv(
            args.input,
            args.output,
            fmt=fmt,
            value_cols=value_cols,
            threshold=args.threshold,
            strict=args.strict,
            na_to_zero=args.na_to_zero,
            chunksize=args.chunksize,
            has_header=has_header,
        )


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        sys.stderr.write(f"Error: {e}\n")
        sys.exit(1)
------------------------------------------------------------

PROMPT 403:
"Generate Python code that imports the necessary modules, creates a JACK client instance, and sets up a callback function to handle message events."
RESPONSE:
# Requires: python-jack-client (pip install jack-client)

import sys
import time
import signal

import jack


# Create a JACK client
client = jack.Client("PythonJACK_MIDI_Listener")

# Register a MIDI input port
midi_in = client.midi_inports.register("midi_in")


def handle_midi_event(timestamp_sec: float, data: bytes) -> None:
    """
    Handle a single MIDI message event.
    timestamp_sec: absolute time in seconds (derived from JACK frame time)
    data: raw MIDI message bytes
    """
    if not data:
        return

    status = data[0]
    msg_type = status & 0xF0
    channel = (status & 0x0F) + 1

    msg_name = {
        0x80: "Note Off",
        0x90: "Note On",
        0xA0: "Poly Aftertouch",
        0xB0: "Control Change",
        0xC0: "Program Change",
        0xD0: "Channel Aftertouch",
        0xE0: "Pitch Bend",
    }.get(msg_type, "System/Other")

    print(f"[{timestamp_sec:12.6f}s] {msg_name:16} ch{channel:02} data={list(data)}")


@client.set_process_callback
def process(frames: int) -> None:
    """
    JACK process callback. Called for each audio/MIDI period.
    Iterates over incoming MIDI events and dispatches them to the handler.
    """
    base_frame = client.last_frame_time
    sr = client.samplerate

    for offset, data in midi_in.incoming_midi_events():
        # Convert JACK frame time to seconds for readability
        ts_sec = (base_frame + offset) / sr
        handle_midi_event(ts_sec, bytes(data))


@client.set_shutdown_callback
def shutdown(status, reason) -> None:
    print(f"JACK server shutdown. status={status} reason={reason}", file=sys.stderr)
    sys.exit(1)


def main():
    # Activate client (connects to JACK and starts callbacks)
    with client:
        # Optional: auto-connect first available physical MIDI output to our input
        try:
            sources = client.get_ports(is_midi=True, is_output=True, is_physical=True)
            if sources:
                try:
                    client.connect(sources[0], midi_in)
                    print(f"Auto-connected: {sources[0]} -> {midi_in}")
                except jack.JackError as e:
                    print(f"Auto-connect failed: {e}", file=sys.stderr)
        except jack.JackError as e:
            print(f"Error querying ports: {e}", file=sys.stderr)

        print("JACK client active. Listening for MIDI message events. Press Ctrl+C to quit.")
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nExiting.")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 404:
"Generate Python code that imports the necessary modules and creates a Datadog provider for Terraform CDK (cdktf) using the prebuilt Datadog provider."
RESPONSE:
import os
from constructs import Construct
from cdktf import App, TerraformStack
from cdktf_cdktf_provider_datadog.provider import DatadogProvider

class DatadogStack(TerraformStack):
    def __init__(self, scope: Construct, ns: str):
        super().__init__(scope, ns)

        # Create the Datadog provider (reads keys from env by default)
        DatadogProvider(
            self,
            "datadog",
            api_key=os.getenv("DATADOG_API_KEY"),
            app_key=os.getenv("DATADOG_APP_KEY"),
            api_url=os.getenv("DATADOG_API_URL", "https://api.datadoghq.com"),
        )

app = App()
DatadogStack(app, "datadog-provider-example")
app.synth()
------------------------------------------------------------

PROMPT 405:
"Generate Python code that creates an XBlock for a course, with a title, a description, and a button that triggers a JavaScript function when clicked."
RESPONSE:
# button_xblock.py

from xblock.core import XBlock
from xblock.fields import Scope, String
from xblock.fragment import Fragment
import html as _html


class ButtonXBlock(XBlock):
    """
    A minimal XBlock that shows a title, a description, and a button
    that triggers a JavaScript function when clicked.
    """

    # Fields editable in Studio
    display_name = String(
        default="Button XBlock",
        scope=Scope.settings,
        help="The title shown to learners."
    )

    description = String(
        default="Click the button to trigger a JavaScript action.",
        scope=Scope.content,
        help="A short description displayed to learners."
    )

    def student_view(self, context=None):
        """
        The primary view of the XBlock, shown to learners.
        """
        title = _html.escape(self.display_name or "")
        desc = _html.escape(self.description or "")

        body = f"""
            <div class="button-xblock">
                <h3 class="button-xblock__title">{title}</h3>
                <p class="button-xblock__desc">{desc}</p>
                <button type="button" class="button-xblock__btn">Click me</button>
                <div class="button-xblock__output" aria-live="polite"></div>
            </div>
        """

        css = """
            .button-xblock__btn {
                padding: 8px 12px;
                background: #0b5fff;
                color: #fff;
                border: 0;
                border-radius: 4px;
                cursor: pointer;
            }
            .button-xblock__btn:hover { background: #0a52e6; }
            .button-xblock__output { margin-top: 8px; font-style: italic; }
        """

        # Note: edX runtime provides jQuery globally in LMS/Studio contexts.
        js = """
        function ButtonXBlock(runtime, element) {
            var $root = $(element);
            var $btn = $root.find('.button-xblock__btn');
            var $out = $root.find('.button-xblock__output');

            var count = 0;

            $btn.on('click', function() {
                count += 1;
                // Example JS behavior: update the UI with a click count.
                $out.text('Button clicked ' + count + ' time' + (count === 1 ? '' : 's') + '.');
            });
        }
        """

        frag = Fragment(body)
        frag.add_css(css)
        frag.add_javascript(js)
        frag.initialize("ButtonXBlock")
        return frag

    # Optional: simple author view in Studio (shows current values)
    def author_view(self, context=None):
        return self.student_view(context)
------------------------------------------------------------

PROMPT 406:
"Generate Python code that imports the necessary AWS CDK modules and creates a cross-region global Aurora RDS database using the CDK construct library."
RESPONSE:
# app.py
import os
from aws_cdk import (
    App,
    Stack,
    Environment,
    RemovalPolicy,
    Arn,
    ArnComponents,
    aws_ec2 as ec2,
    aws_rds as rds,
)
from constructs import Construct


# Configuration (replace with your values or pass via context/env)
ACCOUNT = os.getenv("CDK_DEFAULT_ACCOUNT", "123456789012")
PRIMARY_REGION = os.getenv("PRIMARY_REGION", "us-east-1")
SECONDARY_REGION = os.getenv("SECONDARY_REGION", "us-west-2")

GLOBAL_CLUSTER_ID = "my-global-aurora"
PRIMARY_CLUSTER_ID = "primary-aurora-cluster"

# Choose an Aurora MySQL version supported by Global Database
AURORA_MYSQL_VERSION = rds.AuroraMysqlEngineVersion.VER_3_05_2  # Aurora MySQL 3.x (compatible with MySQL 8.0)
AURORA_MYSQL_VERSION_STR = AURORA_MYSQL_VERSION.full_version


class PrimaryAuroraGlobalStack(Stack):
    def __init__(self, scope: Construct, construct_id: str, *, global_cluster_id: str, primary_cluster_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # Networking for RDS
        vpc = ec2.Vpc(self, "PrimaryVpc", max_azs=2, nat_gateways=1)

        # Security group that allows MySQL within the SG (adjust as needed)
        sg = ec2.SecurityGroup(self, "PrimaryRdsSg", vpc=vpc, allow_all_outbound=True)
        sg.add_ingress_rule(peer=sg, connection=ec2.Port.tcp(3306), description="Self (Aurora intra-cluster)")

        # Subnet group for DB
        subnet_group = rds.SubnetGroup(
            self,
            "PrimarySubnetGroup",
            description="Primary Aurora subnet group",
            vpc=vpc,
            vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS),
        )

        # Primary Aurora DB cluster (provisioned) - L2 construct creates credentials in Secrets Manager
        primary_cluster = rds.DatabaseCluster(
            self,
            "PrimaryAuroraCluster",
            cluster_identifier=primary_cluster_id,
            engine=rds.DatabaseClusterEngine.aurora_mysql(version=AURORA_MYSQL_VERSION),
            credentials=rds.Credentials.from_generated_secret("admin"),
            writer=rds.ClusterInstance.provisioned(
                "Writer",
                instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE3, ec2.InstanceSize.MEDIUM),
            ),
            readers=[
                rds.ClusterInstance.provisioned(
                    "Reader1",
                    instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE3, ec2.InstanceSize.MEDIUM),
                )
            ],
            default_database_name="appdb",
            vpc=vpc,
            security_groups=[sg],
            vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS),
            storage_encrypted=True,
            removal_policy=RemovalPolicy.DESTROY,  # do not use DESTROY for production
        )

        # Create the Global Cluster and attach the primary DB cluster to it
        # Using the L1 because global cluster is not fully modeled in L2
        rds.CfnGlobalCluster(
            self,
            "GlobalCluster",
            engine="aurora-mysql",
            global_cluster_identifier=global_cluster_id,
            source_db_cluster_identifier=primary_cluster.cluster_arn,
        )


class SecondaryAuroraGlobalStack(Stack):
    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        *,
        global_cluster_id: str,
        primary_region: str,
        primary_cluster_id: str,
        **kwargs,
    ) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # Networking for secondary region
        vpc = ec2.Vpc(self, "SecondaryVpc", max_azs=2, nat_gateways=1)

        sg = ec2.SecurityGroup(self, "SecondaryRdsSg", vpc=vpc, allow_all_outbound=True)
        sg.add_ingress_rule(peer=sg, connection=ec2.Port.tcp(3306), description="Self (Aurora intra-cluster)")

        subnet_group = rds.SubnetGroup(
            self,
            "SecondarySubnetGroup",
            description="Secondary Aurora subnet group",
            vpc=vpc,
            vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS),
        )

        # Build the ARN of the primary cluster deterministically so we don't create a cross-region reference
        primary_cluster_arn = Arn.format(
            ArnComponents(
                service="rds",
                region=primary_region,
                account=self.account,
                resource="cluster",
                resource_name=primary_cluster_id,
                sep=":",
            ),
            stack=self,
        )

        # Secondary (global) DB cluster - use L1 to set GlobalClusterIdentifier + ReplicationSourceIdentifier
        secondary_cluster = rds.CfnDBCluster(
            self,
            "SecondaryDbCluster",
            engine="aurora-mysql",
            engine_version=AURORA_MYSQL_VERSION_STR,
            db_subnet_group_name=subnet_group.subnet_group_name,
            vpc_security_group_ids=[sg.security_group_id],
            deletion_protection=False,
            # Join the same Global Cluster and replicate from the primary cluster
            global_cluster_identifier=global_cluster_id,
            replication_source_identifier=primary_cluster_arn,
        )

        # Add one or more instances to the secondary cluster (provisioned instances)
        rds.CfnDBInstance(
            self,
            "SecondaryDbInstance1",
            engine="aurora-mysql",
            db_instance_class="db.r6g.large",  # choose an Aurora-supported instance class
            db_cluster_identifier=secondary_cluster.ref,
            publicly_accessible=False,
        )


app = App()

primary_env = Environment(account=ACCOUNT, region=PRIMARY_REGION)
secondary_env = Environment(account=ACCOUNT, region=SECONDARY_REGION)

primary_stack = PrimaryAuroraGlobalStack(
    app,
    "PrimaryAuroraGlobalStack",
    env=primary_env,
    global_cluster_id=GLOBAL_CLUSTER_ID,
    primary_cluster_id=PRIMARY_CLUSTER_ID,
)

# Note: Deploy the primary stack first, then the secondary stack.
secondary_stack = SecondaryAuroraGlobalStack(
    app,
    "SecondaryAuroraGlobalStack",
    env=secondary_env,
    global_cluster_id=GLOBAL_CLUSTER_ID,
    primary_region=PRIMARY_REGION,
    primary_cluster_id=PRIMARY_CLUSTER_ID,
)

app.synth()
------------------------------------------------------------

PROMPT 407:
"Generate Python code that uses the `cffi` module to create typing stubs for a C library, allowing for type-safe interaction with the library in Python."
RESPONSE:
Below is a self-contained Python code generator that:

- Uses cffi to parse a C header signature (the cdef string).
- Emits a Python runtime module that loads the C library via cffi.
- Emits a .pyi stub for the module that exposes a Protocol-based interface so type checkers (e.g., mypy, pyright) can type-check your calls.

It avoids writing per-function wrappers by generating a Protocol that matches your C API; you get type-checked calls while still using the raw cffi library object at runtime.

Save as gen_cffi_stubs.py and run it to generate a typed binding module and its stub.

Code (generator)
- Generates:
  - myclib.py: runtime loader using cffi
  - myclib.pyi: typing stub with a Protocol describing the C API

You can modify the CDEF string and library name to match your library.

from __future__ import annotations

import os
import sys
import textwrap
from typing import Iterable, Tuple, List
from cffi import FFI
from ctypes.util import find_library

# 1) Put your C API here (or load it from a header and preprocess)
# Example: a tiny subset of libm so this works out-of-the-box on POSIX.
CDEF = r"""
    double cos(double x);
    double sin(double x);
    double sqrt(double x);
    double pow(double x, double y);

    // Examples of various types (uncomment if your library provides them):
    // const char *library_version(void);
    // size_t vector_sum(const double *arr, size_t n, double *out); // returns number of items processed
    // void fill_buffer(char *buf, size_t n);
    // struct point { double x; double y; };
    // void translate(struct point *p, double dx, double dy);
"""

# 2) How to locate your library at runtime
# Provide a base name; find_library will resolve the full name cross-platform.
# For libm on POSIX, "m" typically works. For Windows, try "ucrtbase" as a fallback.
def default_lib_name() -> str:
    if os.name == "nt":
        for cand in ("ucrtbase", "msvcrt", "vcruntime140", "vcruntime140_1"):
            path = find_library(cand)
            if path:
                return path
        # As a last resort, try raw candidate (cffi may still load it if it's on PATH)
        return "ucrtbase"
    else:
        # POSIX: libm
        lib = find_library("m")
        return lib or "m"


# 3) Minimal C type to Python type mapping for stubs
# We keep pointers as CData (accurate for cffi) and map scalars to Python types.
# You can extend these rules if you want friendlier wrappers.
def ctype_to_pyann(ffi: FFI, ctype, position: str) -> str:
    """
    Convert a cffi ctype to a string annotation suitable for .pyi.
    position in {"arg", "ret"} can be used to fine-tune special cases.
    """
    # String form to make rule-based decisions
    cdecl = ffi.getctype(ctype)
    cdecl = " ".join(cdecl.split())  # normalize whitespace

    # Basic classifiers
    def is_pointer() -> bool:
        # cffi formats pointers with a trailing '*' (except function ptrs)
        return "*" in cdecl or cdecl.endswith(" *")

    def is_func_ptr() -> bool:
        # Function pointers look like: 'double(*)(double)'
        return "(*" in cdecl and ")(" in cdecl

    # Primitive scalars
    integer_types = {
        "char", "signed char", "unsigned char",
        "short", "short int", "signed short", "signed short int",
        "unsigned short", "unsigned short int",
        "int", "signed int", "unsigned", "unsigned int",
        "long", "long int", "signed long", "signed long int",
        "unsigned long", "unsigned long int",
        "long long", "long long int", "signed long long", "signed long long int",
        "unsigned long long", "unsigned long long int",
        "size_t", "ssize_t", "ptrdiff_t",
    }
    float_types = {"float", "double", "long double"}
    bool_types = {"_Bool", "bool"}  # _Bool is the C11 type; bool with <stdbool.h>

    # Strip qualifiers
    base = cdecl
    for qual in ("const ", "volatile ", "restrict "):
        base = base.replace(qual, "")
    base = base.strip()

    # Normalize 'long long int' -> 'long long int' handled by set above
    # 1) Function pointer => CData
    if is_func_ptr():
        return "CData"

    # 2) Void
    if base == "void":
        return "None" if position == "ret" else "None"  # arguments rarely are pure void

    # 3) Scalars
    if base in integer_types:
        return "int"
    if base in float_types:
        return "float"
    if base in bool_types:
        return "bool"

    # 4) Pointers and arrays
    if is_pointer():
        # You can choose to treat 'const char *' arguments as 'bytes | str | CData' if desired.
        # For simplicity and accuracy (cffi returns CData for pointers), keep CData.
        return "CData"

    # 5) Structs/unions/enums by value (rare)
    # Map enums/struct by value to int/CData conservatively.
    if base.startswith("enum "):
        return "int"
    if base.startswith("struct ") or base.startswith("union "):
        return "CData"

    # Fallback: unknown -> CData
    return "CData"


def extract_functions(ffi: FFI) -> List[Tuple[str, object]]:
    """
    Return a list of (func_name, func_ctype) from the cdef.
    Relies on cffi's private parser; acceptable for codegen.
    """
    decls = getattr(ffi, "_parser")._declarations  # type: ignore[attr-defined]
    items = []
    for (kind, name), ctype in decls.items():
        if kind == "function":
            items.append((name, ctype))
    # Stable order
    items.sort()
    return items


def extract_variables(ffi: FFI) -> List[Tuple[str, object]]:
    decls = getattr(ffi, "_parser")._declarations  # type: ignore[attr-defined]
    items = []
    for (kind, name), ctype in decls.items():
        if kind == "variable":
            items.append((name, ctype))
    items.sort()
    return items


def gen_protocol_pyi(module_name: str, cdef: str, proto_name: str = "CLib") -> str:
    """
    Build .pyi content for the module. Exposes:
      - class CLib(Protocol): ... with all C functions
      - lib: CLib
      - ffi: FFI
    """
    ffi = FFI()
    ffi.cdef(cdef)

    funcs = extract_functions(ffi)
    vars_ = extract_variables(ffi)

    lines: List[str] = []
    lines.append("# This file is auto-generated. Do not edit by hand.")
    lines.append("from typing import Protocol")
    lines.append("from cffi import FFI")
    lines.append("from _cffi_backend import CData")
    lines.append("")
    lines.append(f"class {proto_name}(Protocol):")

    if not funcs and not vars_:
        lines.append("    pass")
    else:
        for fname, fctype in funcs:
            # ctype has .result and .args, and maybe .ellipsis
            ret = ctype_to_pyann(ffi, fctype.result, "ret")

            # args might have names or not
            params: List[str] = []
            if getattr(fctype, "ellipsis", False):
                # Variadic function: generate a minimal signature
                # We add *args: CData to cover extra
                params.append("*args: CData")
            else:
                for i, argt in enumerate(fctype.args or []):
                    # argt may have .name in newer cffi. Fall back to a{i}
                    name = getattr(argt, "name", None) or f"a{i}"
                    ann = ctype_to_pyann(ffi, argt, "arg")
                    params.append(f"{name}: {ann}")

            sig = ", ".join(params)
            # Methods in Protocol must have 'self' if this were a class, but here we model attributes
            # of a library object which are callable objects. In stubs, we write them as methods to
            # benefit from Protocol subtyping by attribute.
            # Trick: Represent library functions as methods on Protocol so that an object with callables
            # with matching signatures conforms. Type checkers treat attribute callability appropriately.
            lines.append(f"    def {fname}(self, {sig}) -> {ret}: ...")

        # Add variables as read-write attributes
        for vname, vctype in vars_:
            ann = ctype_to_pyann(ffi, vctype, "ret")
            lines.append(f"    {vname}: {ann}")

    lines.append("")
    lines.append(f"ffi: FFI")
    lines.append(f"lib: {proto_name}")
    lines.append("")
    return "\n".join(lines)


def gen_runtime_module(module_name: str, cdef: str, lib_path: str | None = None) -> str:
    """
    Build the runtime .py content that:
      - creates ffi
      - cdef(cdef)
      - dlopen(lib)
      - exposes 'ffi' and 'lib'
    The .pyi will supply precise types; the runtime is simple.
    """
    lib_expr = repr(lib_path) if lib_path else "None"
    code = f'''
# This file is auto-generated. Do not edit by hand.
from __future__ import annotations

from cffi import FFI
from ctypes.util import find_library
from typing import Optional

_ffi = FFI()

_CDEF = r"""{cdef}"""

_ffi.cdef(_CDEF)

def _resolve_library(user_lib: Optional[str]) -> str:
    if user_lib:
        return user_lib
    # Try to resolve from a base name (edit this function for your library)
    # Falls back to platform defaults if needed.
    # You can also hardcode a full path here.
    # For this generated file, we accept user_lib=None and leave resolution
    # to the OS loader (may work if the library is on default loader paths).
    return user_lib or ""

_user_lib = {lib_expr}
lib = _ffi.dlopen(_resolve_library(_user_lib))
ffi = _ffi
'''
    return textwrap.dedent(code).lstrip()


def write_out(module_name: str, pyi_text: str, py_text: str, out_dir: str = ".") -> Tuple[str, str]:
    pyi_path = os.path.join(out_dir, f"{module_name}.pyi")
    py_path = os.path.join(out_dir, f"{module_name}.py")
    with open(pyi_path, "w", encoding="utf-8") as f:
        f.write(pyi_text)
    with open(py_path, "w", encoding="utf-8") as f:
        f.write(py_text)
    return pyi_path, py_path


def main() -> None:
    module_name = "myclib"

    # Choose/resolve a library for the demo. Adjust to your library.
    lib_path = default_lib_name()  # a filename or base name is fine for cffi.dlopen

    pyi = gen_protocol_pyi(module_name, CDEF, proto_name="CLib")
    py = gen_runtime_module(module_name, CDEF, lib_path=lib_path)

    pyi_path, py_path = write_out(module_name, pyi, py, out_dir=".")
    print(f"Wrote:\n  {py_path}\n  {pyi_path}")
    print("\nTry it:")
    print("  python -c \"import myclib; print(myclib.lib.cos(0.0))\"")
    print("Type-check with mypy/pyright to see typed signatures from myclib.pyi.")


if __name__ == "__main__":
    main()

How to use
- Put your actual C function signatures into CDEF.
- Adjust default_lib_name() or pass lib_path to gen_runtime_module if you know the exact shared library filename.
- Run: python gen_cffi_stubs.py
  - This writes myclib.py (runtime) and myclib.pyi (types).
- Use in code:
  - import myclib
  - myclib.lib.cos(0.5)  # type-checked
- Run mypy/pyright for static type checking; they will read myclib.pyi.

Notes and customization
- The generator maps C scalars to Python types (int, float, bool) and leaves pointers, arrays, and function pointers as CData (from _cffi_backend). This is accurate for raw cffi usage.
- If you prefer more Pythonic types for certain patterns (e.g., const char* as bytes for returns or arguments), extend ctype_to_pyann and optionally wrap such functions at runtime.
- This approach intentionally uses a .pyi stub alongside a minimal runtime; type checkers will enforce the Protocol signatures while you keep the performance and simplicity of cffi’s dlopen.
------------------------------------------------------------

PROMPT 408:
"Generate Python code that creates a simple web framework for learning purposes, including routing, template rendering, and basic HTTP request handling."
RESPONSE:
# miniweb.py
# A tiny educational web framework with routing, template rendering, and basic HTTP handling.
# Standard-library only. Run: python miniweb.py, then visit http://127.0.0.1:8000
# Create a directory named "templates" next to this file and add "home.html" (example at bottom).

import re
import io
import os
import json
import html
import traceback
from http import HTTPStatus
from wsgiref.simple_server import make_server
from urllib.parse import parse_qs, urlencode, unquote, quote


class Safe:
    """Wrap a string to mark it as safe (do not HTML-escape)."""
    def __init__(self, s):
        self.s = s
    def __str__(self):
        return self.s


class Request:
    """Represents an HTTP request (WSGI)."""
    def __init__(self, environ):
        self.environ = environ
        self.method = environ.get('REQUEST_METHOD', 'GET').upper()
        self.path = unquote(environ.get('PATH_INFO', '') or '/')
        self.query_string = environ.get('QUERY_STRING', '')
        self.query = {k: v[0] if len(v) == 1 else v for k, v in parse_qs(self.query_string, keep_blank_values=True).items()}
        self._body = None
        self._text = None
        self._form = None
        self._json = None
        self.headers = self._parse_headers(environ)
        self.cookies = self._parse_cookies(self.headers.get('cookie'))

    def _parse_headers(self, environ):
        headers = {}
        for k, v in environ.items():
            if k.startswith('HTTP_'):
                name = k[5:].replace('_', '-').lower()
                headers[name] = v
        # Some headers are special-cased in WSGI
        if 'CONTENT_TYPE' in environ:
            headers['content-type'] = environ['CONTENT_TYPE']
        if 'CONTENT_LENGTH' in environ:
            headers['content-length'] = environ['CONTENT_LENGTH']
        return headers

    def _parse_cookies(self, cookie_header):
        cookies = {}
        if not cookie_header:
            return cookies
        parts = cookie_header.split(';')
        for part in parts:
            if '=' in part:
                name, val = part.split('=', 1)
                cookies[name.strip()] = unquote(val.strip())
        return cookies

    @property
    def body(self):
        if self._body is None:
            wsgi_input = self.environ['wsgi.input']
            length = self.environ.get('CONTENT_LENGTH')
            try:
                length = int(length) if length else 0
            except ValueError:
                length = 0
            self._body = wsgi_input.read(length) if length > 0 else b''
        return self._body

    @property
    def text(self):
        if self._text is None:
            self._text = self.body.decode(self.charset or 'utf-8', errors='replace')
        return self._text

    @property
    def charset(self):
        ctype = self.headers.get('content-type', '')
        # very minimal parsing for charset=...
        if 'charset=' in ctype:
            return ctype.split('charset=', 1)[1].split(';', 1)[0].strip()
        return 'utf-8'

    @property
    def form(self):
        if self._form is None:
            ctype = (self.headers.get('content-type') or '').split(';', 1)[0].strip().lower()
            if self.method in ('POST', 'PUT', 'PATCH') and ctype == 'application/x-www-form-urlencoded':
                self._form = {k: v[0] if len(v) == 1 else v for k, v in parse_qs(self.text, keep_blank_values=True).items()}
            else:
                self._form = {}
        return self._form

    def json(self, default=None):
        if self._json is None:
            try:
                self._json = json.loads(self.text)
            except Exception:
                self._json = default
        return self._json


class Response:
    """Represents an HTTP response."""
    def __init__(self, body=b'', status=200, headers=None, content_type='text/plain; charset=utf-8'):
        self.status = status
        self.headers = [('Content-Type', content_type)]
        if headers:
            for k, v in headers:
                self.headers.append((k, v))
        self.body = body if isinstance(body, (bytes, bytearray)) else str(body).encode('utf-8')

    @classmethod
    def text(cls, content, status=200, headers=None):
        return cls(str(content), status=status, headers=headers, content_type='text/plain; charset=utf-8')

    @classmethod
    def html(cls, content, status=200, headers=None):
        return cls(str(content), status=status, headers=headers, content_type='text/html; charset=utf-8')

    @classmethod
    def json(cls, data, status=200, headers=None):
        body = json.dumps(data, ensure_ascii=False, separators=(',', ':')).encode('utf-8')
        return cls(body, status=status, headers=headers, content_type='application/json; charset=utf-8')

    @classmethod
    def redirect(cls, location, status=302):
        return cls('', status=status, headers=[('Location', location)])

    def set_header(self, name, value):
        # remove any existing header with same name (case-insensitive), then add
        self.headers = [(k, v) for (k, v) in self.headers if k.lower() != name.lower()]
        self.headers.append((name, value))

    def set_cookie(self, name, value, path='/', http_only=True, secure=False, max_age=None, same_site='Lax'):
        parts = [f"{name}={quote(str(value))}"]
        if path:
            parts.append(f"Path={path}")
        if http_only:
            parts.append("HttpOnly")
        if secure:
            parts.append("Secure")
        if same_site:
            parts.append(f"SameSite={same_site}")
        if max_age is not None:
            parts.append(f"Max-Age={int(max_age)}")
        self.headers.append(('Set-Cookie', '; '.join(parts)))

    def to_wsgi(self):
        status_text = f"{self.status} {HTTPStatus(self.status).phrase if self.status in HTTPStatus._value2member_map_ else ''}".strip()
        return status_text, self.headers, [self.body]


class TemplateRenderer:
    """Very simple HTML template renderer: replaces {{ expr }} with escaped values from context.
    Supports dotted lookups (e.g., user.name). Everything is escaped by default unless wrapped with Safe(...).
    """
    var_re = re.compile(r"{{\s*(.*?)\s*}}")

    def __init__(self, templates_dir='templates'):
        self.templates_dir = templates_dir

    def render(self, template_name, context=None):
        context = context or {}
        path = os.path.join(self.templates_dir, template_name)
        with open(path, 'r', encoding='utf-8') as f:
            src = f.read()
        return self._render_src(src, context)

    def _render_src(self, src, context):
        def replace(match):
            expr = match.group(1)
            value = self._resolve(context, expr)
            if isinstance(value, Safe):
                return str(value)
            return html.escape('' if value is None else str(value), quote=False)
        return self.var_re.sub(replace, src)

    def _resolve(self, ctx, expr):
        expr = expr.strip()
        # Allow piping to Safe via "|safe"
        if '|safe' in expr:
            expr = expr.replace('|safe', '').strip()
            return Safe(self._resolve(ctx, expr))
        parts = expr.split('.')
        val = ctx
        for part in parts:
            part = part.strip()
            # Try dict
            if isinstance(val, dict) and part in val:
                val = val[part]
                continue
            # Try attribute
            if hasattr(val, part):
                val = getattr(val, part)
                continue
            # Try list index
            if isinstance(val, (list, tuple)):
                try:
                    idx = int(part)
                    val = val[idx]
                    continue
                except Exception:
                    return None
            # Unknown
            return None
        # Call zero-arg callables to allow simple funcs in context
        if callable(val):
            try:
                return val()
            except TypeError:
                return val
        return val


class Router:
    """Stores routes and matches incoming requests."""
    def __init__(self):
        self.routes = []  # each: dict(method, pattern, converters, handler, raw_path)

    def add(self, method, path, handler):
        pattern, converters = self._compile_path(path)
        self.routes.append({
            'method': method.upper(),
            'pattern': pattern,
            'converters': converters,
            'handler': handler,
            'raw_path': path
        })

    def match(self, method, path):
        allowed = set()
        for r in self.routes:
            m = r['pattern'].match(path)
            if not m:
                continue
            if r['method'] == method:
                params = {k: r['converters'].get(k, str)(v) for k, v in m.groupdict().items()}
                return r['handler'], params, None
            allowed.add(r['method'])
        return None, None, allowed

    def _compile_path(self, path):
        # Convert paths like /users/<id> or /post/<int:pid> or /files/<path:rest>
        converters = {}
        token_re = re.compile(r"<(?:(int|path|str):)?([a-zA-Z_][a-zA-Z0-9_]*)>")
        idx = 0
        regex = '^'
        for m in token_re.finditer(path):
            start, end = m.span()
            regex += re.escape(path[idx:start])
            type_name = m.group(1) or 'str'
            name = m.group(2)
            if type_name == 'int':
                regex += fr"(?P<{name}>\d+)"
                converters[name] = int
            elif type_name == 'path':
                regex += fr"(?P<{name}>.+)"
                converters[name] = str
            else:
                regex += fr"(?P<{name}>[^/]+)"
                converters[name] = str
            idx = end
        regex += re.escape(path[idx:]) + '$'
        return re.compile(regex), converters


class MiniWeb:
    """Main application: routing, rendering, and WSGI interface."""
    def __init__(self, templates_dir='templates', debug=True):
        self.debug = debug
        self.router = Router()
        self.templates = TemplateRenderer(templates_dir=templates_dir)
        self._error_handlers = {
            404: self._default_404,
            405: self._default_405,
            500: self._default_500,
        }

    # Routing API
    def route(self, path, methods=('GET',)):
        def decorator(func):
            for m in methods:
                self.router.add(m, path, func)
            return func
        return decorator

    # Rendering API
    def render(self, template_name, context=None, status=200, headers=None):
        html_str = self.templates.render(template_name, context or {})
        return Response.html(html_str, status=status, headers=headers)

    # Error handling
    def errorhandler(self, code):
        def decorator(func):
            self._error_handlers[int(code)] = func
            return func
        return decorator

    def _default_404(self, request, exc=None):
        return Response.html("<h1>404 Not Found</h1>", status=404)

    def _default_405(self, request, allowed=None):
        resp = Response.html("<h1>405 Method Not Allowed</h1>", status=405)
        if allowed:
            resp.set_header('Allow', ', '.join(sorted(allowed)))
        return resp

    def _default_500(self, request, exc=None):
        if self.debug and exc is not None:
            tb = html.escape(''.join(traceback.format_exception(type(exc), exc, exc.__traceback__)))
            return Response.html(f"<h1>500 Internal Server Error</h1><pre>{tb}</pre>", status=500)
        return Response.html("<h1>500 Internal Server Error</h1>", status=500)

    # WSGI app callable
    def __call__(self, environ, start_response):
        request = Request(environ)
        try:
            handler, params, allowed = self.router.match(request.method, request.path)
            if handler is None:
                if allowed:
                    resp = self._error_handlers.get(405, self._default_405)(request, allowed=allowed)
                else:
                    resp = self._error_handlers.get(404, self._default_404)(request, None)
            else:
                result = handler(request, **params)
                resp = self._to_response(result)
        except Exception as exc:
            resp = self._error_handlers.get(500, self._default_500)(request, exc if self.debug else None)

        status, headers, body_iter = resp.to_wsgi()
        start_response(status, headers)
        return body_iter

    def _to_response(self, result):
        if isinstance(result, Response):
            return result
        if isinstance(result, (bytes, bytearray)):
            return Response(result)
        if isinstance(result, str):
            return Response.text(result)
        if isinstance(result, (dict, list)):
            return Response.json(result)
        # Fallback
        return Response.text(str(result))

    # Development server
    def run(self, host='127.0.0.1', port=8000):
        print(f"Serving on http://{host}:{port}")
        with make_server(host, port, self) as httpd:
            try:
                httpd.serve_forever()
            except KeyboardInterrupt:
                print("\nShutting down...")


# Example usage of the framework
app = MiniWeb(templates_dir='templates', debug=True)

@app.route('/', methods=('GET',))
def home(request):
    # Renders templates/home.html with context
    return app.render('home.html', {
        'title': 'MiniWeb',
        'message': 'Hello from MiniWeb!',
        'intro': 'This page was rendered with a tiny template engine.',
        'name': request.query.get('name', 'friend'),
        'encoded_query': urlencode(request.query),
        'helper': lambda: 'Computed value',
    })

@app.route('/hello', methods=('GET',))
def hello(request):
    name = request.query.get('name', 'world')
    return Response.text(f"Hello, {name}!")

@app.route('/users/<int:user_id>', methods=('GET',))
def user_detail(request, user_id):
    return Response.json({'user_id': user_id, 'info': 'A demo JSON response'})

@app.route('/echo', methods=('POST',))
def echo(request):
    ctype = (request.headers.get('content-type') or '').split(';', 1)[0].strip().lower()
    if ctype == 'application/json':
        data = request.json(default={})
        return Response.json({'you_posted_json': data})
    elif ctype == 'application/x-www-form-urlencoded':
        return Response.json({'you_posted_form': request.form})
    else:
        return Response.text(request.text or '(empty body)')

@app.route('/redirect', methods=('GET',))
def do_redirect(request):
    return Response.redirect('/')

# Optional custom error pages:
@app.errorhandler(404)
def not_found(request, exc=None):
    return Response.html('<h1>Custom 404</h1><p>Sorry, the page was not found.</p>', status=404)


if __name__ == '__main__':
    # Create a sample template if not present, for convenience.
    os.makedirs('templates', exist_ok=True)
    home_tpl = os.path.join('templates', 'home.html')
    if not os.path.exists(home_tpl):
        with open(home_tpl, 'w', encoding='utf-8') as f:
            f.write("""<!DOCTYPE html>
<html>
<head><meta charset="utf-8"><title>{{ title }}</title></head>
<body>
  <h1>{{ message }}</h1>
  <p>{{ intro }}</p>
  <p>Welcome, {{ name }}!</p>
  <p>Helper says: {{ helper }}</p>
  <p>Try routes:</p>
  <ul>
    <li><a href="/hello?name=MiniWeb">/hello?name=MiniWeb</a></li>
    <li><a href="/users/42">/users/42</a></li>
    <li><a href="/redirect">/redirect</a></li>
  </ul>
  <form method="post" action="/echo">
    <h3>POST form to /echo</h3>
    <input name="a" placeholder="a">
    <input name="b" placeholder="b">
    <button type="submit">Send</button>
  </form>
  <p>Rendered at: {{ helper }}</p>
</body>
</html>
""")
            print("Created templates/home.html sample template.")
    app.run()


# Notes
# - Routing: app.route('/path', methods=('GET','POST',...)) with path params like /users/<int:id>, /files/<path:rest>
# - Request: request.method, request.path, request.query, request.headers, request.cookies, request.form, request.json()
# - Responses: Response.text(...), Response.html(...), Response.json(...), Response.redirect(...)
# - Templates: {{ variable }} with dot lookups, escaping by default. Use |safe (e.g., {{ html|safe }}) to disable escaping.
# - This is intentionally minimal and not suitable for production.
------------------------------------------------------------

PROMPT 409:
"Generate Python code that imports the Apache DataSketches Library and creates a Sketch object to estimate the frequency of unique elements in a dataset."
RESPONSE:
# pip install datasketches

from datasketches import cpc_sketch

# Example dataset (could be any iterable of hashable items: strings, ints, etc.)
data = ["apple", "banana", "apple", "cherry", "banana", "date", "apple"]

# Create a CPC sketch to estimate the number of unique elements (cardinality)
sk = cpc_sketch(lg_k=12)  # lg_k controls accuracy vs. memory; 12 is a common default

# Stream items into the sketch
for item in data:
    sk.update(item)

# Query the sketch
estimate = sk.get_estimate()
lb = sk.get_lower_bound(1)  # 95% lower bound
ub = sk.get_upper_bound(1)  # 95% upper bound

print(f"Estimated unique elements: {estimate:.2f} (95% CI: [{lb:.2f}, {ub:.2f}])")
------------------------------------------------------------

PROMPT 410:
"Generate Python code that imports the logging module and creates a custom logger class with a low-boilerplate logging function for library method calls."
RESPONSE:
```python
import logging
import time
import functools
from typing import Any, Callable, Iterable, Optional, Sequence, TypeVar

T = TypeVar("T")


class LibLogger:
    """
    Minimal-boilerplate logger for library method calls.

    Usage:
        log = LibLogger(level=logging.DEBUG)

        # As a call wrapper (low boilerplate)
        import requests
        resp = log(requests.get, "https://example.com", timeout=2, headers={"Authorization": "Bearer abc"})

        # As a decorator for your own functions
        @log.calls(level=logging.INFO, log_result=True)
        def compute(x, y):
            return x + y

        result = compute(2, 3)
    """

    def __init__(
        self,
        name: str = "liblog",
        level: int = logging.INFO,
        handler: Optional[logging.Handler] = None,
        fmt: str = "%(asctime)s %(levelname)s %(name)s - %(message)s",
        datefmt: str = "%H:%M:%S",
        redact_keys: Sequence[str] = ("password", "token", "secret", "authorization", "api_key", "apikey"),
        arg_maxlen: int = 120,
    ) -> None:
        self.logger = logging.getLogger(name)
        self.default_level = level
        self.redact_keys = tuple(k.lower() for k in redact_keys)
        self.arg_maxlen = arg_maxlen

        if handler is None:
            handler = logging.StreamHandler()

        if not self.logger.handlers:
            handler.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))
            self.logger.addHandler(handler)

        self.logger.setLevel(level)

    # Allow instance to be used directly as a call wrapper:
    def __call__(self, func: Callable[..., T], /, *args: Any, **kwargs: Any) -> T:
        return self.call(func, *args, **kwargs)

    def set_level(self, level: int) -> None:
        self.default_level = level
        self.logger.setLevel(level)

    def calls(self, level: Optional[int] = None, log_result: bool = False) -> Callable[[Callable[..., T]], Callable[..., T]]:
        """
        Decorator that logs a function call and its duration.
        """
        def decorator(func: Callable[..., T]) -> Callable[..., T]:
            @functools.wraps(func)
            def wrapper(*args: Any, **kwargs: Any) -> T:
                return self.call(func, *args, level=level, log_result=log_result, **kwargs)
            return wrapper
        return decorator

    def call(
        self,
        func: Callable[..., T],
        /,
        *args: Any,
        level: Optional[int] = None,
        msg: Optional[str] = None,
        log_result: bool = False,
        reraise: bool = True,
        suppress: bool = False,
        redact: Optional[Iterable[str]] = None,
        **kwargs: Any,
    ) -> T:
        """
        Log a library function/method call with arguments, duration, and outcome.

        - func: callable to execute
        - level: logging level (defaults to instance default)
        - msg: optional prefix message
        - log_result: if True, include a short repr of the result
        - reraise: re-raise on exception (default True)
        - suppress: if True and exception occurs, suppress and return None
        - redact: extra keys to redact from kwargs (case-insensitive)
        """
        lvl = self.default_level if level is None else level
        call_name = self._qualname(func)
        call_args = self._format_args(args, kwargs, redact=redact)

        prefix = f"{msg} - " if msg else ""
        self.logger.log(lvl, "%sCALL %s%s", prefix, call_name, call_args)

        start = time.perf_counter()
        try:
            result = func(*args, **kwargs)
            dur = time.perf_counter() - start
            if log_result:
                result_repr = self._short_repr(result)
                self.logger.log(lvl, "%sOK   %s (%.3fs) -> %s", prefix, call_name, dur, result_repr)
            else:
                self.logger.log(lvl, "%sOK   %s (%.3fs)", prefix, call_name, dur)
            return result
        except Exception:
            dur = time.perf_counter() - start
            self.logger.exception("%sERR  %s (%.3fs)", prefix, call_name, dur)
            if reraise:
                raise
            if suppress:
                return None  # type: ignore[return-value]
            # If not re-raising and not suppressing, still return None for convenience
            return None  # type: ignore[return-value]

    # Helpers

    def _qualname(self, func: Callable[..., Any]) -> str:
        # Try to produce something like: module.Class.method or module.function
        mod = getattr(func, "__module__", None) or ""
        qn = getattr(func, "__qualname__", None) or getattr(func, "__name__", repr(func))
        if mod and mod != "__main__":
            return f"{mod}.{qn}"
        return qn

    def _short_repr(self, obj: Any) -> str:
        try:
            s = repr(obj)
        except Exception:
            s = f"<unrepr: {type(obj).__name__}>"
        if len(s) > self.arg_maxlen:
            return s[: self.arg_maxlen - 1] + "…"
        return s

    def _redact_mapping(self, mapping: Any, redact_extra: Optional[Iterable[str]]) -> Any:
        # Only redact top-level keys in dict-like objects
        try:
            items = mapping.items()
        except Exception:
            return mapping
        keys = set(self.redact_keys)
        if redact_extra:
            keys |= {k.lower() for k in redact_extra}
        redacted = {}
        for k, v in items:
            if isinstance(k, str) and k.lower() in keys:
                redacted[k] = "***"
            else:
                redacted[k] = v
        return redacted

    def _format_args(self, args: Sequence[Any], kwargs: dict, redact: Optional[Iterable[str]]) -> str:
        # Redact top-level sensitive kwargs
        if kwargs:
            safe_kwargs = self._redact_mapping(kwargs, redact)
        else:
            safe_kwargs = kwargs

        parts = []
        for a in args:
            parts.append(self._short_repr(a))
        for k, v in safe_kwargs.items():
            parts.append(f"{k}={self._short_repr(v)}")
        return f"({', '.join(parts)})" if parts else "()"


# Example direct run (remove or adapt for your application):
if __name__ == "__main__":
    log = LibLogger(level=logging.DEBUG)

    # Example using a standard library call
    log(len, [1, 2, 3, 4, 5], msg="Measure length", log_result=True)

    # Example decorator
    @log.calls(level=logging.INFO, log_result=True)
    def add(a, b):
        return a + b

    add(2, 3)
```
------------------------------------------------------------

PROMPT 411:
"Generate Python code that imports the Google Cloud Pub/Sub client library and creates a client object to interact with the Google Cloud Pub/Sub service."
RESPONSE:
from google.cloud import pubsub_v1

# Create Pub/Sub clients
publisher_client = pubsub_v1.PublisherClient()
subscriber_client = pubsub_v1.SubscriberClient()
------------------------------------------------------------

PROMPT 412:
"Generate Python code that creates pretty tables in the terminal using the `terminaltables` library, import it, and create a table with headers and data."
RESPONSE:
# pip install terminaltables

from terminaltables import SingleTable

def main():
    data = [
        ["Name", "Age", "City"],
        ["Alice", "29", "Berlin"],
        ["Bob", "34", "New York"],
        ["Charlie", "25", "London"],
    ]

    table = SingleTable(data, title="Team Members")
    table.inner_heading_row_border = True  # Divider under headers
    table.justify_columns[1] = "right"     # Right-align the Age column

    print(table.table)

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 413:
"Generate Python code that creates an HTML file with a JavaScript component that has an ActionScript 3-like interface, using apysc as the frontend library."
RESPONSE:
Here’s a minimal, ready-to-run apysc script that generates an HTML file and JavaScript with an ActionScript 3-like interface (Stage, Sprite, graphics, event listeners).

# pip install apysc
import apysc as ap

def on_rect_click(e: ap.MouseEvent, options: dict) -> None:
    rect: ap.Rectangle = e.this
    # Toggle fill color (AS3-like property access).
    if rect.fill_color == ap.Color("#00aaff"):
        rect.fill_color = ap.Color("#ff6a00")
    else:
        rect.fill_color = ap.Color("#00aaff")
    ap.trace("Rectangle clicked!")

def main() -> None:
    # Create Stage (AS3-like).
    stage: ap.Stage = ap.Stage(
        stage_width=640,
        stage_height=360,
        background_color=ap.Color("#222222"),
    )

    # Create a Sprite and draw a rectangle via graphics API (AS3-like).
    sprite: ap.Sprite = ap.Sprite()
    sprite.graphics.begin_fill(color=ap.Color("#00aaff"), alpha=0.9)
    sprite.graphics.line_style(color=ap.Color("#ffffff"), thickness=3)
    rect: ap.Rectangle = sprite.graphics.draw_rect(x=100, y=100, width=220, height=120)

    # Add an AS3-like click listener.
    rect.click(on_rect_click)

    # Output HTML + JS to the specified directory.
    ap.save_overall_html(dest_dir_path="apysc_as3_demo", minify=False)

if __name__ == "__main__":
    main()

How to use:
- Install: pip install apysc
- Run this script. It will create apysc_as3_demo/index.html and a JS bundle.
- Open the generated index.html in a browser. Click the rectangle to see the color toggle and a console trace message.
------------------------------------------------------------

PROMPT 414:
"Generate Python code that creates an open table format for huge analytic datasets using Apache Iceberg, allowing for efficient querying and data analysis."
RESPONSE:
Below is a single Python script that uses PySpark with Apache Iceberg to create a scalable open table, write large batches efficiently, run analytical queries (including time travel), and perform maintenance/optimization tasks for performance.

Note: You need Java and PySpark installed. Spark will auto-fetch the Iceberg runtime JAR via spark.jars.packages. Adjust versions/warehouse/catalog as needed.

#!/usr/bin/env python3
import os
from pyspark.sql import SparkSession, functions as F

# ------------------------------------------------------------------------------
# Configuration
# ------------------------------------------------------------------------------
SPARK_VERSION_SUFFIX = "3.5"         # Match your Spark major.minor (3.3, 3.4, 3.5, ...)
ICEBERG_VERSION = "1.6.1"            # Iceberg runtime version
SCALA_BINARY = "2.12"                # Spark 3.x uses Scala 2.12

CATALOG = "ice"                      # Spark catalog name
NAMESPACE = "analytics"              # Logical database/namespace
TABLE = "orders"                     # Table name
FQTN = f"{CATALOG}.{NAMESPACE}.{TABLE}"

# Warehouse location (use an S3 path like s3a://bucket/path for cloud)
WAREHOUSE = "file:///tmp/iceberg_warehouse"

# Synthetic data generation
NUM_BATCHES = 5
ROWS_PER_BATCH = 2_000_000           # Increase for larger datasets
TARGET_FILE_SIZE_BYTES = 128 * 1024 * 1024  # 128MB

# ------------------------------------------------------------------------------
# Spark session with Iceberg
# ------------------------------------------------------------------------------
def build_spark():
    iceberg_pkg = f"org.apache.iceberg:iceberg-spark-runtime-{SPARK_VERSION_SUFFIX}_{SCALA_BINARY}:{ICEBERG_VERSION}"
    spark = (
        SparkSession.builder
        .appName("Iceberg-Lakehouse-Demo")
        .config("spark.jars.packages", iceberg_pkg)
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
        .config(f"spark.sql.catalog.{CATALOG}", "org.apache.iceberg.spark.SparkCatalog")
        .config(f"spark.sql.catalog.{CATALOG}.type", "hadoop")              # For simple local demo; prod often uses 'hive' or 'rest'
        .config(f"spark.sql.catalog.{CATALOG}.warehouse", WAREHOUSE)
        # Performance/format defaults
        .config("spark.sql.parquet.enableVectorizedReader", "true")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.files.maxPartitionBytes", TARGET_FILE_SIZE_BYTES)
        .getOrCreate()
    )
    return spark

# ------------------------------------------------------------------------------
# Create namespace and table with partitioning and table properties
# ------------------------------------------------------------------------------
def create_table(spark):
    spark.sql(f"CREATE NAMESPACE IF NOT EXISTS {CATALOG}.{NAMESPACE}")

    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {FQTN} (
          order_id BIGINT,
          customer_id BIGINT,
          order_ts TIMESTAMP,
          country STRING,
          item STRING,
          price DECIMAL(12,2),
          quantity INT
        )
        USING ICEBERG
        PARTITIONED BY (
          days(order_ts),
          bucket(16, customer_id),
          truncate(3, country)
        )
        TBLPROPERTIES (
          'format-version'='2',
          'write.format.default'='parquet',
          'parquet.compression-codec'='zstd',
          'write.target-file-size-bytes'='{TARGET_FILE_SIZE_BYTES}',
          'write.distribution-mode'='hash'
        )
    """)

    # Optional: define write ordering to improve clustering for queries
    spark.sql(f"ALTER TABLE {FQTN} WRITE ORDERED BY (order_ts DESC, customer_id ASC)")

# ------------------------------------------------------------------------------
# Generate and write synthetic data in batches
# ------------------------------------------------------------------------------
def write_batches(spark):
    countries = ["US","CA","DE","FR","BR","IN","CN","AU","ZA","MX"]
    items = ["laptop","phone","tablet","headphones","camera","monitor","keyboard","mouse"]

    for b in range(NUM_BATCHES):
        # Create a DataFrame with random but skew-aware distributions
        df = (
            spark.range(ROWS_PER_BATCH)
            .select(
                (F.lit(b) * F.lit(ROWS_PER_BATCH) + F.col("id")).alias("order_id"),
                (F.floor(F.rand(seed=b) * F.lit(1_000_000))).cast("bigint").alias("customer_id"),
                # Random timestamp within last 365 days
                F.expr(f"timestampadd(DAY, -cast(rand({b})*365 as int), current_timestamp())").alias("order_ts"),
                # Weighted random country choice
                F.expr("element_at(array('US','US','US','CA','DE','FR','BR','IN','CN','AU','ZA','MX'), 1 + cast(rand()*12 as int))").alias("country"),
                F.expr("element_at(array('laptop','phone','tablet','headphones','camera','monitor','keyboard','mouse'), 1 + cast(rand()*8 as int))").alias("item"),
                (F.round(F.rand(seed=b+1) * F.lit(1000.0), 2)).cast("decimal(12,2)").alias("price"),
                (F.floor(F.rand(seed=b+2) * F.lit(10)) + 1).cast("int").alias("quantity"),
            )
        )

        # Efficient Iceberg append using DataFrameWriterV2
        df.writeTo(FQTN).append()
        print(f"Wrote batch {b+1}/{NUM_BATCHES}")

# ------------------------------------------------------------------------------
# Example analytical queries (including time travel)
# ------------------------------------------------------------------------------
def run_queries(spark):
    # Simple row count
    spark.sql(f"SELECT count(*) AS rows FROM {FQTN}").show(truncate=False)

    # Filter + aggregation with partition pruning and predicate pushdown
    spark.sql(f"""
        SELECT country,
               approx_count_distinct(customer_id) AS unique_customers,
               sum(price * quantity) AS gross_revenue
        FROM {FQTN}
        WHERE country IN ('US','DE','IN')
          AND order_ts >= date_sub(current_date(), 30)
        GROUP BY country
        ORDER BY gross_revenue DESC
    """).show(truncate=False)

    # Time travel: find an earlier snapshot and query it
    snaps = spark.sql(f"SELECT snapshot_id, committed_at FROM {FQTN}.snapshots ORDER BY committed_at").collect()
    if len(snaps) > 1:
        old_snapshot = snaps[0]["snapshot_id"]
        spark.sql(f"SELECT count(*) AS rows_old FROM {FQTN} VERSION AS OF {old_snapshot}").show(truncate=False)

# ------------------------------------------------------------------------------
# Optimize/maintenance: compact small files, rewrite manifests, expire snapshots
# ------------------------------------------------------------------------------
def optimize_table(spark):
    # Compact small files to target file size
    # Syntax variants exist across Iceberg versions; try the "table =>" form:
    spark.sql(f"""
        CALL {CATALOG}.system.rewrite_data_files(
          table => '{NAMESPACE}.{TABLE}',
          options => map(
            'min-input-files', '8',
            'target-file-size-bytes', '{TARGET_FILE_SIZE_BYTES}'
          )
        )
    """)

    # Rewrite manifests to speed up planning
    spark.sql(f"CALL {CATALOG}.system.rewrite_manifests(table => '{NAMESPACE}.{TABLE}')")

    # Expire old snapshots and remove orphan files to control metadata bloat and storage
    spark.sql(f"""
        CALL {CATALOG}.system.expire_snapshots(
          table => '{NAMESPACE}.{TABLE}',
          older_than => (current_timestamp() - INTERVAL 7 DAYS),
          retain_last => 2
        )
    """)
    spark.sql(f"""
        CALL {CATALOG}.system.remove_orphan_files(
          table => '{NAMESPACE}.{TABLE}',
          older_than => (current_timestamp() - INTERVAL 7 DAYS)
        )
    """)

# ------------------------------------------------------------------------------
# Main
# ------------------------------------------------------------------------------
if __name__ == "__main__":
    spark = build_spark()
    try:
        create_table(spark)
        write_batches(spark)
        run_queries(spark)
        optimize_table(spark)
    finally:
        spark.stop()

How to adapt for production:
- Catalog: Prefer Hive or REST/Nessie catalogs for multi-engine access and concurrency guarantees.
- Storage: Point WAREHOUSE to cloud object storage (e.g., s3a://bucket/warehouse) and configure Spark/Hadoop credentials.
- Engines: The same Iceberg tables can be queried by Spark, Trino, Presto, Flink, Dremio, DuckDB (read), etc.
- Tuning: Adjust partitioning, write.order, file sizes, compression, and optimization cadence to your workload.
------------------------------------------------------------

PROMPT 415:
"Generate Python code that imports the sip module and uses it to create a PyQt6 application with the same functionality as the original package description."
RESPONSE:
Could you share the “original package description” you want replicated? Without it, here is a minimal PyQt6 application skeleton that imports and uses the sip module; you can drop in the actual UI/logic to match your description.

import sys
import sip
from PyQt6.QtWidgets import (
    QApplication,
    QMainWindow,
    QTextEdit,
    QToolBar,
    QStatusBar,
    QAction,
)
from PyQt6.QtCore import Qt


class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("PyQt6 App (uses sip)")

        # Central widget (replace with widgets/logic matching your package description)
        self.editor = QTextEdit(self)
        self.editor.setReadOnly(True)
        self.editor.setText(
            "Placeholder UI\n\nReplace this with the UI and behavior that matches the original package description."
        )
        self.setCentralWidget(self.editor)

        # Simple toolbar
        toolbar = QToolBar("Main Toolbar", self)
        toolbar.setMovable(False)
        self.addToolBar(Qt.ToolBarArea.TopToolBarArea, toolbar)

        quit_action = QAction("Quit", self)
        quit_action.setShortcut("Ctrl+Q")
        quit_action.triggered.connect(self.close)
        toolbar.addAction(quit_action)

        # Status bar showing sip usage
        status = QStatusBar(self)
        self.setStatusBar(status)
        sip_version = getattr(sip, "__version__", "unknown")
        status.showMessage(f"sip version: {sip_version}")

        # Example of using sip in code (introspection / diagnostics)
        # Here we just read an attribute to "use" sip explicitly.
        # You can expand this if you actually need sip-specific behavior.
        self._sip_info = {
            "module": sip.__name__,
            "version": sip_version,
        }


def main():
    app = QApplication(sys.argv)
    window = MainWindow()
    window.resize(800, 600)
    window.show()
    sys.exit(app.exec())


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 416:
"Generate Python code that imports the necessary modules and creates a simple typing stub for the popular Python library \"tqdm\" using the \"typing\" module."
RESPONSE:
from __future__ import annotations

from typing import (
    Any,
    ContextManager,
    Generic,
    IO,
    Iterable,
    Iterator,
    Mapping,
    Optional,
    TypeVar,
    overload,
)

T = TypeVar("T")


class Tqdm(Generic[T], Iterator[T], ContextManager["Tqdm[Any]"]):
    def __init__(
        self,
        iterable: Optional[Iterable[T]] = ...,
        *,
        total: Optional[int] = ...,
        desc: Optional[str] = ...,
        leave: Optional[bool] = ...,
        file: Optional[IO[str]] = ...,
        ncols: Optional[int] = ...,
        unit: str = ...,
        initial: int = ...,
        disable: Optional[bool] = ...,
        dynamic_ncols: Optional[bool] = ...,
    ) -> None: ...
    def __iter__(self) -> Iterator[T]: ...
    def __next__(self) -> T: ...
    def update(self, n: int = ...) -> None: ...
    def close(self) -> None: ...
    def set_description(self, desc: Optional[str] = ..., refresh: bool = ...) -> None: ...
    def set_postfix(
        self,
        ordered_dict: Optional[Mapping[str, Any]] = ...,
        refresh: bool = ...,
        **kwargs: Any,
    ) -> None: ...
    def reset(self, total: Optional[int] = ...) -> None: ...
    def clear(self, nomove: bool = ...) -> None: ...
    def __enter__(self) -> "Tqdm[Any]": ...
    def __exit__(self, exc_type: Any, exc: Any, tb: Any) -> None: ...


# Public class name as exposed by the tqdm package (e.g., from tqdm import tqdm)
class tqdm(Tqdm[T], Generic[T]): ...


@overload
def trange(
    n: int,
    *,
    desc: Optional[str] = ...,
    leave: Optional[bool] = ...,
    file: Optional[IO[str]] = ...,
    ncols: Optional[int] = ...,
    unit: str = ...,
    initial: int = ...,
    disable: Optional[bool] = ...,
    dynamic_ncols: Optional[bool] = ...,
) -> Iterator[int]: ...
@overload
def trange(
    start: int,
    stop: int,
    step: int = ...,
    *,
    desc: Optional[str] = ...,
    leave: Optional[bool] = ...,
    file: Optional[IO[str]] = ...,
    ncols: Optional[int] = ...,
    unit: str = ...,
    initial: int = ...,
    disable: Optional[bool] = ...,
    dynamic_ncols: Optional[bool] = ...,
) -> Iterator[int]: ...
def trange(*args: Any, **kwargs: Any) -> Iterator[int]: ...


def write(s: str, file: Optional[IO[str]] = ..., end: str = ...) -> None: ...


__all__ = ["tqdm", "trange", "Tqdm", "write"]
------------------------------------------------------------

PROMPT 417:
"Generate Python code that imports the necessary libraries and creates a Google Cloud Functions client object to interact with the Google Cloud Functions API."
RESPONSE:
# First-gen (Cloud Functions v1)
from google.cloud import functions_v1

client_v1 = functions_v1.CloudFunctionsServiceClient()


# Second-gen (Cloud Functions v2)
from google.cloud import functions_v2

client_v2 = functions_v2.FunctionServiceClient()
------------------------------------------------------------

PROMPT 418:
"Generate Python code that creates a composable command line interface toolkit using the `click` library and the `asyncio` module for asynchronous functionality."
RESPONSE:
```python
#!/usr/bin/env python3
"""
Composable async CLI toolkit built on Click + asyncio.

This module provides:
- AsyncCommand/AsyncGroup that transparently run async callbacks.
- Shared option utilities for composable, reusable options.
- Concurrency helpers (gather_limited).
- Async subprocess runner with cancellation/timeout.
- Simple async progress utilities.
- Lightweight plugin loader for mounting external commands/groups.

At the bottom, an example CLI demonstrates usage.
"""

from __future__ import annotations

import asyncio
import contextlib
import dataclasses
import importlib
import inspect
import json
import logging
import os
import signal
import sys
import time
from dataclasses import dataclass
from typing import (
    Any,
    AsyncGenerator,
    AsyncIterable,
    Awaitable,
    Callable,
    Iterable,
    List,
    Optional,
    Sequence,
    Tuple,
    Union,
)

import click


# -------------------------
# Core: Async Click classes
# -------------------------

class AsyncCommand(click.Command):
    """
    A Click Command that can run async callbacks via asyncio.run.
    Synchronous callbacks still work as usual.
    """

    def invoke(self, ctx: click.Context) -> Any:
        # Ensure Click's injection (pass_context/obj/etc.) is respected by calling ctx.invoke
        rv = ctx.invoke(self.callback, **ctx.params) if self.callback is not None else None
        if inspect.isawaitable(rv):
            try:
                return asyncio.run(rv)
            except KeyboardInterrupt:
                raise click.Abort()
        return rv


class AsyncGroup(click.Group):
    """
    A Click Group that defaults to AsyncCommand and AsyncGroup for nested commands.
    """

    command_class = AsyncCommand

    def command(self, *args, **kwargs):
        kwargs.setdefault("cls", self.command_class)
        return super().command(*args, **kwargs)

    def group(self, *args, **kwargs):
        kwargs.setdefault("cls", type(self))
        return super().group(*args, **kwargs)


# ---------------------------------
# Shared state, options, and helpers
# ---------------------------------

@dataclass
class AppState:
    verbose: int = 0
    json_output: bool = False
    start_time: float = dataclasses.field(default_factory=time.time)

    def logger(self, name: str = "cli") -> logging.Logger:
        level = logging.WARNING
        if self.verbose == 1:
            level = logging.INFO
        elif self.verbose >= 2:
            level = logging.DEBUG
        logging.basicConfig(
            level=level,
            format="%(asctime)s %(levelname)s %(name)s: %(message)s",
            datefmt="%H:%M:%S",
        )
        return logging.getLogger(name)


pass_state = click.make_pass_decorator(AppState, ensure=True)


def shared_options(*options: Callable) -> Callable:
    """
    Compose multiple Click option/argument decorators into one reusable decorator.
    Usage:
        common = shared_options(opt1, opt2)
        @common
        def cmd(...): ...
    """
    def _apply(func: Callable) -> Callable:
        for opt in reversed(options):
            func = opt(func)
        return func
    return _apply


# Reusable options
verbose_option = click.option(
    "-v",
    "--verbose",
    count=True,
    help="Increase verbosity (-v, -vv).",
)

json_option = click.option(
    "--json/--no-json",
    "json_output",
    default=False,
    show_default=True,
    help="Output JSON instead of human-readable text.",
)

concurrency_option = click.option(
    "-j",
    "--jobs",
    "concurrency",
    type=click.IntRange(1, 1000),
    default=5,
    show_default=True,
    help="Max concurrent tasks.",
)

timeout_option = click.option(
    "-t",
    "--timeout",
    type=float,
    default=None,
    help="Optional timeout (seconds).",
)


def print_result(data: Any, state: AppState) -> None:
    """
    Print either JSON or pretty text depending on state.json_output.
    """
    if state.json_output:
        def default(o):
            if dataclasses.is_dataclass(o):
                return dataclasses.asdict(o)
            if isinstance(o, (set, frozenset)):
                return list(o)
            return repr(o)
        click.echo(json.dumps(data, indent=2, default=default))
    else:
        # Simple pretty print
        if dataclasses.is_dataclass(data):
            data = dataclasses.asdict(data)
        click.echo(str(data))


# --------------------------
# Async concurrency utilities
# --------------------------

async def gather_limited(
    limit: int,
    coros_or_funcs: Iterable[Union[Awaitable, Callable[[], Awaitable]]],
    return_exceptions: bool = False,
):
    """
    Gather coroutines with a concurrency limit.

    coros_or_funcs may be coroutines or 0-arg callables returning coroutines.
    """
    sem = asyncio.Semaphore(limit)

    async def run_one(item):
        coro = item if inspect.isawaitable(item) else item()
        async with sem:
            return await coro

    tasks = [asyncio.create_task(run_one(c)) for c in coros_or_funcs]
    try:
        return await asyncio.gather(*tasks, return_exceptions=return_exceptions)
    finally:
        # If cancelled/errored, ensure we cancel pending tasks
        for t in tasks:
            if not t.done():
                t.cancel()
        with contextlib.suppress(asyncio.CancelledError):
            await asyncio.gather(*tasks, return_exceptions=True)


# -------------------------
# Async subprocess execution
# -------------------------

@dataclass
class Completed:
    cmd: Union[str, Sequence[str]]
    returncode: int
    stdout: str
    stderr: str
    duration: float


async def run_process(
    cmd: Union[str, Sequence[str]],
    *,
    shell: bool = False,
    env: Optional[dict[str, str]] = None,
    cwd: Optional[str] = None,
    input: Optional[Union[str, bytes]] = None,
    timeout: Optional[float] = None,
    text: bool = True,
) -> Completed:
    """
    Run a subprocess asynchronously with optional timeout and capture stdout/stderr.

    If text=True, input is encoded/outputs are decoded as UTF-8.
    """
    if text and isinstance(input, str):
        input_bytes = input.encode("utf-8")
    else:
        input_bytes = input if isinstance(input, (bytes, type(None))) else None

    start = time.time()
    if shell and isinstance(cmd, str):
        proc = await asyncio.create_subprocess_shell(
            cmd,
            env=env,
            cwd=cwd,
            stdin=asyncio.subprocess.PIPE if input is not None else None,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
    else:
        if isinstance(cmd, str):
            raise TypeError("cmd must be a sequence when shell=False")
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            env=env,
            cwd=cwd,
            stdin=asyncio.subprocess.PIPE if input is not None else None,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

    try:
        out_b, err_b = await asyncio.wait_for(proc.communicate(input_bytes), timeout=timeout)
    except asyncio.TimeoutError:
        with contextlib.suppress(ProcessLookupError):
            proc.kill()
        raise
    finally:
        duration = time.time() - start

    if text:
        out = out_b.decode("utf-8", errors="replace") if out_b is not None else ""
        err = err_b.decode("utf-8", errors="replace") if err_b is not None else ""
    else:
        out, err = out_b, err_b  # type: ignore[assignment]

    return Completed(
        cmd=cmd,
        returncode=proc.returncode,
        stdout=out,
        stderr=err,
        duration=duration,
    )


# -------------------------
# Async progress utilities
# -------------------------

async def async_range(n: int, delay: float = 0.05) -> AsyncGenerator[int, None]:
    for i in range(n):
        await asyncio.sleep(delay)
        yield i


async def with_progress(
    iterable: AsyncIterable[Any],
    *,
    length: Optional[int],
    label: str = "",
) -> AsyncGenerator[Any, None]:
    """
    Wrap an async iterable with a Click progress bar. Requires a known length.
    """
    if length is None:
        # No progress bar without length; just pass through
        async for item in iterable:
            yield item
        return

    with click.progressbar(length=length, label=label) as bar:
        async for item in iterable:
            bar.update(1)
            yield item


# -------------------------
# Plugin loading/composition
# -------------------------

def load_plugins(target_group: click.Group, env_var: str = "CLI_PLUGINS") -> None:
    """
    Load plugin modules listed in CLI_PLUGINS (comma-separated).
    Each plugin module can expose:
      - register(cli): a function called with the target group, or
      - top-level click.Command or click.Group objects to be attached.
    """
    value = os.getenv(env_var, "").strip()
    if not value:
        return

    modules = [m.strip() for m in value.split(",") if m.strip()]
    for modname in modules:
        try:
            mod = importlib.import_module(modname)
        except Exception as e:
            click.echo(f"Failed to import plugin '{modname}': {e}", err=True)
            continue

        reg = getattr(mod, "register", None)
        if callable(reg):
            try:
                reg(target_group)
            except Exception as e:
                click.echo(f"Plugin '{modname}' register() failed: {e}", err=True)
            continue

        # Otherwise, attach any top-level click commands/groups
        for name, obj in vars(mod).items():
            if isinstance(obj, click.Command):
                target_group.add_command(obj)


# -------------------------
# Example CLI using toolkit
# -------------------------

@click.group(cls=AsyncGroup)
@verbose_option
@json_option
@click.pass_context
def cli(ctx: click.Context, verbose: int, json_output: bool):
    """
    Example CLI using the composable async toolkit.
    """
    ctx.obj = AppState(verbose=verbose, json_output=json_output)
    # Load any plugins if configured
    load_plugins(ctx.command)  # type: ignore[arg-type]


@cli.command(help="Run multiple sleeps concurrently.")
@concurrency_option
@click.argument("durations", type=float, nargs=-1, required=True)
@pass_state
async def sleep_many(state: AppState, concurrency: int, durations: Tuple[float, ...]):
    log = state.logger("sleep_many")
    log.debug(f"Starting {len(durations)} sleeps, concurrency={concurrency}")

    async def do_sleep(d: float) -> dict:
        t0 = time.time()
        await asyncio.sleep(d)
        return {"slept": d, "elapsed": round(time.time() - t0, 3)}

    results = await gather_limited(
        concurrency,
        (lambda d=d: do_sleep(d) for d in durations),
    )

    # Sort by input order
    print_result(results, state)


@cli.command(help="Run shell commands concurrently.")
@concurrency_option
@timeout_option
@click.option(
    "-c",
    "--cmd",
    "commands",
    multiple=True,
    required=True,
    help="Shell command to run. Use multiple -c for multiple commands.",
)
@pass_state
async def run_commands(
    state: AppState,
    concurrency: int,
    timeout: Optional[float],
    commands: Tuple[str, ...],
):
    log = state.logger("run_commands")
    log.info(f"Running {len(commands)} commands with concurrency={concurrency}")

    async def run_one(cmd: str) -> dict:
        try:
            res = await run_process(cmd, shell=True, timeout=timeout)
            return {
                "cmd": cmd,
                "returncode": res.returncode,
                "stdout": res.stdout.strip(),
                "stderr": res.stderr.strip(),
                "duration": round(res.duration, 3),
            }
        except asyncio.TimeoutError:
            return {"cmd": cmd, "error": "timeout", "timeout": timeout}

    results = await gather_limited(concurrency, (lambda c=c: run_one(c) for c in commands))
    print_result(results, state)

    # If any failed, set a non-zero exit code
    if any(r.get("returncode", 0) != 0 for r in results if isinstance(r, dict)):
        raise click.exceptions.Exit(1)


@cli.command(help="Demonstrate async progress bar.")
@click.option("--steps", type=int, default=50, show_default=True)
@click.option("--delay", type=float, default=0.03, show_default=True)
@pass_state
async def progress_demo(state: AppState, steps: int, delay: float):
    items = async_range(steps, delay=delay)
    idx = -1
    async for idx in with_progress(items, length=steps, label="Working"):
        pass
    print_result({"completed": idx + 1}, state)


@cli.group(help="A composable subgroup example.", cls=AsyncGroup)
def tools():
    pass


@tools.command(help="Echo arguments after a tiny async wait.")
@click.argument("words", nargs=-1, required=True)
@click.option("--upper/--no-upper", default=False, show_default=True)
@pass_state
async def echo(state: AppState, words: Tuple[str, ...], upper: bool):
    await asyncio.sleep(0.05)
    out = " ".join(words)
    if upper:
        out = out.upper()
    print_result({"echo": out}, state)


# -----------
# Entry point
# -----------

def main():
    # Set up friendly Ctrl+C behavior for sync parts of Click
    if hasattr(signal, "SIGINT"):
        signal.signal(signal.SIGINT, signal.default_int_handler)
    cli(prog_name=os.getenv("CLI_NAME", "aclix"))


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 419:
"Generate Python code that implements a simple JavaScript interpreter using the `ast` module to parse JavaScript code and execute it using the `exec()` function."
RESPONSE:
Short answer: Python’s ast module only parses Python, not JavaScript. So you can’t directly parse JS with ast or execute it with exec. Below is a safe, toy “interpreter” that handles a very small subset of JavaScript by first transpiling it to restricted Python, then parsing that Python with ast, validating it, and executing it with exec in a sandbox. It supports simple statements like variable assignments, arithmetic, comparisons, boolean ops, and console.log. It does not support blocks, braces, functions, or complex JS features.

Use a real JS engine (quickjs, PyMiniRacer/V8, js2py, or Node via subprocess) for anything beyond trivial demos.

Code:
import ast
import io
import re
import sys
from typing import Dict, Any, Tuple, List


class UnsafeJavaScriptFeature(Exception):
    pass


class SimpleJSTranspiler:
    """
    Extremely small JavaScript-to-Python transpiler.

    Supported:
      - let/var/const x = ...
      - Expressions with + - * / % **, parentheses
      - Booleans true/false, null -> None
      - Comparisons ==, !=, <, <=, >, >= and also converts === -> ==, !== -> !=
      - Logical operators && -> and, || -> or, ! -> not
      - console.log(expr)
      - Statement separators via newline or semicolon

    Not supported:
      - Braces, blocks, functions, classes, arrays/objects, attribute access, subscripts, imports, etc.

    The output is Python code that is then validated via ast for safety and executed.
    """

    _line_comment_re = re.compile(r"//.*?$", re.MULTILINE)
    _block_comment_re = re.compile(r"/\*.*?\*/", re.DOTALL)

    def remove_comments(self, code: str) -> str:
        code = re.sub(self._block_comment_re, "", code)
        code = re.sub(self._line_comment_re, "", code)
        return code

    def normalize_whitespace(self, code: str) -> str:
        # Convert semicolons into newlines (preserving line structure)
        lines: List[str] = []
        for raw_line in code.splitlines():
            # Split by semicolons but keep simple strings intact (very naive)
            parts = [p for p in raw_line.split(";")]
            for p in parts:
                if p.strip():
                    lines.append(p)
            # blank lines for semicolons at end are ignored
        return "\n".join(lines)

    def basic_rewrites(self, code: str) -> str:
        # Disallow braces to keep this toy safe/simple
        if "{" in code or "}" in code:
            raise UnsafeJavaScriptFeature("Blocks/braces are not supported in this toy interpreter.")

        # Strict equals to Python equals
        code = re.sub(r"!==", "!=", code)
        code = re.sub(r"===", "==", code)

        # Logical ops
        code = re.sub(r"\b&&\b", " and ", code)
        code = re.sub(r"\b\|\|\b", " or ", code)
        # Replace '!' where it's not part of '!='
        code = re.sub(r"!(?!=)", " not ", code)

        # Declarations
        code = re.sub(r"\b(var|let|const)\s+", "", code)

        # Literals
        code = re.sub(r"\btrue\b", "True", code, flags=re.IGNORECASE)
        code = re.sub(r"\bfalse\b", "False", code, flags=re.IGNORECASE)
        code = re.sub(r"\bnull\b", "None", code, flags=re.IGNORECASE)

        # console.log -> print
        code = re.sub(r"\bconsole\s*\.\s*log\s*\(", "print(", code)

        # Trim trailing spaces
        code = "\n".join(line.rstrip() for line in code.splitlines())
        return code

    def translate(self, js_code: str) -> str:
        js_code = self.remove_comments(js_code)
        js_code = self.normalize_whitespace(js_code)
        py_code = self.basic_rewrites(js_code)
        # Strip empty lines
        py_code = "\n".join(line for line in py_code.splitlines() if line.strip())
        return py_code


class SafePythonValidator(ast.NodeVisitor):
    """
    Very conservative AST validator that allows only a small, safe subset of Python
    corresponding to our tiny JS subset.

    Disallows:
      - Imports, attributes, subscripts, comprehensions, lambdas, with/try, class/def, etc.
      - Calls to anything other than allowlisted names (e.g., print)
    """

    allowed_calls = {"print"}
    allowed_binops = (ast.Add, ast.Sub, ast.Mult, ast.Div, ast.FloorDiv, ast.Mod, ast.Pow)
    allowed_unaryops = (ast.USub, ast.UAdd, ast.Not)
    allowed_boolops = (ast.And, ast.Or)
    allowed_cmps = (ast.Eq, ast.NotEq, ast.Lt, ast.LtE, ast.Gt, ast.GtE)

    def generic_visit(self, node):
        # Disallow many node types outright
        forbidden = (
            ast.Import, ast.ImportFrom, ast.Attribute, ast.Subscript, ast.Lambda,
            ast.Dict, ast.Set, ast.ListComp, ast.SetComp, ast.DictComp, ast.GeneratorExp,
            ast.With, ast.Try, ast.Await, ast.AsyncFunctionDef, ast.Yield, ast.YieldFrom,
            ast.Raise, ast.ClassDef, ast.FunctionDef, ast.For, ast.While, ast.If, ast.Match,
            ast.Global, ast.Nonlocal, ast.Delete, ast.NamedExpr
        )
        if isinstance(node, forbidden):
            raise UnsafeJavaScriptFeature(f"Forbidden Python construct: {type(node).__name__}")

        # Validate allowed constructs
        if isinstance(node, ast.Call):
            if not isinstance(node.func, ast.Name) or node.func.id not in self.allowed_calls:
                raise UnsafeJavaScriptFeature("Only calls to print(...) are allowed.")
        elif isinstance(node, ast.Assign):
            # Only simple names on the left-hand side
            for tgt in node.targets:
                if not isinstance(tgt, ast.Name):
                    raise UnsafeJavaScriptFeature("Only simple variable assignments are allowed.")
        elif isinstance(node, ast.AugAssign):
            if not isinstance(node.target, ast.Name):
                raise UnsafeJavaScriptFeature("Only simple variable assignments are allowed.")
        elif isinstance(node, ast.BinOp):
            if not isinstance(node.op, self.allowed_binops):
                raise UnsafeJavaScriptFeature("Disallowed binary operator.")
        elif isinstance(node, ast.UnaryOp):
            if not isinstance(node.op, self.allowed_unaryops):
                raise UnsafeJavaScriptFeature("Disallowed unary operator.")
        elif isinstance(node, ast.BoolOp):
            if not isinstance(node.op, self.allowed_boolops):
                raise UnsafeJavaScriptFeature("Disallowed boolean operator.")
        elif isinstance(node, ast.Compare):
            for op in node.ops:
                if not isinstance(op, self.allowed_cmps):
                    raise UnsafeJavaScriptFeature("Disallowed comparison operator.")
        elif isinstance(node, ast.Name):
            # Disallow dunder or suspicious names
            if node.id.startswith("__"):
                raise UnsafeJavaScriptFeature("Suspicious identifier.")
        elif isinstance(node, ast.Attribute):
            # Already forbidden above, but just in case
            raise UnsafeJavaScriptFeature("Attribute access is not allowed.")

        super().generic_visit(node)


class SimpleJSInterpreter:
    def __init__(self):
        self.transpiler = SimpleJSTranspiler()
        # A very restricted set of builtins to reduce risk
        self.safe_builtins: Dict[str, Any] = {
            "print": print,
            "abs": abs,
            "min": min,
            "max": max,
            "round": round,
        }

    def parse_and_validate(self, py_code: str) -> ast.AST:
        try:
            tree = ast.parse(py_code, mode="exec")
        except SyntaxError as e:
            raise UnsafeJavaScriptFeature(f"Transpiled code is not valid Python: {e}") from e
        SafePythonValidator().visit(tree)
        return tree

    def run(self, js_code: str) -> Tuple[str, Dict[str, Any]]:
        """
        Runs tiny-subset JS code. Returns (captured_stdout, globals_dict).
        """
        py_code = self.transpiler.translate(js_code)
        tree = self.parse_and_validate(py_code)

        # Sandbox environment
        env: Dict[str, Any] = {"__builtins__": self.safe_builtins}

        # Capture stdout from print
        buf = io.StringIO()
        old_stdout = sys.stdout
        try:
            sys.stdout = buf
            compiled = compile(tree, filename="<js>", mode="exec")
            exec(compiled, env, env)
        finally:
            sys.stdout = old_stdout

        return buf.getvalue(), {k: v for k, v in env.items() if k not in ("__builtins__")}


if __name__ == "__main__":
    js = """
        // Simple demo
        let x = 3 + 4 * 2;
        const y = x === 11;
        var z = y && !false || false;
        console.log("x =", x);
        console.log("y =", y);
        console.log("z =", z);
        x = x + 1;
        console.log("x2 =", x);
    """

    interp = SimpleJSInterpreter()
    try:
        out, state = interp.run(js)
        print("Output from console.log():")
        print(out)
        print("Final variables:", {k: v for k, v in state.items() if not k.startswith("_")})
    except UnsafeJavaScriptFeature as e:
        print("Rejected for safety/unsupported feature:", e)

Notes:
- This is not a real JS interpreter and is intentionally limited.
- For genuine JavaScript execution from Python, consider:
  - quickjs: pip install quickjs
  - PyMiniRacer (V8): pip install py-mini-racer
  - js2py: pip install js2py
  - Node.js via subprocess for full JS support.
------------------------------------------------------------

PROMPT 420:
"Generate Python code that imports the requests library and creates a session object to send HTTP requests with customizable headers, authentication, and cookies."
RESPONSE:
import requests

def create_session(headers=None, auth=None, cookies=None):
    """
    Create and configure a requests.Session with optional default headers, auth, and cookies.
    - headers: dict of default headers to send with every request
    - auth: tuple (username, password) or requests.auth.AuthBase for per-session auth
    - cookies: dict of cookies to attach to the session
    """
    session = requests.Session()

    if headers:
        session.headers.update(headers)

    if auth:
        session.auth = auth  # e.g., ('user', 'pass') or requests.auth.HTTPBasicAuth('user', 'pass')

    if cookies:
        session.cookies.update(cookies)

    return session

def send_request(session, method, url, *,
                 params=None,
                 data=None,
                 json=None,
                 headers=None,
                 cookies=None,
                 auth=None,
                 timeout=30,
                 **kwargs):
    """
    Send an HTTP request using an existing session with optional per-request overrides.
    - method: 'GET', 'POST', etc.
    - url: target URL
    - params: query parameters (dict)
    - data: form data (dict or bytes)
    - json: JSON payload (dict)
    - headers: per-request headers (override/extend session headers)
    - cookies: per-request cookies (merged with session cookies)
    - auth: per-request auth (overrides session auth)
    - timeout: per-request timeout in seconds
    - kwargs: any other requests options (e.g., verify, proxies, files, allow_redirects)
    """
    resp = session.request(
        method=method.upper(),
        url=url,
        params=params,
        data=data,
        json=json,
        headers=headers,
        cookies=cookies,
        auth=auth,
        timeout=timeout,
        **kwargs
    )
    return resp

# Example usage:
if __name__ == "__main__":
    # Create a session with default headers, auth, and cookies
    sess = create_session(
        headers={"User-Agent": "MyApp/1.0", "Accept": "application/json"},
        auth=("username", "password"),  # or requests.auth.HTTPBasicAuth("username", "password")
        cookies={"sessionid": "abc123"}
    )

    # GET request with per-request headers and cookies
    r_get = send_request(
        sess,
        "GET",
        "https://httpbin.org/get",
        params={"q": "example"},
        headers={"X-Custom-Header": "value"},
        cookies={"feature": "on"},
        timeout=10
    )
    print("GET status:", r_get.status_code)
    print("GET response:", r_get.text)

    # POST JSON with per-request auth override
    r_post = send_request(
        sess,
        "POST",
        "https://httpbin.org/post",
        json={"key": "value"},
        auth=("otheruser", "otherpass"),  # overrides session auth for this call
        timeout=10
    )
    print("POST status:", r_post.status_code)
    print("POST response:", r_post.text)
------------------------------------------------------------

PROMPT 421:
"Generate Python code that imports the necessary libraries, creates a Stardog database connection, and executes a SPARQL query to retrieve data from the database."
RESPONSE:
# pip install pystardog

import os
import stardog
from stardog.exceptions import StardogException

def main():
    # Connection settings (override via environment variables if desired)
    endpoint = os.getenv("STARDOG_ENDPOINT", "http://localhost:5820")
    database = os.getenv("STARDOG_DB", "my_database")
    username = os.getenv("STARDOG_USERNAME", "admin")
    password = os.getenv("STARDOG_PASSWORD", "admin")

    # Example SPARQL query
    query = """
    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
    SELECT ?s ?p ?o
    WHERE {
      ?s ?p ?o .
    }
    LIMIT 10
    """

    try:
        # Create connection and execute query
        with stardog.Connection(database, endpoint=endpoint, username=username, password=password) as conn:
            results = conn.select(query)  # returns a dict in SPARQL JSON Results format

            # Print results as simple rows
            for binding in results.get("results", {}).get("bindings", []):
                row = {k: v.get("value") for k, v in binding.items()}
                print(row)

    except StardogException as e:
        print(f"Stardog error: {e}")
    except Exception as e:
        print(f"Unexpected error: {e}")

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 422:
"Generate Python code that imports the faker library and uses its \"phone_number\" function to generate a phone number in E164 format, like \"+1 (800) 123-4567\"."
RESPONSE:
Note: E.164 format does not include spaces or parentheses. An E.164 US number looks like +18001234567 (not "+1 (800) 123-4567").

# pip install Faker phonenumbers

from faker import Faker
import phonenumbers

fake = Faker("en_US")

def e164_phone_number(region="US"):
    # Try a few times because Faker may generate numbers that aren't valid
    for _ in range(50):
        raw = fake.phone_number()
        try:
            num = phonenumbers.parse(raw, region)
            if phonenumbers.is_valid_number(num):
                return phonenumbers.format_number(num, phonenumbers.PhoneNumberFormat.E164)
        except phonenumbers.NumberParseException:
            continue
    raise RuntimeError("Couldn't generate a valid E.164 number from Faker.phone_number()")

print(e164_phone_number())
------------------------------------------------------------

PROMPT 423:
"Generate Python code that imports the `uuid` module and creates a new time-based UUID format suitable for use as a database key, such as `uuid.time_based_uuid()`"
RESPONSE:
# Generate a time-based UUID (UUIDv7 if available; otherwise construct it).
import uuid
import time
import secrets

def time_based_uuid():
    # Use stdlib UUIDv7 if present (Python 3.12+), else build one.
    if hasattr(uuid, "uuid7"):
        return uuid.uuid7()

    # Construct UUIDv7 per RFC 9562:
    # 48-bit Unix ms timestamp | 4-bit version | 12-bit rand_a | 2-bit variant | 62-bit rand_b
    ts_ms = int(time.time_ns() // 1_000_000) & ((1 << 48) - 1)
    rand_a = secrets.randbits(12)
    rand_b = secrets.randbits(62)

    value = 0
    value |= ts_ms << 80          # 48-bit timestamp
    value |= 0x7 << 76            # version 7
    value |= rand_a << 64         # 12-bit rand_a
    value |= 0b10 << 62           # RFC 4122 variant
    value |= rand_b               # 62-bit rand_b

    return uuid.UUID(int=value)

# Optional: expose as uuid.time_based_uuid() for convenient calls.
uuid.time_based_uuid = time_based_uuid

if __name__ == "__main__":
    key = uuid.time_based_uuid()
    print(key)              # canonical string
    # print(key.bytes)      # 16-byte binary (good for compact DB storage)
------------------------------------------------------------

PROMPT 424:
"Generate Python code that uses the whois library to retrieve WHOIS information for a given domain name, and print the resulting data in a neatly formatted table."
RESPONSE:
```python
#!/usr/bin/env python3
# Requires: pip install python-whois
# Usage: python whois_table.py example.com

import sys
import shutil
import textwrap
from datetime import date, datetime

try:
    import whois
except ImportError:
    print("Error: The 'python-whois' package is required. Install it with: pip install python-whois")
    sys.exit(1)


def to_ascii_domain(domain: str) -> str:
    try:
        return domain.encode("idna").decode("ascii")
    except Exception:
        return domain


def stringify(value):
    if value is None:
        return "—"
    if isinstance(value, (datetime, date)):
        return value.isoformat()
    if isinstance(value, (list, tuple, set)):
        return ", ".join(stringify(v) for v in value)
    if isinstance(value, bytes):
        try:
            return value.decode("utf-8", "replace")
        except Exception:
            return str(value)
    return str(value)


def ordered_fields(data: dict):
    preferred = [
        "domain_name",
        "registrar",
        "registrant",
        "registrant_name",
        "registrant_organization",
        "registrant_country",
        "status",
        "creation_date",
        "updated_date",
        "expiration_date",
        "name_servers",
        "nameservers",
        "emails",
        "dnssec",
    ]
    keys = list(data.keys())
    seen = set()
    order = []

    # Add preferred if present
    for k in preferred:
        if k in data and k not in seen:
            order.append(k)
            seen.add(k)

    # Add the rest sorted
    for k in sorted(keys, key=lambda x: x.lower()):
        if k not in seen:
            order.append(k)
            seen.add(k)
    return order


def print_table(data: dict):
    if not data:
        print("No WHOIS data found.")
        return

    # Determine column widths based on terminal size
    term_width = shutil.get_terminal_size((100, 20)).columns
    min_val_width = 40
    max_key_width = 30

    fields = ordered_fields(data)
    key_width = min(max_key_width, max(len("Field"), *(len(k) for k in fields)))
    value_width = max(min_val_width, term_width - (key_width + 5))  # 5 for separators

    hline = f"+-{'-' * key_width}-+-{'-' * value_width}-+"
    header = f"| {'Field'.ljust(key_width)} | {'Value'.ljust(value_width)} |"

    print(hline)
    print(header)
    print(hline)

    for k in fields:
        v = stringify(data.get(k))
        wrapped = textwrap.wrap(v, width=value_width) or [""]
        # First line with key
        print(f"| {k.ljust(key_width)} | {wrapped[0].ljust(value_width)} |")
        # Continuation lines
        for cont in wrapped[1:]:
            print(f"| {' '.ljust(key_width)} | {cont.ljust(value_width)} |")
        print(hline)


def main():
    if len(sys.argv) < 2:
        print("Usage: python whois_table.py <domain>")
        sys.exit(1)

    domain = to_ascii_domain(sys.argv[1].strip())
    if not domain or "." not in domain:
        print("Error: Please provide a valid domain name, e.g., example.com")
        sys.exit(1)

    try:
        result = whois.whois(domain)
    except Exception as e:
        print(f"WHOIS lookup failed: {e}")
        sys.exit(2)

    # The whois result is typically dict-like already
    try:
        data = dict(result)
    except Exception:
        # Fallback: access internal __dict__ or as-is
        data = getattr(result, "__dict__", result)

    print_table(data)


if __name__ == "__main__":
    main()
```
------------------------------------------------------------

PROMPT 425:
"Generate Python code that creates an interactive grid for sorting and filtering DataFrames in Jupyter Notebook using the ipywidgets library and the pandas library."
RESPONSE:
# Interactive DataFrame grid with sorting and filtering (ipywidgets + pandas)
# Paste this into a Jupyter Notebook cell and run.

import pandas as pd
import numpy as np

try:
    import ipywidgets as widgets
    from IPython.display import display, clear_output
    from pandas.api.types import (
        is_numeric_dtype,
        is_integer_dtype,
        is_datetime64_any_dtype,
        is_bool_dtype,
        is_categorical_dtype,
        is_string_dtype,
    )
except ImportError as e:
    raise ImportError(
        "This example requires ipywidgets. Install it with:\n"
        "  pip install ipywidgets\n"
        "and enable for Jupyter if needed."
    ) from e


def create_interactive_grid(df: pd.DataFrame, categorical_unique_limit: int = 50):
    """
    Build an interactive grid for sorting and filtering a pandas DataFrame in Jupyter Notebook.

    - Global search across all text-like columns
    - Per-column filters (type-aware)
    - Two-level sorting (ascending/descending)
    - Row limit control
    """
    df = df.copy()

    # Try to ensure datetime columns are in datetime dtype
    for col in df.columns:
        if is_string_dtype(df[col]) or df[col].dtype == object:
            # Attempt auto-convert only if it looks like datetime for a sample (lightweight heuristic)
            sample = df[col].dropna().astype(str).head(10)
            if len(sample) and sample.str.match(r"\d{4}[-/]\d{1,2}[-/]\d{1,2}").mean() > 0.6:
                try:
                    df[col] = pd.to_datetime(df[col], errors="ignore")
                except Exception:
                    pass

    # Widgets: Top controls
    row_limit = widgets.IntSlider(
        value=min(50, len(df)),
        min=5,
        max=max(10, len(df)),
        step=5,
        description="Show rows",
        continuous_update=False,
        readout=True,
        layout=widgets.Layout(width="350px"),
    )
    global_search = widgets.Text(
        value="",
        placeholder="Search all text columns (case-insensitive)...",
        description="Search",
        layout=widgets.Layout(width="450px"),
    )
    clear_all_btn = widgets.Button(
        description="Clear all filters",
        icon="trash",
        button_style="warning",
        layout=widgets.Layout(width="160px"),
        tooltip="Reset all filters and sorting to defaults",
    )

    # Sorting controls (two levels)
    col_options = [("— none —", None)] + [(c, c) for c in df.columns]
    sort1_col = widgets.Dropdown(options=col_options, value=None, description="Sort 1")
    sort1_asc = widgets.ToggleButtons(
        options=[("Asc", True), ("Desc", False)], value=True, layout=widgets.Layout(width="150px")
    )
    sort2_col = widgets.Dropdown(options=col_options, value=None, description="Sort 2")
    sort2_asc = widgets.ToggleButtons(
        options=[("Asc", True), ("Desc", False)], value=True, layout=widgets.Layout(width="150px")
    )

    sort_box = widgets.HBox([sort1_col, sort1_asc, sort2_col, sort2_asc])

    # Per-column filter widgets
    filter_rows = []
    filter_specs = {}  # col -> dict with 'widgets' and 'mask_fn'

    def _numeric_filter(col, s):
        s_valid = s.dropna()
        if s_valid.empty:
            # No data; provide a disabled slider
            slider = widgets.FloatRangeSlider(
                value=(0.0, 0.0), min=0.0, max=0.0, step=0.1,
                description=col, continuous_update=False, disabled=True, readout=True, layout=widgets.Layout(width="600px")
            )
            include_na = widgets.Checkbox(value=False, description="Include NaN", indent=False)
            row = widgets.HBox([slider, include_na])
            def mask_fn():
                return pd.Series(True, index=df.index) & s.isna() if include_na.value else pd.Series(False, index=df.index)
            return row, {"slider": slider, "include_na": include_na}, mask_fn

        min_v, max_v = float(s_valid.min()), float(s_valid.max())
        if is_integer_dtype(s_valid):
            step = max(1, int(round((max_v - min_v) / 100))) if max_v > min_v else 1
            slider = widgets.IntRangeSlider(
                value=(int(min_v), int(max_v)),
                min=int(min_v),
                max=int(max_v),
                step=step,
                description=col,
                continuous_update=False,
                readout=True,
                layout=widgets.Layout(width="600px"),
            )
        else:
            # Float
            step = (max_v - min_v) / 200 if max_v > min_v else 0.1
            slider = widgets.FloatRangeSlider(
                value=(min_v, max_v),
                min=min_v,
                max=max_v,
                step=step,
                description=col,
                readout_format=".3f",
                continuous_update=False,
                readout=True,
                layout=widgets.Layout(width="600px"),
            )
        include_na = widgets.Checkbox(value=False, description="Include NaN", indent=False)
        row = widgets.HBox([slider, include_na])

        def mask_fn():
            lo, hi = slider.value
            m = s.ge(lo) & s.le(hi)
            if include_na.value:
                m = m | s.isna()
            return m.fillna(False)

        return row, {"slider": slider, "include_na": include_na}, mask_fn

    def _datetime_filter(col, s):
        s = pd.to_datetime(s, errors="coerce")
        s_valid = s.dropna()
        min_d = s_valid.min() if not s_valid.empty else pd.Timestamp("1970-01-01")
        max_d = s_valid.max() if not s_valid.empty else pd.Timestamp("1970-01-02")
        start = widgets.DatePicker(value=min_d.date() if not s_valid.empty else None, description=col + " from")
        end = widgets.DatePicker(value=max_d.date() if not s_valid.empty else None, description="to")
        include_na = widgets.Checkbox(value=False, description="Include NaT", indent=False)
        row = widgets.HBox([start, end, include_na])

        def mask_fn():
            st = pd.to_datetime(start.value) if start.value else min_d
            ed = pd.to_datetime(end.value) if end.value else max_d
            m = s.between(st, ed, inclusive="both")
            if include_na.value:
                m = m | s.isna()
            return m.fillna(False)

        return row, {"start": start, "end": end, "include_na": include_na}, mask_fn

    def _bool_filter(col, s):
        # Allow Any/True/False
        sel = widgets.Dropdown(options=[("Any", None), ("True", True), ("False", False)], value=None, description=col)
        include_na = widgets.Checkbox(value=False, description="Include NA", indent=False)
        row = widgets.HBox([sel, include_na])

        def mask_fn():
            if sel.value is None:
                return pd.Series(True, index=df.index)
            m = s == sel.value
            if include_na.value:
                m = m | s.isna()
            return m.fillna(False)

        return row, {"select": sel, "include_na": include_na}, mask_fn

    def _category_or_text_filter(col, s):
        s_obj = s  # as-is
        nunique = int(s_obj.nunique(dropna=True))
        include_na = widgets.Checkbox(value=False, description="Include NA", indent=False)

        # If low cardinality: SelectMultiple; else: substring search
        if nunique <= categorical_unique_limit:
            opts = list(pd.Series(s_obj.dropna().unique()).astype(str).sort_values())
            chooser = widgets.SelectMultiple(
                options=opts,
                value=tuple(opts),  # default: all selected
                description=col,
                rows=min(8, len(opts)),
                layout=widgets.Layout(width="450px"),
            )
            row = widgets.HBox([chooser, include_na])

            def mask_fn():
                selected = list(chooser.value)
                if len(selected) == 0 and not include_na.value:
                    # nothing selected -> no rows
                    return pd.Series(False, index=df.index)
                m = s_obj.astype(str).isin(selected) if selected else pd.Series(False, index=df.index)
                if include_na.value:
                    m = m | s_obj.isna()
                return m.fillna(False)

            return row, {"chooser": chooser, "include_na": include_na}, mask_fn
        else:
            txt = widgets.Text(value="", placeholder="contains...", description=col)
            row = widgets.HBox([txt, include_na])

            def mask_fn():
                q = txt.value.strip()
                if q == "":
                    return pd.Series(True, index=df.index) if include_na.value else (~s_obj.isna()) | s_obj.isna()
                m = s_obj.astype(str).str.contains(q, case=False, na=False)
                if include_na.value:
                    m = m | s_obj.isna()
                return m.fillna(False)

            return row, {"text": txt, "include_na": include_na}, mask_fn

    # Build per-column filters
    for col in df.columns:
        s = df[col]
        if is_datetime64_any_dtype(s):
            row, widgets_dict, mask_fn = _datetime_filter(col, s)
        elif is_numeric_dtype(s):
            row, widgets_dict, mask_fn = _numeric_filter(col, s)
        elif is_bool_dtype(s):
            row, widgets_dict, mask_fn = _bool_filter(col, s)
        elif is_categorical_dtype(s) or is_string_dtype(s) or s.dtype == object:
            row, widgets_dict, mask_fn = _category_or_text_filter(col, s)
        else:
            # Fallback: treat as text
            row, widgets_dict, mask_fn = _category_or_text_filter(col, s.astype("string"))

        filter_rows.append(row)
        filter_specs[col] = {"widgets": widgets_dict, "mask_fn": mask_fn}

    filters_box = widgets.VBox(filter_rows, layout=widgets.Layout(max_height="350px", overflow="auto"))
    filters_accordion = widgets.Accordion(children=[filters_box])
    filters_accordion.set_title(0, "Column filters (expand to adjust)")

    # Output area and status
    out = widgets.Output()
    status = widgets.HTML("")

    # Combine filters and update display
    text_cols = df.select_dtypes(include=["object", "string", "category"]).columns.tolist()

    def compute_mask():
        mask = pd.Series(True, index=df.index)
        # Global search across text-like cols
        q = global_search.value.strip()
        if q and len(text_cols) > 0:
            any_match = pd.Series(False, index=df.index)
            for c in text_cols:
                any_match = any_match | df[c].astype(str).str.contains(q, case=False, na=False)
            mask = mask & any_match
        # Per-column filters
        for col, spec in filter_specs.items():
            m = spec["mask_fn"]()
            if m is not None:
                mask = mask & m
        return mask

    def update(_=None):
        m = compute_mask()
        filtered = df[m]
        # Sorting
        sort_cols = []
        ascending = []
        if sort1_col.value is not None:
            sort_cols.append(sort1_col.value)
            ascending.append(sort1_asc.value)
        if sort2_col.value is not None and sort2_col.value not in sort_cols:
            sort_cols.append(sort2_col.value)
            ascending.append(sort2_asc.value)
        if sort_cols:
            try:
                filtered = filtered.sort_values(by=sort_cols, ascending=ascending, kind="mergesort")
            except Exception:
                pass

        # Update status and output
        with out:
            out.clear_output(wait=True)
            display(filtered.head(row_limit.value))
        status.value = f"<b>{filtered.shape[0]:,}</b> rows × <b>{filtered.shape[1]:,}</b> columns (showing first {row_limit.value})"

    def clear_all(_=None):
        global_search.value = ""
        # Reset sorts
        sort1_col.value = None
        sort1_asc.value = True
        sort2_col.value = None
        sort2_asc.value = True
        # Reset filters
        for col, spec in filter_specs.items():
            widgets_dict = spec["widgets"]
            # Numeric
            if "slider" in widgets_dict:
                slider = widgets_dict["slider"]
                slider.value = (slider.min, slider.max)
            # Datetime
            if "start" in widgets_dict and "end" in widgets_dict:
                # Try to reset to min/max if they exist in the column
                s = pd.to_datetime(df[col], errors="coerce")
                s_valid = s.dropna()
                if not s_valid.empty:
                    widgets_dict["start"].value = s_valid.min().date()
                    widgets_dict["end"].value = s_valid.max().date()
                else:
                    widgets_dict["start"].value = None
                    widgets_dict["end"].value = None
            # Bool
            if "select" in widgets_dict:
                widgets_dict["select"].value = None
            # Category low-card
            if "chooser" in widgets_dict:
                chooser = widgets_dict["chooser"]
                chooser.value = tuple(chooser.options)
            # Text search
            if "text" in widgets_dict:
                widgets_dict["text"].value = ""
            # NA inclusion
            if "include_na" in widgets_dict:
                widgets_dict["include_na"].value = False

        update()

    # Wire events
    global_search.observe(update, names="value")
    row_limit.observe(update, names="value")
    sort1_col.observe(update, names="value")
    sort1_asc.observe(update, names="value")
    sort2_col.observe(update, names="value")
    sort2_asc.observe(update, names="value")
    clear_all_btn.on_click(clear_all)

    # Per-column widget observers
    for row in filter_rows:
        # Each row is a Box with children that have 'value' trait
        for child in row.children:
            try:
                child.observe(update, names="value")
            except Exception:
                pass

    # Initial render
    update()

    top_bar = widgets.HBox([row_limit, global_search, clear_all_btn])
    ui = widgets.VBox([top_bar, sort_box, filters_accordion, status, out])
    display(ui)
    return ui


# Example usage:
# If you already have a DataFrame named df, just call:
# create_interactive_grid(df)

# Demo DataFrame if none defined:
if "df" not in globals():
    rng = np.random.default_rng(0)
    n = 500
    df = pd.DataFrame(
        {
            "id": np.arange(n),
            "category": rng.choice(list("ABCD") + [None], size=n, p=[0.2, 0.2, 0.2, 0.2, 0.2]),
            "value": rng.normal(0, 1, size=n),
            "amount": rng.integers(0, 1000, size=n),
            "flag": rng.choice([True, False, None], size=n),
            "date": pd.to_datetime("2022-01-01") + pd.to_timedelta(rng.integers(0, 365, size=n), unit="D"),
            "text": rng.choice(["alpha", "beta", "gamma", "delta", "epsilon", None], size=n),
        }
    )

# Launch the interactive grid
create_interactive_grid(df)
------------------------------------------------------------

PROMPT 426:
"Generate Python code that imports the inspect module and creates a signature object for a function, allowing you to manipulate and analyze the function's signature."
RESPONSE:
import inspect

# Example function
def process(a, b: int, c=10, *args, d: str = "x", **kwargs) -> bool:
    """Demo function to analyze and manipulate its signature."""
    return True

# Create a Signature object
sig = inspect.signature(process)
print("Original signature:", sig)

# Analyze parameters
for name, p in sig.parameters.items():
    print(f"- {name}: kind={p.kind.name}, default={p.default!r}, annotation={p.annotation!r}")
print("Return annotation:", sig.return_annotation)

# Bind arguments to see how they map to parameters
bound = sig.bind(1, 2, 30, "extra1", d="hello", flag=True)
print("Bound arguments mapping:", bound.arguments)

# Manipulate the signature: rename 'c' to 'count' and change its default
params = list(sig.parameters.values())
for i, p in enumerate(params):
    if p.name == "c":
        params[i] = p.replace(name="count", default=42)
        break

# Add a new keyword-only parameter 'e' with a default, before **kwargs if present
new_param_e = inspect.Parameter(
    "e",
    kind=inspect.Parameter.KEYWORD_ONLY,
    default=0,
    annotation=int,
)

insert_idx = next((i for i, p in enumerate(params) if p.kind == inspect.Parameter.VAR_KEYWORD), len(params))
params.insert(insert_idx, new_param_e)

# Build a new Signature with modified parameters and a different return annotation
new_sig = sig.replace(parameters=params, return_annotation=str)
print("New signature:", new_sig)

# Optionally, attach the new signature to the function for introspection
process.__signature__ = new_sig
print("Introspected signature after override:", inspect.signature(process))

# Validate binding with the new signature
bound_new = new_sig.bind(1, 2, 99, e=5, d="done")
print("Bound with new signature:", bound_new.arguments)
------------------------------------------------------------

PROMPT 427:
"Generate Python code that creates a CloudFormation template using the AWS CDK library to define a stack, resources, and dependencies for a scalable web application."
RESPONSE:
import os
from constructs import Construct
from aws_cdk import (
    App,
    Stack,
    Duration,
    RemovalPolicy,
    CfnOutput,
    Tags,
    Environment,
    aws_ec2 as ec2,
    aws_ecs as ecs,
    aws_ecs_patterns as ecs_patterns,
    aws_logs as logs,
    aws_dynamodb as dynamodb,
)

class ScalableWebAppStack(Stack):
    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # Networking: VPC across 2 AZs with 1 NAT for cost efficiency
        vpc = ec2.Vpc(
            self,
            "WebAppVpc",
            max_azs=2,
            nat_gateways=1,
        )

        # ECS Cluster
        cluster = ecs.Cluster(self, "WebAppCluster", vpc=vpc)

        # Logging
        log_group = logs.LogGroup(
            self,
            "WebAppLogs",
            retention=logs.RetentionDays.ONE_WEEK,
            removal_policy=RemovalPolicy.DESTROY,
        )

        # Load-balanced Fargate Service (scalable web app)
        service = ecs_patterns.ApplicationLoadBalancedFargateService(
            self,
            "WebAppService",
            cluster=cluster,
            cpu=256,  # 0.25 vCPU
            memory_limit_mib=512,
            desired_count=2,
            public_load_balancer=True,
            task_image_options=ecs_patterns.ApplicationLoadBalancedTaskImageOptions(
                image=ecs.ContainerImage.from_registry("public.ecr.aws/docker/library/nginx:stable-alpine"),
                container_port=80,
                environment={
                    "APP_ENV": "prod",
                },
                log_driver=ecs.LogDriver.aws_logs(
                    stream_prefix="web-app",
                    log_group=log_group,
                ),
            ),
        )

        # Health check tuning (optional)
        service.target_group.configure_health_check(
            path="/",
            healthy_http_codes="200-399",
            interval=Duration.seconds(30),
        )

        # Auto scaling policies
        scaling = service.service.auto_scale_task_count(
            min_capacity=2,
            max_capacity=10,
        )
        scaling.scale_on_cpu_utilization(
            "CpuScaling",
            target_utilization_percent=60,
            scale_in_cooldown=Duration.seconds(60),
            scale_out_cooldown=Duration.seconds(30),
        )
        scaling.scale_on_request_count(
            "RequestScaling",
            requests_per_target=1000,  # per target per minute
            target_group=service.target_group,
        )

        # Backend state: DynamoDB table (serverless, scalable)
        table = dynamodb.Table(
            self,
            "WebAppTable",
            table_name=None,  # auto-generated name
            partition_key=dynamodb.Attribute(name="pk", type=dynamodb.AttributeType.STRING),
            sort_key=dynamodb.Attribute(name="sk", type=dynamodb.AttributeType.STRING),
            billing_mode=dynamodb.BillingMode.PAY_PER_REQUEST,
            point_in_time_recovery=True,
            removal_policy=RemovalPolicy.DESTROY,  # for demo/dev; use RETAIN in production
        )

        # Grant ECS task permission to read/write the table
        table.grant_read_write_data(service.task_definition.task_role)

        # Pass configuration to the container
        if service.task_definition.default_container:
            service.task_definition.default_container.add_environment("TABLE_NAME", table.table_name)

        # Explicit dependency: ensure table is created before service starts
        service.service.node.add_dependency(table)

        # Outputs
        CfnOutput(
            self,
            "LoadBalancerDNS",
            value=service.load_balancer.load_balancer_dns_name,
            description="Public URL of the web application",
        )
        CfnOutput(
            self,
            "DynamoDBTableName",
            value=table.table_name,
            description="DynamoDB table used by the application",
        )

        # Optional tagging
        Tags.of(self).add("Project", "ScalableWebApp")
        Tags.of(self).add("ManagedBy", "CDK")

if __name__ == "__main__":
    app = App()

    env = Environment(
        account=os.getenv("CDK_DEFAULT_ACCOUNT"),
        region=os.getenv("CDK_DEFAULT_REGION"),
    )

    ScalableWebAppStack(app, "ScalableWebAppStack", env=env)
    app.synth()
------------------------------------------------------------

PROMPT 428:
"Generate Python code that creates a CDK construct for an AWS Lambda function using the CDK library, specifying the function name, runtime, memory size, and handler."
RESPONSE:
```python
from constructs import Construct
from aws_cdk import Stack
from aws_cdk import aws_lambda as _lambda

class LambdaFunctionConstruct(Construct):
    def __init__(
        self,
        scope: Construct,
        construct_id: str,
        *,
        function_name: str,
        runtime: _lambda.Runtime,
        memory_size: int,
        handler: str,
        code: _lambda.Code,
    ) -> None:
        super().__init__(scope, construct_id)

        self.function = _lambda.Function(
            self,
            "Function",
            function_name=function_name,
            runtime=runtime,
            memory_size=memory_size,
            handler=handler,
            code=code,
        )

# Example usage inside a stack
class MyStack(Stack):
    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        LambdaFunctionConstruct(
            self,
            "MyLambda",
            function_name="my-lambda-function",
            runtime=_lambda.Runtime.PYTHON_3_12,
            memory_size=512,
            handler="app.handler",
            code=_lambda.Code.from_asset("lambda"),  # directory with your code
        )
```
------------------------------------------------------------

PROMPT 429:
"Generate Python code that uses the `pytest` framework to create a custom plugin for printing summary data in a specific format, using the `pytest.plugin` mechanism."
RESPONSE:
# File: pytest_custom_summary.py
# A pytest plugin that prints a custom summary in plain, JSON, or TAP-like format.
#
# Usage examples:
#   pytest -p pytest_custom_summary
#   pytest -p pytest_custom_summary --summary-format json --summary-topn 10
#   pytest -p pytest_custom_summary --summary-format tap
#
# Alternatively, enable via conftest.py in your repo:
#   pytest_plugins = ["pytest_custom_summary"]

import json
import time
from collections import defaultdict

import pytest


class SummaryPlugin:
    def __init__(self, config):
        self.config = config
        self.session_start_time = None
        self.total_time = 0.0
        self.exitstatus = None

        # Outcome tracking
        self.counts = defaultdict(int)
        self.nodeids_by_outcome = defaultdict(list)  # outcome -> [nodeids]
        self.durations = []  # list of (duration, nodeid)

    @pytest.hookimpl(tryfirst=True)
    def pytest_sessionstart(self, session):
        self.session_start_time = time.time()

    def pytest_runtest_logreport(self, report):
        # Count results at the "call" phase; include setup-phase skips
        is_setup_skip = report.skipped and report.when == "setup"
        if report.when != "call" and not is_setup_skip:
            return

        if report.passed:
            outcome = "xpassed" if getattr(report, "wasxfail", False) else "passed"
        elif report.failed:
            outcome = "xfailed" if getattr(report, "wasxfail", False) else "failed"
        elif report.skipped:
            outcome = "skipped"
        else:
            outcome = "unknown"

        self.counts[outcome] += 1
        self.nodeids_by_outcome[outcome].append(report.nodeid)

        # Track test durations for slowest listing
        self.durations.append((report.duration, report.nodeid))

    def pytest_sessionfinish(self, session, exitstatus):
        self.exitstatus = exitstatus
        if self.session_start_time is not None:
            self.total_time = time.time() - self.session_start_time

    def pytest_terminal_summary(self, terminalreporter, exitstatus):
        fmt = self.config.getoption("--summary-format")
        topn = self.config.getoption("--summary-topn")
        title = self.config.getoption("--summary-title")

        # Build common data
        totals = self._totals()
        total_count = sum(totals.values())
        slowest = self._slowest(topn) if topn else []

        terminalreporter.section(title, sep="=")

        if fmt == "json":
            payload = {
                "title": title,
                "totals": totals,
                "total": total_count,
                "exitstatus": exitstatus,
                "duration_seconds": round(self.total_time, 3),
                "slowest": [{"nodeid": nid, "duration": round(d, 6)} for d, nid in slowest],
                "failed": self.nodeids_by_outcome.get("failed", []),
            }
            terminalreporter.write_line(json.dumps(payload, indent=2, sort_keys=True))
            return

        if fmt == "tap":
            terminalreporter.write_line(f"1..{total_count}")
            i = 0
            order = ["passed", "failed", "skipped", "xfailed", "xpassed", "unknown"]
            for outcome in order:
                for nodeid in self.nodeids_by_outcome.get(outcome, []):
                    i += 1
                    ok = (outcome == "passed")
                    prefix = "ok" if ok else "not ok"
                    note = f" # {outcome}" if outcome != "passed" else ""
                    terminalreporter.write_line(f"{prefix} {i} - {nodeid}{note}")
            terminalreporter.write_line(
                f"# totals: {totals} in {self.total_time:.3f}s (exitstatus={exitstatus})"
            )
            if slowest:
                terminalreporter.write_line("# slowest tests:")
                for d, nid in slowest:
                    terminalreporter.write_line(f"#   {d:.3f}s {nid}")
            return

        # Default: plain single/multi-line human format
        terminalreporter.write_line(
            "results: "
            f"passed={totals.get('passed', 0)} "
            f"failed={totals.get('failed', 0)} "
            f"skipped={totals.get('skipped', 0)} "
            f"xfailed={totals.get('xfailed', 0)} "
            f"xpassed={totals.get('xpassed', 0)} "
            f"unknown={totals.get('unknown', 0)} "
            f"total={total_count}"
        )
        terminalreporter.write_line(f"duration: {self.total_time:.3f}s (exitstatus={exitstatus})")

        if slowest:
            terminalreporter.write_line(f"slowest {len(slowest)} tests:")
            for d, nid in slowest:
                terminalreporter.write_line(f"  {d:.3f}s {nid}")

        failed = self.nodeids_by_outcome.get("failed", [])
        if failed:
            terminalreporter.write_line("failed tests:")
            for nid in failed:
                terminalreporter.write_line(f"  {nid}")

    def _totals(self):
        return {k: int(v) for k, v in sorted(self.counts.items())}

    def _slowest(self, n):
        n = max(0, int(n))
        if n == 0 or not self.durations:
            return []
        return sorted(self.durations, key=lambda x: x[0], reverse=True)[:n]


def pytest_addoption(parser):
    group = parser.getgroup("custom-summary")
    group.addoption(
        "--summary-format",
        action="store",
        default="plain",
        choices=("plain", "json", "tap"),
        help="Format for the custom summary footer.",
    )
    group.addoption(
        "--summary-topn",
        action="store",
        type=int,
        default=5,
        help="Show top-N slowest tests in summary (0 disables).",
    )
    group.addoption(
        "--summary-title",
        action="store",
        default="Custom Test Summary",
        help="Section title for the summary output.",
    )


def pytest_configure(config):
    # Register plugin instance so it can be loaded via the pytest plugin mechanism:
    # - as an external plugin: pytest -p pytest_custom_summary
    # - or via pytest_plugins in conftest.py
    config.pluginmanager.register(SummaryPlugin(config), name="custom-summary-plugin")
------------------------------------------------------------

PROMPT 430:
"Generate Python code that creates a command-line interface for interacting with the Soda Core library, which provides tools for data transformation and manipulation."
RESPONSE:
#!/usr/bin/env python3
"""
A lightweight CLI wrapper around Soda Core to run data quality scans from the command line.

Prerequisites:
- Install Soda Core and the appropriate data source package(s), for example:
    pip install "soda-core>=3" "soda-core-postgres"  # for PostgreSQL
    pip install "soda-core-snowflake"
    pip install "soda-core-bigquery"
    pip install "soda-core-duckdb"
    ...

Basic usage examples:
- Run a scan with config and checks:
    python soda_cli.py scan \
      --config configuration.yml \
      --checks checks.yml \
      --data-source my_postgres \
      --scan-name nightly_orders_check \
      --var env=dev --var threshold=5

- Test a connection (validates config + data source connectivity):
    python soda_cli.py test-connection --config configuration.yml --data-source my_postgres

- Generate a starter checks file for a table:
    python soda_cli.py scaffold-checks --table orders --out checks_orders.yml
"""

import argparse
import json
import logging
import os
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


def _setup_logging(verbosity: int) -> None:
    # Map verbosity to log level. Default INFO; -v adds more verbosity; -q quiets
    if verbosity <= -1:
        level = logging.ERROR
    elif verbosity == 0:
        level = logging.INFO
    elif verbosity == 1:
        level = logging.DEBUG
    else:
        level = logging.NOTSET

    logging.basicConfig(
        level=level,
        format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
        datefmt="%H:%M:%S",
    )


def _parse_vars(var_list: List[str], vars_file: Optional[str]) -> Dict[str, Any]:
    """
    Load variables for SodaCL.
    - Multiple --var key=value args
    - Optional --vars-file path.{json|yaml|yml}
    If both present, command-line --var overrides file values.
    """
    variables: Dict[str, Any] = {}

    # Load from file if provided
    if vars_file:
        p = Path(vars_file)
        if not p.exists():
            raise FileNotFoundError(f"Variables file not found: {vars_file}")
        text = p.read_text(encoding="utf-8")
        lower = p.suffix.lower()
        if lower in (".json",):
            variables.update(json.loads(text))
        elif lower in (".yaml", ".yml"):
            try:
                import yaml  # type: ignore
            except Exception as e:
                raise RuntimeError(
                    "YAML variables file specified but PyYAML is not installed. "
                    "Install with: pip install pyyaml"
                ) from e
            variables.update(yaml.safe_load(text) or {})
        else:
            raise ValueError("Unsupported vars file type. Use .json, .yaml, or .yml")

    # Apply --var key=value overrides
    for item in var_list or []:
        if "=" not in item:
            raise ValueError(f"Invalid --var '{item}'. Expected format key=value")
        k, v = item.split("=", 1)
        k = k.strip()
        v = v.strip()
        # Attempt to coerce JSON types if possible (e.g., numbers, booleans)
        try:
            v_json = json.loads(v)
            variables[k] = v_json
        except Exception:
            variables[k] = v

    return variables


def _summarize_scan(scan: Any) -> Tuple[bool, bool, Dict[str, Any]]:
    """
    Best-effort way to summarize the scan outcome across Soda Core versions.
    Returns (has_errors, has_failures, summary_dict)
    """
    has_errors = False
    has_failures = False
    summary: Dict[str, Any] = {}

    # Prefer explicit methods if available
    for name in ("has_errors", "has_error"):
        if hasattr(scan, name):
            try:
                has_errors = bool(getattr(scan, name)())
                break
            except Exception:
                pass

    # Failures usually correspond to failed checks
    for name in ("has_check_fails", "has_failures", "has_test_failures"):
        if hasattr(scan, name):
            try:
                has_failures = bool(getattr(scan, name)())
                break
            except Exception:
                pass

    # Try to collect more details if available
    # Note: These attributes/methods can differ by version; guard with try/except.
    # We'll fill what we can discover without breaking if the API differs.
    possible_counts = [
        "get_check_count",
        "get_failed_check_count",
        "get_checks_count",
        "get_checks_fail_count",
    ]
    for name in possible_counts:
        if hasattr(scan, name):
            try:
                summary[name] = getattr(scan, name)()
            except Exception:
                pass

    # Logs text can be useful for CI output
    for name in ("get_logs_text",):
        if hasattr(scan, name):
            try:
                summary["logs"] = getattr(scan, name)()
            except Exception:
                pass

    return has_errors, has_failures, summary


def cmd_scan(args: argparse.Namespace) -> int:
    try:
        from soda.scan import Scan  # type: ignore
    except Exception as e:
        logging.error("Could not import soda-core. Install it first: pip install 'soda-core>=3'")
        logging.debug("Import error details", exc_info=True)
        return 2

    variables = _parse_vars(args.var or [], args.vars_file)

    scan = Scan()

    # Data source name can be set here or resolved from configuration yml
    if args.data_source:
        scan.set_data_source_name(args.data_source)

    if args.scan_name:
        # Some versions accept set_scan_definition_name; if not present, ignore.
        for fname in ("set_scan_definition_name", "set_scan_name"):
            if hasattr(scan, fname):
                try:
                    getattr(scan, fname)(args.scan_name)
                    break
                except Exception:
                    pass

    # Add configuration and checks
    if args.config:
        if not Path(args.config).exists():
            logging.error(f"Configuration file not found: {args.config}")
            return 2
        scan.add_configuration_yaml_file(args.config)

    if not args.checks:
        logging.error("No checks file(s) provided. Use --checks path.yaml (can pass multiple).")
        return 2

    for checks_file in args.checks:
        if not Path(checks_file).exists():
            logging.error(f"Checks file not found: {checks_file}")
            return 2
        scan.add_sodacl_yaml_file(checks_file)

    if variables:
        scan.set_variables(variables)

    # Optional: environment and scan metadata
    if args.env:
        # Commonly variables include env; keep both for compatibility.
        try:
            scan.set_environment(args.env)  # may not exist in all versions
        except Exception:
            # Fall back to variables if environment setter is unavailable
            v = variables.copy()
            v.setdefault("env", args.env)
            scan.set_variables(v)

    # Execute the scan
    try:
        scan.execute()
    except SystemExit as se:
        # Some versions can sys.exit; capture and convert to code
        logging.debug("Scan raised SystemExit", exc_info=True)
        code = int(getattr(se, "code", 2) or 2)
        return code
    except Exception as e:
        logging.error(f"Soda scan execution failed: {e}")
        logging.debug("Execution failure details", exc_info=True)
        return 2

    has_errors, has_failures, summary = _summarize_scan(scan)

    # Print a brief summary
    if has_errors:
        logging.error("Scan completed with errors.")
    if has_failures:
        logging.warning("Scan completed with failed checks.")
    if not has_errors and not has_failures:
        logging.info("Scan completed successfully with all checks passing.")

    # Optionally write a JSON artifact
    if args.out_json:
        out = {
            "has_errors": has_errors,
            "has_failures": has_failures,
            "summary": summary,
        }
        try:
            Path(args.out_json).write_text(json.dumps(out, indent=2), encoding="utf-8")
            logging.info(f"Wrote scan result JSON: {args.out_json}")
        except Exception as e:
            logging.error(f"Failed to write JSON output: {e}")

    # Exit code policy
    #  - 2: errors (config issues, connectivity, runtime)
    #  - 1: failed checks
    #  - 0: success
    if has_errors:
        return 2
    if has_failures:
        return 1
    return 0


def cmd_test_connection(args: argparse.Namespace) -> int:
    """
    Attempt to instantiate and execute a no-op scan. If the data source and
    configuration are valid, this should succeed without checks.
    """
    try:
        from soda.scan import Scan  # type: ignore
    except Exception:
        logging.error("Could not import soda-core. Install it first: pip install 'soda-core>=3'")
        return 2

    scan = Scan()

    if args.data_source:
        scan.set_data_source_name(args.data_source)

    if args.config:
        if not Path(args.config).exists():
            logging.error(f"Configuration file not found: {args.config}")
            return 2
        scan.add_configuration_yaml_file(args.config)

    # Execute a scan without checks to validate connectivity
    try:
        scan.execute()
    except SystemExit as se:
        code = int(getattr(se, "code", 2) or 2)
        return code
    except Exception as e:
        logging.error(f"Connection test failed: {e}")
        logging.debug("Connection failure details", exc_info=True)
        return 2

    has_errors, _, _ = _summarize_scan(scan)
    if has_errors:
        logging.error("Connection test completed but reported errors.")
        return 2

    logging.info("Connection test succeeded.")
    return 0


def _scaffold_checks_yaml(table: str) -> str:
    """
    Produce a minimal SodaCL checks YAML scaffold for a single table.
    Users can expand it as needed.
    """
    return f"""checks for {table}:
  - row_count > 0
  - missing_count({{ column: '*' }}) = 0:
      except:
        - optional_id
  - schema:
      warn:
        when schema changes:
          - column add
          - column remove
          - data type change
"""


def cmd_scaffold_checks(args: argparse.Namespace) -> int:
    if not args.table:
        logging.error("Please provide --table name for scaffold generation.")
        return 2
    content = _scaffold_checks_yaml(args.table)
    if args.out:
        p = Path(args.out)
        try:
            p.write_text(content, encoding="utf-8")
            logging.info(f"Wrote checks scaffold to {p}")
        except Exception as e:
            logging.error(f"Failed to write scaffold file: {e}")
            return 2
    else:
        # Print to stdout
        sys.stdout.write(content)
    return 0


def make_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="soda-cli",
        description="Command-line interface for Soda Core scans.",
    )
    parser.add_argument(
        "-v", "--verbose", action="count", default=0, help="Increase verbosity (-v, -vv)"
    )
    parser.add_argument(
        "-q", "--quiet", action="count", default=0, help="Decrease verbosity (-q)"
    )

    sub = parser.add_subparsers(dest="command", required=True)

    # scan
    p_scan = sub.add_parser(
        "scan",
        help="Run a Soda Core scan using configuration and SodaCL checks files.",
    )
    p_scan.add_argument(
        "--config",
        required=True,
        help="Path to Soda configuration YAML (e.g., configuration.yml)",
    )
    p_scan.add_argument(
        "--checks",
        nargs="+",
        required=True,
        help="One or more SodaCL checks YAML files.",
    )
    p_scan.add_argument(
        "--data-source",
        help="Data source name (must match a data source in configuration.yml).",
    )
    p_scan.add_argument(
        "--scan-name",
        help="Optional scan name to tag this run.",
    )
    p_scan.add_argument(
        "--env",
        help="Optional environment label (may be used as a variable 'env').",
    )
    p_scan.add_argument(
        "--var",
        action="append",
        help="Variable override in key=value form. Can be used multiple times.",
    )
    p_scan.add_argument(
        "--vars-file",
        help="Path to a JSON or YAML file providing variables.",
    )
    p_scan.add_argument(
        "--out-json",
        help="Write a JSON summary of results to this file.",
    )
    p_scan.set_defaults(func=cmd_scan)

    # test-connection
    p_test = sub.add_parser(
        "test-connection",
        help="Validate configuration and connectivity without running checks.",
    )
    p_test.add_argument(
        "--config",
        required=True,
        help="Path to Soda configuration YAML.",
    )
    p_test.add_argument(
        "--data-source",
        help="Data source name (must match a data source in configuration.yml).",
    )
    p_test.set_defaults(func=cmd_test_connection)

    # scaffold-checks
    p_scaf = sub.add_parser(
        "scaffold-checks",
        help="Generate a starter SodaCL checks YAML for a given table.",
    )
    p_scaf.add_argument(
        "--table",
        required=True,
        help="Table name to scaffold checks for.",
    )
    p_scaf.add_argument(
        "--out",
        help="Optional path to write the scaffold; if omitted, prints to stdout.",
    )
    p_scaf.set_defaults(func=cmd_scaffold_checks)

    return parser


def main(argv: Optional[List[str]] = None) -> int:
    parser = make_parser()
    args = parser.parse_args(argv)

    # Logging setup
    v = int(args.verbose or 0)
    q = int(args.quiet or 0)
    _setup_logging(v - q)

    try:
        return args.func(args)
    except KeyboardInterrupt:
        logging.error("Interrupted by user.")
        return 130


if __name__ == "__main__":
    sys.exit(main())
------------------------------------------------------------

PROMPT 431:
"Generate Python code that uses the Dagster API to create, run, and manage data pipelines, leveraging dbt's data transformation capabilities within a Dagster workflow."
RESPONSE:
# pip install dagster dagster-webserver dagster-dbt dagster-duckdb-pandas duckdb dbt-core dbt-duckdb
# Save this file (e.g., repo.py) and run: dagster dev -f repo.py
# Ensure you have a dbt project (with dbt-duckdb adapter) and profiles.yml set up.
# This example uses DuckDB so Python and dbt operate on the same database.

import os
from pathlib import Path
from typing import Iterable, Optional

import pandas as pd
from dagster import (
    AssetKey,
    AssetSelection,
    Definitions,
    FreshnessPolicy,
    RunRequest,
    ScheduleDefinition,
    SensorEvaluationContext,
    DefaultSensorStatus,
    asset,
    define_asset_job,
    sensor,
)
from dagster import materialize as materialize_assets
from dagster_duckdb_pandas import DuckDBPandasIOManager
from dagster_dbt import (
    DbtCliResource,
    load_assets_from_dbt_project,
    load_asset_checks_from_dbt_project,
)


# ------------------------------------------------------------------------------
# Paths and configuration
# ------------------------------------------------------------------------------

HERE = Path(__file__).parent.resolve()

# Point these at your dbt project and profiles directories
DBT_PROJECT_DIR = os.fspath(HERE / "dbt_project")      # e.g., contains dbt_project.yml
DBT_PROFILES_DIR = os.fspath(HERE / ".dbt")            # e.g., contains profiles.yml

# DuckDB database shared by Python assets and dbt (via dbt-duckdb)
DUCKDB_PATH = os.fspath(HERE / "warehouse.duckdb")


# ------------------------------------------------------------------------------
# Python asset that prepares raw/source data for dbt
# ------------------------------------------------------------------------------

@asset(
    io_manager_key="duckdb_io_manager",
    key_prefix=["raw"],                   # results in table raw.orders
    group_name="ingestion",
    freshness_policy=FreshnessPolicy(maximum_lag_minutes=60, cron_schedule="0 * * * *"),
)
def orders() -> pd.DataFrame:
    """
    Example raw dataset; in a real pipeline, replace with your extract/ingest logic.
    The DuckDB IO manager writes this DataFrame to warehouse.duckdb as raw.orders.
    """
    df = pd.DataFrame(
        [
            {"order_id": 1, "customer_id": 100, "amount": 39.95, "ts": "2025-10-01"},
            {"order_id": 2, "customer_id": 101, "amount": 19.99, "ts": "2025-10-02"},
            {"order_id": 3, "customer_id": 100, "amount": 10.00, "ts": "2025-10-02"},
        ]
    )
    return df


# ------------------------------------------------------------------------------
# dbt assets and checks
# ------------------------------------------------------------------------------

# Load every model/seed/snapshot/test in the dbt project as Dagster assets/checks.
# Ensure your dbt profile points to the same DuckDB database at DUCKDB_PATH.
dbt_assets = load_assets_from_dbt_project(
    project_dir=DBT_PROJECT_DIR,
    profiles_dir=DBT_PROFILES_DIR,
    # Optionally narrow scope: select="tag:prod", exclude="tag:experimental"
    key_prefix=["warehouse"],             # assets become warehouse.<model_name>
    # group_name per node will be derived; you can override with node_info_to_group_fn
)

dbt_checks = load_asset_checks_from_dbt_project(
    project_dir=DBT_PROJECT_DIR,
    profiles_dir=DBT_PROFILES_DIR,
)


# ------------------------------------------------------------------------------
# Resources
# ------------------------------------------------------------------------------

resources = {
    # Used by Python assets to write to DuckDB tables/view
    "duckdb_io_manager": DuckDBPandasIOManager(database=DUCKDB_PATH),

    # Used by dbt assets to execute dbt CLI commands (build, run, test, snapshot)
    # Target must exist in your profiles.yml
    "dbt": DbtCliResource(
        project_dir=DBT_PROJECT_DIR,
        profiles_dir=DBT_PROFILES_DIR,
        target="dev",
        # env={"DBT_ENV_SECRET_SOMETHING": "value"}  # Example to pass secrets to dbt
    ),
}


# ------------------------------------------------------------------------------
# Jobs: orchestration of assets
# ------------------------------------------------------------------------------

# Build everything (Python + dbt)
build_all = define_asset_job(
    name="build_all",
    selection=AssetSelection.all(),
    description="Materialize raw ingestion and all dbt models/tests.",
)

# Only run dbt models (skip raw ingestion)
build_dbt_only = define_asset_job(
    name="build_dbt_only",
    selection=AssetSelection.assets(*dbt_assets),
    description="Materialize dbt models/tests only.",
)

# Example: run a subset of dbt models by key; replace with your model names
# Asset keys: AssetKey(["warehouse", "<model_name>"])
important_models = [
    AssetKey(["warehouse", "customers"]),
    AssetKey(["warehouse", "orders_enriched"]),
]
build_important = define_asset_job(
    name="build_important_models",
    selection=AssetSelection.keys(*important_models),
    description="Materialize only the most important dbt models.",
)


# ------------------------------------------------------------------------------
# Schedule
# ------------------------------------------------------------------------------

daily_2am_utc = ScheduleDefinition(
    job=build_all,
    cron_schedule="0 2 * * *",
    execution_timezone="UTC",
)


# ------------------------------------------------------------------------------
# Sensor (file-based): trigger when new CSVs are dropped
# ------------------------------------------------------------------------------

DATA_DIR = HERE / "data"
DATA_DIR.mkdir(exist_ok=True)

def iter_new_files(context: SensorEvaluationContext, suffix: str = ".csv") -> Iterable[Path]:
    for p in DATA_DIR.glob(f"*{suffix}"):
        # Use file timestamp as a simple idempotency key
        mtime = p.stat().st_mtime_ns
        key = f"{p.name}:{mtime}"
        if not context.instance.get_run_records(filters=None, limit=1):
            # If needed, you can look up by run_key in custom storage;
            # for simplicity we just always yield here.
            pass
        yield p

@sensor(job=build_all, default_status=DefaultSensorStatus.STOPPED)
def new_data_sensor(context: SensorEvaluationContext):
    """
    Watches DATA_DIR for new CSV files and triggers the 'build_all' job when a new file appears.
    Replace logic with your own external trigger criteria.
    """
    for _ in iter_new_files(context):
        # A real-world sensor would include a unique run_key to avoid duplicate runs.
        yield RunRequest(
            run_key=None,
            tags={"trigger": "new_data"},
        )
        break


# ------------------------------------------------------------------------------
# Definitions: the Dagster "repo"
# ------------------------------------------------------------------------------

defs = Definitions(
    assets=[orders, *dbt_assets],
    asset_checks=list(dbt_checks),
    resources=resources,
    schedules=[daily_2am_utc],
    jobs=[build_all, build_dbt_only, build_important],
    sensors=[new_data_sensor],
)


# ------------------------------------------------------------------------------
# Programmatic run helpers (run and manage)
# ------------------------------------------------------------------------------

def run_all_in_process():
    """
    Run everything in-process (no Dagster daemon/webserver required).
    Good for CI or quick local runs.
    """
    result = build_all.execute_in_process()
    assert result.success
    return result


def materialize_only_model(model_name: str):
    """
    Programmatically re-materialize a single dbt model by name.
    Example: materialize_only_model("customers")
    """
    key = AssetKey(["warehouse", model_name])
    selection = [a for a in dbt_assets if a.key == key]
    if not selection:
        raise ValueError(f"Model not found in loaded dbt assets: {model_name}")
    result = materialize_assets(selection, resources=resources)
    assert result.success
    return result


if __name__ == "__main__":
    # Example runs when executing this file directly.

    # 1) Build everything
    run_all_in_process()

    # 2) Or selectively build a single dbt model
    try:
        materialize_only_model("customers")
    except Exception as e:
        print(f"Selective materialization skipped or failed: {e}")
    
    print("Done.")
------------------------------------------------------------

PROMPT 432:
"Generate Python code that imports the YAML module and extends the pydantic library with YAML support by adding a new mode for loading and dumping data in YAML format."
RESPONSE:
import yaml
import pydantic
from pydantic import BaseModel

def enable_pydantic_yaml_mode(dump_kwargs=None):
    """
    Extends pydantic BaseModel with a new YAML mode:
      - model_dump(mode='yaml') -> str (YAML)
      - BaseModel.model_validate(obj, mode='yaml') -> instance (obj is YAML str/bytes)

    dump_kwargs: optional dict passed to yaml.safe_dump (e.g., {'sort_keys': False})
    """
    dump_kwargs = dict(dump_kwargs or {})
    # Reasonable defaults: keep field order and readable block style
    dump_kwargs.setdefault('sort_keys', False)
    dump_kwargs.setdefault('default_flow_style', False)

    # Only support pydantic v2+ (model_dump/model_validate exist)
    version = getattr(pydantic, 'VERSION', '2')
    major = int(str(version).split('.')[0])
    if major < 2:
        raise RuntimeError('YAML mode patch requires pydantic v2 or newer.')

    # Avoid double patching
    if getattr(BaseModel, '__yaml_mode_enabled__', False):
        return

    # Keep originals
    _orig_model_dump = BaseModel.model_dump
    _orig_model_validate = BaseModel.model_validate

    def _model_dump_with_yaml(self, *args, **kwargs):
        mode = kwargs.get('mode', None)
        if mode == 'yaml':
            # First dump to plain Python data, then to YAML
            kwargs = dict(kwargs)
            kwargs['mode'] = 'python'
            data = _orig_model_dump(self, *args, **kwargs)
            return yaml.safe_dump(data, **dump_kwargs)
        return _orig_model_dump(self, *args, **kwargs)

    @classmethod
    def _model_validate_with_yaml(cls, obj, *args, **kwargs):
        mode = kwargs.pop('mode', None)
        if mode == 'yaml':
            if isinstance(obj, (bytes, str)):
                obj = yaml.safe_load(obj)
            # If obj is already a dict/list, just pass it through
            return _orig_model_validate(obj, *args, **kwargs)
        return _orig_model_validate(obj, *args, **kwargs)

    # Patch BaseModel
    BaseModel.model_dump = _model_dump_with_yaml
    BaseModel.model_validate = _model_validate_with_yaml
    BaseModel.__yaml_mode_enabled__ = True


# Optional convenience helpers without patching signatures further
def model_dump_yaml(model: BaseModel, **dump_kwargs) -> str:
    """
    Dump a BaseModel instance to a YAML string without needing mode='yaml'.
    """
    return yaml.safe_dump(model.model_dump(mode='python'), sort_keys=False, default_flow_style=False, **dump_kwargs)


def model_validate_yaml(model_cls: type[BaseModel], data: str | bytes) -> BaseModel:
    """
    Validate a YAML string/bytes into a BaseModel instance without needing mode='yaml'.
    """
    return model_cls.model_validate(yaml.safe_load(data))


# Enable YAML mode on import; comment out if you prefer manual control.
enable_pydantic_yaml_mode()

# Example usage:
# class User(BaseModel):
#     id: int
#     name: str
#
# u = User(id=1, name='Ada')
# y = u.model_dump(mode='yaml')
# u2 = User.model_validate(y, mode='yaml')
------------------------------------------------------------

PROMPT 433:
"Generate Python code that creates a metadata storage module using Invenio-Records, allowing users to store, manage, and retrieve metadata for various digital objects."
RESPONSE:
Below is a minimal, self-contained Python module that uses Invenio-Records to store, manage, and retrieve metadata for digital objects. It exposes:

- A simple Flask-based REST API (POST/GET/PATCH/PUT/DELETE) for records
- A small programmatic wrapper (MetadataStore) you can import and use inside Python code

Dependencies:
- Flask
- invenio-records
- invenio-db
- marshmallow

Install:
- pip install Flask invenio-records invenio-db marshmallow

Save as metadata_storage.py and run: python metadata_storage.py
It will start a dev server on http://127.0.0.1:5000

Code:

#!/usr/bin/env python3
"""
A minimal metadata storage module built on Invenio-Records.

Features:
- Persistence via SQLAlchemy/Invenio-DB
- CRUD via Invenio-Records API
- REST endpoints for external clients
- Programmatic API for in-process usage

Note:
- Uses SQLite for demo purposes. For production, use PostgreSQL and proper migrations.
"""

import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

from flask import Flask, Blueprint, jsonify, request, abort
from marshmallow import Schema, fields, ValidationError, INCLUDE
from sqlalchemy.exc import SQLAlchemyError

from invenio_db import InvenioDB, db
from invenio_records import InvenioRecords
from invenio_records.api import Record
from invenio_records.models import RecordMetadataBase


# ---------------------------
# Database model and Record API
# ---------------------------

class ObjectMetadata(db.Model, RecordMetadataBase):
    """Database table to store metadata for digital objects."""
    __tablename__ = "object_metadata"


class ObjectRecord(Record):
    """API wrapper around the ObjectMetadata model."""
    model_cls = ObjectMetadata


# ---------------------------
# Validation schema (flexible)
# ---------------------------

class GenericMetadataSchema(Schema):
    """
    Flexible schema: accepts any fields by default.
    Add optional "recommended" fields to guide clients.
    """
    class Meta:
        unknown = INCLUDE

    # Optional, recommended fields (not required)
    resource_type = fields.String(required=False)
    title = fields.String(required=False)
    description = fields.String(required=False)
    creators = fields.List(
        fields.Dict(keys=fields.String(), values=fields.Raw()), required=False
    )
    tags = fields.List(fields.String(), required=False)


schema = GenericMetadataSchema()


# ---------------------------
# Serialization helpers
# ---------------------------

def record_to_dict(rec: ObjectRecord) -> Dict[str, Any]:
    """Serialize an ObjectRecord to a JSON-friendly dict."""
    model = rec.model
    # created/updated are timezone aware (UTC) in Invenio-Records; convert to ISO strings
    created = model.created.isoformat() if isinstance(model.created, datetime) else None
    updated = model.updated.isoformat() if isinstance(model.updated, datetime) else None
    return {
        "id": str(rec.id),
        "metadata": rec.dumps(),  # the JSON document
        "revision_id": getattr(model, "version_id", None),
        "created": created,
        "updated": updated,
    }


def load_metadata(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Validate/normalize incoming payload using marshmallow schema."""
    try:
        return schema.load(payload)
    except ValidationError as err:
        abort(400, description={"message": "Validation error", "errors": err.messages})


# ---------------------------
# Programmatic API
# ---------------------------

class MetadataStore:
    """Convenience API for programmatic access."""

    @staticmethod
    def create(metadata: Dict[str, Any]) -> Dict[str, Any]:
        data = load_metadata(metadata)
        rec = ObjectRecord.create(data)
        db.session.commit()
        return record_to_dict(rec)

    @staticmethod
    def get(record_id: str) -> Dict[str, Any]:
        rec = ObjectRecord.get_record(record_id)
        return record_to_dict(rec)

    @staticmethod
    def update(record_id: str, metadata: Dict[str, Any], merge: bool = True) -> Dict[str, Any]:
        rec = ObjectRecord.get_record(record_id)
        data = load_metadata(metadata)
        if merge:
            rec.update(data)
        else:
            # Replace entire document
            # Ensure we keep $schema if you use JSONSchema in the future
            rec.clear()
            rec.update(data)
        rec.commit()
        db.session.commit()
        return record_to_dict(rec)

    @staticmethod
    def delete(record_id: str) -> None:
        rec = ObjectRecord.get_record(record_id)
        rec.delete()
        db.session.commit()

    @staticmethod
    def list(page: int = 1, size: int = 10) -> Tuple[List[Dict[str, Any]], int]:
        """Return a page of records and the total count."""
        q = ObjectMetadata.query.order_by(ObjectMetadata.created.desc())
        total = q.count()
        items = q.paginate(page=page, per_page=size, error_out=False).items
        results = [record_to_dict(ObjectRecord.from_model(m)) for m in items]
        return results, total


# ---------------------------
# REST API
# ---------------------------

api = Blueprint("metadata_api", __name__)


@api.errorhandler(ValidationError)
def handle_validation_error(e):
    return jsonify({"message": "Validation error", "errors": e.messages}), 400


@api.errorhandler(SQLAlchemyError)
def handle_db_error(e):
    db.session.rollback()
    return jsonify({"message": "Database error"}), 500


@api.route("/records", methods=["POST"])
def create_record():
    payload = request.get_json(force=True, silent=True)
    if payload is None:
        abort(400, description="Invalid or missing JSON body.")
    out = MetadataStore.create(payload)
    return jsonify(out), 201


@api.route("/records/<rid>", methods=["GET"])
def get_record(rid: str):
    try:
        out = MetadataStore.get(rid)
        return jsonify(out), 200
    except Exception:
        abort(404, description="Record not found.")


@api.route("/records/<rid>", methods=["PUT"])
def put_record(rid: str):
    payload = request.get_json(force=True, silent=True)
    if payload is None:
        abort(400, description="Invalid or missing JSON body.")
    try:
        out = MetadataStore.update(rid, payload, merge=False)
        return jsonify(out), 200
    except Exception:
        abort(404, description="Record not found.")


@api.route("/records/<rid>", methods=["PATCH"])
def patch_record(rid: str):
    payload = request.get_json(force=True, silent=True)
    if payload is None:
        abort(400, description="Invalid or missing JSON body.")
    try:
        out = MetadataStore.update(rid, payload, merge=True)
        return jsonify(out), 200
    except Exception:
        abort(404, description="Record not found.")


@api.route("/records/<rid>", methods=["DELETE"])
def delete_record(rid: str):
    try:
        MetadataStore.delete(rid)
        return "", 204
    except Exception:
        abort(404, description="Record not found.")


@api.route("/records", methods=["GET"])
def list_records():
    try:
        page = int(request.args.get("page", 1))
        size = int(request.args.get("size", 10))
    except ValueError:
        abort(400, description="Invalid pagination parameters.")
    items, total = MetadataStore.list(page=page, size=size)
    return jsonify({
        "hits": {"hits": items, "total": total},
        "page": page,
        "size": size
    }), 200


# ---------------------------
# App factory and bootstrap
# ---------------------------

def create_app() -> Flask:
    app = Flask(__name__)
    # For demo: use SQLite. For production, configure PostgreSQL.
    app.config.update(
        SQLALCHEMY_DATABASE_URI="sqlite:///metadata.db",
        SQLALCHEMY_TRACK_MODIFICATIONS=False,
        JSON_SORT_KEYS=False,
    )

    InvenioDB(app)        # Initialize DB extension
    InvenioRecords(app)   # Initialize Records extension

    with app.app_context():
        db.create_all()   # For demo only; use Alembic migrations in production

    app.register_blueprint(api, url_prefix="/")

    @app.route("/health")
    def health():
        return jsonify(status="ok"), 200

    return app


if __name__ == "__main__":
    app = create_app()
    # Development server
    app.run(host="127.0.0.1", port=5000, debug=True)

Usage examples:

- Programmatic API:
  from metadata_storage import create_app, MetadataStore
  app = create_app()
  with app.app_context():
      rec = MetadataStore.create({"title": "Example", "resource_type": "image", "tags": ["demo"]})
      rid = rec["id"]
      fetched = MetadataStore.get(rid)
      updated = MetadataStore.update(rid, {"description": "Updated"}, merge=True)
      MetadataStore.delete(rid)

- REST API via curl:
  Create:
    curl -X POST http://127.0.0.1:5000/records -H "Content-Type: application/json" -d '{"title":"Example","resource_type":"image"}'
  Get:
    curl http://127.0.0.1:5000/records/<id>
  Update (merge):
    curl -X PATCH http://127.0.0.1:5000/records/<id> -H "Content-Type: application/json" -d '{"description":"Updated"}'
  Replace:
    curl -X PUT http://127.0.0.1:5000/records/<id> -H "Content-Type: application/json" -d '{"title":"Replaced"}'
  Delete:
    curl -X DELETE http://127.0.0.1:5000/records/<id>
  List:
    curl http://127.0.0.1:5000/records?page=1&size=10

Notes:
- This example intentionally keeps validation flexible to support varied metadata. You can tighten the schema as needed.
- For production, use PostgreSQL, configure proper connection pooling, and manage schema using Alembic migrations provided by invenio-db. If you need REST resource/service layers, consider invenio-records-resources as a next step.
------------------------------------------------------------

PROMPT 434:
"Generate Python code that creates a template renderer that takes a template file and a dictionary of variables as input, and outputs the rendered template as a string."
RESPONSE:
# template_renderer.py

from pathlib import Path
from string import Template


class TemplateRenderer:
    """
    A simple template renderer using Python's standard library.

    - Template syntax: use $name or ${name} placeholders in the template file.
    - Example content: "Hello, $user! Today is ${day}."
    """

    def __init__(self, encoding: str = "utf-8", safe: bool = False):
        """
        :param encoding: File encoding used to read templates.
        :param safe: If True, leaves unresolved placeholders intact (safe_substitute).
                     If False, raises KeyError when a placeholder is missing (substitute).
        """
        self.encoding = encoding
        self.safe = safe

    def render(self, template_file: str | Path, variables: dict) -> str:
        """
        Render a template file with the provided variables.

        :param template_file: Path to the template file.
        :param variables: Dictionary of variables for substitution.
        :return: Rendered template as a string.
        """
        path = Path(template_file)
        text = path.read_text(encoding=self.encoding)
        tmpl = Template(text)

        if self.safe:
            return tmpl.safe_substitute(variables)

        try:
            return tmpl.substitute(variables)
        except KeyError as e:
            missing = e.args[0]
            raise KeyError(f"Missing variable '{missing}' required by template '{path}'") from None


# Example usage:
# renderer = TemplateRenderer()
# output = renderer.render("email_template.txt", {"user": "Ada", "day": "Monday"})
# print(output)
------------------------------------------------------------

PROMPT 435:
"Generate Python code that creates fake servers for testing using the `socket` library and customize the response data, headers, and HTTP status codes for each request."
RESPONSE:
# Fake HTTP server for tests using only the socket library.
# - Register static or dynamic routes
# - Customize status code, headers, body per request
# - Minimal HTTP/1.1 parsing, closes connection after each response
#
# Example usage is at the bottom under `if __name__ == "__main__":`

import socket
import threading
import re
import time
from datetime import datetime, timezone
from urllib.parse import urlsplit, parse_qs

# --------------------------- Utilities ---------------------------

def http_date(dt=None):
    if dt is None:
        dt = datetime.now(timezone.utc)
    return dt.strftime("%a, %d %b %Y %H:%M:%S GMT")

def to_bytes(body, encoding="utf-8"):
    if body is None:
        return b""
    if isinstance(body, (bytes, bytearray, memoryview)):
        return bytes(body)
    if isinstance(body, str):
        return body.encode(encoding)
    # Fallback: repr
    return repr(body).encode(encoding)

# --------------------------- Data classes ---------------------------

class Request:
    def __init__(self, method, target, version, headers, body, client_addr):
        self.method = method
        self.target = target
        self.http_version = version
        self.headers = headers  # case-insensitive keys normalized to lower
        self.body = body
        self.client_addr = client_addr

        parts = urlsplit(target)
        self.path = parts.path or "/"
        self.query_string = parts.query or ""
        self.query = parse_qs(self.query_string, keep_blank_values=True)

        # Filled when a route with params matches
        self.path_params = {}

    def header(self, name, default=None):
        return self.headers.get(name.lower(), default)


class Response:
    def __init__(self, status=200, headers=None, body=b""):
        self.status = int(status)
        self.headers = dict(headers or {})
        self.body = to_bytes(body)

# --------------------------- Fake HTTP Server ---------------------------

class FakeHTTPServer:
    def __init__(self, host="127.0.0.1", port=0):
        self.host = host
        self.port = port
        self._sock = None
        self._thread = None
        self._stop_event = threading.Event()
        self._routes = []  # list of (method, regex, handler)
        self._routes_lock = threading.Lock()

    @property
    def address(self):
        if not self._sock:
            return None
        return self._sock.getsockname()

    @property
    def url(self):
        addr = self.address
        if not addr:
            return None
        host, port = addr[0], addr[1]
        return f"http://{host}:{port}"

    def start(self, backlog=50):
        if self._sock:
            return
        self._sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self._sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self._sock.bind((self.host, self.port))
        self._sock.listen(backlog)
        self._sock.settimeout(0.5)
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._serve_forever, name="FakeHTTPServer", daemon=True)
        self._thread.start()

    def stop(self):
        self._stop_event.set()
        if self._sock:
            try:
                self._sock.close()
            except OSError:
                pass
            self._sock = None
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=2.0)
        self._thread = None

    def clear_routes(self):
        with self._routes_lock:
            self._routes.clear()

    def add_static(self, method, path_pattern, status=200, headers=None, body=b""):
        """Register a static response for a method + path pattern."""
        resp = Response(status=status, headers=headers, body=body)
        self._add_route(method, path_pattern, handler=lambda req, params: resp)

    def add_handler(self, method, path_pattern, handler):
        """Register a dynamic handler(request, params) -> (status, headers, body) or Response."""
        def wrapped(req, params):
            res = handler(req, params)
            if isinstance(res, Response):
                return res
            if isinstance(res, tuple):
                # Allow: (status, headers, body) or (status, body)
                if len(res) == 3:
                    status, headers, body = res
                elif len(res) == 2:
                    status, body = res
                    headers = {}
                else:
                    raise ValueError("Handler must return Response or (status, headers, body) or (status, body)")
                return Response(status=status, headers=headers, body=body)
            # Fallback: treat as body with 200
            return Response(status=200, headers={}, body=res)
        self._add_route(method, path_pattern, handler=wrapped)

    def _add_route(self, method, path_pattern, handler):
        method = method.upper() if method else "*"
        regex = self._compile_pattern(path_pattern)
        with self._routes_lock:
            self._routes.append((method, regex, handler))

    @staticmethod
    def _compile_pattern(pattern):
        # Supports:
        # - Exact paths: "/ping"
        # - Path params: "/users/{id}" -> (?P<id>[^/]+)
        # - Raw regex via re.Pattern
        if isinstance(pattern, re.Pattern):
            return pattern
        if not pattern.startswith("/"):
            raise ValueError("path_pattern must start with '/'")
        # Escape and replace {param} with named groups
        def repl(match):
            name = match.group(1)
            return f"(?P<{name}>[^/]+)"

        # Convert to regex: treat as literal except {name}
        # First, escape regex metacharacters, then un-escape braces we plan to handle
        esc = re.escape(pattern)
        esc = esc.replace(r"\{", "{").replace(r"\}", "}")
        esc = re.sub(r"\{([A-Za-z_][A-Za-z0-9_]*)\}", repl, esc)
        regex = f"^{esc}$"
        return re.compile(regex)

    def _serve_forever(self):
        while not self._stop_event.is_set():
            try:
                conn, addr = self._sock.accept()
            except socket.timeout:
                continue
            except OSError:
                # Socket closed during stop
                break
            threading.Thread(target=self._handle_client, args=(conn, addr), daemon=True).start()

    def _handle_client(self, conn, addr):
        try:
            conn.settimeout(5.0)
            request = self._read_request(conn, addr)
            if request is None:
                # Malformed or timeout
                return
            response = self._dispatch(request)
            self._send_response(conn, request, response)
        except Exception:
            # Best-effort internal error response
            try:
                resp = Response(status=500, headers={"Content-Type": "text/plain"}, body=b"Internal Server Error")
                # Fake a minimal request to figure out HEAD vs others
                self._send_response(conn, Request("GET", "/", "HTTP/1.1", {}, b"", addr), resp)
            except Exception:
                pass
        finally:
            try:
                conn.shutdown(socket.SHUT_RDWR)
            except Exception:
                pass
            try:
                conn.close()
            except Exception:
                pass

    def _read_request(self, conn, addr):
        # Read headers
        data = b""
        header_end = -1
        while True:
            chunk = conn.recv(4096)
            if not chunk:
                break
            data += chunk
            header_end = data.find(b"\r\n\r\n")
            if header_end != -1:
                break
            if len(data) > 65536:  # 64 KiB header cap
                return None
        if header_end == -1:
            return None

        header_bytes = data[:header_end].decode("iso-8859-1", errors="replace")
        rest = data[header_end + 4:]

        lines = header_bytes.split("\r\n")
        if not lines:
            return None

        # Request line: METHOD SP REQUEST_TARGET SP HTTP_VERSION
        try:
            method, target, version = lines[0].split(" ", 2)
        except ValueError:
            return None

        headers = {}
        for line in lines[1:]:
            if not line or ":" not in line:
                continue
            name, value = line.split(":", 1)
            headers[name.strip().lower()] = value.strip()

        # Determine body length
        body = b""
        content_length = headers.get("content-length")
        if content_length is not None:
            try:
                length = int(content_length)
                need = max(0, length - len(rest))
                body = rest
                while need > 0:
                    chunk = conn.recv(min(4096, need))
                    if not chunk:
                        break
                    body += chunk
                    need -= len(chunk)
                body = body[:length]
            except ValueError:
                body = rest
        else:
            # Ignore chunked for simplicity in this fake server
            body = rest

        return Request(method.upper(), target, version, headers, body, addr)

    def _dispatch(self, request):
        path = request.path
        method = request.method
        with self._routes_lock:
            routes = list(self._routes)

        for m, regex, handler in routes:
            if m != "*" and m != method:
                continue
            match = regex.match(path)
            if not match:
                continue
            request.path_params = match.groupdict()
            try:
                return handler(request, request.path_params)
            except Exception as e:
                return Response(
                    status=500,
                    headers={"Content-Type": "text/plain"},
                    body=f"Handler error: {e}"
                )

        return Response(
            status=404,
            headers={"Content-Type": "text/plain"},
            body=f"No route for {method} {path}"
        )

    def _send_response(self, conn, request, response):
        # Ensure required headers
        headers = {
            "Date": http_date(),
            "Server": "FakeHTTPServer/1.0",
            "Connection": "close",
        }
        # Merge custom headers, allowing user to override defaults
        for k, v in (response.headers or {}).items():
            headers[k] = v

        body = response.body or b""
        # For HEAD, do not send a body but include Content-Length of what GET would return
        is_head = (request.method == "HEAD")
        if is_head:
            headers["Content-Length"] = str(len(body))
            body_to_send = b""
        else:
            headers["Content-Length"] = str(len(body))
            body_to_send = body

        status_line = f"HTTP/1.1 {response.status} {self._reason_phrase(response.status)}\r\n"

        header_lines = "".join(f"{k}: {v}\r\n" for k, v in headers.items())
        raw = status_line.encode("ascii") + header_lines.encode("iso-8859-1") + b"\r\n" + body_to_send
        conn.sendall(raw)

    @staticmethod
    def _reason_phrase(status):
        # Minimal mapping
        reasons = {
            200: "OK",
            201: "Created",
            202: "Accepted",
            204: "No Content",
            301: "Moved Permanently",
            302: "Found",
            304: "Not Modified",
            400: "Bad Request",
            401: "Unauthorized",
            403: "Forbidden",
            404: "Not Found",
            405: "Method Not Allowed",
            409: "Conflict",
            415: "Unsupported Media Type",
            418: "I'm a teapot",
            429: "Too Many Requests",
            500: "Internal Server Error",
            503: "Service Unavailable",
        }
        return reasons.get(status, "OK")

# --------------------------- Example usage ---------------------------

if __name__ == "__main__":
    import http.client
    import json

    server = FakeHTTPServer()
    server.start()
    print("Server listening at:", server.url)

    # Static route with custom headers and status
    server.add_static(
        "GET",
        "/ping",
        status=200,
        headers={"Content-Type": "text/plain", "X-Test": "yes"},
        body="pong"
    )

    # Dynamic route using path params and query
    def user_handler(req, params):
        user_id = params.get("id")
        verbose = req.query.get("verbose", ["0"])[0] == "1"
        payload = {"id": user_id, "verbose": verbose, "client": req.client_addr[0]}
        body = json.dumps(payload)
        return 200, {"Content-Type": "application/json"}, body

    server.add_handler("GET", "/users/{id}", user_handler)

    # Echo POST body, set 201 status and custom header
    def echo_handler(req, params):
        return Response(
            status=201,
            headers={"Content-Type": req.header("content-type", "application/octet-stream"),
                     "X-Echoed": "true"},
            body=req.body
        )

    server.add_handler("POST", "/echo", echo_handler)

    # Example client calls using http.client
    time.sleep(0.1)  # allow server to start
    host, port = server.address

    def do_request(method, path, body=None, headers=None):
        conn = http.client.HTTPConnection(host, port, timeout=3)
        conn.request(method, path, body=body, headers=headers or {})
        resp = conn.getresponse()
        data = resp.read()
        conn.close()
        print(f"{method} {path} -> {resp.status} {resp.reason}")
        for k, v in resp.getheaders():
            if k.lower().startswith("x-") or k.lower() in ("content-type", "content-length"):
                print(f"  {k}: {v}")
        print("  Body:", data.decode("utf-8", errors="replace"))
        print()

    do_request("GET", "/ping")
    do_request("GET", "/users/42?verbose=1")
    do_request("POST", "/echo", body=b'{"hello":"world"}', headers={"Content-Type": "application/json"})
    do_request("HEAD", "/ping")

    server.stop()
    print("Server stopped.")
------------------------------------------------------------

PROMPT 436:
"Generate Python code that imports the necessary modules, creates a Y-CRDT data structure using the yrs library, and provides methods for updating and querying the CRDT."
RESPONSE:
# Requires: pip install y-py
# Note: y_py is the official Python binding for Yrs (the Yjs CRDT core in Rust).

from typing import Any, Optional, Dict, List
from y_py import YDoc


class YCRDT:
    """
    A small wrapper around a Y-CRDT document using y_py (Yrs Python bindings).
    Provides basic update and query operations for a shared text, map, and array.
    """

    def __init__(self, text_name: str = "text", map_name: str = "root", array_name: str = "items") -> None:
        self.doc = YDoc()
        # Named top-level shared types
        self._text = self.doc.get_text(text_name)
        self._map = self.doc.get_map(map_name)
        self._array = self.doc.get_array(array_name)

    # ----------------
    # Text operations
    # ----------------
    def insert_text(self, index: int, content: str) -> None:
        """Insert content into the shared text at a given index."""
        with self.doc.begin_transaction() as txn:
            self._text.insert(txn, index, content)

    def delete_text(self, index: int, length: int) -> None:
        """Delete a range of characters from the shared text."""
        with self.doc.begin_transaction() as txn:
            self._text.remove_range(txn, index, length)

    def get_text(self) -> str:
        """Return the entire shared text as a Python string."""
        with self.doc.begin_transaction() as txn:
            return self._text.to_string(txn)

    # ----------------
    # Map operations
    # ----------------
    def set_key(self, key: str, value: Any) -> None:
        """
        Set a key in the shared map.
        Values should be JSON-serializable primitives or nested Y types created via the doc.
        """
        with self.doc.begin_transaction() as txn:
            self._map.set(txn, key, value)

    def get_key(self, key: str, default: Any = None) -> Any:
        """Get a key from the shared map, returning default if missing."""
        with self.doc.begin_transaction() as txn:
            val = self._map.get(txn, key)
            return default if val is None else val

    def map_to_dict(self) -> Dict[str, Any]:
        """Return the shared map as a JSON-serializable Python dict."""
        with self.doc.begin_transaction() as txn:
            return self._map.to_json(txn)

    # ----------------
    # Array operations
    # ----------------
    def append_item(self, value: Any) -> None:
        """Append a value to the shared array."""
        with self.doc.begin_transaction() as txn:
            self._array.append(txn, value)

    def insert_items(self, index: int, values: List[Any]) -> None:
        """Insert a range of values at a specific index in the shared array."""
        with self.doc.begin_transaction() as txn:
            # insert_range places the provided iterable at index
            self._array.insert_range(txn, index, values)

    def get_items(self) -> List[Any]:
        """Return the shared array as a JSON-serializable Python list."""
        with self.doc.begin_transaction() as txn:
            return self._array.to_json(txn)

    # ----------------
    # Document sync (CRDT updates)
    # ----------------
    def state_vector(self) -> bytes:
        """
        Return the document's state vector (compact summary of known updates).
        Peers exchange this to request only missing updates.
        """
        return self.doc.state_vector_v1()

    def diff_update(self, remote_state_vector: Optional[bytes] = None) -> bytes:
        """
        Return an encoded update containing all changes the remote (described
        by remote_state_vector) is missing. If remote_state_vector is None,
        the full document state is returned.
        """
        if remote_state_vector is None:
            # An empty state vector (b"") requests the full state
            remote_state_vector = b""
        return self.doc.encode_state_as_update_v1(remote_state_vector)

    def apply_update(self, update: bytes) -> None:
        """
        Apply an encoded update from a peer. Safe to call repeatedly and out-of-order.
        """
        self.doc.apply_update_v1(update)

    # ----------------
    # Snapshot helpers
    # ----------------
    def to_json(self) -> Dict[str, Any]:
        """Materialize a JSON-serializable snapshot of the top-level shared types."""
        with self.doc.begin_transaction() as txn:
            return {
                "text": self._text.to_string(txn),
                "map": self._map.to_json(txn),
                "array": self._array.to_json(txn),
            }


if __name__ == "__main__":
    # Example usage
    alice = YCRDT()
    bob = YCRDT()

    # Alice updates
    alice.insert_text(0, "Hello")
    alice.set_key("counter", 1)
    alice.append_item("A")

    # Generate update for Bob and apply
    update_for_bob = alice.diff_update(bob.state_vector())
    bob.apply_update(update_for_bob)

    # Bob updates
    bob.insert_text(5, " world")
    bob.set_key("counter", bob.get_key("counter", 0) + 1)
    bob.append_item("B")

    # Sync back to Alice
    update_for_alice = bob.diff_update(alice.state_vector())
    alice.apply_update(update_for_alice)

    # Query both CRDTs; they converge
    print("Alice:", alice.to_json())
    print("Bob:  ", bob.to_json())
------------------------------------------------------------

PROMPT 437:
"Generate Python code that uses the Sklearn library to create a decision tree classifier, trains it on a dataset, and then uses the tree to make predictions on new data."
RESPONSE:
# pip install scikit-learn numpy
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1) Load a dataset (Iris)
iris = load_iris()
X, y = iris.data, iris.target

# 2) Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 3) Create and train the Decision Tree classifier
clf = DecisionTreeClassifier(
    criterion="gini",    # or "entropy", "log_loss"
    max_depth=None,      # you can set e.g., 3 to limit depth
    random_state=42
)
clf.fit(X_train, y_train)

# 4) Evaluate on the test set
y_pred = clf.predict(X_test)
print("Test accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification report:\n", classification_report(y_test, y_pred, target_names=iris.target_names))

# 5) Use the trained tree to make predictions on new data
# Each sample must have 4 features: [sepal length, sepal width, petal length, petal width]
new_samples = np.array([
    [5.1, 3.5, 1.4, 0.2],  # likely setosa
    [6.7, 3.1, 4.7, 1.5],  # likely versicolor
    [7.2, 3.0, 5.8, 1.6]   # likely virginica
])

new_preds = clf.predict(new_samples)
new_pred_names = iris.target_names[new_preds]
new_probas = clf.predict_proba(new_samples)

for i, (x, pred_idx, pred_name, proba) in enumerate(zip(new_samples, new_preds, new_pred_names, new_probas)):
    print(f"Sample {i}: {x} -> class {pred_idx} ({pred_name}), probabilities={np.round(proba, 3)}")
------------------------------------------------------------

PROMPT 438:
"Generate Python code that creates a database using SQLite, allows users to create accounts and log in, and includes a dashboard for managing user accounts and databases."
RESPONSE:
# Simple Flask + SQLite app with user accounts, login, and an admin dashboard
# ---------------------------------------------------------------
# Requirements:
#   pip install flask werkzeug
#
# Run:
#   python app.py
#
# Notes:
# - Uses SQLite for the main application database (app.db).
# - Stores additional user-created SQLite databases under ./data/.
# - First registered user becomes an admin automatically.
# - Admins can manage users and create/delete/list extra databases,
#   inspect tables, and run read-only SELECT queries.
# - This is a minimal example and omits CSRF protection and email verification.
#   For production, add CSRF protection and harden security settings.
# ---------------------------------------------------------------

import os
import re
import sqlite3
from pathlib import Path
from functools import wraps
from datetime import datetime

from flask import (
    Flask, request, redirect, url_for, session, abort,
    render_template_string, flash
)
from werkzeug.security import generate_password_hash, check_password_hash

APP_DB_PATH = "app.db"
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

app = Flask(__name__)
app.config["SECRET_KEY"] = os.environ.get("SECRET_KEY", "dev-secret-change-me")
app.config["SESSION_COOKIE_HTTPONLY"] = True
app.config["SESSION_COOKIE_SAMESITE"] = "Lax"  # Consider "Strict" and HTTPS in production

# -------------------- Database helpers --------------------

def get_app_db():
    conn = sqlite3.connect(APP_DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

def init_app_db():
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            username TEXT UNIQUE NOT NULL,
            password_hash TEXT NOT NULL,
            is_admin INTEGER NOT NULL DEFAULT 0,
            is_active INTEGER NOT NULL DEFAULT 1,
            created_at TEXT NOT NULL DEFAULT (DATETIME('now'))
        )
    """)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS databases (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT UNIQUE NOT NULL,
            path TEXT UNIQUE NOT NULL,
            created_by INTEGER NOT NULL,
            created_at TEXT NOT NULL DEFAULT (DATETIME('now')),
            FOREIGN KEY(created_by) REFERENCES users(id)
        )
    """)
    conn.commit()
    conn.close()

def count_users():
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) AS c FROM users")
    c = cur.fetchone()["c"]
    conn.close()
    return c

def load_user(user_id):
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("SELECT * FROM users WHERE id = ?", (user_id,))
    row = cur.fetchone()
    conn.close()
    return row

def create_user(username, password, is_admin=False):
    username = username.strip()
    if not username or not password:
        raise ValueError("Username and password are required.")
    if not re.fullmatch(r"[A-Za-z0-9_.-]{3,32}", username):
        raise ValueError("Username must be 3-32 chars (letters, digits, _ . -).")

    password_hash = generate_password_hash(password)
    conn = get_app_db()
    cur = conn.cursor()
    try:
        cur.execute(
            "INSERT INTO users (username, password_hash, is_admin) VALUES (?, ?, ?)",
            (username, password_hash, 1 if is_admin else 0),
        )
        conn.commit()
    except sqlite3.IntegrityError as e:
        raise ValueError(f"Username '{username}' is already taken.") from e
    finally:
        conn.close()

def authenticate(username, password):
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("SELECT * FROM users WHERE username = ?", (username,))
    row = cur.fetchone()
    conn.close()
    if not row:
        return None
    if row["is_active"] != 1:
        return None
    if check_password_hash(row["password_hash"], password):
        return row
    return None

# -------------------- Auth helpers --------------------

def login_required(f):
    @wraps(f)
    def wrapper(*args, **kwargs):
        if "user_id" not in session:
            return redirect(url_for("login", next=request.path))
        user = load_user(session["user_id"])
        if not user or user["is_active"] != 1:
            session.clear()
            return redirect(url_for("login"))
        return f(*args, **kwargs)
    return wrapper

def admin_required(f):
    @wraps(f)
    def wrapper(*args, **kwargs):
        if "user_id" not in session:
            return redirect(url_for("login", next=request.path))
        user = load_user(session["user_id"])
        if not user or user["is_active"] != 1:
            session.clear()
            return redirect(url_for("login"))
        if user["is_admin"] != 1:
            abort(403)
        return f(*args, **kwargs)
    return wrapper

# -------------------- Utility for managing extra SQLite DBs --------------------

VALID_DB_NAME = re.compile(r"^[A-Za-z0-9_.-]{1,64}$")

def safe_db_filename(name: str) -> Path:
    name = name.strip()
    if not name or not VALID_DB_NAME.fullmatch(name):
        raise ValueError("Invalid database name. Use letters, digits, _ . - (max 64).")
    filename = name if name.endswith(".db") else f"{name}.db"
    p = DATA_DIR / filename
    # Ensure under DATA_DIR only
    p = p.resolve()
    if DATA_DIR.resolve() not in p.parents:
        raise ValueError("Unsafe path.")
    return p

def create_sqlite_db_file(db_path: Path):
    if db_path.exists():
        raise ValueError("Database file already exists.")
    conn = sqlite3.connect(str(db_path))
    try:
        conn.execute("PRAGMA journal_mode=WAL")
        conn.commit()
    finally:
        conn.close()

def list_tables(db_path: Path):
    conn = sqlite3.connect(str(db_path))
    conn.row_factory = sqlite3.Row
    try:
        cur = conn.cursor()
        cur.execute("SELECT name, type FROM sqlite_master WHERE type IN ('table','view') ORDER BY name")
        return [dict(row) for row in cur.fetchall()]
    finally:
        conn.close()

def run_safe_select(db_path: Path, query: str, params: tuple = ()):
    q = query.strip().strip(";")
    # Allow only single-statement SELECT (no semicolons, no PRAGMA/ATTACH/etc.)
    if not q.lower().startswith("select "):
        raise ValueError("Only SELECT queries are allowed.")
    if ";" in q:
        raise ValueError("Multiple statements are not allowed.")
    # Very basic blocklist to mitigate obvious dangers
    blocked = [" attach ", " detach ", " pragma ", " insert ", " update ", " delete ", " drop ", " create ", " alter ", " vacuum ", " reindex "]
    low = " " + q.lower() + " "
    if any(b in low for b in blocked):
        raise ValueError("Query contains disallowed keywords.")
    conn = sqlite3.connect(str(db_path))
    conn.row_factory = sqlite3.Row
    try:
        cur = conn.cursor()
        cur.execute(q, params)
        cols = [d[0] for d in cur.description] if cur.description else []
        rows = cur.fetchall()
        return cols, [tuple(r) for r in rows]
    finally:
        conn.close()

# -------------------- Templates --------------------

base_tpl = """
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>{{ title or "App" }}</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; margin: 0; background: #f6f7fb; color: #222; }
    header, footer { background: #1f2937; color: #fff; padding: 10px 16px; }
    a { color: #2563eb; text-decoration: none; }
    .container { max-width: 1000px; margin: 0 auto; padding: 16px; }
    nav a { margin-right: 10px; color: #e5e7eb; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 8px; padding: 16px; margin: 12px 0; }
    .row { display: flex; gap: 12px; flex-wrap: wrap; }
    .col { flex: 1; min-width: 240px; }
    label { display: block; margin-top: 8px; }
    input, select, textarea { width: 100%; padding: 8px; border: 1px solid #cbd5e1; border-radius: 6px; }
    button { padding: 8px 12px; border: none; border-radius: 6px; background: #2563eb; color: white; cursor: pointer; }
    button.danger { background: #dc2626; }
    .flash { padding: 10px; border-radius: 6px; margin-bottom: 12px; }
    .ok { background: #ecfdf5; color: #065f46; border: 1px solid #10b981; }
    .err { background: #fef2f2; color: #991b1b; border: 1px solid #ef4444; }
    table { width: 100%; border-collapse: collapse; margin-top: 8px; }
    th, td { padding: 8px; border-bottom: 1px solid #e5e7eb; text-align: left; }
    .muted { color: #6b7280; font-size: 0.9em; }
  </style>
</head>
<body>
<header>
  <div class="container">
    <div style="display:flex;align-items:center;justify-content:space-between;">
      <div><strong>SQLite Admin</strong></div>
      <nav>
        {% if session.get('user_id') %}
          <a href="{{ url_for('dashboard') }}">Dashboard</a>
          <a href="{{ url_for('users') }}">Users</a>
          <a href="{{ url_for('databases') }}">Databases</a>
          <a href="{{ url_for('logout') }}">Logout</a>
        {% else %}
          <a href="{{ url_for('login') }}">Login</a>
          <a href="{{ url_for('register') }}">Register</a>
        {% endif %}
      </nav>
    </div>
  </div>
</header>
<div class="container">
  {% with msgs = get_flashed_messages(with_categories=true) %}
    {% if msgs %}
      {% for cat, m in msgs %}
        <div class="flash {{ 'ok' if cat=='ok' else 'err' }}">{{ m }}</div>
      {% endfor %}
    {% endif %}
  {% endwith %}
  {% block content %}{% endblock %}
</div>
<footer>
  <div class="container muted">SQLite Admin Example • {{ now }}</div>
</footer>
</body>
</html>
"""

login_tpl = """
{% extends "base" %}
{% block content %}
<div class="card" style="max-width:420px;margin:40px auto;">
  <h2>Login</h2>
  <form method="post">
    <label>Username
      <input name="username" required>
    </label>
    <label>Password
      <input name="password" type="password" required>
    </label>
    <input type="hidden" name="next" value="{{ request.args.get('next','') }}">
    <div style="margin-top:12px;">
      <button type="submit">Login</button>
    </div>
  </form>
  <div class="muted" style="margin-top:8px;">No account? <a href="{{ url_for('register') }}">Register</a></div>
</div>
{% endblock %}
"""

register_tpl = """
{% extends "base" %}
{% block content %}
<div class="card" style="max-width:480px;margin:40px auto;">
  <h2>Register</h2>
  {% if first_user %}
    <div class="flash ok">No users yet. The first account will be an admin.</div>
  {% endif %}
  <form method="post">
    <label>Username
      <input name="username" required pattern="[A-Za-z0-9_.-]{3,32}" title="3-32 chars: letters, digits, _ . -">
    </label>
    <label>Password
      <input name="password" type="password" required minlength="6">
    </label>
    <div style="margin-top:12px;">
      <button type="submit">Create account</button>
    </div>
  </form>
  <div class="muted" style="margin-top:8px;">Already have an account? <a href="{{ url_for('login') }}">Login</a></div>
</div>
{% endblock %}
"""

dashboard_tpl = """
{% extends "base" %}
{% block content %}
<h2>Dashboard</h2>
<div class="row">
  <div class="col">
    <div class="card">
      <h3>Welcome, {{ user['username'] }}</h3>
      <p class="muted">Joined {{ user['created_at'] }}{% if user['is_admin']==1 %} • Admin{% endif %}</p>
      <form method="post" action="{{ url_for('change_password') }}">
        <h4>Change password</h4>
        <label>Current password
          <input type="password" name="current_password" required>
        </label>
        <label>New password
          <input type="password" name="new_password" required minlength="6">
        </label>
        <div style="margin-top:8px;"><button type="submit">Update password</button></div>
      </form>
    </div>
  </div>
  <div class="col">
    <div class="card">
      <h3>Quick stats</h3>
      <p>Total users: <strong>{{ total_users }}</strong></p>
      <p>Managed databases: <strong>{{ total_dbs }}</strong></p>
      <div style="margin-top:8px;">
        <a href="{{ url_for('users') }}"><button>Manage Users</button></a>
        <a href="{{ url_for('databases') }}"><button>Manage Databases</button></a>
      </div>
    </div>
  </div>
</div>
{% if user['is_admin']==1 %}
<div class="card">
  <h3>Admin shortcuts</h3>
  <form method="post" action="{{ url_for('create_db') }}">
    <div class="row">
      <div class="col">
        <label>New database name
          <input name="name" placeholder="example.db" required>
        </label>
      </div>
      <div class="col" style="display:flex;align-items:flex-end;">
        <button type="submit">Create database</button>
      </div>
    </div>
  </form>
</div>
{% endif %}
{% endblock %}
"""

users_tpl = """
{% extends "base" %}
{% block content %}
<h2>Users</h2>
{% if current_user['is_admin'] != 1 %}
  <div class="flash err">You must be an admin to view this page.</div>
{% else %}
<div class="card">
  <h3>Create user</h3>
  <form method="post" action="{{ url_for('users_create') }}">
    <div class="row">
      <div class="col">
        <label>Username
          <input name="username" required pattern="[A-Za-z0-9_.-]{3,32}">
        </label>
      </div>
      <div class="col">
        <label>Password
          <input name="password" type="password" required minlength="6">
        </label>
      </div>
      <div class="col">
        <label>Admin?
          <select name="is_admin">
            <option value="0">No</option>
            <option value="1">Yes</option>
          </select>
        </label>
      </div>
    </div>
    <div style="margin-top:8px;"><button type="submit">Add user</button></div>
  </form>
</div>

<div class="card">
  <h3>All users</h3>
  <table>
    <thead><tr><th>ID</th><th>Username</th><th>Admin</th><th>Active</th><th>Created</th><th>Actions</th></tr></thead>
    <tbody>
      {% for u in users %}
      <tr>
        <td>{{ u.id }}</td>
        <td>{{ u.username }}</td>
        <td>{{ 'Yes' if u.is_admin==1 else 'No' }}</td>
        <td>{{ 'Yes' if u.is_active==1 else 'No' }}</td>
        <td>{{ u.created_at }}</td>
        <td>
          {% if current_user['id'] != u.id %}
            <form method="post" action="{{ url_for('users_toggle_active', user_id=u.id) }}" style="display:inline;">
              <button type="submit">{{ 'Disable' if u.is_active==1 else 'Enable' }}</button>
            </form>
            <form method="post" action="{{ url_for('users_toggle_admin', user_id=u.id) }}" style="display:inline;">
              <button type="submit">{{ 'Make user' if u.is_admin==1 else 'Make admin' }}</button>
            </form>
            <form method="post" action="{{ url_for('users_delete', user_id=u.id) }}" style="display:inline;" onsubmit="return confirm('Delete user {{ u.username }}?');">
              <button type="submit" class="danger">Delete</button>
            </form>
          {% else %}
            <span class="muted">This is you</span>
          {% endif %}
        </td>
      </tr>
      {% endfor %}
    </tbody>
  </table>
</div>
{% endif %}
{% endblock %}
"""

databases_tpl = """
{% extends "base" %}
{% block content %}
<h2>Databases</h2>
{% if current_user['is_admin'] != 1 %}
  <div class="flash err">You must be an admin to view this page.</div>
{% else %}
<div class="card">
  <h3>Create new database</h3>
  <form method="post" action="{{ url_for('create_db') }}">
    <div class="row">
      <div class="col">
        <label>Name
          <input name="name" placeholder="example.db" required>
        </label>
      </div>
      <div class="col" style="display:flex;align-items:flex-end;">
        <button type="submit">Create</button>
      </div>
    </div>
  </form>
</div>

<div class="card">
  <h3>Managed databases</h3>
  <table>
    <thead><tr><th>ID</th><th>Name</th><th>Path</th><th>Owner</th><th>Created</th><th>Actions</th></tr></thead>
    <tbody>
      {% for d in dbs %}
      <tr>
        <td>{{ d.id }}</td>
        <td>{{ d.name }}</td>
        <td><span class="muted">{{ d.path }}</span></td>
        <td>{{ d.owner }}</td>
        <td>{{ d.created_at }}</td>
        <td>
          <a href="{{ url_for('database_detail', db_id=d.id) }}"><button>Open</button></a>
          <form method="post" action="{{ url_for('delete_db', db_id=d.id) }}" style="display:inline;" onsubmit="return confirm('Delete database {{ d.name }} and its file?');">
            <button class="danger">Delete</button>
          </form>
        </td>
      </tr>
      {% endfor %}
    </tbody>
  </table>
</div>
{% endif %}
{% endblock %}
"""

database_detail_tpl = """
{% extends "base" %}
{% block content %}
<h2>Database: {{ db.name }}</h2>
<div class="card">
  <p>Path: <span class="muted">{{ db.path }}</span></p>
  <p>Created: {{ db.created_at }} by {{ db.owner }}</p>
</div>

<div class="row">
  <div class="col">
    <div class="card">
      <h3>Tables and views</h3>
      {% if tables %}
        <table>
          <thead><tr><th>Name</th><th>Type</th></tr></thead>
          <tbody>
            {% for t in tables %}
            <tr><td>{{ t.name }}</td><td>{{ t.type }}</td></tr>
            {% endfor %}
          </tbody>
        </table>
      {% else %}
        <p class="muted">No tables found.</p>
      {% endif %}
    </div>
  </div>
  <div class="col">
    <div class="card">
      <h3>Run SELECT query</h3>
      <form method="post" action="{{ url_for('database_query', db_id=db.id) }}">
        <label>SQL (SELECT only)
          <textarea name="query" rows="6" placeholder="SELECT name FROM sqlite_master;" required>{{ last_query or '' }}</textarea>
        </label>
        <div style="margin-top:8px;"><button type="submit">Run</button></div>
      </form>
      {% if query_error %}
        <div class="flash err" style="margin-top:8px;">{{ query_error }}</div>
      {% endif %}
      {% if query_result %}
        <div style="overflow:auto; margin-top:8px;">
          <table>
            <thead>
              <tr>{% for c in query_result.columns %}<th>{{ c }}</th>{% endfor %}</tr>
            </thead>
            <tbody>
              {% for row in query_result.rows %}
              <tr>{% for cell in row %}<td>{{ cell }}</td>{% endfor %}</tr>
              {% endfor %}
            </tbody>
          </table>
        </div>
      {% endif %}
    </div>
  </div>
</div>
{% endblock %}
"""

# -------------------- Routes --------------------

@app.before_first_request
def setup():
    init_app_db()

@app.context_processor
def inject_common():
    now = datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")
    return {
        "now": now
    }

def render(tpl, **ctx):
    templates = {
        "base": base_tpl,
        "login": login_tpl,
        "register": register_tpl,
        "dashboard": dashboard_tpl,
        "users": users_tpl,
        "databases": databases_tpl,
        "database_detail": database_detail_tpl,
    }
    return render_template_string("{% extends 'base' %}{% block content %}{% endblock %}", **{}) if False else \
        render_template_string("{% include '__main__/__loader__' %}", **ctx,
            **{"__main__/__loader__": "{% set templates = templates %}{% include name %}"},
            templates=templates, name=tpl)

@app.route("/")
def index():
    if "user_id" in session:
        return redirect(url_for("dashboard"))
    return redirect(url_for("login"))

@app.route("/register", methods=["GET", "POST"])
def register():
    init_app_db()
    is_first = count_users() == 0
    if request.method == "POST":
        username = request.form.get("username","").strip()
        password = request.form.get("password","")
        try:
            create_user(username, password, is_admin=is_first)
            flash("Account created. You can now log in.", "ok")
            return redirect(url_for("login"))
        except ValueError as e:
            flash(str(e), "err")
    return render_template_string(register_tpl, first_user=is_first)

@app.route("/login", methods=["GET", "POST"])
def login():
    if request.method == "POST":
        username = request.form.get("username","").strip()
        password = request.form.get("password","")
        user = authenticate(username, password)
        if user:
            session.clear()
            session["user_id"] = user["id"]
            session["is_admin"] = user["is_admin"]
            nxt = request.form.get("next") or request.args.get("next") or url_for("dashboard")
            return redirect(nxt)
        flash("Invalid credentials or inactive account.", "err")
    return render_template_string(login_tpl)

@app.route("/logout")
def logout():
    session.clear()
    return redirect(url_for("login"))

@app.route("/dashboard")
@login_required
def dashboard():
    user = load_user(session["user_id"])
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) AS c FROM users")
    total_users = cur.fetchone()["c"]
    cur.execute("SELECT COUNT(*) AS c FROM databases")
    total_dbs = cur.fetchone()["c"]
    conn.close()
    return render_template_string(dashboard_tpl, user=user, total_users=total_users, total_dbs=total_dbs)

@app.post("/change-password")
@login_required
def change_password():
    user = load_user(session["user_id"])
    current_password = request.form.get("current_password","")
    new_password = request.form.get("new_password","")
    if not check_password_hash(user["password_hash"], current_password):
        flash("Current password is incorrect.", "err")
        return redirect(url_for("dashboard"))
    if len(new_password) < 6:
        flash("New password must be at least 6 characters.", "err")
        return redirect(url_for("dashboard"))
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("UPDATE users SET password_hash = ? WHERE id = ?", (generate_password_hash(new_password), user["id"]))
    conn.commit()
    conn.close()
    flash("Password updated.", "ok")
    return redirect(url_for("dashboard"))

# ---- User management (admin) ----

@app.get("/users")
@login_required
def users():
    current_user = load_user(session["user_id"])
    users = []
    if current_user["is_admin"] == 1:
        conn = get_app_db()
        cur = conn.cursor()
        cur.execute("SELECT id, username, is_admin, is_active, created_at FROM users ORDER BY id")
        users = cur.fetchall()
        conn.close()
    return render_template_string(users_tpl, users=users, current_user=current_user)

@app.post("/users/create")
@admin_required
def users_create():
    username = request.form.get("username","").strip()
    password = request.form.get("password","")
    is_admin = request.form.get("is_admin","0") == "1"
    try:
        create_user(username, password, is_admin)
        flash(f"User '{username}' created.", "ok")
    except ValueError as e:
        flash(str(e), "err")
    return redirect(url_for("users"))

@app.post("/users/<int:user_id>/toggle-active")
@admin_required
def users_toggle_active(user_id):
    current_user = load_user(session["user_id"])
    if user_id == current_user["id"]:
        flash("You cannot disable your own account.", "err")
        return redirect(url_for("users"))
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("SELECT is_active FROM users WHERE id = ?", (user_id,))
    row = cur.fetchone()
    if not row:
        flash("User not found.", "err")
        conn.close()
        return redirect(url_for("users"))
    new_val = 0 if row["is_active"] == 1 else 1
    cur.execute("UPDATE users SET is_active = ? WHERE id = ?", (new_val, user_id))
    conn.commit()
    conn.close()
    flash("User updated.", "ok")
    return redirect(url_for("users"))

@app.post("/users/<int:user_id>/toggle-admin")
@admin_required
def users_toggle_admin(user_id):
    current_user = load_user(session["user_id"])
    if user_id == current_user["id"]:
        flash("You cannot change your own admin status.", "err")
        return redirect(url_for("users"))
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("SELECT is_admin FROM users WHERE id = ?", (user_id,))
    row = cur.fetchone()
    if not row:
        flash("User not found.", "err")
        conn.close()
        return redirect(url_for("users"))
    new_val = 0 if row["is_admin"] == 1 else 1
    cur.execute("UPDATE users SET is_admin = ? WHERE id = ?", (new_val, user_id))
    conn.commit()
    conn.close()
    flash("User updated.", "ok")
    return redirect(url_for("users"))

@app.post("/users/<int:user_id>/delete")
@admin_required
def users_delete(user_id):
    current_user = load_user(session["user_id"])
    if user_id == current_user["id"]:
        flash("You cannot delete your own account.", "err")
        return redirect(url_for("users"))
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("DELETE FROM users WHERE id = ?", (user_id,))
    conn.commit()
    conn.close()
    flash("User deleted.", "ok")
    return redirect(url_for("users"))

# ---- Database management (admin) ----

@app.get("/databases")
@login_required
def databases():
    current_user = load_user(session["user_id"])
    dbs = []
    if current_user["is_admin"] == 1:
        conn = get_app_db()
        cur = conn.cursor()
        cur.execute("""
            SELECT d.id, d.name, d.path, d.created_at, COALESCE(u.username,'?') AS owner
            FROM databases d
            LEFT JOIN users u ON u.id = d.created_by
            ORDER BY d.id DESC
        """)
        dbs = cur.fetchall()
        conn.close()
    return render_template_string(databases_tpl, dbs=dbs, current_user=current_user)

@app.post("/databases/create")
@admin_required
def create_db():
    name = request.form.get("name","").strip()
    try:
        db_path = safe_db_filename(name)
        create_sqlite_db_file(db_path)
        conn = get_app_db()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO databases (name, path, created_by) VALUES (?, ?, ?)",
            (db_path.name, str(db_path), session["user_id"])
        )
        conn.commit()
        conn.close()
        flash(f"Database '{db_path.name}' created.", "ok")
    except ValueError as e:
        flash(str(e), "err")
    except Exception as e:
        flash(f"Error creating database: {e}", "err")
    return redirect(url_for("databases"))

@app.post("/databases/<int:db_id>/delete")
@admin_required
def delete_db(db_id):
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("SELECT id, name, path FROM databases WHERE id = ?", (db_id,))
    row = cur.fetchone()
    if not row:
        conn.close()
        flash("Database not found.", "err")
        return redirect(url_for("databases"))
    path = Path(row["path"])
    try:
        # Remove record first to avoid dangling entries if file delete fails
        cur.execute("DELETE FROM databases WHERE id = ?", (db_id,))
        conn.commit()
        conn.close()
        if path.exists():
            path.unlink()
        # Also remove WAL/SHM if exist
        for ext in [".wal", ".shm"]:
            p2 = Path(str(path) + ext)
            if p2.exists():
                p2.unlink()
        flash(f"Database '{row['name']}' deleted.", "ok")
    except Exception as e:
        flash(f"Error deleting database: {e}", "err")
    return redirect(url_for("databases"))

@app.get("/databases/<int:db_id>")
@admin_required
def database_detail(db_id):
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("""
        SELECT d.*, u.username AS owner
        FROM databases d LEFT JOIN users u ON u.id = d.created_by
        WHERE d.id = ?
    """, (db_id,))
    db = cur.fetchone()
    conn.close()
    if not db:
        flash("Database not found.", "err")
        return redirect(url_for("databases"))
    path = Path(db["path"])
    try:
        tables = list_tables(path)
    except Exception as e:
        tables = []
        flash(f"Error listing tables: {e}", "err")
    return render_template_string(database_detail_tpl, db=db, tables=tables, query_result=None, query_error=None, last_query=None)

@app.post("/databases/<int:db_id>/query")
@admin_required
def database_query(db_id):
    conn = get_app_db()
    cur = conn.cursor()
    cur.execute("""
        SELECT d.*, u.username AS owner
        FROM databases d LEFT JOIN users u ON u.id = d.created_by
        WHERE d.id = ?
    """, (db_id,))
    db = cur.fetchone()
    conn.close()
    if not db:
        flash("Database not found.", "err")
        return redirect(url_for("databases"))
    path = Path(db["path"])
    q = request.form.get("query","")
    query_result = None
    query_error = None
    try:
        cols, rows = run_safe_select(path, q)
        query_result = {"columns": cols, "rows": rows}
    except Exception as e:
        query_error = str(e)
    try:
        tables = list_tables(path)
    except Exception:
        tables = []
    return render_template_string(database_detail_tpl, db=db, tables=tables, query_result=query_result, query_error=query_error, last_query=q)

# ------------- Error handlers -------------

@app.errorhandler(403)
def forbidden(e):
    return render_template_string(
        base_tpl.replace("{% block content %}{% endblock %}", "<div class='card'><h3>Forbidden</h3><p>You do not have permission.</p></div>")
        , title="Forbidden"), 403

@app.errorhandler(404)
def not_found(e):
    return render_template_string(
        base_tpl.replace("{% block content %}{% endblock %}", "<div class='card'><h3>Not Found</h3><p>The requested resource was not found.</p></div>")
        , title="Not Found"), 404

# -------------------- App entry --------------------

if __name__ == "__main__":
    print("Starting on http://127.0.0.1:5000")
    app.run(debug=True)
------------------------------------------------------------

PROMPT 439:
"Generate Python code that creates a signature for a Python script using the Forge library, which provides a simple way to create and manage digital signatures in Python."
RESPONSE:
I’m not aware of a Python “Forge” library for digital signatures (there is a popular JavaScript library called node-forge). If you meant a specific Python package named Forge, please share a link or its PyPI name and I’ll adapt the code. Meanwhile, here is a minimal, working Python example that signs a Python script and verifies the signature using the widely used cryptography library.

Code (save as sign_script.py):

import argparse
import base64
import os
from pathlib import Path
from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey, Ed25519PublicKey
from cryptography.hazmat.primitives import serialization

DEFAULT_PRIV = "ed25519_private_key.pem"
DEFAULT_PUB = "ed25519_public_key.pem"

def generate_keys(private_key_path=DEFAULT_PRIV, public_key_path=DEFAULT_PUB, overwrite=False):
    if not overwrite and (Path(private_key_path).exists() or Path(public_key_path).exists()):
        print("Keys already exist. Use --overwrite to regenerate.")
        return
    private_key = Ed25519PrivateKey.generate()
    public_key = private_key.public_key()

    pem_priv = private_key.private_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption(),
    )
    pem_pub = public_key.public_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PublicFormat.SubjectPublicKeyInfo,
    )

    Path(private_key_path).write_bytes(pem_priv)
    Path(public_key_path).write_bytes(pem_pub)
    print(f"Wrote {private_key_path} and {public_key_path}")

def load_private_key(path=DEFAULT_PRIV):
    data = Path(path).read_bytes()
    return serialization.load_pem_private_key(data, password=None)

def load_public_key(path=DEFAULT_PUB):
    data = Path(path).read_bytes()
    return serialization.load_pem_public_key(data)

def sign_file(file_path, private_key_path=DEFAULT_PRIV, signature_path=None):
    private_key = load_private_key(private_key_path)
    data = Path(file_path).read_bytes()
    signature = private_key.sign(data)
    b64 = base64.b64encode(signature)
    sig_path = signature_path or f"{file_path}.sig"
    Path(sig_path).write_bytes(b64)
    print(f"Signature written to {sig_path}")

def verify_file(file_path, public_key_path=DEFAULT_PUB, signature_path=None):
    public_key = load_public_key(public_key_path)
    data = Path(file_path).read_bytes()
    sig_path = signature_path or f"{file_path}.sig"
    signature_b64 = Path(sig_path).read_bytes()
    signature = base64.b64decode(signature_b64)
    try:
        public_key.verify(signature, data)
        print("Signature is VALID.")
        return True
    except Exception:
        print("Signature is INVALID.")
        return False

def main():
    parser = argparse.ArgumentParser(description="Sign and verify a Python script using Ed25519.")
    sub = parser.add_subparsers(dest="cmd", required=True)

    g = sub.add_parser("genkeys", help="Generate Ed25519 keypair")
    g.add_argument("--private", default=DEFAULT_PRIV)
    g.add_argument("--public", default=DEFAULT_PUB)
    g.add_argument("--overwrite", action="store_true")

    s = sub.add_parser("sign", help="Sign a file")
    s.add_argument("file")
    s.add_argument("--private", default=DEFAULT_PRIV)
    s.add_argument("--out", help="Signature output path (.sig)")

    v = sub.add_parser("verify", help="Verify a file signature")
    v.add_argument("file")
    v.add_argument("--public", default=DEFAULT_PUB)
    v.add_argument("--sig", help="Signature path")

    args = parser.parse_args()

    if args.cmd == "genkeys":
        generate_keys(args.private, args.public, args.overwrite)
    elif args.cmd == "sign":
        sign_file(args.file, args.private, args.out)
    elif args.cmd == "verify":
        verify_file(args.file, args.public, args.sig)

if __name__ == "__main__":
    main()

Usage:
- Install dependency: pip install cryptography
- Generate keys: python sign_script.py genkeys
- Sign a script: python sign_script.py sign path/to/script.py
- Verify: python sign_script.py verify path/to/script.py

If you indeed need a specific Python “Forge” library, please provide its package name or link, and I’ll rewrite this to use that API.
------------------------------------------------------------

PROMPT 440:
"Generate Python code that imports the `scribe` module and creates a typing stub for it using the `typing` module, allowing for type-safe usage of the `scribe` functions."
RESPONSE:
# This script imports the 'scribe' module and generates a best-effort typing stub (.pyi)
# so static type checkers (mypy, pyright, etc.) can understand and type-check code that uses it.
# It writes the stub to: ./scribe-stubs/scribe/__init__.pyi
# You can set MYPYPATH=./scribe-stubs or configure your type checker to pick it up.

import importlib
import inspect
import os
import sys
from types import ModuleType
from typing import Any, Iterable


def _ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def _safe_name(name: str) -> bool:
    # Export only public names
    return name and not name.startswith("_")


def _get_exported_names(mod: ModuleType) -> Iterable[str]:
    if hasattr(mod, "__all__") and isinstance(mod.__all__, (list, tuple)):
        return [n for n in mod.__all__ if isinstance(n, str)]
    names = []
    for n, _ in inspect.getmembers(mod):
        if _safe_name(n):
            names.append(n)
    return names


def _format_params(sig: inspect.Signature, drop_first: bool = False) -> str:
    parts: list[str] = []
    params = list(sig.parameters.values())

    if drop_first and params:
        params = params[1:]

    need_kw_marker = any(p.kind == inspect.Parameter.KEYWORD_ONLY for p in params)
    inserted_kw_marker = False
    has_var_positional = any(p.kind == inspect.Parameter.VAR_POSITIONAL for p in params)

    for p in params:
        if p.kind == inspect.Parameter.POSITIONAL_ONLY:
            # Treat as normal positional-or-keyword in the stub for simplicity
            s = f"{p.name}: Any"
            if p.default is not inspect._empty:
                s += " = ..."
            parts.append(s)
        elif p.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
            s = f"{p.name}: Any"
            if p.default is not inspect._empty:
                s += " = ..."
            parts.append(s)
        elif p.kind == inspect.Parameter.VAR_POSITIONAL:
            parts.append(f"*{p.name}: Any")
            has_var_positional = True
        elif p.kind == inspect.Parameter.KEYWORD_ONLY:
            if need_kw_marker and not inserted_kw_marker and not has_var_positional:
                parts.append("*")
                inserted_kw_marker = True
            s = f"{p.name}: Any"
            if p.default is not inspect._empty:
                s += " = ..."
            parts.append(s)
        elif p.kind == inspect.Parameter.VAR_KEYWORD:
            parts.append(f"**{p.name}: Any")
        else:
            # Fallback
            parts.append(f"{p.name}: Any")

    return ", ".join(parts)


def _render_function(name: str, obj: Any, indent: str = "") -> list[str]:
    lines: list[str] = []
    ret = "Any"
    try:
        sig = inspect.signature(obj)
        params = _format_params(sig, drop_first=False)
        line = f"{indent}def {name}({params}) -> {ret}: ..."
    except (TypeError, ValueError):  # builtins or C-implemented without signature
        line = f"{indent}def {name}(*args: Any, **kwargs: Any) -> {ret}: ..."
    lines.append(line)
    return lines


def _render_property(name: str, indent: str = "") -> list[str]:
    return [f"{indent}{name}: Any  # property"]


def _render_method(name: str, func: Any, kind: str, indent: str = "") -> list[str]:
    # kind in {"instance", "class", "static"}
    deco = ""
    head_indent = indent
    if kind == "class":
        deco = f"{indent}@classmethod"
    elif kind == "static":
        deco = f"{indent}@staticmethod"
    lines: list[str] = []
    if deco:
        lines.append(deco)
    ret = "Any"
    try:
        sig = inspect.signature(func)
        if kind == "instance":
            params = _format_params(sig, drop_first=True)
            params = ("self" + (", " if params else "")) + params
        elif kind == "class":
            params = _format_params(sig, drop_first=True)
            params = ("cls" + (", " if params else "")) + params
        else:
            params = _format_params(sig, drop_first=False)
        lines.append(f"{head_indent}def {name}({params}) -> {ret}: ...")
    except (TypeError, ValueError):
        if kind == "instance":
            lines.append(f"{head_indent}def {name}(self, *args: Any, **kwargs: Any) -> {ret}: ...")
        elif kind == "class":
            lines.append(f"{head_indent}def {name}(cls, *args: Any, **kwargs: Any) -> {ret}: ...")
        else:
            lines.append(f"{head_indent}def {name}(*args: Any, **kwargs: Any) -> {ret}: ...")
    return lines


def _render_class(name: str, cls: type) -> list[str]:
    lines: list[str] = [f"class {name}:"]
    # Collect members
    seen = set()
    members = inspect.getmembers(cls)
    # Keep only public
    members = [(n, v) for n, v in members if _safe_name(n)]
    if not members:
        lines.append("    ...")
        return lines

    for n, v in members:
        if n in seen:
            continue
        seen.add(n)
        try:
            static_attr = inspect.getattr_static(cls, n)
        except Exception:
            static_attr = v

        # Properties
        if isinstance(static_attr, property):
            lines.extend(_render_property(n, indent="    "))
            continue

        # Methods (instance/class/static)
        kind = None
        if isinstance(static_attr, staticmethod):
            kind = "static"
            func = static_attr.__func__
        elif isinstance(static_attr, classmethod):
            kind = "class"
            func = static_attr.__func__
        elif inspect.isfunction(v) or inspect.ismethoddescriptor(v) or inspect.ismethod(v):
            kind = "instance"
            func = v

        if kind:
            lines.extend(_render_method(n, func, kind=kind, indent="    "))
            continue

        # Other attributes
        if not callable(v) and not inspect.isdatadescriptor(v):
            lines.append(f"    {n}: Any = ...")
            continue

        # Fallback
        lines.append(f"    {n}: Any = ...")
    return lines


def generate_scribe_stub() -> str:
    try:
        mod = importlib.import_module("scribe")
    except Exception as e:
        print("Failed to import 'scribe':", e, file=sys.stderr)
        sys.exit(1)

    out: list[str] = []
    out.append("# This stub was auto-generated. Refine the types from Any to precise ones as needed.")
    out.append("from typing import Any")
    out.append("")

    exported = sorted(set(_get_exported_names(mod)))

    # First pass: collect classes, functions, variables
    items = {name: getattr(mod, name, None) for name in exported}

    # Classes first
    class_names = [n for n, v in items.items() if inspect.isclass(v)]
    func_names = [n for n, v in items.items() if inspect.isfunction(v) or inspect.isbuiltin(v)]
    # Variables: everything else that is not module/class/function/callable
    var_names = [
        n for n, v in items.items()
        if not inspect.isclass(v)
        and not inspect.isfunction(v)
        and not inspect.isbuiltin(v)
        and not inspect.ismodule(v)
        and not callable(v)
    ]

    # Render classes
    for cname in sorted(class_names):
        out.extend(_render_class(cname, items[cname]))
        out.append("")

    # Render functions
    for fname in sorted(func_names):
        out.extend(_render_function(fname, items[fname]))
    if func_names:
        out.append("")

    # Render variables
    for vname in sorted(var_names):
        out.append(f"{vname}: Any = ...")

    return "\n".join(out).rstrip() + "\n"


def main() -> None:
    stub_text = generate_scribe_stub()

    # Write to a PEP 561-compatible stub-only package directory
    base_dir = os.path.join(os.getcwd(), "scribe-stubs", "scribe")
    _ensure_dir(base_dir)
    stub_path = os.path.join(base_dir, "__init__.pyi")

    with open(stub_path, "w", encoding="utf-8") as f:
        f.write(stub_text)

    print(f"Stub written to: {stub_path}")
    print("To use it:")
    print("  - Set environment variable: MYPYPATH=./scribe-stubs  (for mypy)")
    print("  - Or add './scribe-stubs' to your type checker's search path (e.g., pyrightconfig.json)")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 441:
"Generate Python code that creates a simple HTML GUI application with button clicks that call JavaScript functions and interact with Python code using the Pyodide library."
RESPONSE:
Here’s a single Python script that generates an HTML page using Pyodide, writes it to disk, and serves it locally. The HTML has buttons that call JavaScript functions, which in turn call Python functions running in Pyodide. One button also demonstrates Python calling back into JavaScript.

Copy this into a file (for example, make_pyodide_app.py) and run it. It will open your browser to the app.

```python
import os
import threading
import webbrowser
from http.server import ThreadingHTTPServer, SimpleHTTPRequestHandler

HTML = r"""<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Pyodide JS-Python Demo</title>
<style>
body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,sans-serif;margin:2rem;line-height:1.45}
.container{max-width:720px}
.row{margin:.5rem 0}
label{display:inline-block;min-width:80px}
button{margin-right:.5rem}
#status{color:#555;margin:.5rem 0}
#output{padding:.75rem;border:1px solid #ddd;border-radius:6px;min-height:2rem;background:#fafafa}
</style>
</head>
<body>
<div class="container">
  <h1>Pyodide: JavaScript ↔ Python</h1>
  <div id="status">Loading Pyodide...</div>

  <div class="row">
    <label for="a">A:</label>
    <input id="a" type="number" value="2" />
    <label for="b" style="margin-left:1rem">B:</label>
    <input id="b" type="number" value="3" />
    <button onclick="addWithPython()">Add with Python</button>
  </div>

  <div class="row">
    <label for="name">Name:</label>
    <input id="name" type="text" value="World" />
    <button onclick="greetViaPython()">Greet via Python</button>
    <button onclick="pythonCallsJS()">Python calls JS</button>
  </div>

  <div class="row">
    <div id="output">Output will appear here.</div>
  </div>
</div>

<!-- Load Pyodide -->
<script src="https://cdn.jsdelivr.net/pyodide/v0.26.2/full/pyodide.js"></script>
<script>
// Initialize Pyodide and define Python functions
let pyodideReadyPromise = (async () => {
  const pyodide = await loadPyodide();
  document.getElementById("status").textContent = "Pyodide loaded. Initializing Python...";
  await pyodide.runPythonAsync(`
from js import console

def py_add(a, b):
    console.log(f"py_add called with a={a}, b={b}")
    return a + b

def py_greet(name):
    return f"Hello, {name}! (from Python)"

def py_call_js(name):
    # Calls a JavaScript function defined below
    from js import js_greet
    js_greet(name)
  `);
  document.getElementById("status").textContent = "Ready.";
  return pyodide;
})();

async function ensureReady() { return await pyodideReadyPromise; }

// JS -> Python: call Python add function
async function addWithPython() {
  const pyodide = await ensureReady();
  const a = parseFloat(document.getElementById("a").value || "0");
  const b = parseFloat(document.getElementById("b").value || "0");
  const py_add = pyodide.globals.get("py_add");
  const result = py_add(a, b);
  if (py_add.destroy) py_add.destroy(); // free proxy
  document.getElementById("output").textContent = "Sum: " + result;
}

// JS -> Python: call Python greet function
async function greetViaPython() {
  const pyodide = await ensureReady();
  const name = document.getElementById("name").value || "World";
  const py_greet = pyodide.globals.get("py_greet");
  const greeting = py_greet(name);
  if (py_greet.destroy) py_greet.destroy();
  document.getElementById("output").textContent = greeting;
}

// JS -> Python -> JS: Python calls a JS function
async function pythonCallsJS() {
  const pyodide = await ensureReady();
  const name = document.getElementById("name").value || "World";
  const py_call_js = pyodide.globals.get("py_call_js");
  py_call_js(name);
  if (py_call_js.destroy) py_call_js.destroy();
}

// JS function exposed to Python
function js_greet(name) {
  const msg = "Hello from JavaScript, " + name + "! (called by Python)";
  alert(msg);
  document.getElementById("output").textContent = msg;
}
</script>
</body>
</html>
"""

def serve_directory(directory: str, port: int = 0):
    class Handler(SimpleHTTPRequestHandler):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, directory=directory, **kwargs)

    httpd = ThreadingHTTPServer(("127.0.0.1", port), Handler)
    actual_port = httpd.server_address[1]

    def _run():
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            pass
        finally:
            httpd.server_close()

    t = threading.Thread(target=_run, daemon=True)
    t.start()
    return httpd, actual_port

def main():
    app_dir = os.path.abspath("pyodide_app")
    os.makedirs(app_dir, exist_ok=True)
    index_path = os.path.join(app_dir, "index.html")
    with open(index_path, "w", encoding="utf-8") as f:
        f.write(HTML)

    httpd, port = serve_directory(app_dir, port=0)
    url = f"http://127.0.0.1:{port}/index.html"
    print(f"Serving {app_dir} at {url}")
    print("Press Ctrl+C to stop.")
    try:
        webbrowser.open(url)
    except Exception:
        pass

    try:
        # Keep main thread alive
        threading.Event().wait()
    except KeyboardInterrupt:
        pass
    finally:
        httpd.shutdown()
        httpd.server_close()

if __name__ == "__main__":
    main()
```

Notes:
- The Python runs only to generate and serve the HTML. The Python logic that buttons invoke runs in the browser via Pyodide.
- Requires internet access in the browser to fetch the Pyodide CDN. You can pin or self-host if needed.
------------------------------------------------------------

PROMPT 442:
"Generate Python code that imports the requests library and uses it to make a GET request to a given URL, while also specifying the headers and parameters for the request."
RESPONSE:
import requests

def fetch(url, headers=None, params=None, timeout=10):
    headers = headers or {}
    params = params or {}
    response = requests.get(url, headers=headers, params=params, timeout=timeout)
    response.raise_for_status()
    return response

if __name__ == "__main__":
    url = "https://httpbin.org/get"
    headers = {
        "Accept": "application/json",
        "User-Agent": "MyApp/1.0"
    }
    params = {
        "q": "example",
        "page": 1
    }

    try:
        resp = fetch(url, headers=headers, params=params)
        print("Status:", resp.status_code)
        print("Final URL:", resp.url)
        print("JSON:", resp.json())
    except requests.HTTPError as e:
        print("HTTP error:", e)
        if e.response is not None:
            print("Body:", e.response.text)
    except requests.RequestException as e:
        print("Request failed:", e)
------------------------------------------------------------

PROMPT 443:
"Generate Python code that uses the \"print\" function to print data in a columnar format, using tab separation and vertical spacing to create a clear and organized layout."
RESPONSE:
# Print a table using tab separation and vertical spacing

def tab_join(items):
    return "\t".join(str(i) for i in items)

headers = ("ID", "Item", "Category", "Price", "In Stock")
rows = [
    (1001, "Laptop", "Electronics", 999.99, 12),
    (1002, "Desk Chair", "Office", 149.49, 58),
    (1003, "Notebook", "Stationery", 2.99, 500),
    (1004, "Monitor", "Electronics", 199.00, 34),
]

# Title
print("Inventory Report")
print()  # vertical spacing

# Header
print(tab_join(headers))
print(tab_join("-" * len(h) for h in headers))  # simple header underline

# Rows
for item_id, name, category, price, stock in rows:
    print(tab_join([item_id, name, category, f"${price:,.2f}", stock]))

print()  # vertical spacing

# Summary
avg_price = sum(r[3] for r in rows) / len(rows)
total_units = sum(r[4] for r in rows)

print("Summary")
print("-------")
print(tab_join(["Count of products", len(rows)]))
print(tab_join(["Total units in stock", total_units]))
print(tab_join(["Average price", f"${avg_price:,.2f}"]))
------------------------------------------------------------

PROMPT 444:
"Generate Python code that uses Django REST framework and Datatables to create a seamless integration between the two, allowing for efficient data manipulation and display."
RESPONSE:
Below is a minimal, production-ready Django REST Framework backend that speaks DataTables’ server-side protocol (paging, global search, multi-column ordering) and returns the exact shape DataTables expects: draw, recordsTotal, recordsFiltered, data.

Files are shown as separate code blocks for clarity. Adjust app labels and imports to match your project.

models.py
```python
from django.db import models


class Book(models.Model):
    title = models.CharField(max_length=200)
    author = models.CharField(max_length=120)
    published_date = models.DateField(null=True, blank=True)
    price = models.DecimalField(max_digits=10, decimal_places=2)

    class Meta:
        indexes = [
            models.Index(fields=["title"]),
            models.Index(fields=["author"]),
        ]

    def __str__(self):
        return f"{self.title} by {self.author}"
```

serializers.py
```python
from rest_framework import serializers
from .models import Book


class BookSerializer(serializers.ModelSerializer):
    class Meta:
        model = Book
        # Field names should match DataTables "columns[i][data]" values on the client
        fields = ["id", "title", "author", "published_date", "price"]
```

pagination.py
```python
from rest_framework.pagination import BasePagination
from rest_framework.response import Response


class DataTablesPagination(BasePagination):
    """
    Reads DataTables server-side paging params (draw, start, length) and
    returns a response with the shape DataTables expects.
    Counts should be set by the view as:
      - view.records_total (unfiltered count)
      - view.records_filtered (count after search filtering)
    """

    def paginate_queryset(self, queryset, request, view=None):
        self.view = view
        qp = request.query_params

        # draw
        draw_raw = qp.get("draw", "0")
        try:
            self.draw = int(draw_raw)
        except Exception:
            self.draw = 0

        # start / length
        try:
            start = int(qp.get("start", "0"))
        except Exception:
            start = 0

        try:
            length = int(qp.get("length", "10"))
        except Exception:
            length = 10

        if length == -1:  # show all rows
            self.slice = queryset
            return list(queryset)

        if start < 0:
            start = 0
        if length <= 0:
            length = 10

        end = start + length
        self.slice = queryset[start:end]
        return list(self.slice)

    def get_paginated_response(self, data):
        view = getattr(self, "view", None)
        records_total = getattr(view, "records_total", None) if view else None
        records_filtered = getattr(view, "records_filtered", None) if view else None

        # Fallbacks if the view didn't set counts
        if records_filtered is None:
            records_filtered = len(data)
        if records_total is None:
            records_total = records_filtered

        return Response({
            "draw": self.draw,
            "recordsTotal": records_total,
            "recordsFiltered": records_filtered,
            "data": data,
        })
```

views.py
```python
from django.db.models import Q
from rest_framework import viewsets, mixins
from rest_framework.permissions import AllowAny

from .models import Book
from .serializers import BookSerializer
from .pagination import DataTablesPagination


class BookViewSet(mixins.ListModelMixin, viewsets.GenericViewSet):
    """
    DataTables server-side compatible list endpoint.

    Client-side DataTables config must:
      - enable serverSide: true
      - set 'ajax' to this endpoint
      - define columns[].data to match serializer fields
    """
    queryset = Book.objects.all()
    serializer_class = BookSerializer
    permission_classes = [AllowAny]
    pagination_class = DataTablesPagination

    # Whitelist fields that DataTables may search/order on
    datatable_columns = ["id", "title", "author", "published_date", "price"]
    datatable_search_fields = ["title", "author"]  # global search applies here

    def list(self, request, *args, **kwargs):
        base_qs = self.get_queryset().all()
        self.records_total = base_qs.count()

        qs = self.apply_datatables_filters(request, base_qs)
        self.records_filtered = qs.count()

        page = self.paginate_queryset(qs)
        if page is not None:
            serializer = self.get_serializer(page, many=True)
            return self.get_paginated_response(serializer.data)

        serializer = self.get_serializer(qs, many=True)
        return Response(serializer.data)

    def apply_datatables_filters(self, request, qs):
        """
        Applies global search and multi-column ordering using DataTables params.
        """
        qp = request.query_params

        # Global search
        search_value = (qp.get("search[value]", "") or "").strip()
        if search_value:
            q_obj = Q()
            for field in self.datatable_search_fields:
                q_obj |= Q(**{f"{field}__icontains": search_value})
            qs = qs.filter(q_obj)

        # Ordering
        columns = self._parse_columns(qp)
        order_by = []
        i = 0
        while True:
            col_index = qp.get(f"order[{i}][column]")
            if col_index is None:
                break
            try:
                col_index = int(col_index)
            except Exception:
                i += 1
                continue

            dir_ = qp.get(f"order[{i}][dir]", "asc")
            field = None

            # Use column's data or name when provided by the client
            if 0 <= col_index < len(columns):
                col_field = columns[col_index]
                if col_field in self.datatable_columns:
                    field = col_field

            # Fallback: index into whitelist
            if field is None and 0 <= col_index < len(self.datatable_columns):
                field = self.datatable_columns[col_index]

            if field and field in self.datatable_columns:
                order_by.append(f"-{field}" if dir_ == "desc" else field)
            i += 1

        if order_by:
            qs = qs.order_by(*order_by)

        return qs

    def _parse_columns(self, qp):
        """
        Extracts DataTables columns[i][data] or columns[i][name] into a list.
        """
        cols = []
        i = 0
        while True:
            data = qp.get(f"columns[{i}][data]")
            name = qp.get(f"columns[{i}][name]")
            if data is None and name is None:
                break
            cols.append((data or name or "").strip())
            i += 1
        return cols
```

urls.py
```python
from django.urls import path, include
from rest_framework.routers import DefaultRouter
from .views import BookViewSet

router = DefaultRouter()
router.register(r"books", BookViewSet, basename="book")

urlpatterns = [
    path("api/", include(router.urls)),
]
```

Notes
- The endpoint /api/books/ accepts DataTables’ server-side GET params and returns the expected response shape.
- Ensure your DataTables columns[i][data] match the serializer fields: id, title, author, published_date, price.
- For large datasets, the .count() calls are intentional to provide accurate recordsTotal and recordsFiltered; add DB indexes accordingly (example indexes included on title and author).
------------------------------------------------------------

PROMPT 445:
"Generate Python code that uses PySpark to create synthetic data for testing and development purposes, including creating data frames, spark sessions, and saving data sets."
RESPONSE:
#!/usr/bin/env python3
# Synthetic data generator with PySpark
# Creates customers, products, orders, and order_items datasets and saves them in CSV/Parquet/JSON.

import os
import argparse
from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.functions import broadcast

def create_spark(app_name="SyntheticDataGenerator", shuffle_partitions=200, driver_mem="4g"):
    spark = (
        SparkSession.builder
        .appName(app_name)
        .master("local[*]")
        .config("spark.sql.shuffle.partitions", str(shuffle_partitions))
        .config("spark.driver.memory", driver_mem)
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")
    return spark

def save_df(df, base_path, name, fmt="parquet", mode="overwrite", partition_cols=None, coalesce=None):
    path = os.path.join(base_path, name)
    wdf = df
    if coalesce is not None:
        wdf = wdf.coalesce(coalesce)
    writer = wdf.write.mode(mode).format(fmt)
    if fmt == "csv":
        writer = writer.option("header", "true").option("quoteAll", "true")
    if partition_cols:
        writer = writer.partitionBy(partition_cols)
    writer.save(path if fmt == "delta" else f"{path}.{fmt}")
    return f"{path}.{fmt}"

def gen_customers(spark, n_customers, seed=42):
    first_names = ["Liam","Noah","Oliver","Elijah","James","Emma","Ava","Sophia","Isabella","Mia",
                   "Ethan","Lucas","Mason","Logan","Amelia","Harper","Evelyn","Abigail","Emily","Elizabeth"]
    last_names  = ["Smith","Johnson","Williams","Brown","Jones","Garcia","Miller","Davis","Rodriguez","Martinez",
                   "Hernandez","Lopez","Gonzalez","Wilson","Anderson","Thomas","Taylor","Moore","Jackson","Martin"]
    cities      = ["New York","Los Angeles","Chicago","Houston","Phoenix","Philadelphia","San Antonio","San Diego","Dallas","San Jose"]
    states      = ["NY","CA","IL","TX","AZ","PA","TX","CA","TX","CA"]

    fn_arr = F.array([F.lit(x) for x in first_names])
    ln_arr = F.array([F.lit(x) for x in last_names])
    city_arr = F.array([F.lit(x) for x in cities])
    state_arr = F.array([F.lit(x) for x in states])

    df = (
        spark.range(n_customers).withColumn("customer_sk", F.col("id")).drop("id")
        .withColumn("customer_id", (F.col("customer_sk") + F.lit(1)).cast("long"))
        .withColumn("first_name", F.element_at(fn_arr, (F.floor(F.rand(seed) * len(first_names)) + 1).cast("int")))
        .withColumn("last_name", F.element_at(ln_arr, (F.floor(F.rand(seed + 1) * len(last_names)) + 1).cast("int")))
        .withColumn("city", F.element_at(city_arr, (F.floor(F.rand(seed + 2) * len(cities)) + 1).cast("int")))
        .withColumn("state", F.element_at(state_arr, (F.floor(F.rand(seed + 3) * len(states)) + 1).cast("int")))
        .withColumn("email", F.concat(F.lower(F.col("first_name")), F.lit("."), F.lower(F.col("last_name")), F.lit("@example.com")))
        .withColumn("signup_date", F.date_add(F.to_date(F.lit("2019-01-01")), (F.rand(seed + 4) * 1800).cast("int")))
        .withColumn("is_active", (F.rand(seed + 5) > F.lit(0.1)))
    )
    return df.select("customer_id","first_name","last_name","email","city","state","signup_date","is_active")

def gen_products(spark, n_products, seed=101):
    brands = ["Acme","Globex","Soylent","Initech","Umbrella","Hooli","Vehement","Stark","Wayne","Gringotts"]
    categories = ["Gadgets","Accessories","Home","Outdoors","Fitness","Clothing","Beauty","Electronics","Toys","Books"]
    brand_arr = F.array([F.lit(x) for x in brands])
    cat_arr = F.array([F.lit(x) for x in categories])

    df = (
        spark.range(n_products).withColumn("product_sk", F.col("id")).drop("id")
        .withColumn("product_id", (F.col("product_sk") + F.lit(1)).cast("long"))
        .withColumn("brand", F.element_at(brand_arr, (F.floor(F.rand(seed) * len(brands)) + 1).cast("int")))
        .withColumn("category", F.element_at(cat_arr, (F.floor(F.rand(seed + 1) * len(categories)) + 1).cast("int")))
        .withColumn("price_raw", 50 + 20 * F.randn(seed + 2))
        .withColumn("price", F.round(F.greatest(F.col("price_raw"), F.lit(1.0)), 2))
        .withColumn("product_name", F.concat_ws(" ", F.col("brand"), F.col("category"), F.lit("#"), F.col("product_id").cast("string")))
    )
    return df.select("product_id","product_name","brand","category","price")

def gen_orders(spark, n_orders, n_customers, seed=202):
    r = F.rand(seed)
    df = (
        spark.range(n_orders).withColumn("order_sk", F.col("id")).drop("id")
        .withColumn("order_id", (F.col("order_sk") + F.lit(1)).cast("long"))
        .withColumn("customer_id", (F.floor(F.rand(seed + 1) * n_customers) + 1).cast("long"))
        .withColumn("order_date", F.date_add(F.to_date(F.lit("2022-01-01")), (F.rand(seed + 2) * 730).cast("int")))
        .withColumn("status_r", F.rand(seed + 3))
        .withColumn(
            "order_status",
            F.when(F.col("status_r") < 0.05, F.lit("CANCELLED"))
             .when(F.col("status_r") < 0.15, F.lit("PENDING"))
             .when(F.col("status_r") < 0.70, F.lit("SHIPPED"))
             .otherwise(F.lit("DELIVERED"))
        )
        .withColumn(
            "payment_method",
            F.when(r < 0.50, "CREDIT_CARD")
             .when(r < 0.75, "PAYPAL")
             .when(r < 0.90, "APPLE_PAY")
             .otherwise("BANK_TRANSFER")
        )
        .withColumn("order_year", F.year("order_date"))
        .withColumn("order_month", F.month("order_date"))
    )
    return df.select("order_id","customer_id","order_date","order_year","order_month","order_status","payment_method")

def gen_order_items(spark, orders_df, products_df, n_products, seed=303):
    # For each order, create 1-5 items, random product and quantity; price comes from products
    items = (
        orders_df
        .withColumn("item_count", (F.floor(F.rand(seed) * 5) + 1).cast("int"))
        .withColumn("item_seq", F.expr("sequence(1, item_count)"))
        .withColumn("item_no", F.explode("item_seq"))
        .drop("item_seq","item_count")
        .withColumn("product_id", (F.floor(F.rand(seed + 1) * n_products) + 1).cast("long"))
        .withColumn("quantity", (F.floor(F.rand(seed + 2) * 4) + 1).cast("int"))
    )

    items = (
        items.join(broadcast(products_df.select("product_id","price")), on="product_id", how="left")
        .withColumn("discount",
            F.when(F.rand(seed + 3) < 0.02, F.lit(0.20))
             .when(F.rand(seed + 4) < 0.10, F.lit(0.05))
             .otherwise(F.lit(0.0))
        )
        .withColumn("unit_price", F.col("price"))
        .drop("price")
        .withColumn("extended_price", F.round(F.col("quantity") * F.col("unit_price") * (1 - F.col("discount")), 2))
    )

    # Derive item_id per order (dense rank by item_no) or global sequence
    items = (
        items.withColumn("item_id", F.monotonically_increasing_id())
             .select("item_id","order_id","product_id","quantity","unit_price","discount","extended_price","order_date","order_year","order_month")
    )

    # Compute order totals and return both items and updated orders with totals
    order_totals = items.groupBy("order_id").agg(F.round(F.sum("extended_price"), 2).alias("order_total"))
    return items, order_totals

def attach_order_totals(orders_df, order_totals):
    return (
        orders_df.join(order_totals, on="order_id", how="left")
                 .withColumn("order_total", F.coalesce(F.col("order_total"), F.lit(0.0)))
    )

def main():
    p = argparse.ArgumentParser(description="Generate synthetic datasets with PySpark")
    p.add_argument("--out", default="./synthetic_data", help="Base output path (local or cloud, e.g., s3://bucket/path)")
    p.add_argument("--formats", default="parquet", help="Comma-separated formats: parquet,csv,json")
    p.add_argument("--mode", default="overwrite", choices=["overwrite","append"], help="Save mode")
    p.add_argument("--customers", type=int, default=10000, help="Number of customers")
    p.add_argument("--products", type=int, default=1000, help="Number of products")
    p.add_argument("--orders", type=int, default=50000, help="Number of orders")
    p.add_argument("--seed", type=int, default=1234, help="Base random seed")
    p.add_argument("--partitions", type=int, default=200, help="spark.sql.shuffle.partitions")
    p.add_argument("--driver-mem", default="4g", help="Spark driver memory")
    p.add_argument("--coalesce-small", type=int, default=1, help="Coalesce small dimension tables to N files")
    args = p.parse_args()

    formats = [f.strip().lower() for f in args.formats.split(",") if f.strip()]
    os.makedirs(args.out, exist_ok=True)

    spark = create_spark(shuffle_partitions=args.partitions, driver_mem=args.driver_mem)

    # Generate datasets
    customers_df = gen_customers(spark, args.customers, seed=args.seed + 1)
    products_df = gen_products(spark, args.products, seed=args.seed + 2)
    orders_df = gen_orders(spark, args.orders, args.customers, seed=args.seed + 3)
    order_items_df, order_totals_df = gen_order_items(spark, orders_df, products_df, args.products, seed=args.seed + 4)
    orders_enriched_df = attach_order_totals(orders_df, order_totals_df)

    # Optional repartitioning for larger datasets
    big_parts = max(1, min(args.partitions, 200))
    orders_enriched_df = orders_enriched_df.repartition(big_parts, "order_year","order_month")
    order_items_df = order_items_df.repartition(big_parts, "order_year","order_month")

    # Save datasets in desired formats
    for fmt in formats:
        save_df(customers_df, args.out, "customers", fmt=fmt, mode=args.mode, coalesce=args.coalesce_small)
        save_df(products_df, args.out, "products", fmt=fmt, mode=args.mode, coalesce=args.coalesce_small)
        save_df(orders_enriched_df, args.out, "orders", fmt=fmt, mode=args.mode, partition_cols=["order_year","order_month"])
        save_df(order_items_df, args.out, "order_items", fmt=fmt, mode=args.mode, partition_cols=["order_year","order_month"])

    # Show a quick summary
    print("Row counts:")
    print(f"  customers:     {customers_df.count():,}")
    print(f"  products:      {products_df.count():,}")
    print(f"  orders:        {orders_enriched_df.count():,}")
    print(f"  order_items:   {order_items_df.count():,}")

    # Example: read back Parquet orders
    if "parquet" in formats:
        sample_orders = spark.read.parquet(os.path.join(args.out, "orders.parquet"))
        sample_orders.orderBy(F.desc("order_total")).show(5, truncate=False)

    spark.stop()

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 446:
"Generate Python code that imports the necessary libraries and creates a wrapper function for each MetalFX function, allowing users to call the functions in a Pythonic way."
RESPONSE:
import sys
import platform
from typing import Optional, Tuple, Any, Dict

# Requires: pip install pyobjc-core pyobjc-framework-Cocoa pyobjc-framework-Metal
try:
    import objc
    from Foundation import NSBundle
    import Metal
except Exception as exc:
    raise RuntimeError("This code requires macOS with PyObjC and the Metal framework installed.") from exc


def _ensure_macos_13_plus() -> None:
    ver = tuple(int(v) for v in platform.mac_ver()[0].split(".")[:2] if v.isdigit())
    if not ver or ver < (13, 0):
        raise RuntimeError("MetalFX requires macOS 13 or newer.")


def _load_metalfx() -> None:
    # Load MetalFX framework so its classes are available via objc.lookupClass
    path = "/System/Library/Frameworks/MetalFX.framework"
    bundle = NSBundle.bundleWithPath_(path)
    if bundle is None:
        raise RuntimeError("MetalFX.framework not found (requires macOS 13+).")
    objc.loadBundle("MetalFX", globals(), bundle_path=path)


def _lookup(cls_name: str):
    try:
        return objc.lookUpClass(cls_name)
    except Exception as exc:
        raise RuntimeError(f"Required MetalFX class '{cls_name}' not found. Make sure the system supports MetalFX.") from exc


def _set_if_has(obj: Any, prop: str, value: Any) -> bool:
    try:
        setattr(obj, prop, value)
        return True
    except Exception:
        setter = "set" + prop[0].upper() + prop[1:] + "_"
        if hasattr(obj, setter):
            getattr(obj, setter)(value)
            return True
    return False


# Public: device/queue helpers

def mtl_default_device():
    """Return the system default MTLDevice."""
    dev = Metal.MTLCreateSystemDefaultDevice()
    if dev is None:
        raise RuntimeError("No compatible Metal device found.")
    return dev


def mtl_command_queue(device) -> Any:
    """Create a command queue from a device."""
    q = device.newCommandQueue()
    if q is None:
        raise RuntimeError("Failed to create MTLCommandQueue.")
    return q


# Public: MetalFX wrappers

def create_spatial_scaler(
    device: Any,
    input_size: Tuple[int, int],
    output_size: Tuple[int, int],
    *,
    color_processing_mode: Optional[int] = None,
    # Known property on some OS/hw versions. Often range [0, 1].
    sharpness: Optional[float] = None,
    label: Optional[str] = None,
) -> Any:
    """
    Create and return an MTLFXSpatialScaler.
    - input_size/output_size: (width, height)
    - color_processing_mode: enum value if available (e.g., SDR/HDR)
    - sharpness: optional per-scaler default sharpness
    """
    _ensure_macos_13_plus()
    _load_metalfx()

    MTLFXSpatialScalerDescriptor = _lookup("MTLFXSpatialScalerDescriptor")

    in_w, in_h = input_size
    out_w, out_h = output_size

    desc = MTLFXSpatialScalerDescriptor.new()
    _set_if_has(desc, "inputWidth", int(in_w))
    _set_if_has(desc, "inputHeight", int(in_h))
    _set_if_has(desc, "outputWidth", int(out_w))
    _set_if_has(desc, "outputHeight", int(out_h))
    if color_processing_mode is not None:
        _set_if_has(desc, "colorProcessingMode", int(color_processing_mode))

    scaler = device.newSpatialScalerWithDescriptor_(desc)
    if scaler is None:
        raise RuntimeError("Device failed to create MTLFXSpatialScaler (check device/OS support).")

    if label:
        _set_if_has(scaler, "label", label)
    if sharpness is not None:
        _set_if_has(scaler, "sharpness", float(sharpness))

    return scaler


def encode_spatial_scaling(
    scaler: Any,
    command_buffer: Any,
    *,
    color_texture: Any,
    output_texture: Any,
    # These are optional per-encode overrides, if supported by your OS/device version
    sharpness: Optional[float] = None,
) -> None:
    """
    Encode a spatial scaling pass.
    Required:
      - color_texture: input color MTLTexture
      - output_texture: output MTLTexture
    Optional:
      - sharpness: per-encode override (if property exists)
    """
    if sharpness is not None:
        _set_if_has(scaler, "sharpness", float(sharpness))

    _set_if_has(scaler, "colorTexture", color_texture)
    _set_if_has(scaler, "outputTexture", output_texture)
    scaler.encodeToCommandBuffer_(command_buffer)


def spatial_prefetch_resources(scaler: Any) -> None:
    """Optionally prefetch resources for a spatial scaler."""
    if hasattr(scaler, "prefetchResources"):
        scaler.prefetchResources()


def spatial_purge_resources(scaler: Any) -> None:
    """Optionally purge resources for a spatial scaler."""
    if hasattr(scaler, "purgeResources"):
        scaler.purgeResources()


def create_temporal_scaler(
    device: Any,
    input_size: Tuple[int, int],
    output_size: Tuple[int, int],
    *,
    # Optional content vs. render input region (if using dynamic upscaling window)
    input_content_size: Optional[Tuple[int, int]] = None,
    color_processing_mode: Optional[int] = None,
    # Optional behavior knobs (availability varies by OS/hw)
    motion_vector_scale: Optional[Tuple[float, float]] = None,
    exposure_is_hdr: Optional[bool] = None,
    pre_exposure: Optional[float] = None,
    label: Optional[str] = None,
) -> Any:
    """
    Create and return an MTLFXTemporalScaler.
    - input_size/output_size: (width, height)
    - input_content_size: (width, height) of active content in input
    - color_processing_mode: enum value if available (e.g., SDR/HDR)
    """
    _ensure_macos_13_plus()
    _load_metalfx()

    MTLFXTemporalScalerDescriptor = _lookup("MTLFXTemporalScalerDescriptor")

    in_w, in_h = input_size
    out_w, out_h = output_size
    desc = MTLFXTemporalScalerDescriptor.new()
    _set_if_has(desc, "inputWidth", int(in_w))
    _set_if_has(desc, "inputHeight", int(in_h))
    _set_if_has(desc, "outputWidth", int(out_w))
    _set_if_has(desc, "outputHeight", int(out_h))

    if input_content_size is not None:
        cw, ch = input_content_size
        _set_if_has(desc, "inputContentWidth", int(cw))
        _set_if_has(desc, "inputContentHeight", int(ch))

    if color_processing_mode is not None:
        _set_if_has(desc, "colorProcessingMode", int(color_processing_mode))
    if motion_vector_scale is not None:
        sx, sy = motion_vector_scale
        _set_if_has(desc, "motionVectorScaleX", float(sx))
        _set_if_has(desc, "motionVectorScaleY", float(sy))
    if exposure_is_hdr is not None:
        _set_if_has(desc, "exposureIsHDR", bool(exposure_is_hdr))
    if pre_exposure is not None:
        _set_if_has(desc, "preExposure", float(pre_exposure))

    scaler = device.newTemporalScalerWithDescriptor_(desc)
    if scaler is None:
        raise RuntimeError("Device failed to create MTLFXTemporalScaler (check device/OS support).")

    if label:
        _set_if_has(scaler, "label", label)

    return scaler


def reset_temporal_history(scaler: Any) -> None:
    """
    Request resetting the temporal history on the next encode.
    Many OS versions expose 'reset' or 'resetHistory' toggle.
    """
    if not (_set_if_has(scaler, "reset", True) or _set_if_has(scaler, "resetHistory", True)):
        # Fallback: if no reset toggle is available, no-op
        pass


def encode_temporal_scaling(
    scaler: Any,
    command_buffer: Any,
    *,
    color_texture: Any,
    output_texture: Any,
    depth_texture: Optional[Any] = None,
    motion_vectors_texture: Optional[Any] = None,
    exposure_texture: Optional[Any] = None,
    reactive_mask_texture: Optional[Any] = None,
    dilated_motion_texture: Optional[Any] = None,
    # Per-encode optional parameters
    jitter_offset: Optional[Tuple[float, float]] = None,
    motion_vector_scale: Optional[Tuple[float, float]] = None,
    pre_exposure: Optional[float] = None,
    reset_history: Optional[bool] = None,
) -> None:
    """
    Encode a temporal scaling pass. Provide the textures you have; unavailable ones are skipped if optional.
    Required:
      - color_texture, output_texture
    Optional (commonly used):
      - depth_texture, motion_vectors_texture, exposure_texture
      - reactive_mask_texture, dilated_motion_texture
      - jitter_offset=(x, y), motion_vector_scale=(sx, sy), pre_exposure, reset_history
    """
    # Per-encode configuration
    if jitter_offset is not None:
        jx, jy = jitter_offset
        _set_if_has(scaler, "jitterOffsetX", float(jx))
        _set_if_has(scaler, "jitterOffsetY", float(jy))
    if motion_vector_scale is not None:
        sx, sy = motion_vector_scale
        _set_if_has(scaler, "motionVectorScaleX", float(sx))
        _set_if_has(scaler, "motionVectorScaleY", float(sy))
    if pre_exposure is not None:
        _set_if_has(scaler, "preExposure", float(pre_exposure))
    if reset_history is not None:
        if not (_set_if_has(scaler, "reset", bool(reset_history)) or _set_if_has(scaler, "resetHistory", bool(reset_history))):
            pass

    # Required textures
    _set_if_has(scaler, "colorTexture", color_texture)
    _set_if_has(scaler, "outputTexture", output_texture)

    # Optional textures (set if provided and property exists)
    if depth_texture is not None:
        _set_if_has(scaler, "depthTexture", depth_texture)
    if motion_vectors_texture is not None:
        _set_if_has(scaler, "motionVectorTexture", motion_vectors_texture) or _set_if_has(scaler, "motionVectorsTexture", motion_vectors_texture)
    if exposure_texture is not None:
        _set_if_has(scaler, "exposureTexture", exposure_texture)
    if reactive_mask_texture is not None:
        _set_if_has(scaler, "reactiveMaskTexture", reactive_mask_texture)
    if dilated_motion_texture is not None:
        _set_if_has(scaler, "dilatedMotionVectorTexture", dilated_motion_texture) or _set_if_has(scaler, "dilatedMotionVectorsTexture", dilated_motion_texture)

    scaler.encodeToCommandBuffer_(command_buffer)


def temporal_prefetch_resources(scaler: Any) -> None:
    """Optionally prefetch resources for a temporal scaler."""
    if hasattr(scaler, "prefetchResources"):
        scaler.prefetchResources()


def temporal_purge_resources(scaler: Any) -> None:
    """Optionally purge resources for a temporal scaler."""
    if hasattr(scaler, "purgeResources"):
        scaler.purgeResources()


# Example usage (commented):
# device = mtl_default_device()
# queue = mtl_command_queue(device)
# cmd_buf = queue.commandBuffer()
#
# spatial = create_spatial_scaler(device, (1280, 720), (2560, 1440), sharpness=0.2)
# encode_spatial_scaling(spatial, cmd_buf, color_texture=input_tex, output_texture=out_tex)
#
# temporal = create_temporal_scaler(device, (1280, 720), (2560, 1440))
# encode_temporal_scaling(temporal, cmd_buf,
#                         color_texture=input_tex,
#                         output_texture=out_tex,
#                         depth_texture=depth_tex,
#                         motion_vectors_texture=mv_tex,
#                         jitter_offset=(0.5, -0.5))
# cmd_buf.commit()
------------------------------------------------------------

PROMPT 447:
"Generate Python code that imports the grpc module and creates a function that takes a server_call_details argument, which is then used to create a CallCredentials instance."
RESPONSE:
import grpc

def call_credentials_from_server_details(server_call_details):
    """
    Create a grpc.CallCredentials from server_call_details.
    server_call_details is expected to have:
      - method: str
      - invocation_metadata: iterable of (key, value) pairs
    """
    method = getattr(server_call_details, 'method', None)
    metadata = getattr(server_call_details, 'invocation_metadata', ()) or ()

    # Example: extract an authorization header if present
    auth_value = None
    for key, value in metadata:
        if key.lower() == 'authorization':
            auth_value = value
            break

    def metadata_callback(context, callback):
        md = []
        if auth_value:
            md.append(('authorization', auth_value))
        if method:
            md.append(('x-origin-method', method))
        callback(tuple(md), None)

    return grpc.metadata_call_credentials(metadata_callback)
------------------------------------------------------------

PROMPT 448:
"Generate Python code that imports the necessary modules, creates a VMware API client, and defines a function that takes a VM name and returns its corresponding typing stub."
RESPONSE:
# Requires: pip install pyvmomi
import os
import ssl
import atexit
from typing import List, Optional, TypedDict

from pyVim.connect import SmartConnect, Disconnect
from pyVmomi import vim


class VMStub(TypedDict, total=False):
    # Minimal, strongly-typed "stub" of VM information
    name: str
    moid: str
    power_state: str
    cpu_count: int
    memory_mb: int
    guest_os: str
    inventory_path: str
    datastore_names: List[str]
    host_name: str
    cluster_name: str


def create_vsphere_client(
    host: str,
    username: str,
    password: str,
    port: int = 443,
    insecure: bool = True,
) -> vim.ServiceInstance:
    """
    Connect to vSphere and return a ServiceInstance (client handle).
    """
    ssl_ctx = None
    if insecure:
        ssl_ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)
        ssl_ctx.verify_mode = ssl.CERT_NONE

    si = SmartConnect(host=host, user=username, pwd=password, port=port, sslContext=ssl_ctx)
    atexit.register(Disconnect, si)
    return si


def _find_vm_by_name(si: vim.ServiceInstance, vm_name: str) -> Optional[vim.VirtualMachine]:
    """
    Locate a VM by its display name by traversing inventory.
    """
    content = si.RetrieveContent()
    view = content.viewManager.CreateContainerView(content.rootFolder, [vim.VirtualMachine], True)
    try:
        for vm in view.view:
            # vm.name can be non-unique; this returns the first match
            if vm.name == vm_name:
                return vm
    finally:
        view.Destroy()
    return None


def _get_inventory_path(obj) -> str:
    """
    Build an inventory path like: Datacenter/VMs/Folder/.../VM
    """
    parts = []
    while obj:
        label = getattr(obj, "name", None)
        if label:
            parts.append(label)
        obj = getattr(obj, "parent", None)
    return "/".join(reversed(parts))


def _get_parent_of_type(obj, t):
    """
    Walk parents until we find the requested type t.
    """
    while obj:
        if isinstance(obj, t):
            return obj
        obj = getattr(obj, "parent", None)
    return None


def get_vm_typing_stub(si: vim.ServiceInstance, vm_name: str) -> Optional[VMStub]:
    """
    Return a typed "stub" (TypedDict) of key VM properties for the given VM name.
    If multiple VMs share the same name, returns the first match encountered.
    """
    vm = _find_vm_by_name(si, vm_name)
    if not vm:
        return None

    summary = vm.summary
    config = summary.config if summary else None
    runtime = vm.runtime

    # Derive fields safely
    moid = getattr(vm, "_moId", "")
    power_state = str(runtime.powerState) if runtime and runtime.powerState else ""
    cpu_count = config.numCpu if config and hasattr(config, "numCpu") else 0
    memory_mb = config.memorySizeMB if config and hasattr(config, "memorySizeMB") else 0
    guest_os = ""
    if summary and summary.config and summary.config.guestFullName:
        guest_os = summary.config.guestFullName
    elif config and hasattr(config, "guestId") and config.guestId:
        guest_os = config.guestId

    inventory_path = _get_inventory_path(vm)
    datastore_names = [ds.name for ds in getattr(vm, "datastore", [])]

    host_name = ""
    if runtime and runtime.host and runtime.host.name:
        host_name = runtime.host.name

    cluster_name = ""
    compute_res = _get_parent_of_type(vm, vim.ComputeResource)
    if compute_res and compute_res.name:
        cluster_name = compute_res.name

    return VMStub(
        name=vm.name,
        moid=moid,
        power_state=power_state,
        cpu_count=cpu_count,
        memory_mb=memory_mb,
        guest_os=guest_os,
        inventory_path=inventory_path,
        datastore_names=datastore_names,
        host_name=host_name,
        cluster_name=cluster_name,
    )


# Example usage (uncomment to run):
# if __name__ == "__main__":
#     si = create_vsphere_client(
#         host=os.environ["VSPHERE_HOST"],
#         username=os.environ["VSPHERE_USER"],
#         password=os.environ["VSPHERE_PASSWORD"],
#         insecure=True,
#     )
#     stub = get_vm_typing_stub(si, vm_name="MyVM")
#     print(stub)
------------------------------------------------------------

PROMPT 449:
"Generate Python code that imports the argparse module and creates a custom parser class with predefined arguments and cleanup functionality for simplified script execution."
RESPONSE:
import argparse
import contextlib
import logging
import os
import shutil
import sys
import tempfile
from typing import Callable, Iterator, Optional, List


class ScriptParser(argparse.ArgumentParser):
    """
    Custom argument parser with common arguments and cleanup/session helpers
    for simplified script execution.
    """

    def __init__(self, *args, add_default_args: bool = True, **kwargs):
        if "formatter_class" not in kwargs:
            kwargs["formatter_class"] = argparse.ArgumentDefaultsHelpFormatter
        super().__init__(*args, **kwargs)
        if add_default_args:
            self._add_common_args()

    def _add_common_args(self) -> None:
        self.add_argument("-c", "--config", help="Path to configuration file")
        self.add_argument("--dry-run", action="store_true", help="Do not perform changes")
        self.add_argument(
            "-v",
            "--verbose",
            action="count",
            default=0,
            help="Increase verbosity (repeatable)",
        )
        self.add_argument("-q", "--quiet", action="store_true", help="Suppress non-error output")
        self.add_argument(
            "--log-level",
            choices=["CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG", "NOTSET"],
            help="Override log level",
        )
        self.add_argument(
            "--tmpdir",
            default=None,
            help="Use specific temp directory; if not set, a temporary directory will be created and removed on exit",
        )
        self.add_argument("--keep-tmp", action="store_true", help="Do not remove auto-created temp directory on exit")
        self.add_argument("--version", action="version", version="%(prog)s 1.0")
        self.add_argument("--seed", type=int, help="Random seed for reproducibility")

    def parse(self, argv: Optional[List[str]] = None) -> argparse.Namespace:
        args = self.parse_args(argv)
        # Derive effective log level
        if args.log_level:
            level_name = args.log_level
        else:
            if args.quiet:
                level_name = "ERROR"
            elif args.verbose >= 2:
                level_name = "DEBUG"
            elif args.verbose == 1:
                level_name = "INFO"
            else:
                level_name = "WARNING"
        args._log_level_name = level_name
        args._tmp_created = False
        return args

    @contextlib.contextmanager
    def session(self, argv: Optional[List[str]] = None) -> Iterator[argparse.Namespace]:
        """
        Context manager that:
          - Parses args
          - Configures logging
          - Manages a temporary directory lifecycle
          - Seeds randomness if requested
        Yields the parsed args.
        """
        args = self.parse(argv)

        # Setup logging
        logger = logging.getLogger()
        previous_handlers = list(logger.handlers)
        logger.handlers = []
        handler = logging.StreamHandler(sys.stderr)
        handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
        logger.addHandler(handler)
        logger.setLevel(getattr(logging, args._log_level_name))

        # Temp directory management
        created_tmp = False
        tmpdir = args.tmpdir
        if not tmpdir:
            tmpdir = tempfile.mkdtemp(prefix=f"{self.prog}-")
            created_tmp = True
        args.tmpdir = tmpdir
        args._tmp_created = created_tmp

        # Optional seeding
        if getattr(args, "seed", None) is not None:
            try:
                import random

                random.seed(args.seed)
                try:
                    import numpy as _np  # optional

                    _np.random.seed(args.seed)
                except Exception:
                    pass
            except Exception:
                pass

        try:
            yield args
        finally:
            # Cleanup temp directory if we created it
            try:
                if created_tmp and not args.keep_tmp:
                    shutil.rmtree(tmpdir, ignore_errors=True)
            finally:
                # Restore previous logging handlers
                logger.handlers = previous_handlers

    def run(self, main: Callable[[argparse.Namespace], Optional[int]], argv: Optional[List[str]] = None) -> int:
        """
        Execute a main(args) callable within a managed session, handling cleanup and exit codes.
        Returns the exit code (0 on success by default).
        """
        exit_code = 0
        try:
            with self.session(argv) as args:
                result = main(args)
                if isinstance(result, int):
                    exit_code = result
        except KeyboardInterrupt:
            # 130 is conventional for SIGINT
            exit_code = 130
            print("", file=sys.stderr)  # newline after ^C
        except SystemExit as e:
            try:
                exit_code = int(e.code) if e.code is not None else 0
            except Exception:
                exit_code = 1
        except Exception:
            logging.exception("Unhandled exception")
            exit_code = 1

        return exit_code


# Example usage
def _main(args: argparse.Namespace) -> int:
    logging.info("Starting with tmpdir=%s, dry_run=%s", args.tmpdir, args.dry_run)
    # ... your script logic here ...
    return 0


if __name__ == "__main__":
    parser = ScriptParser(description="Example script with simplified parser and cleanup")
    sys.exit(parser.run(_main))
------------------------------------------------------------

PROMPT 450:
"Generate Python code that uses the pandas library to create a profile report for a given DataFrame, including summary statistics, data distribution, and feature correlation."
RESPONSE:
# Option A: Use ydata-profiling (recommended for a rich, single-file HTML report)
# pip install ydata-profiling
import pandas as pd

def create_profile_report(df: pd.DataFrame, output_html: str = "profile_report.html", title: str = "DataFrame Profile", minimal: bool = False):
    """
    Create a data profiling HTML report including summary stats, distributions, and correlations.
    Uses ydata-profiling if available; falls back to a lightweight pandas/matplotlib report otherwise.
    """
    try:
        from ydata_profiling import ProfileReport

        profile = ProfileReport(
            df,
            title=title,
            explorative=True,   # adds extra insights, toggle off if you want a lighter report
            minimal=minimal      # True -> faster, fewer stats/plots
        )
        profile.to_file(output_html)
        print(f"Saved profiling report to: {output_html} (ydata-profiling)")
        return
    except ImportError:
        print("ydata-profiling not installed. Falling back to a basic pandas/matplotlib report...")
        _create_basic_pandas_report(df, output_html, title)


# Option B: Lightweight fallback using only pandas + matplotlib + seaborn
def _create_basic_pandas_report(
    df: pd.DataFrame,
    output_html: str,
    title: str = "DataFrame Profile (Basic)",
    max_numeric_plots: int = 20,
    max_categorical_plots: int = 20,
    top_k_categories: int = 30
):
    import matplotlib
    matplotlib.use("Agg")  # headless rendering
    import matplotlib.pyplot as plt
    import seaborn as sns
    import base64
    from io import BytesIO
    import html

    def fig_to_img_tag(fig):
        buf = BytesIO()
        fig.tight_layout()
        fig.savefig(buf, format="png", dpi=120, bbox_inches="tight")
        plt.close(fig)
        buf.seek(0)
        b64 = base64.b64encode(buf.read()).decode("ascii")
        return f'<img style="max-width:100%;" src="data:image/png;base64,{b64}" />'

    # Start assembling HTML
    parts = []
    parts.append(f"<h1>{html.escape(title)}</h1>")
    parts.append("<h2>Data Snapshot</h2>")
    parts.append(f"<p>Rows: {df.shape[0]:,} | Columns: {df.shape[1]:,}</p>")
    parts.append("<h3>Head</h3>")
    parts.append(df.head().to_html(border=1))
    parts.append("<h3>Tail</h3>")
    parts.append(df.tail().to_html(border=1))

    # Summary statistics
    parts.append("<h2>Summary Statistics</h2>")
    desc_all = df.describe(include="all", datetime_is_numeric=True).transpose()
    parts.append(desc_all.to_html(border=1))

    # Missing values
    parts.append("<h2>Missing Values</h2>")
    miss = df.isna().sum().sort_values(ascending=False)
    miss_df = miss.to_frame("missing_count")
    miss_df["missing_ratio"] = (miss_df["missing_count"] / len(df)).round(4)
    parts.append(miss_df.to_html(border=1))

    # Distributions
    numeric_cols = df.select_dtypes(include="number").columns.tolist()
    categorical_cols = df.select_dtypes(exclude="number").columns.tolist()

    if numeric_cols:
        parts.append("<h2>Numeric Feature Distributions</h2>")
        for col in numeric_cols[:max_numeric_plots]:
            s = df[col].dropna()
            if s.empty:
                continue
            fig, ax = plt.subplots(figsize=(6, 4))
            sns.histplot(s, kde=True, ax=ax)
            ax.set_title(f"Distribution: {col}")
            ax.set_xlabel(col)
            parts.append(fig_to_img_tag(fig))

    if categorical_cols:
        parts.append("<h2>Categorical Feature Distributions</h2>")
        for col in categorical_cols[:max_categorical_plots]:
            s = df[col].astype("object")
            counts = s.value_counts(dropna=False).head(top_k_categories)
            if counts.empty:
                continue
            fig, ax = plt.subplots(figsize=(6, max(3, 0.2 * len(counts))))
            sns.barplot(x=counts.values, y=[str(x) for x in counts.index], ax=ax, orient="h")
            ax.set_title(f"Top {min(top_k_categories, len(counts))} Categories: {col}")
            ax.set_xlabel("Count")
            ax.set_ylabel(col)
            parts.append(fig_to_img_tag(fig))

    # Correlations
    if len(numeric_cols) >= 2:
        parts.append("<h2>Feature Correlation (Pearson)</h2>")
        corr = df[numeric_cols].corr(method="pearson")
        parts.append("<h3>Correlation Matrix (table)</h3>")
        parts.append(corr.round(3).to_html(border=1))

        fig, ax = plt.subplots(figsize=(min(12, 0.6 * len(numeric_cols) + 3), min(12, 0.6 * len(numeric_cols) + 3)))
        sns.heatmap(corr, cmap="coolwarm", center=0, square=True, cbar_kws={"shrink": 0.8}, ax=ax)
        ax.set_title("Correlation Heatmap (Pearson)")
        parts.append(fig_to_img_tag(fig))

    # Write to file
    html_doc = """
<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<title>{title}</title>
<style>
body {{ font-family: Arial, sans-serif; margin: 20px; }}
table {{ border-collapse: collapse; font-size: 14px; }}
th, td {{ padding: 6px 8px; border: 1px solid #ddd; }}
h1, h2, h3 {{ margin-top: 24px; }}
</style>
</head>
<body>
{content}
</body>
</html>
""".format(title=html.escape(title), content="\n".join(parts))

    with open(output_html, "w", encoding="utf-8") as f:
        f.write(html_doc)

    print(f"Saved basic profiling report to: {output_html} (pandas/matplotlib)")

# Example usage:
if __name__ == "__main__":
    # Replace this with your actual DataFrame
    df_example = pd.DataFrame({
        "age": [23, 45, 31, 52, 41, 23, 37, 29, 48, 33],
        "income": [50000, 82000, 61000, 120000, 95000, 48000, 70000, 54000, 101000, 68000],
        "city": ["NY", "SF", "NY", "LA", "LA", "NY", "CHI", "SF", "LA", "CHI"],
        "signed_up": [True, True, False, True, False, True, True, False, True, True]
    })

    create_profile_report(df_example, output_html="profile_report.html", title="Sample DataFrame Profile", minimal=False)
------------------------------------------------------------

PROMPT 451:
"Generate Python code that implements a simple data-dependent workflow using the `workflow` module, defining tasks as functions and connecting them through data dependencies."
RESPONSE:
# pip install -U "ray[default]"
import ray
from ray import workflow

# Define tasks as functions
@workflow.step
def fetch_numbers():
    # Simulate fetching data
    return [1, 2, 3, 4, 5, 6]

@workflow.step
def filter_even(nums):
    return [n for n in nums if n % 2 == 0]

@workflow.step
def square(nums):
    return [n * n for n in nums]

@workflow.step
def sum_all(nums):
    return sum(nums)

if __name__ == "__main__":
    # Initialize Ray and the workflow storage (for checkpoints)
    ray.init(ignore_reinit_error=True)
    workflow.init(storage="./workflow_data")

    # Connect tasks through data dependencies
    nums_ref = fetch_numbers.step()
    evens_ref = filter_even.step(nums_ref)
    squared_ref = square.step(evens_ref)
    total_ref = sum_all.step(squared_ref)

    # Execute the workflow and get the final result
    result = workflow.run(total_ref)
    print("Sum of squares of even numbers:", result)
------------------------------------------------------------

PROMPT 452:
"Generate Python code that uses the scikit-learn library to create a pipeline of machine learning models, and then converts the pipeline to PMML format using the pmml library."
RESPONSE:
# pip install -U scikit-learn pandas sklearn2pmml sklearn-pandas
# Note: sklearn2pmml requires a Java Runtime Environment (JRE) available on PATH.

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression

from sklearn_pandas import DataFrameMapper
from sklearn2pmml import PMMLPipeline, sklearn2pmml

# 1) Load data into a pandas DataFrame
iris = load_iris(as_frame=True)
df = iris.frame.copy()

# Make column names simple (no spaces)
df.columns = [c.replace(" (cm)", "").replace(" ", "_") for c in df.columns]

target_col = "target"
feature_cols = [c for c in df.columns if c != target_col]

X = df[feature_cols]
y = df[target_col]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 2) Build an sklearn pipeline with preprocessing + model
# Use DataFrameMapper so that sklearn2pmml can keep track of field metadata
mapper = DataFrameMapper([(col, StandardScaler()) for col in feature_cols], df_out=True)

pipeline = PMMLPipeline(
    steps=[
        ("mapper", mapper),
        ("pca", PCA(n_components=2, random_state=42)),
        ("classifier", LogisticRegression(max_iter=1000, multi_class="auto", random_state=42)),
    ],
    active_fields=feature_cols,
    target_fields=[target_col],
)

# 3) Train the pipeline
pipeline.fit(X_train, y_train)

# Optional: quick evaluation
print("Train accuracy:", pipeline.score(X_train, y_train))
print("Test accuracy:", pipeline.score(X_test, y_test))

# 4) Export the trained pipeline to PMML
# This will create a file named 'iris_pipeline.pmml' in the current directory
sklearn2pmml(pipeline, "iris_pipeline.pmml", with_repr=True)
print("PMML model saved to iris_pipeline.pmml")
------------------------------------------------------------

PROMPT 453:
"Generate Python code that imports the asyncio module and creates an IMAP4rev1 client using the aioimaplib library, allowing for asynchronous email retrieval and manipulation."
RESPONSE:
# pip install aioimaplib
import asyncio
import aioimaplib
import email
from email.message import EmailMessage

IMAP_HOST = "imap.example.com"
IMAP_PORT = 993
USERNAME = "your-username"
PASSWORD = "your-password"


async def main():
    client = aioimaplib.IMAP4_SSL(IMAP_HOST, IMAP_PORT)

    try:
        # Wait for server greeting and log in
        await client.wait_hello_from_server()
        status, _ = await client.login(USERNAME, PASSWORD)
        if status != "OK":
            raise RuntimeError("Login failed")

        # Select a mailbox (read/write)
        status, _ = await client.select("INBOX")
        if status != "OK":
            raise RuntimeError("Cannot select INBOX")

        # Search for unread messages using UID (safer than sequence numbers)
        status, data = await client.uid("SEARCH", "UNSEEN")
        if status != "OK":
            raise RuntimeError("Search failed")

        uids = data[0].decode().split() if data and data[0] else []
        print(f"Found {len(uids)} unseen message(s)")

        # Fetch and mark messages as seen
        for uid in uids:
            status, fetch_data = await client.uid("FETCH", uid, "(RFC822 FLAGS)")
            if status != "OK":
                print(f"Fetch failed for UID {uid}")
                continue

            # fetch_data is typically a list; the raw message bytes are in a tuple entry
            for part in fetch_data:
                if isinstance(part, tuple) and len(part) > 1:
                    raw_msg = part[1]
                    msg = email.message_from_bytes(raw_msg)
                    print("Subject:", msg.get("Subject"))

            # Mark as seen
            await client.uid("STORE", uid, "+FLAGS", "(\\Seen)")

        # Append a new message to INBOX (as an example of manipulation)
        new_msg = EmailMessage()
        new_msg["From"] = USERNAME
        new_msg["To"] = USERNAME
        new_msg["Subject"] = "Hello from aioimaplib"
        new_msg.set_content("This is a test message appended asynchronously.")
        status, _ = await client.append("INBOX", new_msg.as_bytes(), flags="(\\Seen)")
        if status == "OK":
            print("Appended a new message to INBOX")

        # Optionally delete the first unseen message (if any) and expunge
        if uids:
            await client.uid("STORE", uids[0], "+FLAGS", "(\\Deleted)")
            await client.expunge()
            print(f"Deleted UID {uids[0]} and expunged")

    finally:
        # Always try to logout gracefully
        try:
            await client.logout()
        except Exception:
            pass


if __name__ == "__main__":
    asyncio.run(main())
------------------------------------------------------------

PROMPT 454:
"Generate Python code that uses the `pyorb` library to interact with an ORB (Object Request Broker) and perform operations such as publishing, subscribing, and requesting data."
RESPONSE:
I want to confirm one thing before you run this: by “pyorb” do you mean the Python CORBA bindings commonly used via omniORBpy? That’s the de-facto way to interact with an ORB from Python. If you meant a different package named “pyorb,” please share a link or version so I can adapt the code. Below is fully working Python CORBA code using omniORBpy that covers:
- Request/Reply: a simple KeyValue service (server + client).
- Publish/Subscribe: a push supplier and push consumer using the CORBA Event Service (CosEvent).

Files:

1) IDL: kv_service.idl
module Demo {
  interface KeyValue {
    string get_data(in string key);
    void put_data(in string key, in string value);
  };
};

2) Generate Python stubs/skeletons
Run:
- omniidl -bpython kv_service.idl

This generates kv_service_idl.py which registers Demo and Demo__POA modules.

3) Server (request/reply): server_kv.py
import sys
from omniORB import CORBA, PortableServer
import CosNaming
import Demo, Demo__POA  # from kv_service.idl (generated)

class KeyValue_i(Demo__POA.KeyValue):
    def __init__(self):
        self.store = {}

    def get_data(self, key):
        return self.store.get(key, "")

    def put_data(self, key, value):
        self.store[key] = value
        return None

def main():
    orb = CORBA.ORB_init(sys.argv, CORBA.ORB_ID)
    poa = orb.resolve_initial_references("RootPOA")
    poa_mgr = poa._get_the_POAManager()

    servant = KeyValue_i()
    oid = poa.activate_object(servant)
    objref = servant._this()

    # Bind in Naming Service under Demo/KeyValue
    ns = orb.resolve_initial_references("NameService")
    nc = ns._narrow(CosNaming.NamingContext)
    if nc is None:
        print("NameService narrow failed")
        sys.exit(1)

    name = [CosNaming.NameComponent("Demo", ""),
            CosNaming.NameComponent("KeyValue", "")]
    try:
        nc.bind(name, objref)
    except CosNaming.NamingContext.AlreadyBound:
        nc.rebind(name, objref)

    print("KeyValue server ready.")
    poa_mgr.activate()
    orb.run()

if __name__ == "__main__":
    main()

4) Client (request/reply): client_kv.py
import sys
from omniORB import CORBA
import CosNaming
import Demo  # from kv_service.idl (generated)

def main():
    orb = CORBA.ORB_init(sys.argv, CORBA.ORB_ID)
    ns = orb.resolve_initial_references("NameService")
    nc = ns._narrow(CosNaming.NamingContext)

    name = [CosNaming.NameComponent("Demo", ""),
            CosNaming.NameComponent("KeyValue", "")]
    obj = nc.resolve(name)
    kv = obj._narrow(Demo.KeyValue)

    kv.put_data("alpha", "42")
    print("alpha =", kv.get_data("alpha"))

if __name__ == "__main__":
    main()

5) Publisher (CosEvent push supplier): publisher.py
import sys
import time
from omniORB import CORBA
import CosEventChannelAdmin, CosEventComm

def main():
    if len(sys.argv) != 2:
        print("Usage: publisher.py <EventChannel_IOR_file>")
        sys.exit(1)
    orb = CORBA.ORB_init(sys.argv, CORBA.ORB_ID)

    # Read EventChannel IOR from file
    with open(sys.argv[1], "r") as f:
        ior = f.read().strip()
    ec = orb.string_to_object(ior)
    ec = ec._narrow(CosEventChannelAdmin.EventChannel)
    if ec is None:
        print("Failed to narrow EventChannel")
        sys.exit(1)

    supp_admin = ec.for_suppliers()
    proxy_consumer = supp_admin.obtain_push_consumer()

    # We can pass a nil PushSupplier reference
    supplier_ref = CosEventComm.PushSupplier._nil()
    proxy_consumer.connect_push_supplier(supplier_ref)

    try:
        for i in range(5):
            msg = f"update {i}"
            any_msg = CORBA.Any(CORBA.TC_string, msg)
            print("Publishing:", msg)
            proxy_consumer.push(any_msg)
            time.sleep(1.0)
    finally:
        try:
            proxy_consumer.disconnect_push_consumer()
        except Exception:
            pass

if __name__ == "__main__":
    main()

6) Subscriber (CosEvent push consumer): subscriber.py
import sys
from omniORB import CORBA, PortableServer
import CosEventChannelAdmin, CosEventComm, CosEventComm__POA

class PushConsumer_i(CosEventComm__POA.PushConsumer):
    def push(self, data):
        # data is a CORBA.Any
        try:
            print("Received:", data.value())
        except Exception:
            print("Received (raw Any):", data)

    def disconnect_push_consumer(self):
        print("Disconnected by supplier")

def main():
    if len(sys.argv) != 2:
        print("Usage: subscriber.py <EventChannel_IOR_file>")
        sys.exit(1)

    orb = CORBA.ORB_init(sys.argv, CORBA.ORB_ID)
    poa = orb.resolve_initial_references("RootPOA")
    poa._get_the_POAManager().activate()

    with open(sys.argv[1], "r") as f:
        ior = f.read().strip()
    ec = orb.string_to_object(ior)
    ec = ec._narrow(CosEventChannelAdmin.EventChannel)
    if ec is None:
        print("Failed to narrow EventChannel")
        sys.exit(1)

    cons_admin = ec.for_consumers()
    proxy_supplier = cons_admin.obtain_push_supplier()

    servant = PushConsumer_i()
    objref = servant._this()
    proxy_supplier.connect_push_consumer(objref)

    print("Subscriber attached. Waiting for events...")
    orb.run()

if __name__ == "__main__":
    main()

How to run (example with omniORB/omniEvents):
- Install: pip install omniORBpy
- Start Naming Service (one-time):
  - omniNames -start 2809
- Ensure clients/servers can find the NameService, e.g. export ORBInitRef:
  - On Linux/macOS: export ORBInitRef=NameService=corbaloc::localhost:2809/NameService
  - On Windows PowerShell: $env:ORBInitRef="NameService=corbaloc::localhost:2809/NameService"
- Build stubs: omniidl -bpython kv_service.idl
- Start server: python server_kv.py
- Run client: python client_kv.py
- Start Event Service (omniEvents):
  - omniEvents (it will print an EventChannel IOR; save it to a file, e.g., channel.ior)
- In separate terminals:
  - Subscriber: python subscriber.py channel.ior
  - Publisher: python publisher.py channel.ior

Notes:
- The request/reply part uses the Naming Service to resolve the KeyValue object.
- The publish/subscribe part uses the CORBA Event Service (CosEvent); both publisher and subscriber connect to the same EventChannel IOR. Many deployments bind the EventChannel in the Naming Service as well; if you do that, you can resolve it instead of passing an IOR file.
- If you intended a different “pyorb” package than omniORBpy, please specify which one, and I’ll translate this into the correct API.
------------------------------------------------------------

PROMPT 455:
"Generate Python code that uses machine learning algorithms to infer the date format from a set of example dates, saving the inferred format as a Python datetime format string."
RESPONSE:
Below is a self-contained Python script that uses a character n-gram machine learning classifier (scikit-learn) trained on synthetic date strings to infer the most likely Python datetime format string for a given set of example dates. It then validates the top predictions by actually parsing your examples and saves the inferred format to a file.

Notes:
- Requires Python 3.8+ and scikit-learn.
- Works for many common date and datetime formats, including ISO-8601 variants with or without timezone offsets and milliseconds.
- If no single format parses all examples, it returns the one that parses the most and warns.

Save as infer_date_format.py and run it either with a file containing one date per line:
  python infer_date_format.py examples.txt
or by passing the dates directly as arguments:
  python infer_date_format.py "2024-09-07" "2024-10-01"

Code:

import sys
import os
import random
import math
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Dict

# You need scikit-learn installed:
#   pip install scikit-learn
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression


def candidate_formats() -> List[str]:
    """
    A curated set of common Python datetime format strings that cover:
    - Date-only with various separators and orders (YMD, DMY, MDY)
    - Month names (short/long)
    - Date-time with spaces or 'T'
    - Seconds and fractional seconds
    - Time zones as %z or literal 'Z'
    - 12-hour clock with AM/PM
    """
    return [
        # Date only
        "%Y-%m-%d",
        "%d-%m-%Y",
        "%m-%d-%Y",
        "%Y/%m/%d",
        "%d/%m/%Y",
        "%m/%d/%Y",
        "%Y.%m.%d",
        "%d.%m.%Y",
        "%Y%m%d",
        "%d%m%Y",
        "%m%d%Y",
        "%d %b %Y",
        "%d %B %Y",
        "%b %d, %Y",
        "%B %d, %Y",
        "%d-%b-%Y",
        "%d-%B-%Y",

        # Date + time (space)
        "%Y-%m-%d %H:%M",
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%d %H:%M:%S.%f",
        "%d/%m/%Y %H:%M",
        "%d/%m/%Y %H:%M:%S",
        "%m/%d/%Y %H:%M",
        "%m/%d/%Y %H:%M:%S",
        "%d-%m-%Y %H:%M:%S",
        "%m-%d-%Y %H:%M:%S",
        "%d.%m.%Y %H:%M:%S",
        "%b %d, %Y %H:%M",
        "%b %d, %Y %H:%M:%S",
        "%B %d, %Y %H:%M",
        "%B %d, %Y %H:%M:%S",
        "%d %b %Y %H:%M",
        "%d %b %Y %H:%M:%S",
        "%d %B %Y %H:%M",
        "%d %B %Y %H:%M:%S",

        # 12-hour with AM/PM
        "%Y-%m-%d %I:%M %p",
        "%m/%d/%Y %I:%M %p",
        "%d/%m/%Y %I:%M %p",
        "%b %d, %Y %I:%M %p",

        # ISO-like
        "%Y-%m-%dT%H:%M",
        "%Y-%m-%dT%H:%M:%S",
        "%Y-%m-%dT%H:%M:%S.%f",

        # ISO with offset
        "%Y-%m-%dT%H:%M%z",
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%dT%H:%M:%S.%f%z",

        # ISO with literal Z (UTC)
        "%Y-%m-%dT%H:%MZ",
        "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%dT%H:%M:%S.%fZ",
    ]


def random_datetime(start_year=1990, end_year=2035) -> datetime:
    start = datetime(start_year, 1, 1, 0, 0, 0)
    end = datetime(end_year, 12, 31, 23, 59, 59)
    delta = end - start
    seconds = random.randint(0, int(delta.total_seconds()))
    dt = start + timedelta(seconds=seconds)
    # Randomly include microseconds to exercise %f
    if random.random() < 0.5:
        dt = dt.replace(microsecond=random.randint(0, 999999))
    return dt


def random_tzinfo() -> timezone:
    # Offsets from -12:00 to +14:00 in 30-min increments
    # These cover real-world offsets including +05:30, +09:30, etc.
    half_hour_steps = list(range(-24, 29))  # -12:00 -> +14:00
    hhmm = random.choice(half_hour_steps)
    hours = hhmm // 2
    minutes = 30 if hhmm % 2 else 0
    return timezone(timedelta(hours=hours, minutes=minutes))


def insert_colon_in_offset(s: str) -> str:
    # Convert +HHMM or -HHMM to +HH:MM or -HH:MM if present at end or before Z char replacement
    # Find last +/- followed by 4 digits
    # E.g., 2023-03-01T12:34:56+0530 -> +05:30
    for sign in ['+', '-']:
        idx = s.rfind(sign)
        if idx != -1 and len(s) >= idx + 5:
            chunk = s[idx:idx+5]
            if chunk[1:].isdigit():
                return s[:idx] + chunk[:3] + ":" + chunk[3:] + s[idx+5:]
    return s


def synthesize_for_format(fmt: str, n: int = 200) -> List[str]:
    """
    Generate n synthetic examples for a given format string.
    If the format contains %z, we create timezone-aware datetimes.
    If the format ends with literal 'Z', we enforce UTC and literal Z.
    We also include colon and non-colon offset variants to make the classifier robust.
    """
    out = []
    z_literal = fmt.endswith('Z')
    has_offset = '%z' in fmt

    for _ in range(n):
        dt = random_datetime()
        tz = None

        if z_literal:
            # Force UTC
            tz = timezone.utc
            dt = dt.replace(tzinfo=tz)
            base = dt.strftime(fmt[:-1])  # format up to before 'Z'
            s = base + 'Z'
            out.append(s)
        elif has_offset:
            tz = random_tzinfo()
            dt = dt.replace(tzinfo=tz)
            s = dt.strftime(fmt)
            out.append(s)
            # Also add a colon-in-offset variant, which %z can parse since Python 3.7
            out.append(insert_colon_in_offset(s))
        else:
            s = dt.strftime(fmt)
            out.append(s)

    return out


def build_training_corpus(formats: List[str], per_format: int = 200) -> Tuple[List[str], List[str]]:
    X, y = [], []
    for fmt in formats:
        strings = synthesize_for_format(fmt, per_format)
        X.extend(strings)
        y.extend([fmt] * len(strings))
    return X, y


def train_model() -> Tuple[Pipeline, List[str]]:
    random.seed(42)
    formats = candidate_formats()
    X, y = build_training_corpus(formats, per_format=120)  # total ~ formats * 120, manageable
    pipe = Pipeline([
        ("tfidf", TfidfVectorizer(analyzer='char', ngram_range=(2, 5), min_df=1)),
        ("clf", LogisticRegression(max_iter=300, n_jobs=None, multi_class='auto'))
    ])
    pipe.fit(X, y)
    return pipe, sorted(list(set(y)))


def try_parse_with_format(s: str, fmt: str) -> bool:
    try:
        datetime.strptime(s, fmt)
        return True
    except Exception:
        return False


def infer_format(example_dates: List[str], save_path: str = "inferred_format.txt") -> str:
    # Clean inputs
    examples = [e.strip() for e in example_dates if e and e.strip()]
    if not examples:
        raise ValueError("No non-empty example dates provided.")

    model, classes = train_model()

    # Predict class probabilities for each example
    # classes_ corresponds to the labels order for proba columns
    probs = model.predict_proba(examples)
    class_indices = {c: i for i, c in enumerate(model.classes_)}

    # Aggregate score per class by summing log probabilities across examples
    scores: Dict[str, float] = {c: 0.0 for c in model.classes_}
    eps = 1e-12
    for c in model.classes_:
        idx = class_indices[c]
        score = 0.0
        for row in probs:
            score += math.log(max(row[idx], eps))
        scores[c] = score

    # Rank classes by score
    ranked = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)
    top_candidates = [c for c, _ in ranked[:8]]  # try top 8

    # Validate by actual parsing. Prefer the first that parses all.
    best_format = None
    for fmt in top_candidates:
        if all(try_parse_with_format(s, fmt) for s in examples):
            best_format = fmt
            break

    # If none parse all examples, choose the one that parses the most and warn
    if best_format is None:
        parse_counts = []
        for fmt in top_candidates:
            count = sum(1 for s in examples if try_parse_with_format(s, fmt))
            parse_counts.append((fmt, count))
        parse_counts.sort(key=lambda kv: kv[1], reverse=True)
        best_format, best_count = parse_counts[0]
        if best_count == 0:
            raise ValueError("Could not find a matching format; none of the top candidates parsed any example.")
        print(f"Warning: no single format parsed all examples. Choosing the format that parsed the most ({best_count}/{len(examples)}).")

    # Save to file
    with open(save_path, "w", encoding="utf-8") as f:
        f.write(best_format)

    return best_format


def _read_examples_from_argv() -> List[str]:
    """
    Reads examples from argv.
    - If one argument is passed and it's a file path, read one date per line.
    - Otherwise, treat each argument as a date example.
    """
    args = sys.argv[1:]
    if not args:
        print("Usage:")
        print("  python infer_date_format.py path/to/examples.txt")
        print('  python infer_date_format.py "2024-09-07" "2024-10-01"')
        sys.exit(1)

    if len(args) == 1 and os.path.exists(args[0]) and os.path.isfile(args[0]):
        with open(args[0], "r", encoding="utf-8") as f:
            lines = [line.rstrip("\n") for line in f]
        return [x for x in lines if x.strip()]
    else:
        return args


if __name__ == "__main__":
    examples = _read_examples_from_argv()
    fmt = infer_format(examples, save_path="inferred_format.txt")
    print(f"Inferred Python datetime format: {fmt}")
    print("Saved to inferred_format.txt")

How it works in brief:
- It synthesizes a labeled training set across many common date/time formats by formatting random datetimes, then trains a character n-gram logistic regression classifier to recognize format patterns.
- For your inputs, it scores each candidate format, validates by attempting to parse the examples, and returns/saves the best format string.
------------------------------------------------------------

PROMPT 456:
"Generate Python code that reads and writes R RData and Rds files into/from pandas data frames using the pandas library and the pyarrow library for interacting with R packages."
RESPONSE:
# Python utilities to read/write R .RData and .rds files to/from pandas DataFrames.
# Notes:
# - Apache Arrow (pyarrow) cannot read/write R's native .RData or .rds formats.
# - Use pyreadr (no R install required) or rpy2 (requires an R installation) for .RData/.rds.
# - For modern, high-performance interop with R via the arrow package, prefer Feather/Parquet
#   using pyarrow (functions provided below).
#
# pip install pandas pyreadr rpy2 pyarrow

from typing import Dict, Optional, Union, Iterable
import os

import pandas as pd

# Optional deps
try:
    import pyreadr  # type: ignore
except Exception:
    pyreadr = None

try:
    import pyarrow as pa  # type: ignore
    import pyarrow.feather as feather  # type: ignore
    import pyarrow.parquet as pq  # type: ignore
except Exception:
    pa = None
    feather = None
    pq = None

try:
    from rpy2 import robjects
    from rpy2.robjects import pandas2ri
    from rpy2.robjects.conversion import localconverter
except Exception:
    robjects = None
    pandas2ri = None
    localconverter = None


# ---------------------------
# RDS (.rds) Read/Write
# ---------------------------

def read_rds(path: str, backend: str = "pyreadr") -> pd.DataFrame:
    """
    Read an R .rds file that contains a single data.frame into a pandas DataFrame.

    backend:
      - "pyreadr": uses pyreadr.read_r (no R installation needed)
      - "rpy2": uses R's readRDS via rpy2 (requires R + rpy2)
    """
    if backend == "pyreadr":
        if pyreadr is None:
            raise ImportError("pyreadr is not installed. pip install pyreadr")
        res = pyreadr.read_r(path)
        if len(res) != 1:
            raise ValueError("RDS did not contain exactly one object.")
        df = next(iter(res.values()))
        if not isinstance(df, pd.DataFrame):
            raise TypeError("RDS object is not an R data.frame convertible to pandas DataFrame.")
        return df

    elif backend == "rpy2":
        if robjects is None or pandas2ri is None:
            raise ImportError("rpy2 is not installed or not importable. pip install rpy2 (and ensure R is installed).")
        with localconverter(robjects.default_converter + pandas2ri.converter):
            r_obj = robjects.r["readRDS"](path)
            py_obj = robjects.conversion.rpy2py(r_obj)
        if not isinstance(py_obj, pd.DataFrame):
            raise TypeError("RDS object is not an R data.frame convertible to pandas DataFrame.")
        return py_obj

    else:
        raise ValueError("backend must be 'pyreadr' or 'rpy2'.")


def write_rds(df: pd.DataFrame, path: str, backend: str = "rpy2") -> None:
    """
    Write a pandas DataFrame to an R .rds file (data.frame).

    backend:
      - "rpy2": uses R's saveRDS (requires R + rpy2)
      - "pyreadr": uses pyreadr to write (no R required; if available)
    """
    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)

    if backend == "rpy2":
        if robjects is None or pandas2ri is None:
            raise ImportError("rpy2 is not installed or not importable. pip install rpy2 (and ensure R is installed).")
        with localconverter(robjects.default_converter + pandas2ri.converter):
            r_df = robjects.conversion.py2rpy(df)
        robjects.r["saveRDS"](r_df, path)
        return

    elif backend == "pyreadr":
        if pyreadr is None:
            raise ImportError("pyreadr is not installed. pip install pyreadr")
        # pyreadr supports writing; API expects (df, path)
        # If your pyreadr version differs, consult its documentation.
        pyreadr.write_rds(df, path)
        return

    else:
        raise ValueError("backend must be 'rpy2' or 'pyreadr'.")


# ---------------------------
# RData (.RData) Read/Write
# ---------------------------

def read_rdata(path: str, backend: str = "pyreadr", select: Optional[Union[str, Iterable[str]]] = None) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
    """
    Read an R .RData file into pandas DataFrames.

    Returns:
      - If select is a single name, returns a single DataFrame.
      - If select is None, returns a dict name -> DataFrame for all data.frame objects found.
      - If select is an iterable of names, returns a dict for those names.

    backend:
      - "pyreadr": uses pyreadr.read_r (no R required)
      - "rpy2": loads objects via R's load() and converts
    """
    if backend == "pyreadr":
        if pyreadr is None:
            raise ImportError("pyreadr is not installed. pip install pyreadr")
        res = pyreadr.read_r(path)  # dict-like: name -> pandas object(s)
        def filter_to_df(d):
            return {k: v for k, v in d.items() if isinstance(v, pd.DataFrame)}

        if select is None:
            return filter_to_df(res)

        if isinstance(select, str):
            if select not in res or not isinstance(res[select], pd.DataFrame):
                raise KeyError(f"Object '{select}' not found as a data.frame in RData.")
            return res[select]

        # iterable of names
        out = {}
        for name in select:
            if name not in res or not isinstance(res[name], pd.DataFrame):
                raise KeyError(f"Object '{name}' not found as a data.frame in RData.")
            out[name] = res[name]
        return out

    elif backend == "rpy2":
        if robjects is None or pandas2ri is None:
            raise ImportError("rpy2 is not installed or not importable. pip install rpy2 (and ensure R is installed).")

        # Load returns a character vector of object names loaded into the global env
        loaded_names = list(robjects.r["load"](path))
        want_names: Iterable[str]
        if select is None:
            want_names = loaded_names
        elif isinstance(select, str):
            want_names = [select]
        else:
            want_names = list(select)

        out_dict: Dict[str, pd.DataFrame] = {}
        with localconverter(robjects.default_converter + pandas2ri.converter):
            for name in want_names:
                if name not in loaded_names:
                    raise KeyError(f"Object '{name}' not found in RData.")
                r_obj = robjects.globalenv[name]
                py_obj = robjects.conversion.rpy2py(r_obj)
                if isinstance(py_obj, pd.DataFrame):
                    out_dict[name] = py_obj
                else:
                    raise TypeError(f"Object '{name}' is not an R data.frame convertible to pandas DataFrame.")

        if isinstance(select, str):
            return out_dict[select]
        return out_dict

    else:
        raise ValueError("backend must be 'pyreadr' or 'rpy2'.")


def write_rdata(objs: Union[pd.DataFrame, Dict[str, pd.DataFrame]], path: str, backend: str = "rpy2") -> None:
    """
    Write one or multiple pandas DataFrames to an .RData file.

    objs:
      - pandas DataFrame -> saved as object named 'df'
      - dict name -> DataFrame -> saved with given names

    backend:
      - "rpy2": uses R's save() with a temporary environment (requires R + rpy2)
      - "pyreadr": uses pyreadr.write_rdata (no R required; if available)
    """
    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)

    if isinstance(objs, pd.DataFrame):
        obj_map = {"df": objs}
    else:
        obj_map = dict(objs)

    if backend == "rpy2":
        if robjects is None or pandas2ri is None:
            raise ImportError("rpy2 is not installed or not importable. pip install rpy2 (and ensure R is installed).")

        # Create a new R environment to avoid polluting the global env
        new_env = robjects.r["new.env"]()
        names = list(obj_map.keys())

        with localconverter(robjects.default_converter + pandas2ri.converter):
            for name, df in obj_map.items():
                r_df = robjects.conversion.py2rpy(df)
                # assign(name, value, envir)
                robjects.r["assign"](name, r_df, new_env)

        # save(list=c(...), file=..., envir=env)
        robjects.r["save"](robjects.StrVector(names), file=path, envir=new_env)
        return

    elif backend == "pyreadr":
        if pyreadr is None:
            raise ImportError("pyreadr is not installed. pip install pyreadr")
        # pyreadr expects a dict name->DataFrame for RData
        pyreadr.write_rdata(path, obj_map)
        return

    else:
        raise ValueError("backend must be 'rpy2' or 'pyreadr'.")


# ---------------------------
# Arrow-based interop (Feather/Parquet) using pyarrow
# These are the recommended formats for frictionless R<->Python via the 'arrow' R package.
# ---------------------------

def to_feather_arrow(df: pd.DataFrame, path: str, compression: str = "zstd") -> None:
    if pa is None or feather is None:
        raise ImportError("pyarrow is not installed. pip install pyarrow")
    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
    table = pa.Table.from_pandas(df, preserve_index=False)
    feather.write_feather(table, path, compression=compression)


def read_feather_arrow(path: str) -> pd.DataFrame:
    if pa is None or feather is None:
        raise ImportError("pyarrow is not installed. pip install pyarrow")
    table = feather.read_table(path)
    return table.to_pandas()


def to_parquet_arrow(df: pd.DataFrame, path: str, compression: str = "zstd") -> None:
    if pa is None or pq is None:
        raise ImportError("pyarrow is not installed. pip install pyarrow")
    os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)
    table = pa.Table.from_pandas(df, preserve_index=False)
    pq.write_table(table, path, compression=compression)


def read_parquet_arrow(path: str) -> pd.DataFrame:
    if pa is None or pq is None:
        raise ImportError("pyarrow is not installed. pip install pyarrow")
    table = pq.read_table(path)
    return table.to_pandas()


# ---------------------------
# Example usage (commented)
# ---------------------------
# df = pd.DataFrame({"x": [1, 2], "y": ["a", "b"]})
#
# # RDS
# write_rds(df, "out.rds", backend="rpy2")      # or backend="pyreadr"
# df2 = read_rds("out.rds", backend="pyreadr")  # or backend="rpy2"
#
# # RData with two frames
# write_rdata({"d1": df, "d2": df}, "out.RData", backend="rpy2")  # or backend="pyreadr"
# frames = read_rdata("out.RData", backend="pyreadr")             # dict name->DataFrame
# d1 = read_rdata("out.RData", backend="pyreadr", select="d1")    # single DataFrame
#
# # Arrow interop (read in R with library(arrow): read_feather("file.feather") / read_parquet("file.parquet"))
# to_feather_arrow(df, "out.feather")
# df3 = read_feather_arrow("out.feather")
# to_parquet_arrow(df, "out.parquet")
# df4 = read_parquet_arrow("out.parquet")
------------------------------------------------------------

PROMPT 457:
"Generate Python code that imports the core klio library and uses its functionalities to perform common tasks such as data cleaning, feature engineering, and data manipulation."
RESPONSE:
I can write this, but I need to confirm which “klio” you mean. There are multiple Python packages named klio with very different APIs:

- Spotify’s Klio (klio-core/klio-cli), built on Apache Beam for data/audio pipelines
- Another “klio” for tabular/ML workflows (if you have a specific GitHub/PyPI link)

Please share the exact package (and version) or a link. Also let me know your data shape (e.g., CSV/Parquet tabular vs. audio/media) and whether you want single-machine or distributed execution. Then I’ll generate ready-to-run code that does data cleaning, feature engineering, and data manipulation using that klio library.
------------------------------------------------------------

PROMPT 458:
"Generate Python code that imports the necessary modules and creates an AWS Redshift client object, allowing you to interact with your Redshift cluster using the Boto3 library."
RESPONSE:
import boto3
from botocore.config import Config

def get_redshift_client(region=None, profile=None, retries=10, timeout=60):
    # Create a boto3 Session (optionally with a profile and region)
    session_kwargs = {}
    if profile:
        session_kwargs["profile_name"] = profile
    if region:
        session_kwargs["region_name"] = region
    session = boto3.Session(**session_kwargs)

    # Optional: tune retries and timeouts
    cfg = Config(
        retries={"max_attempts": retries, "mode": "standard"},
        read_timeout=timeout,
        connect_timeout=timeout,
    )

    # Create the Redshift client (control plane: clusters, snapshots, etc.)
    return session.client("redshift", config=cfg)

if __name__ == "__main__":
    # Example usage: pick your AWS region (or rely on environment/instance profile)
    redshift = get_redshift_client(region="us-east-1")

    # Quick sanity check call (lists clusters your credentials can see)
    resp = redshift.describe_clusters()
    print([c["ClusterIdentifier"] for c in resp.get("Clusters", [])])
------------------------------------------------------------

PROMPT 459:
"Generate Python code that imports the necessary modules for creating a useful tools library with classes for singletons, dynamic function pointers, and other utility functions."
RESPONSE:
# tools_lib_imports.py
# Imports for a tools library with singletons, dynamic function pointers,
# and general-purpose utility helpers.

from __future__ import annotations

# Standard Library: core
import sys
import os
import types
import inspect
import importlib
import logging
import warnings
import traceback
import functools
import operator
import threading
import contextlib
import weakref
import time
import uuid
import copy
import enum
import pathlib
import collections
import itertools

# Typing
from typing import (
    Any,
    Callable,
    Optional,
    Type,
    TypeVar,
    Generic,
    Protocol,
    Iterable,
    Iterator,
    Mapping,
    MutableMapping,
    Sequence,
    MutableSequence,
    Union,
    overload,
    runtime_checkable,
    Final,
)

# Functional utilities
from functools import wraps, partial, lru_cache, singledispatch

# cached_property (fallback for older Python)
try:
    from functools import cached_property  # Py3.8+
except Exception:  # pragma: no cover
    def cached_property(func):  # very lightweight fallback
        return property(func)

# Inspect helpers
from inspect import (
    signature,
    isfunction,
    ismethod,
    isclass,
    iscoroutinefunction,
    isgeneratorfunction,
    getmembers,
    getmodule,
    getdoc,
)

# Types helpers
from types import (
    MethodType,
    FunctionType,
    ModuleType,
    MappingProxyType,
    SimpleNamespace,
)

# Concurrency
from threading import Lock, RLock, Thread, Event

# Weak references
from weakref import WeakKeyDictionary, WeakValueDictionary, ref

# Import machinery
from importlib import import_module, reload, resources

# Dataclasses
from dataclasses import dataclass, field, replace, asdict

# Context helpers
from contextlib import contextmanager, suppress, ExitStack

# Collections
from collections import deque, defaultdict, Counter, ChainMap, UserDict, namedtuple

# Operator helpers
from operator import attrgetter, itemgetter, methodcaller

# Time and filesystem
from time import perf_counter, monotonic, sleep
from pathlib import Path

# OS and environment
from os import getenv, fspath
from os.path import abspath, dirname, join, exists, expanduser, expandvars

# Optional compatibility (typing_extensions) for older interpreters
try:
    from typing_extensions import TypedDict, NotRequired, Self, ParamSpec
except Exception:  # pragma: no cover
    # Graceful degradation if typing_extensions isn't available
    TypedDict = dict  # type: ignore
    NotRequired = Optional  # type: ignore
    Self = Any  # type: ignore
    class ParamSpec:  # type: ignore
        pass

# Public API marker (populate in your implementation module)
__all__: list[str] = []
------------------------------------------------------------

PROMPT 460:
"Generate Python code that imports the psycopg2 library and creates a typing stub for it using the `typing` module, allowing for type-safe interactions with PostgreSQL databases."
RESPONSE:
from __future__ import annotations

from typing import (
    Any,
    Generic,
    Iterable,
    Iterator,
    Mapping,
    Protocol,
    Sequence,
    TypeVar,
    runtime_checkable,
)

import psycopg2

# Public export surface for type-checkers
__all__ = [
    "Params",
    "RowT",
    "CursorProto",
    "ConnectionProto",
    "connect",
    # Common error classes re-exported for type checking
    "Error",
    "Warning",
    "DatabaseError",
    "DataError",
    "OperationalError",
    "IntegrityError",
    "InternalError",
    "ProgrammingError",
    "NotSupportedError",
    "InterfaceError",
]

# ---------------------------------------------------------------------------
# Typing helpers
# ---------------------------------------------------------------------------

# DB-API parameter styles: either positional (sequence) or named (mapping)
Params = Sequence[Any] | Mapping[str, Any]

# Default row type returned by cursors; can be specialized (e.g., Mapping[str, Any])
RowT = TypeVar("RowT", covariant=True, bound=object)


@runtime_checkable
class CursorProto(Protocol, Generic[RowT]):
    # Optional attributes per PEP 249 / psycopg2 behavior
    description: Sequence[tuple[Any, ...]] | None
    rowcount: int

    def callproc(self, procname: str, parameters: Sequence[Any] | None = ...) -> None: ...
    def close(self) -> None: ...
    def execute(self, operation: str, parameters: Params | None = ...) -> CursorProto[RowT]: ...
    def executemany(self, operation: str, seq_of_parameters: Iterable[Params]) -> None: ...
    def fetchone(self) -> RowT | None: ...
    def fetchmany(self, size: int = ...) -> list[RowT]: ...
    def fetchall(self) -> list[RowT]: ...
    def setinputsizes(self, sizes: Any) -> None: ...
    def setoutputsize(self, size: int, column: int | None = ...) -> None: ...

    # Iteration protocol
    def __iter__(self) -> Iterator[RowT]: ...

    # Context manager support (psycopg2 cursors support this)
    def __enter__(self) -> CursorProto[RowT]: ...
    def __exit__(self, exc_type, exc, tb) -> bool | None: ...


@runtime_checkable
class ConnectionProto(Protocol):
    autocommit: bool

    def close(self) -> None: ...
    def commit(self) -> None: ...
    def rollback(self) -> None: ...

    # Returning a cursor that yields tuples by default; callers may narrow RowT
    def cursor(
        self,
        name: str | None = ...,
        cursor_factory: Any | None = ...,
    ) -> CursorProto[tuple[Any, ...]]: ...

    # Context manager support (commit/rollback on exit depending on errors)
    def __enter__(self) -> ConnectionProto: ...
    def __exit__(self, exc_type, exc, tb) -> bool | None: ...


# ---------------------------------------------------------------------------
# Typed connect wrapper
# ---------------------------------------------------------------------------

def connect(
    dsn: str | None = None,
    *,
    host: str | None = None,
    database: str | None = None,
    user: str | None = None,
    password: str | None = None,
    port: int | str | None = None,
    options: str | None = None,
    connection_factory: Any | None = None,
    cursor_factory: Any | None = None,
    async_: bool | None = None,  # legacy async mode flag in psycopg2
    connect_timeout: int | None = None,
    **kwargs: Any,
) -> ConnectionProto:
    """
    A thin, typed wrapper around psycopg2.connect that returns a ConnectionProto.
    This lets static type checkers understand the DB-API surface while still
    delegating to psycopg2 at runtime.
    """
    _kwargs: dict[str, Any] = dict(
        dsn=dsn,
        host=host,
        database=database,
        user=user,
        password=password,
        port=port,
        options=options,
        connection_factory=connection_factory,
        cursor_factory=cursor_factory,
        connect_timeout=connect_timeout,
        **kwargs,
    )
    # Only include async_ if explicitly provided to avoid passing None
    if async_ is not None:
        _kwargs["async_"] = async_

    return psycopg2.connect(**_kwargs)


# ---------------------------------------------------------------------------
# Re-export common DB-API exceptions for type checking and ergonomic imports
# ---------------------------------------------------------------------------

Error = psycopg2.Error
Warning = psycopg2.Warning
DatabaseError = psycopg2.DatabaseError
DataError = psycopg2.DataError
OperationalError = psycopg2.OperationalError
IntegrityError = psycopg2.IntegrityError
InternalError = psycopg2.InternalError
ProgrammingError = psycopg2.ProgrammingError
NotSupportedError = psycopg2.NotSupportedError
InterfaceError = psycopg2.InterfaceError

# ---------------------------------------------------------------------------
# Example (commented) usage:
#
# from psycopg2_types import connect, ConnectionProto, CursorProto
# from typing import Mapping, Any
#
# with connect(database="db", user="u", password="p") as conn:
#     cur: CursorProto[tuple[Any, ...]] = conn.cursor()
#     cur.execute("SELECT id, name FROM users WHERE id = %s", (123,))
#     row = cur.fetchone()
#     if row:
#         user_id, name = row  # type-checked as tuple[Any, ...]
#
# If you use RealDictCursor:
# from psycopg2.extras import RealDictCursor
# cur = conn.cursor(cursor_factory=RealDictCursor)
# dict_cur: CursorProto[Mapping[str, Any]] = cur
# for r in dict_cur:
#     reveal_type(r)  # Mapping[str, Any]
# ---------------------------------------------------------------------------
------------------------------------------------------------

PROMPT 461:
"Generate Python code that reformats and auto-formats robot framework test cases using a specified format, such as pytest or unittest, and supports multiple programming languages."
RESPONSE:
#!/usr/bin/env python3
"""
rf2code: Convert Robot Framework (.robot) test cases into code for multiple unit-test formats
             (pytest, unittest) and languages (Python, Java JUnit, JavaScript Jest), with auto-formatting.

Requirements:
  - Python 3.8+
Optional (for better formatting):
  - pip install black        # Python formatter
  - pip install jsbeautifier # JavaScript formatter
  - google-java-format       # Java formatter (CLI in PATH)

Usage:
  python rf2code.py --target pytest path/to/tests.robot -o out_dir
  python rf2code.py --target unittest path/to/tests.robot -o out_dir
  python rf2code.py --target junit path/to/tests.robot -o out_dir
  python rf2code.py --target jest path/to/tests.robot -o out_dir

Notes:
  - This is a pragmatic converter. It parses the Test Cases table and translates keyword steps
    into best-effort code. Unknown keywords become TODO comments.
  - For non-trivial keywords, provide or extend mappings as needed.
"""

import argparse
import os
import re
import shutil
import subprocess
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Optional, Tuple


# ----------------------------
# Data models
# ----------------------------

@dataclass
class Step:
    keyword: str
    args: List[str]
    raw: str
    assigned: List[str] = field(default_factory=list)


@dataclass
class RobotTest:
    name: str
    steps: List[Step] = field(default_factory=list)


@dataclass
class RobotSuite:
    name: str
    tests: List[RobotTest] = field(default_factory=list)
    source: Optional[Path] = None


# ----------------------------
# Utility functions
# ----------------------------

SECTION_RE = re.compile(r"^\*{3}\s*([A-Za-z][A-Za-z _-]*)\s*\*{3}\s*$")
TOKEN_SPLIT_RE = re.compile(r"\s{2,}|\t")


def slugify(name: str, allow_period: bool = False) -> str:
    s = name.strip()
    s = re.sub(r"[^\w\s\.-]" if allow_period else r"[^\w\s-]", "", s)
    s = s.replace(" ", "_")
    s = re.sub(r"_+", "_", s).strip("_")
    if s and s[0].isdigit():
        s = f"t_{s}"
    return s or "test_case"


def strip_robot_comment(line: str) -> str:
    # Robot comments start with '#' after at least two spaces or at the beginning.
    # Simple heuristic: split on '  #' (two spaces and hash) or leading '#'
    if line.lstrip().startswith("#"):
        return ""
    parts = re.split(r"\s{2,}#.*$", line)
    return parts[0]


def is_blank(line: str) -> bool:
    return not line.strip()


def parse_robot_file(path: Path) -> RobotSuite:
    """
    Naive parser for Robot Framework .robot files focusing on the *** Test Cases *** section.
    Supports:
      - Test case names
      - Steps with keyword + args, tokens split by 2+ spaces or tabs
      - Continuation lines starting with '...'
      - Multiple variable assignment tokens (e.g., ${a}  ${b}=  Keyword  arg)
    Ignores:
      - Settings, Variables, Keywords sections content
      - Control structures (FOR/IF/TRY) beyond TODO comments
    """
    suite = RobotSuite(name=path.stem, source=path)
    with path.open("r", encoding="utf-8") as f:
        lines = f.readlines()

    current_section = None
    current_test: Optional[RobotTest] = None
    last_step: Optional[Step] = None

    for raw_line in lines:
        line = raw_line.rstrip("\n")
        sec_match = SECTION_RE.match(line.strip())
        if sec_match:
            current_section = sec_match.group(1).strip().lower()
            continue

        if current_section not in ("test cases", "testcases"):
            continue

        clean = strip_robot_comment(line.rstrip())
        if is_blank(clean):
            continue

        # Continuation lines
        if clean.lstrip().startswith("..."):
            cont = clean.strip()[3:].strip()
            if last_step and cont:
                more = [t for t in TOKEN_SPLIT_RE.split(cont) if t]
                last_step.args.extend(more)
            continue

        # Test name (no leading whitespace)
        if not line.startswith((" ", "\t")):
            test_name = clean.strip()
            current_test = RobotTest(name=test_name)
            suite.tests.append(current_test)
            last_step = None
            continue

        # Step line
        if current_test is None:
            continue

        tokens = [t for t in TOKEN_SPLIT_RE.split(clean.strip()) if t]
        if not tokens:
            continue

        # Handle settings lines within test: lines starting with [Something]
        if tokens[0].startswith("[") and tokens[0].endswith("]"):
            # ignore e.g., [Tags], [Setup]
            continue

        # Handle assignments: tokens up to token that endswith '=' are assignments list
        assigned_vars = []
        idx = 0
        while idx < len(tokens) and tokens[idx].endswith("="):
            # handle multi-assign: previous tokens (without '=') plus current token ending '='
            # Robot often uses "${a}    ${b}=" (only last ends with '=')
            # Collect all tokens up to idx inclusive as assigned (trim '=' from last)
            # Reconstruct assigned list:
            if "=" in tokens[idx]:
                # The final assigned token carries '=' suffix
                assignees = tokens[: idx + 1]
                # Normalize: strip '=' from the last token
                assignees[-1] = assignees[-1][:-1]
                assigned_vars = assignees
            idx += 1

        if assigned_vars:
            rest = tokens[len(assigned_vars):]
        else:
            # Alternative form: sometimes both assignees may not add '=' until later token
            # We try to detect pattern: tokens containing ${...} followed by token '='
            rest = tokens

        # If no keyword due to malformed line
        if not rest:
            continue

        keyword = rest[0]
        args = rest[1:] if len(rest) > 1 else []

        # Normalize assigned var names (strip ${}, @{}, &{})
        def norm_var(v: str) -> str:
            v = v.strip()
            v = v.lstrip("$@&{").rstrip("}")
            v = v.replace(" ", "_")
            return v

        assigned_clean = [norm_var(v) for v in assigned_vars] if assigned_vars else []

        step = Step(keyword=keyword.strip(), args=args, raw=clean, assigned=assigned_clean)
        current_test.steps.append(step)
        last_step = step

    return suite


# ----------------------------
# Mapping and Generators
# ----------------------------

def py_literal(value: str) -> str:
    """Best-effort conversion of Robot token to Python literal."""
    v = value.strip()
    # Remove variable sigils if any; treat as string literal (since we don't track variables)
    if re.match(r"^[\$@&]\{.*\}$", v):
        inner = v[2:-1].strip()
        return f'"{inner}"'
    # Booleans
    if v.lower() in ("true", "false"):
        return "True" if v.lower() == "true" else "False"
    # None/null
    if v.lower() in ("none", "null"):
        return "None"
    # Numbers
    if re.match(r"^[+-]?\d+$", v):
        return v
    if re.match(r"^[+-]?(\d+\.\d*|\.\d+)([eE][+-]?\d+)?$", v):
        return v
    # Strings (escape quotes)
    v = v.replace("\\", "\\\\").replace('"', '\\"')
    return f'"{v}"'


def join_args_as_py(args: List[str]) -> str:
    return ", ".join(py_literal(a) for a in args)


def map_step_pypytest(step: Step) -> List[str]:
    """Map Robot step to pytest code lines."""
    k = step.keyword.strip().lower()
    a = step.args

    lines = []
    # Basic BuiltIn mappings
    if k in ("log", "log to console"):
        if a:
            lines.append(f"print({join_args_as_py(a)})")
        else:
            lines.append('print("")')
    elif k == "sleep":
        if a:
            lines.append(f"time.sleep(float({py_literal(a[0])}))")
        else:
            lines.append("time.sleep(1)")
    elif k in ("should be equal", "should be equal as strings", "should be equal as integers"):
        if len(a) >= 2:
            lines.append(f"assert {py_literal(a[0])} == {py_literal(a[1])}")
        else:
            lines.append(f"# TODO: Should Be Equal requires 2 args, got: {a!r}")
    elif k in ("should not be equal",):
        if len(a) >= 2:
            lines.append(f"assert {py_literal(a[0])} != {py_literal(a[1])}")
        else:
            lines.append(f"# TODO: Should Not Be Equal requires 2 args, got: {a!r}")
    elif k in ("should be true",):
        if a:
            expr = a[0]
            lines.append(f"assert eval({py_literal(expr)})")
        else:
            lines.append("assert True")
    elif k in ("should be false",):
        if a:
            expr = a[0]
            lines.append(f"assert not eval({py_literal(expr)})")
        else:
            lines.append("assert False is False")
    elif k in ("fail",):
        msg = py_literal(a[0]) if a else '"Failed by Robot Fail"'
        lines.append(f"pytest.fail({msg})")
    elif k == "set variable":
        # Assignment expected
        if step.assigned:
            if a:
                lines.append(f"{step.assigned[0]} = {py_literal(a[0])}")
            else:
                lines.append(f"{step.assigned[0]} = None")
        else:
            lines.append(f"# TODO: Set Variable without assignment: args={a!r}")
    elif k == "evaluate":
        if step.assigned and a:
            expr = a[0]
            lines.append(f"{step.assigned[0]} = eval({py_literal(expr)})")
        else:
            lines.append(f"# TODO: Evaluate mapping requires assignment and expr: args={a!r}")
    elif k == "create list":
        if step.assigned:
            elems = ", ".join(py_literal(x) for x in a)
            lines.append(f"{step.assigned[0]} = [{elems}]")
        else:
            lines.append(f"# TODO: Create List without assignment: args={a!r}")
    elif k == "create dictionary":
        if step.assigned:
            # args are pairs key value
            pairs = []
            it = iter(a)
            for key in it:
                try:
                    val = next(it)
                except StopIteration:
                    val = ""
                pairs.append(f"{py_literal(key)}: {py_literal(val)}")
            lines.append(f"{step.assigned[0]} = {{{', '.join(pairs)}}}")
        else:
            lines.append(f"# TODO: Create Dictionary without assignment: args={a!r}")
    else:
        # Unknown keyword -> comment TODO
        comment_args = ", ".join(a)
        if step.assigned:
            ass = ", ".join(step.assigned)
            lines.append(f"# TODO: {ass} = {step.keyword}    {comment_args}")
        else:
            lines.append(f"# TODO: {step.keyword}    {comment_args}")

    return lines


def map_step_pyunittest(step: Step) -> List[str]:
    """Map Robot step to unittest code lines (within a TestCase method context)."""
    k = step.keyword.strip().lower()
    a = step.args

    lines = []
    if k in ("log", "log to console"):
        if a:
            lines.append(f"print({join_args_as_py(a)})")
        else:
            lines.append('print("")')
    elif k == "sleep":
        if a:
            lines.append(f"time.sleep(float({py_literal(a[0])}))")
        else:
            lines.append("time.sleep(1)")
    elif k in ("should be equal", "should be equal as strings", "should be equal as integers"):
        if len(a) >= 2:
            lines.append(f"self.assertEqual({py_literal(a[0])}, {py_literal(a[1])})")
        else:
            lines.append(f"# TODO: Should Be Equal requires 2 args, got: {a!r}")
    elif k in ("should not be equal",):
        if len(a) >= 2:
            lines.append(f"self.assertNotEqual({py_literal(a[0])}, {py_literal(a[1])})")
        else:
            lines.append(f"# TODO: Should Not Be Equal requires 2 args, got: {a!r}")
    elif k in ("should be true",):
        if a:
            expr = a[0]
            lines.append(f"self.assertTrue(eval({py_literal(expr)}))")
        else:
            lines.append("self.assertTrue(True)")
    elif k in ("should be false",):
        if a:
            expr = a[0]
            lines.append(f"self.assertFalse(eval({py_literal(expr)}))")
        else:
            lines.append("self.assertFalse(False)")
    elif k in ("fail",):
        msg = py_literal(a[0]) if a else '"Failed by Robot Fail"'
        lines.append(f"self.fail({msg})")
    elif k == "set variable":
        if step.assigned:
            if a:
                lines.append(f"{step.assigned[0]} = {py_literal(a[0])}")
            else:
                lines.append(f"{step.assigned[0]} = None")
        else:
            lines.append(f"# TODO: Set Variable without assignment: args={a!r}")
    elif k == "evaluate":
        if step.assigned and a:
            expr = a[0]
            lines.append(f"{step.assigned[0]} = eval({py_literal(expr)})")
        else:
            lines.append(f"# TODO: Evaluate mapping requires assignment and expr: args={a!r}")
    elif k == "create list":
        if step.assigned:
            elems = ", ".join(py_literal(x) for x in a)
            lines.append(f"{step.assigned[0]} = [{elems}]")
        else:
            lines.append(f"# TODO: Create List without assignment: args={a!r}")
    elif k == "create dictionary":
        if step.assigned:
            pairs = []
            it = iter(a)
            for key in it:
                try:
                    val = next(it)
                except StopIteration:
                    val = ""
                pairs.append(f"{py_literal(key)}: {py_literal(val)}")
            lines.append(f"{step.assigned[0]} = {{{', '.join(pairs)}}}")
        else:
            lines.append(f"# TODO: Create Dictionary without assignment: args={a!r}")
    else:
        comment_args = ", ".join(a)
        if step.assigned:
            ass = ", ".join(step.assigned)
            lines.append(f"# TODO: {ass} = {step.keyword}    {comment_args}")
        else:
            lines.append(f"# TODO: {step.keyword}    {comment_args}")

    return lines


def generate_pytest_code(suite: RobotSuite) -> str:
    lines = []
    lines.append('"""Generated from Robot Framework by rf2code (pytest)."""')
    lines.append("import time")
    lines.append("import pytest")
    lines.append("")
    for test in suite.tests:
        fn = f"test_{slugify(test.name)}"
        lines.append(f"def {fn}():")
        if not test.steps:
            lines.append("    pass")
        else:
            for step in test.steps:
                step_lines = map_step_pypytest(step)
                for sl in step_lines:
                    lines.append(f"    {sl}")
        lines.append("")
    return "\n".join(lines)


def generate_unittest_code(suite: RobotSuite) -> str:
    cls_name = slugify(suite.name, allow_period=False).title().replace("_", "")
    if not cls_name:
        cls_name = "GeneratedTests"

    lines = []
    lines.append('"""Generated from Robot Framework by rf2code (unittest)."""')
    lines.append("import time")
    lines.append("import unittest")
    lines.append("")
    lines.append(f"class Test{cls_name}(unittest.TestCase):")
    if not suite.tests:
        lines.append("    pass")
    for test in suite.tests:
        fn = f"test_{slugify(test.name)}"
        lines.append(f"    def {fn}(self):")
        if not test.steps:
            lines.append("        pass")
        else:
            for step in test.steps:
                step_lines = map_step_pyunittest(step)
                for sl in step_lines:
                    lines.append(f"        {sl}")
        lines.append("")
    lines.append("")
    lines.append("if __name__ == '__main__':")
    lines.append("    unittest.main()")
    return "\n".join(lines)


def java_literal(value: str) -> str:
    v = value.strip()
    if v.lower() in ("true", "false"):
        return v.lower()
    if re.match(r"^[+-]?\d+$", v):
        return v
    if re.match(r"^[+-]?(\d+\.\d*|\.\d+)([eE][+-]?\d+)?$", v):
        return v
    # escape quotes/backslashes
    v = v.replace("\\", "\\\\").replace('"', '\\"')
    return f"\"{v}\""


def map_step_junit(step: Step) -> List[str]:
    k = step.keyword.strip().lower()
    a = step.args
    lines = []
    if k in ("log", "log to console"):
        if a:
            lines.append(f'System.out.println({java_literal(a[0])});')
        else:
            lines.append('System.out.println("");')
    elif k == "sleep":
        if a:
            lines.append(f"Thread.sleep((long)(Double.parseDouble({java_literal(a[0])})*1000));")
        else:
            lines.append("Thread.sleep(1000);")
    elif k in ("should be equal", "should be equal as strings", "should be equal as integers"):
        if len(a) >= 2:
            lines.append(f"org.junit.jupiter.api.Assertions.assertEquals({java_literal(a[1])}, {java_literal(a[0])});")
        else:
            lines.append(f"// TODO: Should Be Equal requires 2 args, got: {a!r}")
    elif k in ("should not be equal",):
        if len(a) >= 2:
            lines.append(f"org.junit.jupiter.api.Assertions.assertNotEquals({java_literal(a[1])}, {java_literal(a[0])});")
        else:
            lines.append(f"// TODO: Should Not Be Equal requires 2 args, got: {a!r}")
    elif k in ("fail",):
        msg = java_literal(a[0]) if a else '"Failed by Robot Fail"'
        lines.append(f"org.junit.jupiter.api.Assertions.fail({msg});")
    else:
        comment_args = ", ".join(a)
        if step.assigned:
            ass = ", ".join(step.assigned)
            lines.append(f"// TODO: {ass} = {step.keyword}    {comment_args}")
        else:
            lines.append(f"// TODO: {step.keyword}    {comment_args}")
    return lines


def generate_junit_code(suite: RobotSuite) -> str:
    cls_name = slugify(suite.name).title().replace("_", "")
    if not cls_name:
        cls_name = "GeneratedTests"
    lines = []
    lines.append("// Generated from Robot Framework by rf2code (JUnit 5).")
    lines.append("import org.junit.jupiter.api.Test;")
    lines.append("")
    lines.append(f"public class {cls_name} " + "{")
    for test in suite.tests:
        mname = f"test_{slugify(test.name)}"
        lines.append("    @Test")
        lines.append(f"    public void {mname}() throws Exception " + "{")
        if not test.steps:
            lines.append("    }")
            continue
        for step in test.steps:
            for sl in map_step_junit(step):
                lines.append(f"        {sl}")
        lines.append("    }")
        lines.append("")
    lines.append("}")
    return "\n".join(lines)


def js_literal(value: str) -> str:
    v = value.strip()
    if v.lower() in ("true", "false"):
        return v.lower()
    if re.match(r"^[+-]?\d+$", v):
        return v
    if re.match(r"^[+-]?(\d+\.\d*|\.\d+)([eE][+-]?\d+)?$", v):
        return v
    v = v.replace("\\", "\\\\").replace("`", "\\`")
    return f"`{v}`"


def map_step_jest(step: Step) -> List[str]:
    k = step.keyword.strip().lower()
    a = step.args
    lines = []
    if k in ("log", "log to console"):
        if a:
            lines.append(f"console.log({js_literal(a[0])});")
        else:
            lines.append("console.log();")
    elif k == "sleep":
        if a:
            lines.append(f"await new Promise(r => setTimeout(r, Math.floor(parseFloat({js_literal(a[0])})*1000)));")
        else:
            lines.append("await new Promise(r => setTimeout(r, 1000));")
    elif k in ("should be equal", "should be equal as strings", "should be equal as integers"):
        if len(a) >= 2:
            lines.append(f"expect({js_literal(a[0])}).toBe({js_literal(a[1])});")
        else:
            lines.append(f"// TODO: Should Be Equal requires 2 args, got: {a!r}")
    elif k in ("should not be equal",):
        if len(a) >= 2:
            lines.append(f"expect({js_literal(a[0])}).not.toBe({js_literal(a[1])});")
        else:
            lines.append(f"// TODO: Should Not Be Equal requires 2 args, got: {a!r}")
    elif k in ("fail",):
        msg = js_literal(a[0]) if a else "`Failed by Robot Fail`"
        lines.append(f"throw new Error({msg});")
    else:
        comment_args = ", ".join(a)
        if step.assigned:
            ass = ", ".join(step.assigned)
            lines.append(f"// TODO: {ass} = {step.keyword}    {comment_args}")
        else:
            lines.append(f"// TODO: {step.keyword}    {comment_args}")
    return lines


def generate_jest_code(suite: RobotSuite) -> str:
    sname = suite.name or "suite"
    lines = []
    lines.append("// Generated from Robot Framework by rf2code (Jest).")
    lines.append(f"describe('{sname}', () => " + "{")
    for test in suite.tests:
        tname = test.name.replace("'", "\\'")
        async_needed = any(step.keyword.strip().lower() == "sleep" for step in test.steps)
        if async_needed:
            lines.append(f"  test('{tname}', async () => " + "{")
        else:
            lines.append(f"  test('{tname}', () => " + "{")
        if not test.steps:
            lines.append("  });")
            lines.append("")
            continue
        for step in test.steps:
            for sl in map_step_jest(step):
                lines.append(f"    {sl}")
        lines.append("  });")
        lines.append("")
    lines.append("});")
    return "\n".join(lines)


# ----------------------------
# Formatting helpers
# ----------------------------

def format_python(code: str) -> str:
    # Try black
    try:
        import black
        mode = black.Mode()
        return black.format_str(code, mode=mode)
    except Exception:
        pass
    # Try autopep8
    try:
        import autopep8  # type: ignore
        return autopep8.fix_code(code)
    except Exception:
        pass
    return code


def which(cmd: str) -> Optional[str]:
    return shutil.which(cmd)


def format_java(code: str) -> str:
    # Try google-java-format CLI
    gjf = which("google-java-format")
    if gjf:
        try:
            p = subprocess.run(
                [gjf, "-"],
                input=code.encode("utf-8"),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
            )
            return p.stdout.decode("utf-8")
        except Exception:
            return code
    return code


def format_js(code: str) -> str:
    # Try jsbeautifier (pure-Python)
    try:
        import jsbeautifier  # type: ignore

        opts = jsbeautifier.default_options()
        opts.indent_size = 2
        return jsbeautifier.beautify(code, opts)
    except Exception:
        pass
    return code


# ----------------------------
# CLI
# ----------------------------

def convert_file(path: Path, target: str) -> Tuple[str, str]:
    """
    Convert a single .robot file into code and suggest output filename.
    Returns (code, filename).
    """
    suite = parse_robot_file(path)
    if target == "pytest":
        code = generate_pytest_code(suite)
        code = format_python(code)
        out_name = f"test_{slugify(suite.name)}.py"
    elif target == "unittest":
        code = generate_unittest_code(suite)
        code = format_python(code)
        out_name = f"test_{slugify(suite.name)}.py"
    elif target == "junit":
        code = generate_junit_code(suite)
        code = format_java(code)
        out_name = f"{slugify(suite.name).title().replace('_', '')}.java"
    elif target == "jest":
        code = generate_jest_code(suite)
        code = format_js(code)
        out_name = f"{slugify(suite.name)}.test.js"
    else:
        raise ValueError(f"Unknown target: {target}")
    return code, out_name


def main():
    parser = argparse.ArgumentParser(description="Convert Robot Framework tests to pytest/unittest/JUnit/Jest.")
    parser.add_argument("input", help="Path to a .robot file or a directory containing .robot files.")
    parser.add_argument("-t", "--target", choices=["pytest", "unittest", "junit", "jest"], default="pytest",
                        help="Output test framework/language.")
    parser.add_argument("-o", "--out-dir", default="rf2code_out", help="Output directory.")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing files.")
    args = parser.parse_args()

    in_path = Path(args.input)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    files: List[Path] = []
    if in_path.is_file() and in_path.suffix.lower() == ".robot":
        files = [in_path]
    elif in_path.is_dir():
        files = sorted([p for p in in_path.rglob("*.robot") if p.is_file()])
    else:
        print("Input must be a .robot file or a directory containing .robot files.", file=sys.stderr)
        sys.exit(2)

    if not files:
        print("No .robot files found.", file=sys.stderr)
        sys.exit(1)

    for rf in files:
        try:
            code, out_name = convert_file(rf, target=args.target)
            out_path = out_dir / out_name
            if out_path.exists() and not args.overwrite:
                print(f"Skipping existing: {out_path} (use --overwrite to replace)")
                continue
            with out_path.open("w", encoding="utf-8") as f:
                f.write(code)
            print(f"Wrote: {out_path}")
        except Exception as e:
            print(f"Error converting {rf}: {e}", file=sys.stderr)

    print("Done.")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 462:
"Generate Python code that imports the ctypes module and wraps Bob Jenkins' hash functions in a Python class, allowing users to easily call the hash functions from Python scripts."
RESPONSE:
# Bob Jenkins' lookup3 hash (hashlittle/hashbig) ctypes wrapper for Python.
# 
# This module expects a compiled shared library that exports the standard
# lookup3 symbols:
#   uint32_t hashlittle(const void *key, size_t length, uint32_t initval);
#   void     hashlittle2(const void *key, size_t length, uint32_t *pc, uint32_t *pb);
#   uint32_t hashbig(const void *key, size_t length, uint32_t initval);
#   void     hashbig2(const void *key, size_t length, uint32_t *pc, uint32_t *pb);
#
# Build instructions (examples):
#   Linux:  gcc -O3 -fPIC -shared -o liblookup3.so lookup3.c
#   macOS:  clang -O3 -fPIC -dynamiclib -o liblookup3.dylib lookup3.c
#   Windows (MSVC): cl /O2 /LD lookup3.c /Fe:lookup3.dll
#
# Place the resulting shared library where your OS can find it or next to this file.

import ctypes
import os
import sys
from typing import Optional, Tuple


class JenkinsHash:
    """
    ctypes-based wrapper for Bob Jenkins' lookup3 hash functions.

    By default, this tries to load a shared library named like:
      - Windows:  lookup3.dll, jenkins.dll
      - Linux:    liblookup3.so, libjenkins.so
      - macOS:    liblookup3.dylib, libjenkins.dylib

    You can also pass an explicit path to the shared library.
    """

    def __init__(self, lib_path: Optional[str] = None):
        self._lib = self._load_library(lib_path)
        self._bind_symbols()

    # Public API -------------------------------------------------------------

    def hashlittle(self, data, initval: int = 0) -> int:
        """
        Compute 32-bit hashlittle(key, initval).

        data: bytes-like or str (str encoded as UTF-8)
        initval: 32-bit seed (0 by default)
        """
        buf, n = self._as_c_buffer(data)
        ptr = ctypes.c_void_p(ctypes.addressof(buf))
        res = self._hashlittle(ptr, ctypes.c_size_t(n), ctypes.c_uint32(initval))
        return int(res) & 0xFFFFFFFF

    def hashlittle2(self, data, seed1: int = 0, seed2: int = 0) -> Tuple[int, int]:
        """
        Compute two 32-bit hashes via hashlittle2(key, pc, pb).

        seed1, seed2: input seeds; on return they become the two hash outputs.
        Returns (h1, h2).
        """
        buf, n = self._as_c_buffer(data)
        ptr = ctypes.c_void_p(ctypes.addressof(buf))
        pc = ctypes.c_uint32(seed1 & 0xFFFFFFFF)
        pb = ctypes.c_uint32(seed2 & 0xFFFFFFFF)
        self._hashlittle2(ptr, ctypes.c_size_t(n), ctypes.byref(pc), ctypes.byref(pb))
        return (int(pc.value) & 0xFFFFFFFF, int(pb.value) & 0xFFFFFFFF)

    def hashbig(self, data, initval: int = 0) -> int:
        """
        Compute 32-bit hashbig(key, initval).

        data: bytes-like or str (str encoded as UTF-8)
        initval: 32-bit seed (0 by default)
        """
        if self._hashbig is None:
            raise NotImplementedError("hashbig not found in loaded library")
        buf, n = self._as_c_buffer(data)
        ptr = ctypes.c_void_p(ctypes.addressof(buf))
        res = self._hashbig(ptr, ctypes.c_size_t(n), ctypes.c_uint32(initval))
        return int(res) & 0xFFFFFFFF

    def hashbig2(self, data, seed1: int = 0, seed2: int = 0) -> Tuple[int, int]:
        """
        Compute two 32-bit hashes via hashbig2(key, pc, pb).

        seed1, seed2: input seeds; on return they become the two hash outputs.
        Returns (h1, h2).
        """
        if self._hashbig2 is None:
            raise NotImplementedError("hashbig2 not found in loaded library")
        buf, n = self._as_c_buffer(data)
        ptr = ctypes.c_void_p(ctypes.addressof(buf))
        pc = ctypes.c_uint32(seed1 & 0xFFFFFFFF)
        pb = ctypes.c_uint32(seed2 & 0xFFFFFFFF)
        self._hashbig2(ptr, ctypes.c_size_t(n), ctypes.byref(pc), ctypes.byref(pb))
        return (int(pc.value) & 0xFFFFFFFF, int(pb.value) & 0xFFFFFFFF)

    # Internal helpers -------------------------------------------------------

    def _as_c_buffer(self, data) -> Tuple[ctypes.Array, int]:
        """
        Convert input to a ctypes buffer and its length.
        Accepts bytes, bytearray, memoryview, or str (UTF-8 encoded).
        """
        if data is None:
            b = b""
        elif isinstance(data, bytes):
            b = data
        elif isinstance(data, bytearray):
            b = bytes(data)
        elif isinstance(data, memoryview):
            b = data.tobytes()
        elif isinstance(data, str):
            b = data.encode("utf-8")
        else:
            # Try duck-typing to bytes
            try:
                b = bytes(data)
            except Exception as exc:
                raise TypeError(f"Unsupported data type for hashing: {type(data)!r}") from exc

        # create_string_buffer copies bytes and adds a trailing NUL; we pass the true length explicitly
        buf = ctypes.create_string_buffer(b)
        return buf, len(b)

    def _bind_symbols(self):
        # Required functions
        self._hashlittle = self._lib.hashlittle
        self._hashlittle.argtypes = [ctypes.c_void_p, ctypes.c_size_t, ctypes.c_uint32]
        self._hashlittle.restype = ctypes.c_uint32

        self._hashlittle2 = self._lib.hashlittle2
        self._hashlittle2.argtypes = [
            ctypes.c_void_p,
            ctypes.c_size_t,
            ctypes.POINTER(ctypes.c_uint32),
            ctypes.POINTER(ctypes.c_uint32),
        ]
        self._hashlittle2.restype = None

        # Optional functions (present in full lookup3)
        self._hashbig = getattr(self._lib, "hashbig", None)
        if self._hashbig is not None:
            self._hashbig.argtypes = [ctypes.c_void_p, ctypes.c_size_t, ctypes.c_uint32]
            self._hashbig.restype = ctypes.c_uint32

        self._hashbig2 = getattr(self._lib, "hashbig2", None)
        if self._hashbig2 is not None:
            self._hashbig2.argtypes = [
                ctypes.c_void_p,
                ctypes.c_size_t,
                ctypes.POINTER(ctypes.c_uint32),
                ctypes.POINTER(ctypes.c_uint32),
            ]
            self._hashbig2.restype = None

    @staticmethod
    def _candidate_library_names():
        if sys.platform.startswith("win"):
            return ["lookup3.dll", "jenkins.dll", "liblookup3.dll", "libjenkins.dll"]
        elif sys.platform == "darwin":
            return ["liblookup3.dylib", "libjenkins.dylib", "lookup3.dylib"]
        else:
            return ["liblookup3.so", "libjenkins.so", "lookup3.so"]

    def _load_library(self, lib_path: Optional[str]) -> ctypes.CDLL:
        # If an explicit path was given, try loading it directly.
        if lib_path:
            try:
                return ctypes.CDLL(lib_path)
            except OSError as e:
                raise OSError(f"Failed to load library at '{lib_path}': {e}") from e

        # Try common names in likely locations
        search_dirs = [
            os.getcwd(),
            os.path.dirname(os.path.abspath(__file__)),
            None,  # allow system search path via bare name
        ]
        names = self._candidate_library_names()

        errors = []
        for name in names:
            for d in search_dirs:
                candidate = name if d is None else os.path.join(d, name)
                try:
                    return ctypes.CDLL(candidate)
                except OSError as e:
                    errors.append((candidate, str(e)))

        msg = [
            "Could not locate the lookup3 shared library. Tried:",
            *[f"  - {path}: {err}" for path, err in errors],
            "Build lookup3.c as a shared library and place it on your system's library path or next to this script.",
        ]
        raise OSError("\n".join(msg))


# Example usage
if __name__ == "__main__":
    j = JenkinsHash()  # or JenkinsHash("/full/path/to/liblookup3.so")

    data = b"hello world"
    print("hashlittle:", hex(j.hashlittle(data)))
    print("hashlittle2:", tuple(hex(x) for x in j.hashlittle2(data, 0, 0)))

    try:
        print("hashbig:", hex(j.hashbig(data)))
        print("hashbig2:", tuple(hex(x) for x in j.hashbig2(data, 0, 0)))
    except NotImplementedError as e:
        print("hashbig/hashbig2 not available in this library:", e)
------------------------------------------------------------

PROMPT 463:
"Generate Python code that implements a Liquid template engine using the `pyliquid` library, allowing you to render Liquid templates with dynamic data and custom filters and tags."
RESPONSE:
# pip install liquid
# If your environment uses a package named "pyliquid" instead of "liquid",
# this code will try to import that first and fall back to "liquid".
try:
    import pyliquid as liquid
except ImportError:
    import liquid

from datetime import datetime
from typing import Any, Dict, Optional


class LiquidEngine:
    """
    A small wrapper around a Liquid environment that:
      - renders templates from strings or files
      - accepts dynamic data (dicts)
      - supports registering custom filters
      - supports registering custom tags

    Requires the python-liquid library (PyPI: liquid), sometimes referred to as "pyliquid".
    """

    def __init__(self, template_search_path: Optional[str] = None):
        # Create an Environment, optionally with a filesystem loader.
        loader = None
        if template_search_path:
            # FileSystemLoader allows loading templates by name from a directory.
            loader = liquid.FileSystemLoader(template_search_path)

        self.env = liquid.Environment(loader=loader)

    # -----------------------------
    # Filter registration
    # -----------------------------
    def register_filter(self, name: str, func):
        """
        Register a custom filter by name.
        A filter takes a value as the first argument and optional parameters.
        Example usage in Liquid: {{ name | shout: "!!" }}
        """
        self.env.add_filter(name, func)

    # -----------------------------
    # Tag registration
    # -----------------------------
    def register_tag(self, tag_cls):
        """
        Register a custom tag class. The class should subclass liquid.Tag and
        implement a parse() method that returns a node with render_to_output().
        """
        self.env.add_tag(tag_cls)

    # -----------------------------
    # Rendering helpers
    # -----------------------------
    def render_from_string(self, template_source: str, data: Dict[str, Any]) -> str:
        """Render a template from a string with the given data context."""
        tmpl = self.env.from_string(template_source)
        return tmpl.render(data)

    def render_from_file(self, template_name: str, data: Dict[str, Any]) -> str:
        """Render a template by file name (requires template_search_path)."""
        tmpl = self.env.get_template(template_name)
        return tmpl.render(data)


# -----------------------------
# Example custom filters
# -----------------------------
def shout_filter(value: Any, suffix: str = "!") -> str:
    """Uppercase the value and append a suffix."""
    return f"{str(value).upper()}{suffix}"


def slugify_filter(value: Any, sep: str = "-") -> str:
    """Simple slugify: lowercase, alphanumeric and sep between words."""
    import re

    text = str(value).lower()
    text = re.sub(r"[^a-z0-9]+", sep, text).strip(sep)
    return text


# -----------------------------
# Example custom tag(s)
# -----------------------------
# A simple tag: {% now: "%Y-%m-%d %H:%M" %}
# Prints the current time formatted with strftime. The argument is a Liquid expression,
# so you can pass a string literal or a variable. Example:
#   {% assign fmt = "%b %d, %Y" %}
#   {% now: fmt %}
try:
    # Prefer API from python-liquid (PyPI: liquid).
    from liquid import Tag
except Exception:  # pragma: no cover
    Tag = object  # fallback to avoid NameErrors if environment is odd


class NowTag(Tag):
    # The name used in templates, e.g. {% now: "%Y" %}
    name = "now"

    # Note: The Tag API here targets python-liquid >= 1.10 style.
    # It relies on Tag.parse() returning a Node-like object that implements
    # render_to_output(context, buffer).
    def parse(self, stream):  # type: ignore[override]
        # Parse a single expression after the tag name, separated by ":" or space.
        # Many versions of python-liquid allow using helper methods on Tag:
        #   expr = self.parse_expression(stream)
        # To be more tolerant, we'll try to parse an expression if available.
        parse_expression = getattr(self, "parse_expression", None)
        expect = getattr(self, "expect", None)

        expr = None
        if callable(parse_expression):
            # Try to parse a single expression (string literal or variable)
            expr = parse_expression(stream)

        # If the environment provides an expect() helper to consume TAG_END, use it.
        if callable(expect):
            try:
                expect(stream, "TAG_END")
            except Exception:
                # If expect is not compatible, ignore to keep broad compatibility.
                pass

        # Build a renderable node that writes the current date/time
        class _NowNode:
            def __init__(self, engine_expr):
                self.engine_expr = engine_expr

            def render_to_output(self, context, buffer):
                # Evaluate the expression if one was parsed; default format if not
                fmt = "%Y-%m-%d %H:%M:%S"
                if self.engine_expr is not None:
                    try:
                        # Most python-liquid versions provide evaluate() on expressions
                        fmt_val = self.engine_expr.evaluate(context)
                        if fmt_val:
                            fmt = str(fmt_val)
                    except Exception:
                        # Fallback: try context lookup in case expr is just a variable name
                        try:
                            fmt_val = context.resolve(self.engine_expr)  # may fail on older versions
                            if fmt_val:
                                fmt = str(fmt_val)
                        except Exception:
                            pass

                buffer.write(datetime.now().strftime(fmt))

        return _NowNode(expr)


# Another custom tag example: {% shout: user.name %}
# Uppercases the evaluated expression and writes it out.
class ShoutTag(Tag):
    name = "shout"

    def parse(self, stream):  # type: ignore[override]
        parse_expression = getattr(self, "parse_expression", None)
        expect = getattr(self, "expect", None)

        expr = None
        if callable(parse_expression):
            expr = parse_expression(stream)

        if callable(expect):
            try:
                expect(stream, "TAG_END")
            except Exception:
                pass

        class _ShoutNode:
            def __init__(self, engine_expr):
                self.engine_expr = engine_expr

            def render_to_output(self, context, buffer):
                value = ""
                if self.engine_expr is not None:
                    try:
                        value = self.engine_expr.evaluate(context)
                    except Exception:
                        # Fallback best-effort
                        try:
                            value = context.resolve(self.engine_expr)
                        except Exception:
                            value = ""
                buffer.write(str(value).upper())

        return _ShoutNode(expr)


# -----------------------------
# Example usage
# -----------------------------
if __name__ == "__main__":
    # Initialize engine with an optional template directory (uncomment to use)
    # engine = LiquidEngine(template_search_path="templates")
    engine = LiquidEngine()

    # Register custom filters
    engine.register_filter("shout", shout_filter)
    engine.register_filter("slugify", slugify_filter)

    # Register custom tags
    engine.register_tag(NowTag)
    engine.register_tag(ShoutTag)

    # Example template with dynamic data, custom filters, and custom tags
    template = """
Hello, {{ user.name | shout: "!!" }}
Your slug: {{ user.name | slugify }}
Right now: {% now: "%b %d, %Y %H:%M" %}
Shout via tag: {% shout: user.name %}

Items:
{% for item in items %}
- {{ forloop.index }}. {{ item.title | slugify }} ({{ item.title }})
{% endfor %}
""".strip()

    data = {
        "user": {"name": "Ada Lovelace"},
        "items": [
            {"title": "Introduction to Liquid"},
            {"title": "Custom Filters & Tags"},
            {"title": "Prod Tips"},
        ],
    }

    output = engine.render_from_string(template, data)
    print(output)
------------------------------------------------------------

PROMPT 464:
"Generate Python code that imports the Cython module and creates a Python wrapper for the Brotli library, allowing users to compress and decompress data using the Brotli algorithm."
RESPONSE:
Below is a minimal Cython-based wrapper around the native Brotli C library (libbrotli) that exposes compress and decompress functions to Python.

Files to create:

1) brotli_wrapper.pyx
--------------------------------
# cython: language_level=3
import cython

from libc.stdlib cimport malloc, free
from libc.stdint cimport uint8_t, uint32_t
from libc.stddef cimport size_t
from cpython.bytes cimport PyBytes_FromStringAndSize

cdef extern from "brotli/encode.h":
    ctypedef enum BrotliEncoderMode:
        BROTLI_MODE_GENERIC
        BROTLI_MODE_TEXT
        BROTLI_MODE_FONT

    size_t BrotliEncoderMaxCompressedSize(size_t input_size)
    int BrotliEncoderCompress(int quality,
                              int lgwin,
                              BrotliEncoderMode mode,
                              size_t input_size,
                              const uint8_t* input_buffer,
                              size_t* encoded_size,
                              uint8_t* encoded_buffer)

cdef extern from "brotli/decode.h":
    ctypedef enum BrotliDecoderResult:
        BROTLI_DECODER_RESULT_ERROR
        BROTLI_DECODER_RESULT_SUCCESS
        BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT
        BROTLI_DECODER_RESULT_NEEDS_MORE_OUTPUT

    cdef struct BrotliDecoderState

    BrotliDecoderState* BrotliDecoderCreateInstance(void* alloc_func, void* free_func, void* opaque)
    void BrotliDecoderDestroyInstance(BrotliDecoderState* state)

    BrotliDecoderResult BrotliDecoderDecompressStream(BrotliDecoderState* state,
                                                      size_t* available_in,
                                                      const uint8_t** next_in,
                                                      size_t* available_out,
                                                      uint8_t** next_out,
                                                      size_t* total_out)

    uint32_t BrotliDecoderGetErrorCode(const BrotliDecoderState* state)
    const char* BrotliDecoderErrorString(uint32_t error_code)


cdef inline BrotliEncoderMode _mode_from_py(object mode):
    if mode is None or mode == "generic":
        return BROTLI_MODE_GENERIC
    elif mode == "text":
        return BROTLI_MODE_TEXT
    elif mode == "font":
        return BROTLI_MODE_FONT
    else:
        raise ValueError("Invalid Brotli mode. Use 'generic', 'text', or 'font'.")


def compress(data, int quality=11, int lgwin=22, mode="generic"):
    """
    Compress bytes using Brotli.
    - quality: 0..11 (higher is slower but better compression)
    - lgwin: window size as log2 (10..24 typical; default 22)
    - mode: 'generic', 'text', or 'font'
    """
    cdef bytes src = bytes(data)
    cdef size_t src_len = len(src)
    cdef const uint8_t* src_ptr = <const uint8_t*> src

    cdef size_t max_out = BrotliEncoderMaxCompressedSize(src_len)
    if max_out == 0:
        raise MemoryError("Unable to determine max compressed size")

    cdef uint8_t* out_ptr = <uint8_t*> malloc(max_out)
    if out_ptr == NULL:
        raise MemoryError("Failed to allocate output buffer")

    cdef size_t out_size = max_out
    cdef int ok = BrotliEncoderCompress(quality, lgwin, _mode_from_py(mode),
                                        src_len, src_ptr,
                                        &out_size, out_ptr)
    if ok == 0:
        free(out_ptr)
        raise ValueError("Brotli compression failed")

    cdef object result = PyBytes_FromStringAndSize(<char*> out_ptr, out_size)
    free(out_ptr)
    return result


def decompress(data, size_t chunk_size=65536):
    """
    Decompress bytes using Brotli (streaming).
    - chunk_size: temporary output buffer size for streaming
    """
    cdef bytes src = bytes(data)
    cdef size_t available_in = len(src)
    cdef const uint8_t* next_in = <const uint8_t*> src

    cdef size_t total_out = 0
    cdef BrotliDecoderState* st = BrotliDecoderCreateInstance(NULL, NULL, NULL)
    if st == NULL:
        raise MemoryError("Failed to create Brotli decoder")

    cdef list chunks = []
    while True:
        cdef size_t available_out = chunk_size
        cdef uint8_t* out_buf = <uint8_t*> malloc(available_out)
        if out_buf == NULL:
            BrotliDecoderDestroyInstance(st)
            raise MemoryError("Failed to allocate decompression buffer")
        cdef uint8_t* next_out = out_buf

        cdef BrotliDecoderResult r = BrotliDecoderDecompressStream(
            st,
            &available_in, &next_in,
            &available_out, &next_out,
            &total_out
        )

        cdef size_t produced = chunk_size - available_out
        if produced:
            chunks.append(PyBytes_FromStringAndSize(<char*> out_buf, produced))
        free(out_buf)

        if r == BROTLI_DECODER_RESULT_SUCCESS:
            break
        elif r == BROTLI_DECODER_RESULT_NEEDS_MORE_OUTPUT:
            # continue loop to provide another output buffer
            continue
        elif r == BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT:
            BrotliDecoderDestroyInstance(st)
            raise ValueError("Brotli decompression failed: truncated input")
        else:  # BROTLI_DECODER_RESULT_ERROR
            cdef uint32_t ec = BrotliDecoderGetErrorCode(st)
            cdef const char* msg = BrotliDecoderErrorString(ec)
            BrotliDecoderDestroyInstance(st)
            if msg != NULL:
                cdef bytes m = msg
                raise ValueError("Brotli decompression error: " + m.decode("ascii", "replace"))
            raise ValueError("Brotli decompression error")

    BrotliDecoderDestroyInstance(st)
    return b"".join(chunks)
--------------------------------

2) setup.py
--------------------------------
from setuptools import setup, Extension
from Cython.Build import cythonize
import sys
import os

ext_modules = [
    Extension(
        "brotli_wrapper",
        sources=["brotli_wrapper.pyx"],
        libraries=["brotlienc", "brotlidec", "brotlicommon"],
        # If headers/libraries are in non-standard locations, add:
        # include_dirs=[...],
        # library_dirs=[...],
    )
]

setup(
    name="brotli_wrapper",
    ext_modules=cythonize(ext_modules, language_level="3"),
)
--------------------------------

Build and install steps:
- Ensure the native Brotli library and headers are installed (e.g., on Debian/Ubuntu: sudo apt-get install libbrotli-dev; on macOS with Homebrew: brew install brotli).
- Install Cython: pip install cython
- Build in place: python setup.py build_ext --inplace

Example usage:
--------------------------------
from brotli_wrapper import compress, decompress

data = b"hello brotli!" * 100
c = compress(data, quality=9, lgwin=20, mode="generic")
d = decompress(c)
assert d == data
print("Original:", len(data), "Compressed:", len(c))
--------------------------------

Notes:
- This wrapper uses the one-shot compression API and the streaming decompression API from libbrotli.
- If your system uses different library names or custom paths, edit libraries, include_dirs, and library_dirs in setup.py accordingly.
------------------------------------------------------------

PROMPT 465:
"Generate Python code that reads and writes data from/to Parquet files using the pyarrow library, which provides efficient and flexible access to data stored in the Parquet format."
RESPONSE:
# PyArrow Parquet read/write examples
# Requires: pyarrow>=8.0.0
# Optional: pandas for DataFrame interop (commented)

import os
from datetime import datetime

import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds
# import pandas as pd  # optional


def make_example_table() -> pa.Table:
    schema = pa.schema(
        [
            ("id", pa.int64()),
            ("name", pa.string()),
            ("country", pa.string()),
            ("value", pa.float64()),
            ("ts", pa.timestamp("us")),
        ]
    )
    data = {
        "id": [1, 2, 3, 4, 5],
        "name": ["Alice", "Bob", "Carol", "Dan", "Eve"],
        "country": ["US", "US", "CA", "DE", "US"],
        "value": [10.5, 20.25, 15.0, 11.75, 99.9],
        "ts": [
            datetime(2024, 1, 1, 12, 0, 0),
            datetime(2024, 1, 2, 12, 0, 0),
            datetime(2024, 1, 3, 12, 0, 0),
            datetime(2024, 1, 4, 12, 0, 0),
            datetime(2024, 1, 5, 12, 0, 0),
        ],
    }
    return pa.Table.from_pydict(data, schema=schema)


def write_single_parquet(table: pa.Table, path: str) -> None:
    pq.write_table(
        table,
        path,
        compression="snappy",          # or "zstd", "gzip", "brotli", "lz4", None
        version="2.0",                 # Parquet writer version: "1.0" or "2.0"
        data_page_version="2.0",       # Encoding improvements
        use_dictionary=True,
        write_statistics=True,
    )
    print(f"Wrote {path} (rows={table.num_rows}, cols={table.num_columns})")


def read_single_parquet(path: str) -> pa.Table:
    # Read entire file
    table = pq.read_table(path, memory_map=True)
    print("Read table schema:", table.schema)

    # Column projection
    projected = pq.read_table(path, columns=["id", "value"])
    print("Projected columns:", projected.column_names)

    # Batch (streaming) read for large files
    pf = pq.ParquetFile(path)
    total_rows = 0
    for batch in pf.iter_batches(batch_size=1024, columns=["id", "value"]):
        total_rows += batch.num_rows
    print("Streamed rows via batches:", total_rows)

    # Optional: convert to pandas
    # df = table.to_pandas()
    # print(df.head())

    return table


def write_multiple_row_groups(table: pa.Table, path: str, row_group_size: int = 2) -> None:
    # Write many row groups using ParquetWriter (useful for append-like patterns)
    if os.path.exists(path):
        os.remove(path)
    with pq.ParquetWriter(
        path,
        schema=table.schema,
        compression="zstd",
        version="2.0",
        data_page_version="2.0",
        use_dictionary=True,
        write_statistics=True,
    ) as writer:
        for batch in table.to_batches(max_chunksize=row_group_size):
            writer.write_table(pa.Table.from_batches([batch]))
    print(f"Wrote {path} with multiple row groups")


def write_partitioned_dataset(table: pa.Table, base_dir: str) -> None:
    # Partition on "country" into a directory of Parquet files (Hive-style)
    ds.write_dataset(
        table,
        base_dir=base_dir,
        format="parquet",
        partitioning=["country"],  # creates country=XX/ subdirs
        existing_data_behavior="overwrite_or_ignore",
        file_options=pq.ParquetWriter,
        # You can set format-specific options via parquet_file_options in newer PyArrow versions
    )
    print(f"Wrote partitioned dataset at {base_dir}")


def read_partitioned_dataset(base_dir: str) -> pa.Table:
    dataset = ds.dataset(base_dir, format="parquet")

    # Pushdown filter and column projection
    filt = (ds.field("country") == "US") & (ds.field("value") > 10)
    table = dataset.to_table(filter=filt, columns=["id", "country", "value"])

    print("Filtered rows:", table.num_rows)
    # scanner = dataset.scan(columns=["id", "value"], filter=filt)
    # table = scanner.to_table()
    return table


def read_metadata(path: str) -> None:
    md = pq.read_metadata(path)
    print("Row groups:", md.num_row_groups)
    print("Created by:", md.created_by)
    print("Key-value metadata:", md.metadata)


def main():
    tbl = make_example_table()

    # 1) Single-file Parquet write/read
    single_path = "example.parquet"
    write_single_parquet(tbl, single_path)
    _ = read_single_parquet(single_path)
    read_metadata(single_path)

    # 2) Multi row-group write
    multi_rg_path = "example_multi_rg.parquet"
    write_multiple_row_groups(tbl, multi_rg_path)

    # 3) Partitioned dataset write/read
    base_dir = "example_dataset"
    write_partitioned_dataset(tbl, base_dir)
    filtered_tbl = read_partitioned_dataset(base_dir)
    print("Filtered table preview:", filtered_tbl.to_pydict())

    # 4) In-memory round-trip (no filesystem)
    sink = pa.BufferOutputStream()
    pq.write_table(tbl, sink, compression="zstd")
    buf = sink.getvalue()
    roundtrip = pq.read_table(pa.BufferReader(buf))
    print("In-memory rows:", roundtrip.num_rows)


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 466:
"Generate Python code that imports the LOESS library and uses it to perform robust locally-weighted regression in one or two dimensions, smoothing data to create a continuous curve."
RESPONSE:
# pip install scikit-misc matplotlib numpy
import numpy as np
import matplotlib.pyplot as plt

try:
    from skmisc.loess import loess  # LOESS in 1D or multi-D
except ImportError as e:
    raise ImportError("Please install scikit-misc: pip install scikit-misc") from e


def loess_smooth_1d(x, y, span=0.3, degree=2, robust=True, n_points=500):
    """
    Robust LOESS smoothing for 1D data (y vs x). Returns smooth curve and optional CI.
    """
    l = loess(x, y)
    l.model.span = span                # fraction of data in each local fit
    l.model.degree = degree            # 1=linear, 2=quadratic
    l.model.family = 'symmetric' if robust else 'gaussian'  # 'symmetric' does robust reweighting
    l.control.surface = 'direct'       # exact local fitting
    l.fit()

    xs = np.linspace(np.min(x), np.max(x), n_points)
    pred = l.predict(xs, stderror=True)
    conf = pred.confidence()           # pointwise confidence bands

    return xs, pred.values, conf.lower, conf.upper


def loess_smooth_2d(xy, z, xg, yg, span=0.3, degree=1, robust=True):
    """
    Robust LOESS smoothing for 2D inputs.
      xy: shape (n_samples, 2)
      z:  shape (n_samples,)
      xg, yg: 1D grid vectors for prediction
    Returns Xg, Yg, Zhat for plotting a smooth surface.
    """
    l = loess(xy, z)
    l.model.span = span
    l.model.degree = degree
    l.model.family = 'symmetric' if robust else 'gaussian'
    l.control.surface = 'interpolate'  # faster prediction via interpolation
    l.fit()

    Xg, Yg = np.meshgrid(xg, yg, indexing='xy')
    grid_pts = np.column_stack([Xg.ravel(), Yg.ravel()])
    pred = l.predict(grid_pts, stderror=False)
    Zhat = pred.values.reshape(Xg.shape)
    return Xg, Yg, Zhat


if __name__ == "__main__":
    rng = np.random.default_rng(42)

    # 1D example: smooth a noisy, outlier-contaminated curve
    x = np.linspace(0, 10, 200)
    y_true = np.sin(x) + 0.2 * np.cos(3 * x)
    y = y_true + 0.3 * rng.standard_normal(x.size)
    # add a few outliers
    out_idx = rng.choice(x.size, size=6, replace=False)
    y[out_idx] += rng.choice([-3, 3], size=out_idx.size)

    xs, yhat, lo, hi = loess_smooth_1d(x, y, span=0.25, degree=2, robust=True)

    plt.figure(figsize=(8, 4))
    plt.scatter(x, y, s=15, alpha=0.5, label="Noisy data")
    plt.plot(xs, yhat, color="crimson", lw=2, label="LOESS smooth (robust)")
    plt.fill_between(xs, lo, hi, color="crimson", alpha=0.15, label="Pointwise CI")
    plt.plot(x, y_true, color="gray", ls="--", label="True curve")
    plt.title("1D Robust LOESS")
    plt.legend()
    plt.tight_layout()

    # 2D example: smooth a noisy surface z = f(x, y)
    n = 1500
    X = rng.uniform(-2, 2, size=(n, 2))
    f = np.sin(X[:, 0]) * np.cos(1.5 * X[:, 1])
    z = f + 0.3 * rng.standard_normal(n)
    # add 2D outliers
    oi = rng.choice(n, size=30, replace=False)
    z[oi] += rng.choice([-2.5, 2.5], size=oi.size)

    xg = np.linspace(-2, 2, 120)
    yg = np.linspace(-2, 2, 120)
    Xg, Yg, Zhat = loess_smooth_2d(X, z, xg, yg, span=0.25, degree=1, robust=True)

    plt.figure(figsize=(6, 5))
    im = plt.imshow(
        Zhat,
        extent=[xg.min(), xg.max(), yg.min(), yg.max()],
        origin="lower",
        cmap="viridis",
        aspect="auto",
    )
    plt.scatter(X[:, 0], X[:, 1], c=z, s=8, cmap="viridis", edgecolor="k", linewidth=0.1, alpha=0.6)
    plt.colorbar(im, label="LOESS-predicted z")
    plt.title("2D Robust LOESS Surface")
    plt.xlabel("x")
    plt.ylabel("y")
    plt.tight_layout()
    plt.show()
------------------------------------------------------------

PROMPT 467:
"Generate Python code that creates a markdown formatted text file with the title \"My Markdown Document\" and a list of items in a bullet point format, using the `markdown` library."
RESPONSE:
# pip install markdown
import markdown

def build_markdown(title, items):
    lines = [f"# {title}", ""]
    for item in items:
        lines.append(f"- {item}")
    return "\n".join(lines) + "\n"

def main():
    title = "My Markdown Document"
    items = ["First item", "Second item", "Third item"]

    md_content = build_markdown(title, items)

    # Write the Markdown file
    with open("my_markdown_document.md", "w", encoding="utf-8") as f:
        f.write(md_content)

    # Use the markdown library to also render it to HTML (optional)
    html_content = markdown.markdown(md_content, extensions=["extra"])
    with open("my_markdown_document.html", "w", encoding="utf-8") as f:
        f.write(html_content)

if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 468:
"Generate Python code that imports the necessary modules and creates a SQLAlchemy dialect for interacting with a SAP HANA database, using the `sqlalchemy` and `hanaclient` libraries."
RESPONSE:
# pip install sqlalchemy hanaclient
import sqlalchemy as sa
from sqlalchemy import text
from sqlalchemy.engine import URL
import hanaclient  # ensure the HANA DB-API driver is available

# Build a SQLAlchemy URL for the SAP HANA dialect using the hanaclient driver
url = URL.create(
    drivername="hana+hanaclient",
    username="USER",
    password="PASSWORD",
    host="hana.example.com",
    port=39015,  # adjust to your instance (e.g., 30015/39015)
    # database can be omitted for HANA; schemas are used instead
    # database="",
    # Optional client options can go in query:
    # query={"encrypt": "true", "sslValidateCertificate": "false"},
)

# Create the engine (this constructs the HANA dialect under the hood)
engine = sa.create_engine(url, pool_pre_ping=True, future=True)

# Access the created dialect explicitly if needed
dialect = engine.dialect
print(f"Dialect: {dialect.name}, Driver: {dialect.driver}")

# Simple test query
with engine.connect() as conn:
    result = conn.execute(text("SELECT 'Hello HANA' AS GREETING FROM DUMMY"))
    print(result.scalar_one())
------------------------------------------------------------

PROMPT 469:
"Generate Python code that implements a function to generate Tiny URL- and bit.ly-like URLs using the `urllib` library to truncate long URLs and create a unique shortened identifier."
RESPONSE:
import hashlib
import threading
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

class URLShortener:
    def __init__(self, base_domain="https://sho.rt/", code_length=7):
        self.base_domain = base_domain.rstrip("/") + "/"
        self.code_length = code_length
        self._code_to_url = {}      # short code -> normalized long URL
        self._url_to_code = {}      # normalized long URL -> short code
        self._lock = threading.Lock()

    def shorten(self, long_url: str) -> str:
        # Normalize the URL consistently using urllib.parse
        norm = self._normalize_url(long_url)

        # Return existing code if we've seen this URL
        with self._lock:
            if norm in self._url_to_code:
                return self.base_domain + self._url_to_code[norm]

        # Create a short, unique code; handle rare collisions deterministically
        attempt = 0
        while True:
            code = self._make_code(norm, attempt)
            with self._lock:
                existing = self._code_to_url.get(code)
                if existing is None:
                    # Claim this code
                    self._code_to_url[code] = norm
                    self._url_to_code[norm] = code
                    return self.base_domain + code
                elif existing == norm:
                    # Same URL, same code
                    return self.base_domain + code
                # else: collision; try a new attempt
            attempt += 1

    def expand(self, short_url: str) -> str | None:
        # Returns the original URL or None if not found
        parsed = urlparse(short_url)
        if parsed.scheme and parsed.netloc:
            if (parsed.scheme + "://" + parsed.netloc + "/") != self.base_domain:
                return None
        code = parsed.path.lstrip("/")
        with self._lock:
            return self._code_to_url.get(code)

    def _normalize_url(self, url: str) -> str:
        # Ensure scheme exists
        p = urlparse(url if "://" in url else "http://" + url)

        # Lowercase scheme and host
        scheme = p.scheme.lower()
        netloc = p.netloc.lower()

        # Strip default ports
        if (scheme == "http" and netloc.endswith(":80")) or (scheme == "https" and netloc.endswith(":443")):
            host, _, port = netloc.rpartition(":")
            if (scheme == "http" and port == "80") or (scheme == "https" and port == "443"):
                netloc = host

        # Sort query parameters for canonical form
        query_params = parse_qsl(p.query, keep_blank_values=True)
        query_params.sort()
        query = urlencode(query_params, doseq=True)

        # Remove fragments; rebuild
        normalized = urlunparse((scheme, netloc, p.path or "/", p.params, query, ""))

        return normalized

    def _make_code(self, normalized_url: str, attempt: int) -> str:
        # Hash the normalized URL, include attempt to avoid collisions
        # blake2b is fast, stable; 8 bytes gives plenty of entropy
        h = hashlib.blake2b(
            (normalized_url + f"#{attempt}").encode("utf-8"),
            digest_size=8,
        ).digest()
        n = int.from_bytes(h, "big")
        code = self._base62(n)
        # Truncate to desired length (TinyURL/bitly-like)
        return code[: self.code_length]

    @staticmethod
    def _base62(n: int) -> str:
        alphabet = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
        if n == 0:
            return "0"
        chars = []
        while n > 0:
            n, rem = divmod(n, 62)
            chars.append(alphabet[rem])
        return "".join(reversed(chars))


# Convenience function as requested
_shortener = URLShortener(base_domain="https://sho.rt/", code_length=7)

def generate_short_url(long_url: str) -> str:
    return _shortener.shorten(long_url)


# Example usage
if __name__ == "__main__":
    urls = [
        "https://example.com/some/really/long/path?with=query&and=params",
        "example.com/some/really/long/path?with=query&and=params",  # missing scheme
        "https://example.com:443/some/really/long/path?and=params&with=query",  # same as first (normalized)
        "https://another.example.org/a/different/url",
    ]
    for u in urls:
        print(u, "->", generate_short_url(u))

    # Expanding
    s = generate_short_url("https://example.com/path?x=1")
    print("Short:", s, "Expands to:", _shortener.expand(s))
------------------------------------------------------------

PROMPT 470:
"Generate Python code that uses the requests library to send HTTP GET requests to the Crossref Search API, parsing the JSON responses to extract the desired metadata for a given DOI."
RESPONSE:
# Python 3 code that uses requests to fetch and parse Crossref metadata for a given DOI

import time
import re
import requests
import urllib.parse
from typing import Any, Dict, List, Optional


class CrossrefError(Exception):
    pass


def _strip_tags(text: str) -> str:
    # Minimal HTML/JATS tag stripper for abstracts
    return re.sub(r"<[^>]+>", "", text).strip()


def _format_date(date_parts: List[int]) -> str:
    # date_parts is typically [YYYY], [YYYY, MM], or [YYYY, MM, DD]
    y = date_parts[0] if len(date_parts) > 0 else None
    m = date_parts[1] if len(date_parts) > 1 else 1
    d = date_parts[2] if len(date_parts) > 2 else 1
    if y is None:
        return ""
    return f"{y:04d}-{m:02d}-{d:02d}"


def _best_date(msg: Dict[str, Any]) -> str:
    # Prefer print, then online, then issued, then created
    for key in ("published-print", "published-online", "issued", "created"):
        val = msg.get(key)
        if isinstance(val, dict):
            parts = val.get("date-parts")
            if isinstance(parts, list) and parts and isinstance(parts[0], list):
                return _format_date(parts[0])
    return ""


def _parse_authors(auth_list: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    authors = []
    if not isinstance(auth_list, list):
        return authors
    for a in auth_list:
        given = a.get("given", "")
        family = a.get("family", "")
        name = " ".join(part for part in [given, family] if part).strip() or a.get("name", "")
        orcid = a.get("ORCID")
        if isinstance(orcid, str) and orcid.startswith("http"):
            orcid = orcid.rsplit("/", 1)[-1]
        affiliations = []
        aff = a.get("affiliation")
        if isinstance(aff, list):
            affiliations = [x.get("name") for x in aff if isinstance(x, dict) and x.get("name")]
        authors.append(
            {
                "given": given or None,
                "family": family or None,
                "name": name or None,
                "orcid": orcid or None,
                "affiliations": [x for x in affiliations if x],
            }
        )
    return authors


def fetch_crossref_work_json(
    doi: str,
    mailto: Optional[str] = None,
    timeout: float = 10.0,
    max_retries: int = 3,
    session: Optional[requests.Session] = None,
) -> Dict[str, Any]:
    """
    Fetch the raw Crossref 'work' JSON for a DOI.
    """
    if not doi or not isinstance(doi, str):
        raise ValueError("doi must be a non-empty string")

    encoded_doi = urllib.parse.quote(doi.strip(), safe="")
    url = f"https://api.crossref.org/works/{encoded_doi}"

    headers = {
        # Replace app/version/contact with your details per Crossref's etiquette
        "User-Agent": f"crossref-metadata-fetcher/1.0 ({'mailto:' + mailto if mailto else 'contact:you@example.com'})"
    }

    params = {}
    if mailto:
        params["mailto"] = mailto

    sess = session or requests.Session()

    for attempt in range(max_retries):
        try:
            resp = sess.get(url, headers=headers, params=params, timeout=timeout)
            # Handle rate limiting and transient errors with simple backoff
            if resp.status_code in (429, 502, 503, 504):
                if attempt < max_retries - 1:
                    sleep_s = (2 ** attempt) + 0.1
                    time.sleep(sleep_s)
                    continue
            resp.raise_for_status()
            data = resp.json()
            if not isinstance(data, dict) or data.get("status") != "ok":
                raise CrossrefError(f"Unexpected response structure: {data!r}")
            msg = data.get("message")
            if not isinstance(msg, dict):
                raise CrossrefError("Missing 'message' in Crossref response")
            return msg
        except requests.HTTPError as e:
            # propagate 404 quickly; retry others already handled
            if resp is not None and resp.status_code == 404:
                raise CrossrefError(f"DOI not found: {doi}") from e
            raise
        except requests.RequestException as e:
            if attempt < max_retries - 1:
                time.sleep((2 ** attempt) + 0.1)
                continue
            raise CrossrefError(f"Network error: {e}") from e
        except ValueError as e:
            # JSON decode error
            raise CrossrefError(f"Invalid JSON from Crossref: {e}") from e

    raise CrossrefError("Exhausted retries")


def get_crossref_metadata(
    doi: str,
    mailto: Optional[str] = None,
    timeout: float = 10.0,
    max_retries: int = 3,
    session: Optional[requests.Session] = None,
) -> Dict[str, Any]:
    """
    Return a simplified metadata dict for a DOI using Crossref.
    """
    msg = fetch_crossref_work_json(
        doi=doi, mailto=mailto, timeout=timeout, max_retries=max_retries, session=session
    )

    title = ""
    if isinstance(msg.get("title"), list) and msg["title"]:
        title = msg["title"][0]

    container_title = ""
    if isinstance(msg.get("container-title"), list) and msg["container-title"]:
        container_title = msg["container-title"][0]

    abstract_raw = msg.get("abstract") or ""
    abstract = _strip_tags(abstract_raw) if isinstance(abstract_raw, str) else ""

    issn = msg.get("ISSN") if isinstance(msg.get("ISSN"), list) else []
    isbn = msg.get("ISBN") if isinstance(msg.get("ISBN"), list) else []

    out = {
        "doi": msg.get("DOI"),
        "title": title or None,
        "subtitle": (msg.get("subtitle")[0] if isinstance(msg.get("subtitle"), list) and msg["subtitle"] else None),
        "authors": _parse_authors(msg.get("author")),
        "publisher": msg.get("publisher"),
        "journal": container_title or None,
        "type": msg.get("type"),
        "volume": msg.get("volume"),
        "issue": msg.get("issue") or msg.get("journal-issue", {}).get("issue"),
        "pages": msg.get("page"),
        "published_date": _best_date(msg),
        "url": msg.get("URL"),
        "issn": issn,
        "isbn": isbn,
        "subjects": msg.get("subject") if isinstance(msg.get("subject"), list) else [],
        "license": (msg.get("license")[0].get("URL") if isinstance(msg.get("license"), list) and msg["license"] else None),
        "reference_count": msg.get("reference-count"),
        "is_referenced_by_count": msg.get("is-referenced-by-count"),
        "abstract": abstract or None,
        "source": "Crossref",
        "raw": msg,  # keep the full record if you need more fields
    }
    return out


if __name__ == "__main__":
    # Example usage
    example_dois = [
        "10.1038/nphys1170",
        "10.1145/3368089.3409741",
        "10.1109/5.771073",
    ]

    for d in example_dois:
        try:
            meta = get_crossref_metadata(d, mailto="youremail@example.com")
            print(f"DOI: {meta['doi']}")
            print(f"Title: {meta['title']}")
            print(f"Journal: {meta['journal']}")
            print(f"Published: {meta['published_date']}")
            print(f"Authors: {[a['name'] for a in meta['authors']]}")
            print(f"URL: {meta['url']}")
            print("---")
        except Exception as e:
            print(f"Failed to fetch {d}: {e}")
------------------------------------------------------------

PROMPT 471:
"Generate Python code that imports the Flask and SQLAlchemy modules, creates a Flask app with a SQLAlchemy database connection, and configures a session scope for the Flask requests."
RESPONSE:
import os

from flask import Flask, g
from sqlalchemy import create_engine
from sqlalchemy.orm import scoped_session, sessionmaker, declarative_base

# Create the Flask app
app = Flask(__name__)

# Configure the database URL (override with the DATABASE_URL environment variable)
app.config["DATABASE_URL"] = os.getenv("DATABASE_URL", "sqlite:///app.db")

# Create SQLAlchemy engine and session factory
engine = create_engine(app.config["DATABASE_URL"], pool_pre_ping=True)
SessionLocal = scoped_session(sessionmaker(bind=engine, autocommit=False, autoflush=False))

# Base class for ORM models (if you define any)
Base = declarative_base()

# Create a session for each request
@app.before_request
def create_session():
    g.db = SessionLocal()

# Commit or rollback and remove the session after each request
@app.teardown_request
def remove_session(exception=None):
    db = g.pop("db", None)
    try:
        if db is not None:
            if exception is None:
                db.commit()
            else:
                db.rollback()
    finally:
        # Ensures the scoped session is properly removed from the current context
        SessionLocal.remove()

# Example route
@app.route("/health")
def health():
    return {"status": "ok"}

if __name__ == "__main__":
    app.run(debug=True)
------------------------------------------------------------

PROMPT 472:
"Generate Python code that instruments URLlib requests using OpenTelemetry tracing, adding trace data to outgoing HTTP requests and extracting trace data from incoming HTTP responses."
RESPONSE:
# pip install:
#   opentelemetry-api
#   opentelemetry-sdk
#   opentelemetry-instrumentation-urllib
#   opentelemetry-exporter-otlp  (optional; or use ConsoleSpanExporter below)

import re
import urllib.request
from typing import Optional, Tuple

from opentelemetry import trace
from opentelemetry.instrumentation.urllib import URLLibInstrumentor
from opentelemetry.propagate import set_global_textmap  # optional if you want custom propagators
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor

# --- OpenTelemetry SDK setup ---
resource = Resource.create(
    {
        "service.name": "urllib-client",
        "service.version": "1.0.0",
        "deployment.environment": "dev",
    }
)
provider = TracerProvider(resource=resource)
trace.set_tracer_provider(provider)

# Export to console for demonstration; swap for OTLPSpanExporter to send to a collector
provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))

tracer = trace.get_tracer(__name__)


def _get_header(resp, name: str) -> Optional[str]:
    # Try multiple ways because urllib may return different response objects
    if hasattr(resp, "getheader"):
        v = resp.getheader(name)
        if v:
            return v
    hdrs = getattr(resp, "headers", None)
    if hdrs:
        # Mapping-like (http.client.HTTPMessage)
        v = hdrs.get(name)
        if v:
            return v
    return None


_traceparent_re = re.compile(r"\b00-([0-9a-f]{32})-([0-9a-f]{16})-[0-9a-f]{2}\b", re.IGNORECASE)


def _extract_trace_from_server_timing(server_timing: str) -> Optional[Tuple[str, str, str]]:
    # Looks for 'traceparent;desc="00-<traceid>-<spanid>-<flags>"'
    # or 'traceparent;desc=00-<traceid>-<spanid>-<flags>'
    if not server_timing:
        return None
    # Find the desc=... piece
    m = re.search(r"traceparent\s*;\s*desc\s*=\s*\"?([^\s\",;]+)\"?", server_timing, re.IGNORECASE)
    if not m:
        return None
    tparent = m.group(1)
    m2 = _traceparent_re.search(tparent)
    if not m2:
        return None
    trace_id, span_id = m2.group(1), m2.group(2)
    flags = tparent[-2:]
    return trace_id, span_id, flags


def _extract_server_trace_context(resp) -> dict:
    """
    Extract trace context hints from a server response, if present.
    Preference order:
      1) traceparent/tracestate headers (if the server echoes/returns them)
      2) Server-Timing header with traceparent
    Returns a dict of attributes to set on the client span.
    """
    attrs = {}

    # Direct W3C headers (less common on responses but supported by some systems)
    tparent = _get_header(resp, "traceparent") or _get_header(resp, "Traceparent")
    tstate = _get_header(resp, "tracestate") or _get_header(resp, "Tracestate")
    if tparent:
        attrs["http.response.header.traceparent"] = tparent
        m = _traceparent_re.search(tparent)
        if m:
            attrs["peer.trace_id"] = m.group(1)
            attrs["peer.span_id"] = m.group(2)
    if tstate:
        attrs["http.response.header.tracestate"] = tstate

    # Server-Timing: traceparent;desc="..."
    if "peer.trace_id" not in attrs:
        st = _get_header(resp, "Server-Timing")
        parsed = _extract_trace_from_server_timing(st or "")
        if parsed:
            trace_id, span_id, flags = parsed
            attrs["http.response.header.server_timing"] = st
            attrs["peer.trace_id"] = trace_id
            attrs["peer.span_id"] = span_id
            attrs["peer.trace_flags"] = flags

    return attrs


# --- Instrument urllib with hooks ---
def request_hook(span, request):
    # You can attach custom attributes before the request is sent.
    # OpenTelemetry instrumentation will automatically inject W3C trace context
    # (traceparent/tracestate) into the outgoing request headers.
    try:
        # urllib.request.Request exposes full_url and method in modern Python
        span.set_attribute("http.request.method", getattr(request, "method", "GET"))
        span.set_attribute("http.request.url", getattr(request, "full_url", getattr(request, "get_full_url", lambda: "")()))
    except Exception:
        pass


def response_hook(span, request, response):
    # Attach response-derived trace data (if any) to the span for correlation.
    try:
        ctx_attrs = _extract_server_trace_context(response)
        for k, v in ctx_attrs.items():
            span.set_attribute(k, v)
    except Exception:
        # Avoid breaking requests if parsing fails
        pass


# Activate instrumentation (patches urllib)
URLLibInstrumentor().instrument(
    request_hook=request_hook,
    response_hook=response_hook,
    # By default, the global W3C TraceContext propagator is used for injection.
)

# --- Demo usage making urllib requests ---
def main():
    with tracer.start_as_current_span("client-operation"):
        req = urllib.request.Request(
            url="https://httpbin.org/get",
            method="GET",
            headers={
                # You can add your own headers; OTel will add traceparent/tracestate automatically
                "User-Agent": "urllib-otel-demo/1.0",
            },
        )
        with urllib.request.urlopen(req, timeout=10) as resp:
            _ = resp.read()  # consume the body
            # The response_hook already captured any server trace hints.

    # Flush and shutdown exporter (good practice for short-lived programs)
    trace.get_tracer_provider().shutdown()


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 473:
"Generate Python code that creates a reproducible data science workflow using Metaflow to streamline data ingestion, transformation, and modeling while minimizing engineering efforts."
RESPONSE:
# flow.py
# Reproducible, end-to-end data science workflow using Metaflow:
# - Ingestion: loads a known dataset (no network dependency)
# - Transformation: light feature hygiene (names) and modeling-time preprocessing
# - Modeling: parallel hyperparameter tuning with foreach, deterministic seeds
# - Reproducibility: pinned dependencies via @conda, seeded randomness, dataset hash lineage

from metaflow import FlowSpec, step, Parameter, conda, conda_base, project
import hashlib

# Reusable Conda environment (pin exact versions for reproducibility)
REQS = dict(libraries={
    "scikit-learn": "1.5.2",
    "pandas": "2.2.3",
    "numpy": "1.26.4",
})

@project(name="reproducible_ds_workflow")
@conda_base(python="3.10")
class ReproducibleDSFlow(FlowSpec):
    # Key knobs for experimentation
    model = Parameter(
        "model",
        default="logreg",
        choices=["logreg", "rf"],
        help="Model family: logistic regression (logreg) or random forest (rf).",
    )
    test_size = Parameter(
        "test_size",
        default=0.2,
        help="Hold-out test size fraction.",
        type=float,
    )
    cv = Parameter(
        "cv",
        default=5,
        help="Cross-validation folds for hyperparameter scoring.",
        type=int,
    )
    seed = Parameter(
        "seed",
        default=42,
        help="Random seed for deterministic splits and estimators.",
        type=int,
    )

    @step
    def start(self):
        import numpy as np
        np.random.seed(int(self.seed))
        self.next(self.ingest)

    @step
    @conda(**REQS)
    def ingest(self):
        # Ingest a stable, built-in dataset (no external IO) for reproducibility
        import pandas as pd
        from sklearn.datasets import load_breast_cancer

        ds = load_breast_cancer(as_frame=True)
        df = ds.frame.copy()
        df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]
        self.feature_cols = [c for c in df.columns if c != "target"]
        self.X = df[self.feature_cols]
        self.y = df["target"]

        # Data lineage hash: ties downstream artifacts to exact input values
        x_hash = pd.util.hash_pandas_object(self.X, index=True).values.tobytes()
        y_hash = self.y.values.tobytes()
        self.dataset_hash = hashlib.sha256(x_hash + y_hash).hexdigest()

        print(f"Ingested dataset with shape: {self.X.shape}, hash: {self.dataset_hash}")
        self.next(self.transform)

    @step
    @conda(**REQS)
    def transform(self):
        # Light transform step to illustrate modularity; heavy transforms occur in modeling pipeline.
        # Example: ensure all columns are numeric-friendly, already done above.
        # You could add domain-specific feature engineering here.
        self.X_transformed = self.X.copy()
        print("Transformation complete (no-op for demo).")
        self.next(self.split)

    @step
    @conda(**REQS)
    def split(self):
        from sklearn.model_selection import train_test_split

        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X_transformed,
            self.y,
            test_size=float(self.test_size),
            random_state=int(self.seed),
            stratify=self.y,
        )
        print(
            f"Split: train={self.X_train.shape}, test={self.X_test.shape}, "
            f"seed={self.seed}, test_size={self.test_size}"
        )
        self.next(self.hyperparam_grid)

    @step
    @conda(**REQS)
    def hyperparam_grid(self):
        # Small, sensible grids that run fast locally but can scale out with foreach
        if self.model == "logreg":
            # Logistic regression with standardized features
            self.grid = [
                {"C": c, "penalty": "l2", "solver": "liblinear", "max_iter": 1000}
                for c in [0.01, 0.1, 1.0, 10.0]
            ]
        else:  # rf
            self.grid = [
                {"n_estimators": n, "max_depth": d, "min_samples_split": s}
                for n in [100, 300]
                for d in [None, 5, 10]
                for s in [2, 5]
            ]
        print(f"Prepared {len(self.grid)} hyperparameter candidates for {self.model}.")
        self.next(self.fit_and_score, foreach="grid")

    @step
    @conda(**REQS)
    def fit_and_score(self):
        from sklearn.pipeline import Pipeline
        from sklearn.preprocessing import StandardScaler
        from sklearn.impute import SimpleImputer
        from sklearn.model_selection import cross_val_score
        from sklearn.linear_model import LogisticRegression
        from sklearn.ensemble import RandomForestClassifier

        params = self.input

        if self.model == "logreg":
            estimator = LogisticRegression(random_state=int(self.seed), **params)
            preprocessing = [("imputer", SimpleImputer(strategy="median")),
                             ("scaler", StandardScaler())]
        else:
            estimator = RandomForestClassifier(random_state=int(self.seed), **params)
            # RF doesn't need scaling; keep imputer for safety
            preprocessing = [("imputer", SimpleImputer(strategy="median"))]

        pipeline = Pipeline(preprocessing + [("model", estimator)])

        # Deterministic CV, score by ROC AUC (binary)
        scores = cross_val_score(
            pipeline, self.X_train, self.y_train, cv=int(self.cv), scoring="roc_auc", n_jobs=1
        )

        self.candidate_result = {
            "params": params,
            "mean_cv_roc_auc": float(scores.mean()),
            "std_cv_roc_auc": float(scores.std()),
        }
        print(f"Scored params {params} -> mean ROC AUC: {self.candidate_result['mean_cv_roc_auc']:.4f}")
        self.next(self.select_model)

    @step
    @conda(**REQS)
    def select_model(self, inputs):
        from sklearn.pipeline import Pipeline
        from sklearn.preprocessing import StandardScaler
        from sklearn.impute import SimpleImputer
        from sklearn.linear_model import LogisticRegression
        from sklearn.ensemble import RandomForestClassifier

        # Pick best by mean CV ROC AUC
        best_input = max(inputs, key=lambda i: i.candidate_result["mean_cv_roc_auc"])
        self.best_params = best_input.candidate_result["params"]
        self.cv_summary = [
            inp.candidate_result for inp in inputs
        ]  # for lineage/inspection

        if self.model == "logreg":
            estimator = LogisticRegression(random_state=int(self.seed), **self.best_params)
            preprocessing = [("imputer", SimpleImputer(strategy="median")),
                             ("scaler", StandardScaler())]
        else:
            estimator = RandomForestClassifier(random_state=int(self.seed), **self.best_params)
            preprocessing = [("imputer", SimpleImputer(strategy="median"))]

        self.pipeline = Pipeline(preprocessing + [("model", estimator)])

        # Fit once on full training data
        self.pipeline.fit(self.X_train, self.y_train)

        print(f"Selected best params: {self.best_params}")
        self.next(self.evaluate)

    @step
    @conda(**REQS)
    def evaluate(self):
        from sklearn.metrics import roc_auc_score, accuracy_score, f1_score

        # Predict and evaluate on held-out test set
        y_proba = self.pipeline.predict_proba(self.X_test)[:, 1]
        y_pred = (y_proba >= 0.5).astype(int)

        self.metrics = {
            "dataset_hash": self.dataset_hash,
            "model_family": self.model,
            "best_params": self.best_params,
            "test_roc_auc": float(roc_auc_score(self.y_test, y_proba)),
            "test_accuracy": float(accuracy_score(self.y_test, y_pred)),
            "test_f1": float(f1_score(self.y_test, y_pred)),
            "cv_results": self.cv_summary,
            "seed": int(self.seed),
            "test_size": float(self.test_size),
            "cv_folds": int(self.cv),
        }

        print("Evaluation metrics:", {k: v for k, v in self.metrics.items() if "cv_results" not in k})
        self.next(self.end)

    @step
    def end(self):
        print("Workflow complete. Artifacts available:")
        print("- pipeline (fitted sklearn Pipeline)")
        print("- metrics (dict with test metrics and lineage)")
        print("- best_params, cv_summary, dataset_hash")
        # In Metaflow, all artifacts are versioned and recoverable across runs.


if __name__ == "__main__":
    ReproducibleDSFlow()
------------------------------------------------------------

PROMPT 474:
"Generate Python code that uses the `dataclasses` module to define classes representing JSON objects, and then generates JSON schemas for those classes using the `jsonschema` library."
RESPONSE:
# pip install jsonschema
from __future__ import annotations

import dataclasses
import datetime as _dt
import json
import sys
import types
import uuid
from dataclasses import dataclass, field, MISSING
from typing import (
    Any,
    Dict,
    List,
    Optional,
    Union,
    get_args,
    get_origin,
    get_type_hints,
    Literal,
)

import jsonschema


NoneType = type(None)


def _is_optional(tp: Any) -> bool:
    origin = get_origin(tp)
    if origin is Union:
        return any(a is NoneType for a in get_args(tp))
    return False


def _without_optional(tp: Any) -> Any:
    if _is_optional(tp):
        args = [a for a in get_args(tp) if a is not NoneType]
        if len(args) == 1:
            return args[0]
        else:
            return Union[tuple(args)]  # type: ignore[index]
    return tp


def _is_dataclass_type(tp: Any) -> bool:
    try:
        return dataclasses.is_dataclass(tp)  # type: ignore[arg-type]
    except Exception:
        return False


def _typename(tp: Any) -> str:
    if hasattr(tp, "__module__") and hasattr(tp, "__qualname__"):
        return f"{tp.__module__}.{tp.__qualname__}"
    return str(tp)


class DataclassSchemaGenerator:
    def __init__(self, draft: str = "2020-12"):
        self._defs: Dict[str, Dict[str, Any]] = {}
        self._in_progress: set[str] = set()
        self._draft = draft

    @property
    def defs(self) -> Dict[str, Dict[str, Any]]:
        return self._defs

    def generate(self, cls: type) -> Dict[str, Any]:
        key = self._ensure_defined(cls)
        schema = {
            "$schema": "https://json-schema.org/draft/2020-12/schema"
            if self._draft == "2020-12"
            else "http://json-schema.org/draft-07/schema#",
            "$ref": f"#/$defs/{key}",
        }
        if self._defs:
            schema["$defs"] = self._defs
        return schema

    def _ensure_defined(self, cls: type) -> str:
        key = _typename(cls)
        if key in self._defs:
            return key
        if key in self._in_progress:
            # Recursive reference; definition will be filled later
            return key

        self._in_progress.add(key)
        # Place a placeholder to support recursion/self-references
        self._defs[key] = {"title": getattr(cls, "__name__", key), "type": "object"}

        props: Dict[str, Any] = {}
        required: List[str] = []
        try:
            type_hints = get_type_hints(cls, include_extras=True)
        except TypeError:
            # Python < 3.10: include_extras not available
            type_hints = get_type_hints(cls)

        for f in dataclasses.fields(cls):
            tp = type_hints.get(f.name, f.type)
            field_schema = self._type_to_schema(tp)

            # Merge field metadata into the schema (e.g., description, format, minLength, etc.)
            if f.metadata:
                # Metadata is a mappingproxy; convert and merge
                meta_dict = dict(f.metadata)
                # Allow metadata to override generated schema keys if desired
                field_schema = {**field_schema, **meta_dict}

            props[f.name] = field_schema

            has_default = not (f.default is MISSING and f.default_factory is MISSING)
            optional = _is_optional(tp)
            if not has_default and not optional:
                required.append(f.name)

        obj_schema: Dict[str, Any] = {
            "title": getattr(cls, "__name__", key),
            "type": "object",
            "properties": props,
            "additionalProperties": False,
        }
        if required:
            obj_schema["required"] = required

        self._defs[key] = obj_schema
        self._in_progress.remove(key)
        return key

    def _type_to_schema(self, tp: Any) -> Dict[str, Any]:
        # Handle Optional[T] and general Unions
        origin = get_origin(tp)

        if origin is Union:
            args = list(get_args(tp))
            if NoneType in args:
                non_null = [a for a in args if a is not NoneType]
                if len(non_null) == 1:
                    base_schema = self._type_to_schema(non_null[0])
                    return {"anyOf": [base_schema, {"type": "null"}]}
            # General union of types
            return {"anyOf": [self._type_to_schema(a) for a in args]}

        # Dataclass types -> $ref to $defs
        if _is_dataclass_type(tp):
            key = self._ensure_defined(tp)
            return {"$ref": f"#/$defs/{key}"}

        # Built-in container types
        if origin in (list, List):
            (elem_tp,) = get_args(tp) if get_args(tp) else (Any,)
            return {"type": "array", "items": self._type_to_schema(elem_tp)}

        if origin in (dict, Dict):
            args = get_args(tp)
            if args and len(args) == 2:
                key_tp, val_tp = args
                # JSON object keys must be strings; if annotated otherwise, we still emit object
                val_schema = self._type_to_schema(val_tp)
            else:
                val_schema = {}
            return {"type": "object", "additionalProperties": val_schema}

        # Literals
        if origin is Literal:
            values = list(get_args(tp))
            # Optionally provide "type" as union of literal value types
            types = set()
            for v in values:
                types.add(self._primitive_json_type(type(v)))
            type_val = list(types)
            schema: Dict[str, Any] = {"enum": values}
            if len(type_val) == 1:
                schema["type"] = type_val[0]
            elif len(type_val) > 1:
                schema["type"] = type_val
            return schema

        # Primitives and common types
        prim_type = self._primitive_json_type(tp)
        if prim_type is not None:
            schema: Dict[str, Any] = {"type": prim_type}
            # Add formats for some special classes
            if tp is uuid.UUID:
                schema["format"] = "uuid"
            elif tp is _dt.datetime:
                schema["format"] = "date-time"
            elif tp is _dt.date:
                schema["format"] = "date"
            elif tp is _dt.time:
                schema["format"] = "time"
            return schema

        # Fallback: unknown type -> empty schema
        return {}

    def _primitive_json_type(self, tp: Any) -> Optional[Union[str, List[str]]]:
        # Map Python types to JSON Schema types
        if tp in (str,):
            return "string"
        if tp in (int,):
            return "integer"
        if tp in (float,):
            return "number"
        if tp in (bool,):
            return "boolean"
        if tp in (NoneType,):
            return "null"
        if tp in (_dt.datetime, _dt.date, _dt.time, uuid.UUID):
            return "string"
        # Accept numbers like Decimal as number
        try:
            import decimal

            if tp is decimal.Decimal:
                return "number"
        except Exception:
            pass

        return None


# Example dataclasses

@dataclass
class Address:
    street: str = field(metadata={"minLength": 1, "description": "Street address"})
    city: str = field(metadata={"minLength": 1})
    state: Optional[str] = field(default=None, metadata={"minLength": 2, "maxLength": 2})
    postal_code: Optional[str] = field(
        default=None,
        metadata={
            "description": "US ZIP or ZIP+4",
            "pattern": r"^\d{5}(?:-\d{4})?$",
        },
    )
    country: str = field(default="US", metadata={"minLength": 2, "maxLength": 2})


@dataclass
class Profile:
    display_name: Optional[str] = field(default=None, metadata={"minLength": 1})
    website: Optional[str] = field(default=None, metadata={"format": "uri"})
    birthday: Optional[_dt.date] = None
    interests: List[str] = field(default_factory=list)
    social: Dict[str, str] = field(default_factory=dict, metadata={"description": "Map of network -> handle"})


@dataclass
class User:
    id: uuid.UUID
    email: str = field(metadata={"format": "email"})
    name: str = field(metadata={"minLength": 1})
    is_active: bool = True
    address: Address = field(metadata={"description": "Primary mailing address"})
    profile: Optional[Profile] = None
    # Union example: can be a string literal or integer code
    tier: Union[Literal["free", "pro", "enterprise"], int] = "free"


def main() -> None:
    gen = DataclassSchemaGenerator(draft="2020-12")
    user_schema = gen.generate(User)

    # Validate the generated schema itself against the metaschema
    jsonschema.Draft202012Validator.check_schema(user_schema)

    print("Generated JSON Schema for User:")
    print(json.dumps(user_schema, indent=2))

    # Validate example instances using the generated schema
    valid_instance = {
        "id": str(uuid.uuid4()),
        "email": "alice@example.com",
        "name": "Alice",
        "is_active": True,
        "address": {
            "street": "123 Main St",
            "city": "Springfield",
            "state": "IL",
            "postal_code": "62704-1234",
            "country": "US",
        },
        "profile": {
            "display_name": "alice",
            "website": "https://example.com",
            "birthday": "1990-01-01",
            "interests": ["reading", "hiking"],
            "social": {"twitter": "@alice"},
        },
        "tier": "pro",
    }

    invalid_instance = {
        "id": "not-a-uuid",
        "email": "not-an-email",
        "name": "",
        "address": {
            "street": "",
            "city": "City",
            "state": "Illinois",  # too long; should be 2 letters
            "postal_code": "ABC",
            "country": "USA",  # too long; should be 2 letters
        },
        "tier": {"unexpected": "object"},
    }

    print("\nValidating a correct instance...")
    try:
        jsonschema.validate(valid_instance, user_schema)
        print("Valid instance OK")
    except jsonschema.ValidationError as e:
        print("Valid instance failed validation:", e, file=sys.stderr)

    print("\nValidating an incorrect instance...")
    try:
        jsonschema.validate(invalid_instance, user_schema)
        print("Invalid instance unexpectedly validated")
    except jsonschema.ValidationError as e:
        print("Invalid instance failed validation as expected:")
        print(f"- Path: {'/'.join(map(str, e.path))}")
        print(f"- Message: {e.message}")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 475:
"Generate Python code that uses the `ansible` module to parse Ansible playbooks and extract changelog information from them, then create a changelog file in the format of a YAML file."
RESPONSE:
#!/usr/bin/env python3
"""
ansible_playbook_changelog.py

Generate a YAML changelog from Ansible playbooks using Ansible's Python API
for parsing.

Convention for extracting changelog entries:
- Preferred: add a "changelog" key to a task with either a string summary or a
  dict providing details.
  Example:
    - name: Install nginx
      apt:
        name: nginx
        state: present
      changelog:
        summary: Install nginx
        type: added           # one of: added, changed, fixed, removed, security, deprecated, docs, refactor, perf, misc
        issue: "GH-123"
        pr: "GH-456"
        author: "jdoe"
        breaking: false
        date: "2025-10-01"
        details: "Initial installation of nginx web server"
- Alternative: use tags "changelog" or "changelog:<type>" on a task.
  Example:
    - name: Configure nginx
      template:
        src: nginx.conf.j2
        dest: /etc/nginx/nginx.conf
      tags:
        - changelog
        - changelog:changed

Other notes:
- Task "name" becomes the summary if a changelog dict/string doesn't specify one.
- The script also tries to descend into:
  - import_playbook
  - include_tasks/import_tasks
  - roles/<role>/tasks/main.yml (basic role resolution)

Requirements:
- Python 3.8+
- ansible (tested with 2.9+ and 2.15+; API is semi-internal and may change)
- pyyaml

Usage:
  python ansible_playbook_changelog.py \
    --playbook site.yml playbooks/app.yml \
    --output CHANGELOG.yml \
    --version 1.2.3 \
    --roles-path roles external_roles

Exit codes:
  0 on success
  non-zero on error
"""

import argparse
import datetime as _dt
import os
import sys
import traceback
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union

try:
    # Ansible modules for loading and path management
    from ansible.parsing.dataloader import DataLoader
    from ansible.utils.display import Display
except ImportError as e:
    print("Error: This script requires the 'ansible' Python package. Install it with:\n  pip install ansible", file=sys.stderr)
    sys.exit(2)

try:
    import yaml
except ImportError:
    print("Error: This script requires 'PyYAML'. Install it with:\n  pip install pyyaml", file=sys.stderr)
    sys.exit(2)


# Types
YAML = Union[dict, list, str, int, float, bool, None]


DISPLAY = Display()


CATEGORIES = {"added", "changed", "fixed", "removed", "security", "deprecated", "docs", "refactor", "perf", "misc"}


def iso_now() -> str:
    return _dt.datetime.now(tz=_dt.timezone.utc).astimezone().replace(microsecond=0).isoformat()


def ensure_list(val: Any) -> List[Any]:
    if val is None:
        return []
    if isinstance(val, (list, tuple, set)):
        return list(val)
    return [val]


def normalize_tags(tags: Any) -> List[str]:
    tags = ensure_list(tags)
    out = []
    for t in tags:
        if isinstance(t, str):
            out.append(t)
        else:
            out.append(str(t))
    return out


def guess_category_from_tags(tags: List[str]) -> Optional[str]:
    # Look for tags like "changelog:<type>" to set category
    for t in tags:
        if t.startswith("changelog:"):
            cat = t.split(":", 1)[1].strip().lower()
            if cat in CATEGORIES:
                return cat
    # Fallback: if "changelog" is present without type, leave None
    return None


def resolve_path(base_dir: str, rel: str) -> Optional[str]:
    if not rel:
        return None
    # If rel is absolute, return directly if it exists
    if os.path.isabs(rel):
        return rel if os.path.exists(rel) else None
    # Try relative to base_dir and CWD
    cands = [
        os.path.normpath(os.path.join(base_dir, rel)),
        os.path.normpath(os.path.join(os.getcwd(), rel)),
    ]
    for p in cands:
        if os.path.exists(p):
            return p
    return None


def find_role_task_main(role_name: str, roles_paths: List[str], base_dir: str) -> Optional[str]:
    # Try Ansible-like basic role resolution:
    #   <roles_path>/<role_name>/tasks/main.yml
    # plus base_dir/roles/<role>/tasks/main.yml as default
    candidates = []
    for rp in roles_paths:
        candidates.append(os.path.join(rp, role_name, "tasks", "main.yml"))
    candidates.append(os.path.join(base_dir, "roles", role_name, "tasks", "main.yml"))

    for p in candidates:
        if os.path.exists(p):
            return p
    return None


def load_yaml_file(loader: DataLoader, path: str) -> YAML:
    # Using DataLoader ensures Ansible-specific parsing and vault support
    return loader.load_from_file(path)


def is_task_item(obj: Any) -> bool:
    # A task is typically a mapping with a module or include key
    return isinstance(obj, dict)


def extract_module_name(task: Dict[str, Any]) -> str:
    # Heuristically determine the module name used in the task
    known_control_keys = {
        "name", "tags", "when", "vars", "register", "delegate_to", "become", "become_user", "loop", "with_items",
        "include_tasks", "import_tasks", "include_role", "import_role", "block", "rescue", "always", "notify",
        "changed_when", "failed_when", "check_mode", "environment", "args", "vars_files", "vars_prompt",
        "run_once", "ignore_errors", "retries", "delay", "until", "listen",
    }
    for k in task.keys():
        if k in ("include_tasks", "import_tasks", "include_role", "import_role", "block", "meta"):
            return k
        if k not in known_control_keys:
            return k
    return "task"


def task_to_summary(task: Dict[str, Any]) -> str:
    if "name" in task and isinstance(task["name"], str):
        return task["name"]
    # Fallback to module name and brief args
    mod = extract_module_name(task)
    return f"{mod} task"


def extract_changelog_entry_from_task(
    task: Dict[str, Any],
    context: Dict[str, Any],
) -> Optional[Dict[str, Any]]:
    # Determine if this task has changelog info
    cl = task.get("changelog", None)
    tags = normalize_tags(task.get("tags", []))
    has_tag = any(t == "changelog" or t.startswith("changelog:") for t in tags)

    if cl is None and not has_tag:
        return None

    entry: Dict[str, Any] = {}

    # Pre-populate context fields
    entry["playbook"] = context.get("playbook")
    entry["file"] = context.get("file")
    entry["play"] = context.get("play")
    if context.get("role"):
        entry["role"] = context["role"]

    # Compute summary and details
    if isinstance(cl, str):
        entry["summary"] = cl
    elif isinstance(cl, dict):
        if "summary" in cl and isinstance(cl["summary"], str):
            entry["summary"] = cl["summary"]
        if "details" in cl and isinstance(cl["details"], str):
            entry["details"] = cl["details"]
    # Fallback to task name/module for summary
    entry.setdefault("summary", task_to_summary(task))

    # Category/type
    category = None
    if isinstance(cl, dict) and "type" in cl:
        category = str(cl["type"]).lower().strip()
    if category not in CATEGORIES:
        category = guess_category_from_tags(tags) or "misc"
    entry["type"] = category

    # Other metadata
    if isinstance(cl, dict):
        for key in ("issue", "pr", "author", "breaking", "date", "version"):
            if key in cl:
                entry[key] = cl[key]

    # Add common task context
    entry["tags"] = tags or None
    if "when" in task:
        entry["when"] = task["when"]

    # Store the raw task name if present
    if "name" in task and isinstance(task["name"], str):
        entry["task"] = task["name"]

    return entry


def iter_tasklike_lists(play: Dict[str, Any]) -> Iterable[Tuple[str, List[Dict[str, Any]]]]:
    for key in ("pre_tasks", "tasks", "post_tasks", "handlers"):
        items = play.get(key)
        if isinstance(items, list):
            yield key, items


def parse_tasks_from_list(
    items: List[Any],
    loader: DataLoader,
    base_dir: str,
    roles_paths: List[str],
    context: Dict[str, Any],
    visited_files: Set[str],
) -> List[Dict[str, Any]]:
    entries: List[Dict[str, Any]] = []

    for task in items:
        if not is_task_item(task):
            continue

        # Extract changelog entry for this task if present
        entry = extract_changelog_entry_from_task(task, context)
        if entry:
            entries.append(entry)

        # Handle includes/imports
        if "import_tasks" in task or "include_tasks" in task:
            inc_path = task.get("import_tasks") or task.get("include_tasks")
            if isinstance(inc_path, str):
                resolved = resolve_path(base_dir, inc_path)
                if resolved and os.path.isfile(resolved):
                    if resolved not in visited_files:
                        visited_files.add(resolved)
                        subtasks = load_yaml_file(loader, resolved)
                        if isinstance(subtasks, list):
                            subctx = dict(context)
                            subctx["file"] = os.path.relpath(resolved, start=context.get("project_root", os.getcwd()))
                            entries.extend(parse_tasks_from_list(subtasks, loader, os.path.dirname(resolved), roles_paths, subctx, visited_files))
                else:
                    DISPLAY.warning(f"Could not resolve include_tasks/import_tasks path: {inc_path} (base: {base_dir})")

        # Handle role includes
        if "include_role" in task or "import_role" in task:
            role_spec = task.get("include_role") or task.get("import_role")
            role_name = None
            if isinstance(role_spec, dict):
                role_name = role_spec.get("name") or role_spec.get("role")
            elif isinstance(role_spec, str):
                role_name = role_spec
            if role_name:
                tasks_main = find_role_task_main(role_name, roles_paths, context.get("project_root", base_dir))
                if tasks_main and tasks_main not in visited_files:
                    visited_files.add(tasks_main)
                    subctx = dict(context)
                    subctx["role"] = role_name
                    subctx["file"] = os.path.relpath(tasks_main, start=context.get("project_root", os.getcwd()))
                    role_tasks = load_yaml_file(loader, tasks_main)
                    if isinstance(role_tasks, list):
                        entries.extend(parse_tasks_from_list(role_tasks, loader, os.path.dirname(tasks_main), roles_paths, subctx, visited_files))

        # Handle blocks (block/rescue/always)
        if "block" in task and isinstance(task["block"], list):
            entries.extend(parse_tasks_from_list(task["block"], loader, base_dir, roles_paths, context, visited_files))
        if "rescue" in task and isinstance(task["rescue"], list):
            entries.extend(parse_tasks_from_list(task["rescue"], loader, base_dir, roles_paths, context, visited_files))
        if "always" in task and isinstance(task["always"], list):
            entries.extend(parse_tasks_from_list(task["always"], loader, base_dir, roles_paths, context, visited_files))

    return entries


def parse_playbook(
    playbook_path: str,
    loader: DataLoader,
    project_root: str,
    roles_paths: List[str],
    visited_files: Set[str],
) -> List[Dict[str, Any]]:
    entries: List[Dict[str, Any]] = []
    resolved = os.path.abspath(playbook_path)
    if not os.path.exists(resolved):
        DISPLAY.warning(f"Playbook not found: {playbook_path}")
        return entries

    data = load_yaml_file(loader, resolved)
    base_dir = os.path.dirname(resolved)
    context_base = {
        "playbook": os.path.relpath(resolved, start=project_root),
        "file": os.path.relpath(resolved, start=project_root),
        "project_root": project_root,
    }

    # Two top-level forms are common: list of plays or dict (play) or import_playbook
    if isinstance(data, list):
        plays = data
    elif isinstance(data, dict):
        # Could be a single play or an import_playbook directive
        if "import_playbook" in data:
            # Follow import
            imp = data["import_playbook"]
            if isinstance(imp, str):
                target = resolve_path(base_dir, imp)
                if target and os.path.isfile(target):
                    if target not in visited_files:
                        visited_files.add(target)
                        entries.extend(parse_playbook(target, loader, project_root, roles_paths, visited_files))
                else:
                    DISPLAY.warning(f"Could not resolve import_playbook path: {imp} (base: {base_dir})")
            return entries
        plays = [data]
    else:
        # Unknown content; ignore
        return entries

    for play in plays:
        if not isinstance(play, dict):
            continue

        context = dict(context_base)
        if "name" in play and isinstance(play["name"], str):
            context["play"] = play["name"]

        # Follow import_playbook at the item level as well
        if "import_playbook" in play:
            imp = play["import_playbook"]
            if isinstance(imp, str):
                target = resolve_path(base_dir, imp)
                if target and os.path.isfile(target):
                    if target not in visited_files:
                        visited_files.add(target)
                        entries.extend(parse_playbook(target, loader, project_root, roles_paths, visited_files))
            continue

        # Parse pre_tasks, tasks, post_tasks, handlers
        for section_name, items in iter_tasklike_lists(play):
            entries.extend(parse_tasks_from_list(items, loader, base_dir, roles_paths, context, visited_files))

        # Parse roles main tasks (if any)
        roles = play.get("roles")
        if isinstance(roles, list):
            for r in roles:
                role_name = None
                if isinstance(r, str):
                    role_name = r
                elif isinstance(r, dict):
                    role_name = r.get("role") or r.get("name")
                if not role_name:
                    continue
                tasks_main = find_role_task_main(role_name, roles_paths, project_root)
                if tasks_main and tasks_main not in visited_files:
                    visited_files.add(tasks_main)
                    subctx = dict(context)
                    subctx["role"] = role_name
                    subctx["file"] = os.path.relpath(tasks_main, start=project_root)
                    role_tasks = load_yaml_file(loader, tasks_main)
                    if isinstance(role_tasks, list):
                        entries.extend(parse_tasks_from_list(role_tasks, loader, os.path.dirname(tasks_main), roles_paths, subctx, visited_files))

    return entries


def build_changelog(entries: List[Dict[str, Any]], version: Optional[str]) -> Dict[str, Any]:
    # If entries carry their own version, keep them; otherwise stamp with provided version (or "unreleased")
    stamped_entries: List[Dict[str, Any]] = []
    for e in entries:
        e2 = dict(e)
        e2.setdefault("date", _dt.date.today().isoformat())
        e2.setdefault("version", version or "unreleased")
        stamped_entries.append(e2)

    # Sort entries by type then summary for readability
    sort_key = lambda x: (x.get("version", ""), x.get("type", ""), (x.get("summary") or x.get("task") or ""))
    stamped_entries.sort(key=sort_key)

    changelog = {
        "changelog": {
            "generated_at": iso_now(),
            "entries": stamped_entries,
        }
    }
    return changelog


def write_yaml(path: str, data: Dict[str, Any]) -> None:
    dirn = os.path.dirname(os.path.abspath(path))
    if dirn and not os.path.exists(dirn):
        os.makedirs(dirn, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        yaml.safe_dump(
            data,
            f,
            sort_keys=False,
            default_flow_style=False,
            allow_unicode=True,
        )


def main(argv: Optional[List[str]] = None) -> int:
    ap = argparse.ArgumentParser(description="Extract changelog information from Ansible playbooks and write YAML changelog.")
    ap.add_argument("--playbook", "-p", nargs="+", required=True, help="Playbook file(s) to parse")
    ap.add_argument("--output", "-o", required=True, help="Output changelog YAML file path")
    ap.add_argument("--version", "-v", default=None, help="Version label to apply to entries lacking version (e.g., 1.2.3). Default: 'unreleased'")
    ap.add_argument("--roles-path", "-r", nargs="*", default=[], help="Additional roles paths to search")
    ap.add_argument("--project-root", default=None, help="Project root for relative paths; defaults to common ancestor of playbooks or CWD")
    ap.add_argument("--verbose", action="store_true", help="Verbose logging")
    args = ap.parse_args(argv)

    if args.verbose:
        DISPLAY.verbosity = 3

    playbooks = [os.path.abspath(p) for p in args.playbook]
    for p in playbooks:
        if not os.path.exists(p):
            print(f"Error: Playbook not found: {p}", file=sys.stderr)
            return 2

    # Determine project root
    project_root = args.project_root
    if not project_root:
        # Use common path of provided playbooks, fallback to CWD
        try:
            project_root = os.path.commonpath(playbooks)
        except Exception:
            project_root = os.getcwd()
    project_root = os.path.abspath(project_root)

    # Initialize DataLoader with basedir = project root
    loader = DataLoader()
    try:
        loader.set_basedir(project_root)
    except Exception:
        # set_basedir not present in some versions; ignore
        pass

    roles_paths = [os.path.abspath(rp) for rp in (args.roles_path or []) if rp]
    entries: List[Dict[str, Any]] = []
    visited_files: Set[str] = set()

    try:
        for pb in playbooks:
            visited_files.add(os.path.abspath(pb))
            entries.extend(parse_playbook(pb, loader, project_root, roles_paths, visited_files))
    except Exception as e:
        print("Error while parsing playbooks:", file=sys.stderr)
        traceback.print_exc()
        return 1

    changelog = build_changelog(entries, version=args.version)
    try:
        write_yaml(args.output, changelog)
    except Exception:
        print(f"Failed to write output file: {args.output}", file=sys.stderr)
        traceback.print_exc()
        return 1

    if args.verbose:
        print(f"Wrote {len(changelog['changelog']['entries'])} changelog entries to {args.output}")
    return 0


if __name__ == "__main__":
    sys.exit(main())
------------------------------------------------------------

PROMPT 476:
"Generate Python code that creates a basic HTML report template for Pytest using the `pytest` and `html` modules, including a table of contents, test results, and summary information."
RESPONSE:
# Save this file as conftest.py in your tests root, then run:
#   pytest --html-report=report.html
# It will generate a basic HTML report with a table of contents, test results, and summary.
import datetime
import os
import platform
import re
import sys
import time
from pathlib import Path
from html import escape

import pytest


def pytest_addoption(parser):
    group = parser.getgroup("basic-html-report", "Basic HTML report")
    group.addoption(
        "--html-report",
        action="store",
        metavar="PATH",
        default=None,
        help="Write basic HTML report to the given file path.",
    )


def pytest_configure(config):
    output = config.getoption("--html-report")
    if output:
        reporter = _BasicHTMLReporter(config, output)
        config._basic_html_reporter = reporter  # keep a reference for unconfigure
        config.pluginmanager.register(reporter, name="basic-html-report")


def pytest_unconfigure(config):
    reporter = getattr(config, "_basic_html_reporter", None)
    if reporter is not None:
        config.pluginmanager.unregister(reporter)
        del config._basic_html_reporter


class _BasicHTMLReporter:
    def __init__(self, config, output_path):
        self.config = config
        self.output_path = Path(output_path)
        self.start_time = time.time()
        self.tests = {}   # nodeid -> record dict
        self.order = []   # preserve collection order
        self._used_ids = set()

    # -------- hooks --------
    def pytest_runtest_logreport(self, report):
        nodeid = report.nodeid
        rec = self.tests.get(nodeid)
        if rec is None:
            rec = self.tests[nodeid] = {
                "nodeid": nodeid,
                "outcome": None,           # passed | failed | skipped | error
                "duration": 0.0,
                "longrepr": None,          # traceback or skip reason
                "setup_error": None,
                "teardown_error": None,
                "capstdout": "",
                "capstderr": "",
            }
            self.order.append(nodeid)

        # accumulate duration per phase
        rec["duration"] += float(getattr(report, "duration", 0.0) or 0.0)

        # captured I/O
        if hasattr(report, "capstdout") and report.capstdout:
            rec["capstdout"] += report.capstdout
        if hasattr(report, "capstderr") and report.capstderr:
            rec["capstderr"] += report.capstderr

        # determine outcome
        if report.when == "call":
            if report.passed:
                rec["outcome"] = "passed"
            elif report.failed:
                rec["outcome"] = "failed"
                rec["longrepr"] = getattr(report, "longreprtext", str(report.longrepr))
            elif report.skipped:
                rec["outcome"] = "skipped"
                rec["longrepr"] = getattr(report, "longreprtext", str(report.longrepr))
        elif report.when == "setup":
            if report.failed:
                rec["outcome"] = "error"
                rec["setup_error"] = getattr(report, "longreprtext", str(report.longrepr))
            elif report.skipped:
                # treat skip in setup as skipped test
                if rec["outcome"] is None:
                    rec["outcome"] = "skipped"
                    rec["longrepr"] = getattr(report, "longreprtext", str(report.longrepr))
        elif report.when == "teardown":
            if report.failed:
                rec["outcome"] = "error"
                rec["teardown_error"] = getattr(report, "longreprtext", str(report.longrepr))

    def pytest_sessionfinish(self, session, exitstatus):
        duration = time.time() - self.start_time
        self._write_report(session, duration)

    # -------- helpers --------
    def _write_report(self, session, total_duration):
        # aggregate counts
        counts = {"passed": 0, "failed": 0, "skipped": 0, "error": 0}
        for nodeid in self.order:
            outcome = self.tests[nodeid]["outcome"] or "error"
            if outcome not in counts:
                # normalize anything unexpected as error
                outcome = "error"
                self.tests[nodeid]["outcome"] = "error"
            counts[outcome] += 1

        created = datetime.datetime.now().astimezone().strftime("%Y-%m-%d %H:%M:%S %Z")
        pyver = "{} ({}-bit)".format(platform.python_version(), "64" if sys.maxsize > 2**32 else "32")
        env = {
            "Python": pyver,
            "Platform": platform.platform(),
            "Pytest": pytest.__version__,
            "RootDir": str(getattr(self.config, "rootpath", "")),
        }

        html = []
        html.append("<!doctype html>")
        html.append("<html lang='en'>")
        html.append("<head>")
        html.append("<meta charset='utf-8' />")
        html.append("<meta name='viewport' content='width=device-width, initial-scale=1' />")
        html.append(f"<title>{self._e('Pytest Report')}</title>")
        html.append("<style>")
        html.append(self._css())
        html.append("</style>")
        html.append("</head>")
        html.append("<body>")
        html.append("<header>")
        html.append("<h1>Pytest Report</h1>")
        html.append(f"<div class='meta'>Generated: {self._e(created)}</div>")
        html.append("</header>")

        # summary
        total = sum(counts.values())
        html.append("<section class='summary'>")
        html.append("<h2>Summary</h2>")
        html.append("<ul class='stats'>")
        html.append(f"<li><span class='label'>Total:</span> {total}</li>")
        html.append(f"<li class='passed'><span class='label'>Passed:</span> {counts['passed']}</li>")
        html.append(f"<li class='failed'><span class='label'>Failed:</span> {counts['failed']}</li>")
        html.append(f"<li class='error'><span class='label'>Errors:</span> {counts['error']}</li>")
        html.append(f"<li class='skipped'><span class='label'>Skipped:</span> {counts['skipped']}</li>")
        html.append(f"<li><span class='label'>Duration:</span> {total_duration:.3f}s</li>")
        html.append("</ul>")

        html.append("<h3>Environment</h3>")
        html.append("<ul class='env'>")
        for k, v in env.items():
            html.append(f"<li><span class='label'>{self._e(k)}:</span> {self._e(v)}</li>")
        html.append("</ul>")
        html.append("</section>")

        # table of contents
        html.append("<section class='toc'>")
        html.append("<h2>Table of contents</h2>")
        html.append("<ol>")
        for nodeid in self.order:
            test_id = self._slug(nodeid)
            html.append(f"<li><a href='#{test_id}'>{self._e(nodeid)}</a></li>")
        html.append("</ol>")
        html.append("</section>")

        # test details
        html.append("<section class='details'>")
        html.append("<h2>Test results</h2>")
        for nodeid in self.order:
            rec = self.tests[nodeid]
            test_id = self._slug(nodeid)
            outcome = rec["outcome"] or "error"
            duration = rec["duration"]
            html.append(f"<article id='{test_id}' class='test case {outcome}'>")
            html.append("<div class='test-header'>")
            html.append(f"<h3>{self._e(nodeid)}</h3>")
            html.append(f"<span class='badge {outcome}'>{outcome.upper()}</span>")
            html.append(f"<span class='duration'>{duration:.3f}s</span>")
            html.append("</div>")

            # stdout/stderr
            if rec["capstdout"]:
                html.append("<details class='io'>")
                html.append("<summary>Captured stdout</summary>")
                html.append(f"<pre>{self._e(rec['capstdout'])}</pre>")
                html.append("</details>")
            if rec["capstderr"]:
                html.append("<details class='io'>")
                html.append("<summary>Captured stderr</summary>")
                html.append(f"<pre>{self._e(rec['capstderr'])}</pre>")
                html.append("</details>")

            # errors / tracebacks / reasons
            if rec["setup_error"]:
                html.append("<details open class='error'>")
                html.append("<summary>Setup error</summary>")
                html.append(f"<pre>{self._e(rec['setup_error'])}</pre>")
                html.append("</details>")
            if rec["longrepr"]:
                if outcome == "failed":
                    html.append("<details open class='failed'>")
                    html.append("<summary>Failure traceback</summary>")
                elif outcome == "skipped":
                    html.append("<details class='skipped'>")
                    html.append("<summary>Skip reason</summary>")
                else:
                    html.append("<details>")
                    html.append("<summary>Details</summary>")
                html.append(f"<pre>{self._e(rec['longrepr'])}</pre>")
                html.append("</details>")
            if rec["teardown_error"]:
                html.append("<details open class='error'>")
                html.append("<summary>Teardown error</summary>")
                html.append(f"<pre>{self._e(rec['teardown_error'])}</pre>")
                html.append("</details>")

            html.append("</article>")
        html.append("</section>")

        html.append("<footer>")
        html.append("<p>Report generated by basic HTML reporter.</p>")
        html.append("</footer>")

        html.append("</body>")
        html.append("</html>")

        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        self.output_path.write_text("\n".join(html), encoding="utf-8")

    def _e(self, s):
        # HTML-escape helper
        return escape(str(s), quote=True)

    def _slug(self, nodeid):
        # make a unique HTML id for anchors
        base = re.sub(r"[^a-zA-Z0-9_-]+", "-", nodeid).strip("-")
        if not base:
            base = "test"
        candidate = base
        i = 2
        while candidate in self._used_ids:
            candidate = f"{base}-{i}"
            i += 1
        self._used_ids.add(candidate)
        return candidate

    def _css(self):
        # minimal embedded CSS
        return """
:root {
  --bg: #ffffff;
  --fg: #111418;
  --muted: #6b7280;
  --border: #e5e7eb;
  --passed: #166534;
  --failed: #b91c1c;
  --error:  #7f1d1d;
  --skipped:#b45309;
  --badge-bg: #f3f4f6;
  --code-bg: #f8fafc;
}

* { box-sizing: border-box; }
body {
  font-family: system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Helvetica Neue, Arial, "Apple Color Emoji", "Segoe UI Emoji";
  margin: 1.5rem;
  color: var(--fg);
  background: var(--bg);
  line-height: 1.5;
}
h1, h2, h3 { margin: 0 0 .5rem; }
header { margin-bottom: 1rem; }
.meta { color: var(--muted); }

.summary, .toc, .details { margin: 1rem 0 2rem; }
ul.stats, ul.env { list-style: none; padding: 0; margin: .5rem 0 0; display: flex; flex-wrap: wrap; gap: .75rem 1rem; }
ul.stats li .label, ul.env li .label { color: var(--muted); margin-right: .25rem; }

.passed { color: var(--passed); }
.failed { color: var(--failed); }
.error  { color: var(--error); }
.skipped{ color: var(--skipped); }

.badge {
  display: inline-block;
  font-weight: 600;
  padding: .125rem .5rem;
  border-radius: .375rem;
  margin-left: .5rem;
  background: var(--badge-bg);
  border: 1px solid var(--border);
}
.badge.passed { border-color: color-mix(in srgb, var(--passed) 40%, white); }
.badge.failed { border-color: color-mix(in srgb, var(--failed) 40%, white); }
.badge.error  { border-color: color-mix(in srgb, var(--error)  40%, white); }
.badge.skipped{ border-color: color-mix(in srgb, var(--skipped)40%, white); }

section.toc ol { margin: .5rem 0 0 1.25rem; }

article.test {
  border: 1px solid var(--border);
  border-radius: .5rem;
  padding: .75rem .9rem;
  margin: 1rem 0;
  background: #fff;
}
.test-header { display: flex; align-items: baseline; gap: .5rem; }
.test-header h3 { font-size: 1.05rem; flex: 1; margin-right: .5rem; }
.duration { color: var(--muted); font-variant-numeric: tabular-nums; }

details { margin-top: .5rem; }
details > summary { cursor: pointer; color: #334155; }
pre {
  background: var(--code-bg);
  border: 1px solid var(--border);
  border-radius: .375rem;
  padding: .5rem .75rem;
  overflow: auto;
  white-space: pre-wrap;
}
footer { margin-top: 2rem; color: var(--muted); font-size: .9rem; }
"""
------------------------------------------------------------

PROMPT 477:
"Generate Python code that imports the necessary libraries and uses them to read, manipulate, and write data in ODS format, similar to the functionality provided by a wrapper library."
RESPONSE:
# pip install pyexcel-ods3 pandas
from collections import OrderedDict
from pathlib import Path

import pandas as pd
from pyexcel_ods3 import get_data, save_data


def ensure_sample_input(path: str) -> None:
    """Create a small sample ODS if the file doesn't exist."""
    p = Path(path)
    if p.exists():
        return
    sample = OrderedDict()
    sample["Sales"] = [
        ["Item", "Qty", "Price"],
        ["Apple", 10, 0.50],
        ["Banana", 5, 0.80],
        ["Cherry", 12, 1.25],
        ["Dates", 3, 2.00],
    ]
    save_data(path, sample)


def read_ods_to_dataframe(path: str, sheet_name: str | None = None) -> tuple[pd.DataFrame, str]:
    """
    Read an ODS file into a pandas DataFrame using pyexcel-ods3.
    Returns (df, actual_sheet_name).
    Assumes first row contains headers.
    """
    book = get_data(path)
    if not book:
        raise ValueError(f"No sheets found in {path}")

    # Pick the desired sheet (by name) or the first one.
    if sheet_name is not None:
        if sheet_name not in book:
            raise ValueError(f"Sheet '{sheet_name}' not found. Available: {list(book.keys())}")
        ws_name = sheet_name
    else:
        ws_name = next(iter(book.keys()))

    rows = book[ws_name] or []
    if not rows:
        # Empty sheet
        return pd.DataFrame(), ws_name

    headers = [str(h) if h is not None else "" for h in rows[0]]
    data_rows = rows[1:]

    # Normalize row lengths to match header length
    normalized = [r + [None] * (len(headers) - len(r)) for r in data_rows]

    df = pd.DataFrame(normalized, columns=headers)
    return df, ws_name


def write_dataframe_to_ods(df: pd.DataFrame, path: str, sheet_name: str = "Sheet1") -> None:
    """Write a pandas DataFrame to an ODS file (single-sheet) using pyexcel-ods3."""
    # Convert DataFrame to 2D array with header row
    header = df.columns.tolist()
    # Replace NaN/None with empty strings for ODS
    body = df.fillna("").values.tolist()
    sheet = [header] + body
    book = OrderedDict([(sheet_name, sheet)])
    save_data(path, book)


def append_sheets_to_ods(path: str, sheets: dict[str, list[list]]) -> None:
    """
    Append or overwrite sheets in an existing ODS file.
    'sheets' is a dict of {sheet_name: 2D_array_with_header}.
    """
    try:
        book = OrderedDict(get_data(path))
    except Exception:
        book = OrderedDict()

    for name, data in sheets.items():
        book[name] = data

    save_data(path, book)


def main():
    input_path = "input.ods"
    output_path = "output.ods"

    # 1) Ensure we have an input file to work with
    ensure_sample_input(input_path)

    # 2) Read ODS into a DataFrame
    df, ws_name = read_ods_to_dataframe(input_path, sheet_name="Sales")

    # 3) Manipulate data (examples)
    # 3a) Compute a new column
    if all(col in df.columns for col in ["Qty", "Price"]):
        df["Value"] = pd.to_numeric(df["Qty"], errors="coerce") * pd.to_numeric(df["Price"], errors="coerce")

    # 3b) Filter rows where quantity > 5
    if "Qty" in df.columns:
        df = df[pd.to_numeric(df["Qty"], errors="coerce").fillna(0) > 5]

    # 3c) Update a specific value
    if "Item" in df.columns and "Price" in df.columns:
        df.loc[df["Item"] == "Apple", "Price"] = 0.55
        if "Value" in df.columns:
            df.loc[df["Item"] == "Apple", "Value"] = pd.to_numeric(df.loc[df["Item"] == "Apple", "Qty"], errors="coerce") * 0.55

    # 4) Write manipulated data to a new ODS (single sheet)
    write_dataframe_to_ods(df, output_path, sheet_name=ws_name)

    # 5) Also append a "Summary" sheet to the same output file
    total_rows = len(df)
    total_value = float(pd.to_numeric(df.get("Value", pd.Series(dtype=float)), errors="coerce").sum()) if "Value" in df.columns else ""

    summary_sheet = [
        ["Metric", "Value"],
        ["Row count", total_rows],
        ["Total Value", total_value],
    ]
    append_sheets_to_ods(output_path, {"Summary": summary_sheet})

    print(f"Wrote manipulated data and summary to '{output_path}'")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 478:
"Generate Python code that uses the `click` module to create a CLI application with custom commands and options, and generates man pages for the application using the `manpage` module."
RESPONSE:
# cli_app_with_manpages.py
# Requires: click (>=8), docutils (>=0.19)
# Install: pip install click docutils

import os
import sys
import datetime
from pathlib import Path
from typing import List, Tuple, Optional

import click

# Try to import the manpage writer module from docutils (the "manpage" module).
# We'll use docutils.core.publish_string with writer_name="manpage" to generate roff man pages.
try:
    from docutils.core import publish_string
    from docutils.writers import manpage as manpage  # noqa: F401 (imported to satisfy "uses manpage module")
    HAVE_MANPAGE = True
except Exception:  # pragma: no cover
    HAVE_MANPAGE = False


APP_NAME = "myapp"
APP_VERSION = "0.1.0"
APP_AUTHOR = "Your Name"
APP_MANUAL_GROUP = "User Commands"


# -----------------------
# CLI implementation
# -----------------------
@click.group(context_settings=dict(help_option_names=["-h", "--help"]))
@click.option("-v", "--verbose", count=True, help="Increase verbosity (can be repeated).")
@click.pass_context
def cli(ctx: click.Context, verbose: int):
    """
    Example Click-based CLI demonstrating custom commands, options,
    and man page generation.
    """
    ctx.ensure_object(dict)
    ctx.obj["VERBOSE"] = verbose


@cli.command(help="Greet someone by name.")
@click.option("-n", "--name", default="World", show_default=True, help="Name to greet.")
@click.option("--caps/--no-caps", default=False, show_default=True, help="Uppercase the greeting.")
@click.option("-r", "--repeat", type=click.IntRange(1, 10), default=1, show_default=True, help="Repeat count.")
@click.pass_context
def greet(ctx: click.Context, name: str, caps: bool, repeat: int):
    greeting = f"Hello, {name}!"
    if caps:
        greeting = greeting.upper()
    for _ in range(repeat):
        click.echo(greeting)


@cli.command(name="sum", help="Sum a list of numbers and print the result.")
@click.argument("numbers", type=float, nargs=-1)
@click.option("--prec", type=click.IntRange(0, 10), default=2, show_default=True, help="Decimal places.")
@click.pass_context
def sum_cmd(ctx: click.Context, numbers: Tuple[float, ...], prec: int):
    total = sum(numbers)
    click.echo(f"{total:.{prec}f}")


@cli.command(help="Show application version and exit.")
def version():
    click.echo(f"{APP_NAME} {APP_VERSION}")


# -----------------------
# Manpage generation
# -----------------------

def _today() -> str:
    return datetime.date.today().strftime("%Y-%m-%d")


def _click_usage_line(cmd: click.BaseCommand, prog: str) -> str:
    # Build a single usage line similar to "Usage: ..."
    with click.Context(cmd, info_name=prog) as ctx:
        return ctx.get_usage().strip()


def _short_help(cmd: click.BaseCommand) -> str:
    # Click provides a convenience method, but fall back to .help if needed.
    try:
        return cmd.get_short_help_str()
    except Exception:
        return (cmd.help or "").strip().splitlines()[0] if cmd.help else ""


def _rst_escape(s: str) -> str:
    # Minimal escaping for reStructuredText; manpage writer will handle hyphens etc.
    return s.replace("*", r"\*")


def _format_option_record(param: click.Parameter, prog: str) -> Tuple[str, str]:
    """
    Return (term, body) for a definition list item in RST for an option/argument.
    """
    # Determine name(s) and type/default/required info
    if isinstance(param, click.Option):
        names = list(param.opts) + list(param.secondary_opts)
        term = ", ".join(f"``{n}``" for n in names) or f"``{param.name}``"
        typ = getattr(param.type, "name", str(param.type)) if hasattr(param, "type") else None
        # Help text (with default if shown)
        body_lines = []
        if param.help:
            body_lines.append(_rst_escape(param.help.strip()))
        # Add metadata line
        meta_bits = []
        if typ and typ.lower() not in ("", "text", "string"):
            meta_bits.append(f"type: {typ}")
        if param.required:
            meta_bits.append("required")
        # Use Click's default resolution (can be callable)
        try:
            with click.Context(param.parent, info_name=prog) as ctx:  # type: ignore[attr-defined]
                default_val = param.get_default(ctx)
        except Exception:
            default_val = param.default
        if getattr(param, "show_default", False) and default_val not in (None, ()):
            meta_bits.append(f"default: {default_val!r}")
        if meta_bits:
            body_lines.append(f"({', '.join(meta_bits)})")
        body = "\n".join(body_lines) if body_lines else " "
        return term, body or " "
    elif isinstance(param, click.Argument):
        name = f"<{param.name.upper()}>"
        term = f"``{name}``"
        body_lines = []
        if param.nargs != 1:
            body_lines.append(f"nargs: {param.nargs}")
        if param.required:
            body_lines.append("required")
        # Type info if available
        typ = getattr(param.type, "name", str(param.type)) if hasattr(param, "type") else None
        if typ and typ.lower() not in ("", "text", "string"):
            body_lines.append(f"type: {typ}")
        body = ", ".join(body_lines) if body_lines else " "
        return term, body
    else:
        return f"``{param.name}``", " "


def _command_full_path(parent_prog: Optional[str], name: str) -> str:
    return f"{parent_prog} {name}".strip() if parent_prog else name


def _build_options_definition_list(cmd: click.BaseCommand, prog: str) -> str:
    items: List[str] = []
    for p in cmd.params:
        term, body = _format_option_record(p, prog)
        items.append(f"{term}\n    {body}")
    return "\n\n".join(items) if items else "This command has no options."


def _build_commands_definition_list(group: click.Group, prog: str) -> str:
    items: List[str] = []
    for name, subcmd in sorted(group.commands.items()):
        items.append(f"``{_command_full_path(prog, name)}``\n    {_rst_escape(_short_help(subcmd))}")
    return "\n\n".join(items) if items else "No subcommands."


def _command_docstring(cmd: click.BaseCommand) -> str:
    # Prefer explicit help, fallback to callback docstring, else empty
    if cmd.help:
        return cmd.help.strip()
    cb = getattr(cmd, "callback", None)
    if cb and cb.__doc__:
        return cb.__doc__.strip()
    return ""


def _rst_for_command(
    cmd: click.BaseCommand,
    prog: str,
    title: str,
    section: str,
    short_desc: str,
    is_group: bool,
    see_also: Optional[List[str]] = None,
) -> str:
    """
    Construct an RST document suitable for docutils' manpage writer.
    We explicitly create NAME/SYNOPSIS/etc. sections.
    """
    header = f"""{title}
{'=' * len(title)}

:Manual section: {section}
:Manual group: {APP_MANUAL_GROUP}
:Date: {_today()}
:Version: {APP_VERSION}
:Author: {APP_AUTHOR}

"""
    name_section = f"""NAME
====

{title} - {short_desc or 'command'}

"""

    synopsis = _click_usage_line(cmd, prog)
    synopsis_section = f"""SYNOPSIS
========

::

   {_rst_escape(synopsis)}

"""

    description_text = _command_docstring(cmd) or " "
    description_section = f"""DESCRIPTION
===========

{_rst_escape(description_text)}

"""

    options_section = f"""OPTIONS
=======

{_build_options_definition_list(cmd, prog)}

"""

    commands_section = ""
    if is_group and isinstance(cmd, click.Group):
        commands_section = f"""COMMANDS
========

{_build_commands_definition_list(cmd, prog)}

"""

    see_also_section = ""
    if see_also:
        refs = ", ".join(f"{ref}({section})" for ref in see_also)
        see_also_section = f"""SEE ALSO
========

{refs}

"""

    return header + name_section + synopsis_section + description_section + options_section + commands_section + see_also_section


def _write_manpage(rst_text: str, out_path: Path):
    if not HAVE_MANPAGE:
        raise RuntimeError(
            "docutils (manpage writer) is required. Install with: pip install docutils"
        )
    roff_bytes = publish_string(rst_text, writer_name="manpage")
    out_path.write_bytes(roff_bytes)


def _collect_commands(group: click.Group, prefix_prog: Optional[str] = None) -> List[Tuple[str, click.BaseCommand, str]]:
    """
    Return a list of (full_prog, command, name) for all commands in the tree.
    full_prog: 'myapp greet', name: 'greet'
    """
    out: List[Tuple[str, click.BaseCommand, str]] = []
    for name, subcmd in sorted(group.commands.items()):
        full_prog = _command_full_path(prefix_prog or group.name or APP_NAME, name)
        out.append((full_prog, subcmd, name))
        if isinstance(subcmd, click.Group):
            out.extend(_collect_commands(subcmd, prefix_prog=full_prog))
    return out


@cli.command("gen-man", help="Generate man pages for this application.")
@click.option("-o", "--output-dir", type=click.Path(file_okay=False, dir_okay=True, writable=True, resolve_path=True), default="man", show_default=True, help="Output directory for generated man pages.")
@click.option("-s", "--section", default="1", show_default=True, help="Manual section number (e.g., 1 for user commands).")
@click.option("--per-command/--single", default=True, show_default=True, help="Generate a page per subcommand or a single top-level page.")
@click.option("--compress/--no-compress", default=False, show_default=True, help="Gzip the resulting .{section} files.")
@click.pass_context
def gen_man(ctx: click.Context, output_dir: str, section: str, per_command: bool, compress: bool):
    if not HAVE_MANPAGE:
        raise click.ClickException(
            "docutils with manpage writer is required. Install with: pip install docutils"
        )

    outdir = Path(output_dir)
    outdir.mkdir(parents=True, exist_ok=True)

    # Main app page
    app_title = APP_NAME
    app_short = "Example Click-based CLI"
    # Main app RST
    app_rst = _rst_for_command(
        cli,
        prog=APP_NAME,
        title=app_title,
        section=section,
        short_desc=app_short,
        is_group=True,
        see_also=[f"{APP_NAME}-{name}" for _, _, name in _collect_commands(cli)],
    )
    app_out = outdir / f"{APP_NAME}.{section}"
    _write_manpage(app_rst, app_out)
    if compress:
        _gzip_file(app_out)

    if per_command:
        for full_prog, cmd, name in _collect_commands(cli):
            title = f"{APP_NAME}-{name}"
            rst = _rst_for_command(
                cmd,
                prog=full_prog,
                title=title,
                section=section,
                short_desc=_short_help(cmd),
                is_group=isinstance(cmd, click.Group),
                see_also=[APP_NAME],  # reference back to main page
            )
            out = outdir / f"{title}.{section}"
            _write_manpage(rst, out)
            if compress:
                _gzip_file(out)

    click.echo(f"Man pages written to: {outdir.resolve()}")


def _gzip_file(path: Path):
    import gzip
    data = path.read_bytes()
    gz_path = path.with_suffix(path.suffix + ".gz")
    with gzip.open(gz_path, "wb") as f:
        f.write(data)
    path.unlink()  # remove uncompressed file


# -----------------------
# Entrypoint
# -----------------------
def main():
    cli(obj={})


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 479:
"Generate Python code that imports the necessary modules for searching and indexing, such as NumPy and SciPy, and creates functions for searching and indexing data based on user input."
RESPONSE:
import sys
from typing import Iterable, List, Optional, Sequence, Tuple, Union

import numpy as np
from scipy.spatial import cKDTree
from scipy import sparse


Number = Union[int, float]
ArrayLike = Union[Sequence[Number], np.ndarray]


# ---------------------------
# Core 1D array search/index
# ---------------------------

def index_by_positions(arr: np.ndarray, indices: Sequence[int]) -> np.ndarray:
    """
    Return elements at given integer indices.
    """
    arr = np.asarray(arr)
    idx = np.asarray(indices, dtype=int)
    return arr[idx]


def index_by_boolean_mask(arr: np.ndarray, mask: np.ndarray) -> np.ndarray:
    """
    Return elements where mask is True.
    """
    arr = np.asarray(arr)
    mask = np.asarray(mask, dtype=bool)
    if mask.shape != arr.shape:
        raise ValueError(f"Mask shape {mask.shape} must match array shape {arr.shape}")
    return arr[mask]


def build_mask(arr: np.ndarray, op: str, threshold: Number) -> np.ndarray:
    """
    Build a boolean mask using an operator and threshold.
    op in {'<','<=','==','>=','>','!='}
    """
    arr = np.asarray(arr)
    if op == '<':
        return arr < threshold
    if op == '<=':
        return arr <= threshold
    if op == '==':
        return arr == threshold
    if op == '!=':
        return arr != threshold
    if op == '>=':
        return arr >= threshold
    if op == '>':
        return arr > threshold
    raise ValueError(f"Unsupported operator: {op}")


def search_exact(arr: np.ndarray, value: Number) -> np.ndarray:
    """
    Return indices where arr equals value.
    """
    arr = np.asarray(arr)
    return np.flatnonzero(arr == value)


def search_range(
    arr: np.ndarray,
    low: Optional[Number] = None,
    high: Optional[Number] = None,
    inclusive: Tuple[bool, bool] = (True, False),
    assume_sorted: bool = False,
) -> np.ndarray:
    """
    Return indices of values within [low, high], with inclusivity control.
    Defaults to [low, high) when both bounds given and inclusive=(True, False).

    If assume_sorted is True, uses np.searchsorted on a sorted copy to compute indices efficiently.
    If assume_sorted is False, returns indices via boolean mask (no sort required).
    """
    arr = np.asarray(arr)
    if low is None and high is None:
        raise ValueError("At least one of low or high must be provided.")

    if not assume_sorted:
        mask = np.ones_like(arr, dtype=bool)
        if low is not None:
            mask &= arr >= low if inclusive[0] else arr > low
        if high is not None:
            mask &= arr <= high if inclusive[1] else arr < high
        return np.flatnonzero(mask)

    # assume_sorted=True path
    # Work with sorted indices to map back
    order = np.argsort(arr, kind="mergesort")
    sorted_arr = arr[order]

    left = 0
    right = len(arr)

    if low is not None:
        side = "left" if inclusive[0] else "right"
        left = int(np.searchsorted(sorted_arr, low, side=side))
    if high is not None:
        side = "right" if inclusive[1] else "left"
        right = int(np.searchsorted(sorted_arr, high, side=side))

    # indices in sorted space -> original indices
    return np.sort(order[left:right])


# ---------------------------
# 2D spatial search (k-NN)
# ---------------------------

def nearest_neighbors(
    points: np.ndarray,
    query_points: np.ndarray,
    k: int = 1,
    leafsize: int = 32,
    workers: int = 1,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Use cKDTree to find k nearest neighbors from points to query_points.

    Returns:
      distances: shape (len(query_points), k)
      indices:   shape (len(query_points), k)
    """
    pts = np.asarray(points, dtype=float)
    qry = np.asarray(query_points, dtype=float)

    if pts.ndim != 2 or qry.ndim != 2 or pts.shape[1] != qry.shape[1]:
        raise ValueError("points and query_points must be 2D arrays with the same number of columns.")

    tree = cKDTree(pts, leafsize=leafsize)
    distances, indices = tree.query(qry, k=k, workers=workers)
    # Ensure 2D output for k==1
    if k == 1:
        distances = distances[:, None]
        indices = indices[:, None]
    return distances, indices


# ---------------------------
# 2D dense indexing helpers
# ---------------------------

def index_rows_cols(mat: np.ndarray, rows: Optional[Sequence[int]] = None, cols: Optional[Sequence[int]] = None) -> np.ndarray:
    """
    Return submatrix by selecting rows and/or columns.
    If rows or cols is None, selects all of that dimension.
    """
    M = np.asarray(mat)
    r = slice(None) if rows is None else np.asarray(rows, dtype=int)
    c = slice(None) if cols is None else np.asarray(cols, dtype=int)
    return M[r][:, c]


# ---------------------------
# Sparse matrix indexing
# ---------------------------

def build_csr(
    rows: Sequence[int],
    cols: Sequence[int],
    data: Sequence[Number],
    shape: Tuple[int, int],
) -> sparse.csr_matrix:
    """
    Build a CSR sparse matrix from triplets (rows, cols, data).
    """
    rows = np.asarray(rows, dtype=int)
    cols = np.asarray(cols, dtype=int)
    data = np.asarray(data)
    return sparse.csr_matrix((data, (rows, cols)), shape=shape)


def csr_select_rows(mat: sparse.csr_matrix, rows: Sequence[int]) -> sparse.csr_matrix:
    """
    Fast row selection for CSR.
    """
    if not sparse.isspmatrix_csr(mat):
        mat = mat.tocsr()
    return mat[np.asarray(rows, dtype=int), :]


def csr_get_values(mat: sparse.csr_matrix, row_indices: Sequence[int], col_indices: Sequence[int]) -> np.ndarray:
    """
    Gather values at given row/col index pairs from a CSR matrix.
    Returns a 1D array of values corresponding to zipped pairs.
    """
    if not sparse.isspmatrix_csr(mat):
        mat = mat.tocsr()
    r = np.asarray(row_indices, dtype=int)
    c = np.asarray(col_indices, dtype=int)
    if r.shape != c.shape:
        raise ValueError("row_indices and col_indices must have the same shape.")
    # Advanced indexing on CSR returns dense; gather each pair efficiently
    return np.asarray(mat[r, c]).ravel()


# ---------------------------
# Simple CLI to drive by user input (demo)
# ---------------------------

def _parse_int_list(s: str) -> List[int]:
    return [int(x.strip()) for x in s.split(",") if x.strip()]


def _parse_float_list(s: str) -> List[float]:
    return [float(x.strip()) for x in s.split(",") if x.strip()]


def _parse_points(s: str, dims: int) -> np.ndarray:
    """
    Parse points from a string like: "x1 y1; x2 y2; x3 y3"
    dims is the expected dimensionality per point.
    """
    rows = []
    for chunk in s.split(";"):
        if chunk.strip():
            parts = [float(v) for v in chunk.strip().replace(",", " ").split()]
            if len(parts) != dims:
                raise ValueError(f"Expected {dims} values per point, got {len(parts)} in '{chunk}'.")
            rows.append(parts)
    if not rows:
        raise ValueError("No points parsed.")
    return np.asarray(rows, dtype=float)


def demo_cli() -> None:
    print("Demo data prepared:")
    arr1d = np.random.randint(0, 100, size=50)
    print(f"- 1D array (length={len(arr1d)}), example head: {arr1d[:10].tolist()}")

    pts2d = np.random.randn(100, 2)
    print(f"- 2D points for k-NN (shape={pts2d.shape}), example head: {pts2d[:3].tolist()}")

    rng = np.random.default_rng(42)
    mat_csr = sparse.random(50, 50, density=0.05, format="csr", random_state=rng, data_rvs=lambda n: rng.integers(1, 10, n))
    print(f"- CSR sparse matrix (shape={mat_csr.shape}, nnz={mat_csr.nnz})")

    menu = """
Choose an operation:
  1) Index by positions (1D)
  2) Index by boolean mask (1D) [op threshold]
  3) Search exact value (1D)
  4) Search range (1D)
  5) Nearest neighbors (2D)
  6) Dense matrix index (2D)
  7) Sparse: select rows (CSR)
  8) Sparse: get values at (row,col) pairs (CSR)
  9) Quit
Enter choice [1-9]: """
    while True:
        try:
            choice = input(menu).strip()
        except (EOFError, KeyboardInterrupt):
            print("\nExiting.")
            return

        if choice == "1":
            s = input("Enter comma-separated integer indices (e.g., 0,3,5): ").strip()
            idx = _parse_int_list(s)
            try:
                out = index_by_positions(arr1d, idx)
                print("Result:", out.tolist())
            except Exception as e:
                print("Error:", e)

        elif choice == "2":
            op = input("Operator (<,<=,==,!=,>=,>): ").strip()
            threshold = float(input("Threshold (number): ").strip())
            try:
                mask = build_mask(arr1d, op, threshold)
                out = index_by_boolean_mask(arr1d, mask)
                print("Mask true count:", int(mask.sum()))
                print("Result:", out.tolist())
            except Exception as e:
                print("Error:", e)

        elif choice == "3":
            val = float(input("Exact value to search: ").strip())
            idx = search_exact(arr1d, val)
            print("Matching indices:", idx.tolist())

        elif choice == "4":
            low_s = input("Low bound (blank for None): ").strip()
            high_s = input("High bound (blank for None): ").strip()
            incl_low = input("Inclusive low? [y/N]: ").strip().lower() == "y"
            incl_high = input("Inclusive high? [y/N]: ").strip().lower() == "y"
            assume_sorted = input("Assume sorted? [y/N]: ").strip().lower() == "y"

            low = float(low_s) if low_s else None
            high = float(high_s) if high_s else None
            idx = search_range(arr1d, low=low, high=high, inclusive=(incl_low, incl_high), assume_sorted=assume_sorted)
            print("Indices:", idx.tolist())
            print("Values:", arr1d[idx].tolist())

        elif choice == "5":
            k = int(input("k (number of neighbors): ").strip())
            qp = input("Enter query points 'x y; x y; ...' (spaces or commas ok): ").strip()
            try:
                query = _parse_points(qp, dims=2)
                dists, inds = nearest_neighbors(pts2d, query, k=k)
                print("Indices:\n", inds)
                print("Distances:\n", dists)
            except Exception as e:
                print("Error:", e)

        elif choice == "6":
            rows = input("Row indices (comma-separated, blank for all): ").strip()
            cols = input("Col indices (comma-separated, blank for all): ").strip()
            mat = np.arange(100).reshape(10, 10)
            print("Using demo matrix 10x10 with values 0..99")
            r = None if rows == "" else _parse_int_list(rows)
            c = None if cols == "" else _parse_int_list(cols)
            try:
                sub = index_rows_cols(mat, r, c)
                print(sub)
            except Exception as e:
                print("Error:", e)

        elif choice == "7":
            rows = input("Row indices to select (comma-separated): ").strip()
            r = _parse_int_list(rows)
            try:
                sub = csr_select_rows(mat_csr, r)
                print(f"Selected rows shape={sub.shape}, nnz={sub.nnz}")
                print(sub)
            except Exception as e:
                print("Error:", e)

        elif choice == "8":
            rp = input("Row indices (comma-separated, same length as cols): ").strip()
            cp = input("Col indices (comma-separated, same length as rows): ").strip()
            r = _parse_int_list(rp)
            c = _parse_int_list(cp)
            try:
                vals = csr_get_values(mat_csr, r, c)
                print("Values:", vals.tolist())
            except Exception as e:
                print("Error:", e)

        elif choice == "9":
            print("Goodbye.")
            return
        else:
            print("Invalid choice. Try again.")


if __name__ == "__main__":
    # If the script is executed directly, launch the demo CLI.
    # You can also import this module and call the functions directly in your own code.
    try:
        demo_cli()
    except Exception as exc:
        print("Fatal error:", exc, file=sys.stderr)
        sys.exit(1)
------------------------------------------------------------

PROMPT 480:
"Generate Python code that imports the necessary libraries and creates a wrapper function for each AdSupport function, conforming to the API defined by the AdSupport framework on macOS."
RESPONSE:
# Python wrappers for macOS AdSupport framework via PyObjC

# Requires: pyobjc-core and pyobjc-framework-AdSupport
# pip install pyobjc-core pyobjc-framework-AdSupport

import sys

try:
    # Prefer the stub module provided by PyObjC
    from AdSupport import ASIdentifierManager
except Exception:
    # Fallback: try to load the framework bundle directly
    import objc
    path = objc.pathForFramework("/System/Library/Frameworks/AdSupport.framework")
    if not path:
        raise ImportError("AdSupport framework not found on this system.")
    objc.loadBundle("AdSupport", globals(), bundle_path=path)
    from AdSupport import ASIdentifierManager  # retry after loading


__all__ = [
    "sharedManager",
    "advertisingIdentifier",
    "isAdvertisingTrackingEnabled",
]


def sharedManager():
    """
    Wrapper for +[ASIdentifierManager sharedManager]
    Returns the shared ASIdentifierManager instance.
    """
    return ASIdentifierManager.sharedManager()


def advertisingIdentifier(as_string=False):
    """
    Wrapper for -[ASIdentifierManager advertisingIdentifier]
    Returns:
      - NSUUID instance by default.
      - If as_string=True, returns the UUID string (uppercase, 36 chars).
    May return None in cases where the identifier is unavailable or restricted.
    """
    mgr = ASIdentifierManager.sharedManager()
    uuid = mgr.advertisingIdentifier()
    if uuid is None:
        return None
    return uuid.UUIDString() if as_string else uuid


def isAdvertisingTrackingEnabled():
    """
    Wrapper for -[ASIdentifierManager isAdvertisingTrackingEnabled]
    Returns:
      - bool indicating whether ad tracking is enabled.
    Notes:
      - This API is deprecated on newer OS versions, but may still be present.
      - If unavailable on the running OS/framework, raises NotImplementedError.
    """
    mgr = ASIdentifierManager.sharedManager()

    # Primary (documented) getter
    if hasattr(mgr, "isAdvertisingTrackingEnabled"):
        return bool(mgr.isAdvertisingTrackingEnabled())

    # Some PyObjC builds may expose the property getter without the 'is' prefix
    if hasattr(mgr, "advertisingTrackingEnabled"):
        # Can be a method or a bridged property; call if callable, else cast
        attr = getattr(mgr, "advertisingTrackingEnabled")
        return bool(attr() if callable(attr) else attr)

    raise NotImplementedError("Advertising tracking status is not available on this platform/framework version.")
------------------------------------------------------------

PROMPT 481:
"Generate Python code that imports the necessary libraries, creates a pandas dataframe from an AWS DynamoDB table, modifies the dataframe, and writes the modified data back to DynamoDB."
RESPONSE:
# pip install boto3 pandas
# AWS credentials and region should be configured via environment variables, AWS config, or IAM role.

import os
from decimal import Decimal
from typing import Any, Dict, List

import boto3
import pandas as pd


TABLE_NAME = "YourDynamoDBTableName"  # TODO: set your table name
PRIMARY_KEYS = ["pk", "sk"]           # TODO: set your table's primary key(s), e.g. ["id"] or ["pk", "sk"]
AWS_REGION = os.getenv("AWS_REGION")  # Optional. Leave None to use default resolution.


def get_table(table_name: str):
    session = boto3.session.Session(region_name=AWS_REGION)
    ddb = session.resource("dynamodb")
    return ddb.Table(table_name)


def scan_all_items(table) -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    scan_kwargs: Dict[str, Any] = {}
    while True:
        resp = table.scan(**scan_kwargs)
        items.extend(resp.get("Items", []))
        last_key = resp.get("LastEvaluatedKey")
        if not last_key:
            break
        scan_kwargs["ExclusiveStartKey"] = last_key
    return items


def decimal_to_native(obj: Any) -> Any:
    # Convert DynamoDB Decimal to int/float, sets to lists, recurse into dict/list.
    if isinstance(obj, dict):
        return {k: decimal_to_native(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [decimal_to_native(v) for v in obj]
    if isinstance(obj, set):
        return [decimal_to_native(v) for v in obj]  # lists are friendlier for DataFrame
    if isinstance(obj, Decimal):
        return int(obj) if obj % 1 == 0 else float(obj)
    return obj


def sanitize_for_dynamodb(obj: Any) -> Any:
    # - DynamoDB does not allow NaN/Inf or empty strings; convert to None.
    # - Floats must be Decimal.
    # - Recurse through nested structures.
    if isinstance(obj, dict):
        out = {}
        for k, v in obj.items():
            val = sanitize_for_dynamodb(v)
            # Remove empty strings (use None -> NULL) or keep None
            if val == "":
                val = None
            out[k] = val
        return out
    if isinstance(obj, list):
        return [sanitize_for_dynamodb(v) for v in obj]
    if obj is None:
        return None
    if isinstance(obj, float):
        # Convert to Decimal via string to preserve precision and avoid NaN/Inf
        if pd.isna(obj) or obj == float("inf") or obj == float("-inf"):
            return None
        return Decimal(str(obj))
    if isinstance(obj, int):
        return Decimal(obj)
    # Leave bool, str as-is. Note: empty str handled above.
    return obj


def modify_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    # Example modifications. Adjust to your needs.
    df = df.copy()

    # Example 1: add/overwrite an updated_at ISO timestamp column
    df["updated_at"] = pd.Timestamp.utcnow().isoformat(timespec="seconds") + "Z"

    # Example 2: uppercase a 'name' column if it exists
    if "name" in df.columns:
        df["name"] = df["name"].astype(str).str.upper()

    # Example 3: compute a numeric column if source columns exist
    if {"price", "quantity"}.issubset(df.columns):
        df["total_value"] = pd.to_numeric(df["price"], errors="coerce").fillna(0) * pd.to_numeric(
            df["quantity"], errors="coerce"
        ).fillna(0)

    # Replace NaN with None for DynamoDB NULL mapping
    df = df.where(pd.notnull(df), None)
    return df


def ensure_primary_keys_present(df: pd.DataFrame, keys: List[str]):
    missing = [k for k in keys if k not in df.columns]
    if missing:
        raise ValueError(f"DataFrame is missing primary key column(s): {missing}")


def write_items(table, items: List[Dict[str, Any]], primary_keys: List[str]):
    # batch_writer handles retries/backoff. overwrite_by_pkeys ensures idempotent upserts.
    with table.batch_writer(overwrite_by_pkeys=primary_keys) as batch:
        for item in items:
            batch.put_item(Item=item)


def main():
    table = get_table(TABLE_NAME)

    # 1) Read all items from DynamoDB
    print("Scanning DynamoDB table...")
    raw_items = scan_all_items(table)
    print(f"Fetched {len(raw_items)} item(s).")

    # 2) Convert to native Python types and create DataFrame
    native_items = [decimal_to_native(it) for it in raw_items]
    if not native_items:
        print("Table is empty. Nothing to process.")
        return

    df = pd.DataFrame(native_items)

    # 3) Modify the DataFrame as needed
    ensure_primary_keys_present(df, PRIMARY_KEYS)
    df_modified = modify_dataframe(df)

    # 4) Convert DataFrame rows back to DynamoDB-friendly dicts
    #    - Replace NaN with None
    df_modified = df_modified.where(pd.notnull(df_modified), None)

    #    - Convert row dict to DynamoDB-compatible types (Decimal for numbers, no empty strings)
    records: List[Dict[str, Any]] = []
    for row in df_modified.to_dict(orient="records"):
        item = sanitize_for_dynamodb(row)
        # Optionally ensure primary keys are still present and non-empty
        for k in PRIMARY_KEYS:
            if k not in item or item[k] in (None, ""):
                raise ValueError(f"Primary key '{k}' missing or empty in item: {item}")
        records.append(item)

    # 5) Write back to DynamoDB
    print(f"Writing {len(records)} item(s) back to DynamoDB...")
    write_items(table, records, PRIMARY_KEYS)
    print("Done.")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 482:
"Generate Python code that imports the necessary libraries and creates a wrapper function around the isl library's functions to provide a simple interface for working with integer sets."
RESPONSE:
# Simple Python wrapper for ISL (via islpy) integer sets

# Requires:
#   pip install islpy
#
# This wrapper provides a small, ergonomic layer over islpy's Set to:
# - create sets from strings or simple bound "boxes"
# - union/intersection/difference
# - projection by variable names
# - sampling a point
# - membership testing by coordinates
# - applying affine maps

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

try:
    import islpy as isl
except ImportError as e:
    raise ImportError(
        "This module requires islpy (Python bindings for ISL). "
        "Install it with: pip install islpy"
    ) from e


def _ctx_of(obj) -> isl.Context:
    # islpy objects carry around a context; grab it for constructing new values
    return obj.get_ctx() if hasattr(obj, "get_ctx") else isl.Context()


def _val_to_int(v: isl.Val) -> int:
    # Convert isl.Val to int when possible
    if hasattr(v, "to_python"):
        py = v.to_python()
        if isinstance(py, (int, float)):
            return int(py)
    # Fallback: parse from string (works for integers)
    return int(str(v))


@dataclass(frozen=True)
class IntSet:
    """A tiny convenience wrapper around islpy.Set for integer sets."""
    _set: isl.Set

    # -------- Construction helpers --------

    @staticmethod
    def from_str(spec: str) -> "IntSet":
        """
        Construct from an ISL set string.
        Example: "{ [i, j] : 0 <= i < 10 and j >= i }"
        """
        s = isl.Set(spec)
        return IntSet(s.coalesce())

    @staticmethod
    def box(var_names: Sequence[str],
            lower: Optional[Dict[str, int]] = None,
            upper: Optional[Dict[str, int]] = None) -> "IntSet":
        """
        Construct a rectangular box set: for each var, optional inclusive/exclusive bounds.
        - lower[name] is interpreted as var >= lower[name]
        - upper[name] is interpreted as var < upper[name]
        If a bound is omitted, that side is left unbounded.
        """
        lower = lower or {}
        upper = upper or {}

        dims = ", ".join(var_names)
        constraints: List[str] = []
        for v in var_names:
            if v in lower:
                constraints.append(f"{lower[v]} <= {v}")
            if v in upper:
                constraints.append(f"{v} < {upper[v]}")
        cond = " and ".join(constraints) if constraints else "true"
        spec = f"{{ [{dims}] : {cond} }}"
        return IntSet.from_str(spec)

    # -------- Basic ops --------

    def simplify(self) -> "IntSet":
        return IntSet(self._set.coalesce())

    def union(self, other: "IntSet") -> "IntSet":
        return IntSet(self._set.union(other._set).coalesce())

    def intersect(self, other: "IntSet") -> "IntSet":
        return IntSet(self._set.intersect(other._set).coalesce())

    def difference(self, other: "IntSet") -> "IntSet":
        return IntSet(self._set.subtract(other._set).coalesce())

    def is_empty(self) -> bool:
        return self._set.is_empty()

    # -------- Introspection --------

    def dim_names(self) -> List[str]:
        sp = self._set.get_space()
        n = self._set.dim(isl.dim_type.set)
        return [sp.get_dim_name(isl.dim_type.set, i) or f"d{i}" for i in range(n)]

    def rename_dims(self, new_names: Sequence[str]) -> "IntSet":
        s = self._set
        n = s.dim(isl.dim_type.set)
        if len(new_names) != n:
            raise ValueError(f"Expected {n} names, got {len(new_names)}")
        for i, nm in enumerate(new_names):
            s = s.set_dim_name(isl.dim_type.set, i, nm)
        return IntSet(s)

    # -------- Projection by names --------

    def project_out(self, names_to_drop: Iterable[str]) -> "IntSet":
        """
        Project out (eliminate) the listed set dimensions (by name).
        """
        sp = self._set.get_space()
        # Map names to indices
        indices = []
        for nm in names_to_drop:
            idx = sp.find_dim_by_name(isl.dim_type.set, nm)
            if idx < 0:
                raise KeyError(f"Dimension '{nm}' not found")
            indices.append(idx)
        # Remove from highest index to lowest to keep indices stable
        s = self._set
        for idx in sorted(indices, reverse=True):
            s = s.project_out(isl.dim_type.set, idx, 1)
        return IntSet(s.coalesce())

    def project_keep_only(self, names_to_keep: Iterable[str]) -> "IntSet":
        """
        Keep only the given names, project out the rest.
        """
        keep = set(names_to_keep)
        drop = [n for n in self.dim_names() if n not in keep]
        return self.project_out(drop)

    # -------- Sampling and membership --------

    def sample_point(self) -> Optional[Dict[str, int]]:
        """
        Return one integer point as a dict {name: value}, or None if empty.
        """
        if self.is_empty():
            return None
        try:
            p = self._set.sample_point()
        except Exception:
            return None
        sp = self._set.get_space()
        n = self._set.dim(isl.dim_type.set)
        out: Dict[str, int] = {}
        for i in range(n):
            nm = sp.get_dim_name(isl.dim_type.set, i) or f"d{i}"
            val = p.get_coordinate_val(isl.dim_type.set, i)
            out[nm] = _val_to_int(val)
        return out

    def contains(self, coords: Dict[str, int]) -> bool:
        """
        Membership test: coords is a dict {name: int}.
        Only specified dimensions are set; unspecified are left 0.
        """
        sp = self._set.get_space()
        # Build a point in the same space, initialize with zeros
        pt = isl.Point.zero(sp)
        ctx = _ctx_of(self._set)

        for nm, val in coords.items():
            idx = sp.find_dim_by_name(isl.dim_type.set, nm)
            if idx < 0:
                raise KeyError(f"Dimension '{nm}' not found")
            pt = pt.set_coordinate_val(isl.dim_type.set, idx, isl.Val.int_from_si(ctx, int(val)))

        # Prefer direct contains if available; otherwise, use singleton subset check
        try:
            return bool(self._set.contains(pt))
        except Exception:
            singleton = isl.Set.from_point(pt)
            return singleton.is_subset(self._set)

    # -------- Applying affine maps --------

    def image(self, map_str: str) -> "IntSet":
        """
        Apply an affine map written in ISL syntax to transform the set.
        Example: map_str = "{ [i,j] -> [i + j] }"
        """
        m = isl.Map(map_str)
        return IntSet(self._set.apply(m).coalesce())

    # -------- Conversion --------

    def to_isl(self) -> isl.Set:
        return self._set

    def __str__(self) -> str:
        return str(self._set)

    def __repr__(self) -> str:
        return f"IntSet({self._set})"


# A simple wrapper function (factory) that the caller can use directly:
def intset(spec: str) -> IntSet:
    """
    Wrapper function: build an IntSet from an ISL string.
    Example: intset("{ [i,j] : 0 <= i < 10 and j >= i }")
    """
    return IntSet.from_str(spec)


# Example usage
if __name__ == "__main__":
    # Create sets
    A = intset("{ [i,j] : 0 <= i < 5 and 0 <= j < 5 }").rename_dims(["i", "j"])
    B = intset("{ [i,j] : i + j = 4 }").rename_dims(["i", "j"])

    # Basic operations
    I = A.intersect(B).simplify()
    U = A.union(B)
    D = A.difference(B)

    print("A:", A)
    print("B:", B)
    print("A ∩ B:", I)
    print("A ∪ B:", U)
    print("A \\ B:", D)

    # Sample a point
    print("Sample from A ∩ B:", I.sample_point())

    # Membership test
    print("Contains i=1,j=3:", I.contains({"i": 1, "j": 3}))
    print("Contains i=2,j=2:", I.contains({"i": 2, "j": 2}))

    # Projection
    proj = A.project_keep_only(["i"])
    print("Project A to i only:", proj)

    # Box constructor
    Box = IntSet.box(["x", "y"], lower={"x": 0, "y": 0}, upper={"x": 3, "y": 2})
    print("Box:", Box)

    # Apply affine map: [i,j] -> [i+j]
    Img = A.image("{ [i,j] -> [i + j] }")
    print("Image of A under sum map:", Img)
------------------------------------------------------------

PROMPT 483:
"Generate Python code that uses the Django REST Framework to create API endpoints for serving pandas dataframes, which can be consumed by client-side visualization libraries like d3.js."
RESPONSE:
Below is a minimal, production-friendly Django REST Framework setup to expose pandas DataFrames over JSON for client-side visualizations (e.g., d3.js). It supports pagination, column selection, filtering, ordering, and a schema endpoint.

Install (pip):
- Django
- djangorestframework
- pandas

settings.py (add apps and a data path)
- Ensure you have rest_framework in INSTALLED_APPS.
- Provide a CSV path or adapt the loader for your data source.

Example:
INSTALLED_APPS = [
    # ...
    'rest_framework',
    'data_api',  # your app
]

REST_FRAMEWORK = {
    'DEFAULT_RENDERER_CLASSES': ['rest_framework.renderers.JSONRenderer'],
    'DEFAULT_PARSER_CLASSES': ['rest_framework.parsers.JSONParser'],
}

# Where your data lives (CSV for example)
DATAFRAME_CSV_PATH = BASE_DIR / 'data' / 'sales.csv'  # set this to your file


data_api/views.py
from __future__ import annotations
import threading
from typing import Dict, Any, Iterable
import os
import pandas as pd
from django.conf import settings
from django.utils.timezone import now
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.pagination import LimitOffsetPagination
from rest_framework import status


class DataFrameLimitOffsetPagination(LimitOffsetPagination):
    default_limit = 500
    max_limit = 5000


class DataFrameStore:
    """
    Thread-safe in-memory cache of a pandas DataFrame.
    Replace the loader with your own (CSV, DB, Parquet, etc.).
    """
    _lock = threading.RLock()
    _df: pd.DataFrame | None = None
    _mtime: float | None = None

    @classmethod
    def _load_df(cls) -> pd.DataFrame:
        path = getattr(settings, 'DATAFRAME_CSV_PATH', None)
        if not path:
            # Fallback demo DataFrame
            return pd.DataFrame(
                [
                    {"date": "2024-01-01", "region": "NA", "product": "A", "sales": 100},
                    {"date": "2024-01-02", "region": "EU", "product": "A", "sales": 130},
                    {"date": "2024-01-03", "region": "NA", "product": "B", "sales": 80},
                    {"date": "2024-01-03", "region": "APAC", "product": "B", "sales": 60},
                ]
            )
        df = pd.read_csv(path)
        # Optional: parse datetimes or enforce dtypes
        for col in df.columns:
            if 'date' in col.lower():
                with pd.option_context('mode.chained_assignment', None):
                    df[col] = pd.to_datetime(df[col], errors='ignore')
        return df

    @classmethod
    def get_df(cls) -> pd.DataFrame:
        with cls._lock:
            path = getattr(settings, 'DATAFRAME_CSV_PATH', None)
            file_mtime = None
            if path and os.path.exists(path):
                file_mtime = os.path.getmtime(path)

            # Reload if empty or file changed
            if cls._df is None or (file_mtime and file_mtime != cls._mtime):
                cls._df = cls._load_df()
                cls._mtime = file_mtime
            return cls._df

    @classmethod
    def schema(cls) -> Dict[str, Any]:
        df = cls.get_df()
        return {
            "columns": [
                {
                    "name": c,
                    "dtype": str(df[c].dtype),
                    "nullable": bool(df[c].isna().any()),
                }
                for c in df.columns
            ],
            "count": int(len(df)),
            "last_loaded_at": now().isoformat(),
        }


def _cast_value_for_column(df: pd.DataFrame, col: str, value: str):
    # Cast based on dtype to improve filter correctness
    if col not in df.columns:
        return value
    dtype = df[col].dtype
    if pd.api.types.is_numeric_dtype(dtype):
        try:
            return pd.to_numeric(value)
        except Exception:
            return value
    if pd.api.types.is_datetime64_any_dtype(dtype):
        try:
            return pd.to_datetime(value)
        except Exception:
            return value
    if pd.api.types.is_bool_dtype(dtype):
        v = value.strip().lower()
        if v in ('true', '1', 'yes'):
            return True
        if v in ('false', '0', 'no'):
            return False
    return value


def _apply_filters(df: pd.DataFrame, params: Dict[str, str]) -> pd.DataFrame:
    """
    Supports:
      - column=value (equals)
      - column__lt, __lte, __gt, __gte, __ne
      - column__contains (case-insensitive substring for object dtype)
      - column__in (comma-separated values)
    Reserved params: limit, offset, order_by, columns
    """
    RESERVED = {'limit', 'offset', 'order_by', 'columns'}

    for raw_key, raw_val in params.items():
        if raw_key in RESERVED:
            continue

        if '__' in raw_key:
            col, op = raw_key.split('__', 1)
        else:
            col, op = raw_key, 'eq'

        if col not in df.columns:
            continue

        if op == 'in':
            parts = [p.strip() for p in raw_val.split(',')]
            values = [_cast_value_for_column(df, col, p) for p in parts]
            df = df[df[col].isin(values)]
            continue

        value = _cast_value_for_column(df, col, raw_val)

        if op == 'eq':
            df = df[df[col] == value]
        elif op == 'ne':
            df = df[df[col] != value]
        elif op == 'lt':
            df = df[df[col] < value]
        elif op == 'lte':
            df = df[df[col] <= value]
        elif op == 'gt':
            df = df[df[col] > value]
        elif op == 'gte':
            df = df[df[col] >= value]
        elif op == 'contains':
            # Safe string contains, case-insensitive
            df = df[df[col].astype(str).str.contains(str(value), case=False, na=False)]
        else:
            # Unknown op; ignore
            pass

    return df


def _apply_ordering(df: pd.DataFrame, order_by: str | None) -> pd.DataFrame:
    if not order_by:
        return df
    cols: Iterable[str] = [c.strip() for c in order_by.split(',') if c.strip()]
    if not cols:
        return df

    by = []
    ascending = []
    for token in cols:
        if token.startswith('-'):
            col = token[1:]
            asc = False
        else:
            col = token
            asc = True
        if col in df.columns:
            by.append(col)
            ascending.append(asc)

    if by:
        df = df.sort_values(by=by, ascending=ascending, kind='mergesort')  # stable
    return df


def _select_columns(df: pd.DataFrame, columns_param: str | None) -> pd.DataFrame:
    if not columns_param:
        return df
    wanted = [c.strip() for c in columns_param.split(',') if c.strip()]
    existing = [c for c in wanted if c in df.columns]
    if existing:
        return df.loc[:, existing]
    return df


class DataFrameSchemaView(APIView):
    def get(self, request):
        schema = DataFrameStore.schema()
        return Response(schema, status=status.HTTP_200_OK)


class DataFrameRecordsView(APIView):
    """
    Returns JSON array of records (d3-friendly) with limit/offset pagination:
      GET /api/data?limit=200&offset=0&region=NA&date__gte=2024-01-01&order_by=date,-sales&columns=date,region,sales
    """
    pagination_class = DataFrameLimitOffsetPagination

    def get(self, request):
        df = DataFrameStore.get_df()

        # Filters
        df = _apply_filters(df, request.query_params)

        # Column selection
        df = _select_columns(df, request.query_params.get('columns'))

        # Ordering
        df = _apply_ordering(df, request.query_params.get('order_by'))

        # Paginate DataFrame efficiently (slice first, then to_dict)
        paginator = self.pagination_class()
        page_df = paginator.paginate_queryset(df, request, view=self)
        if page_df is None:
            # No pagination applied
            data = df.to_dict(orient='records')
            return Response(data)

        # page_df is a DataFrame slice; convert only the page
        records = page_df.to_dict(orient='records')
        return paginator.get_paginated_response(records)


data_api/urls.py
from django.urls import path
from .views import DataFrameRecordsView, DataFrameSchemaView

urlpatterns = [
    path('data', DataFrameRecordsView.as_view(), name='data_records'),
    path('data/schema', DataFrameSchemaView.as_view(), name='data_schema'),
]


project urls.py (add the app’s routes)
from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),
    path('api/', include('data_api.urls')),
]


Notes for client-side (d3.js)
- d3.json('/api/data?limit=1000&order_by=date').then(data => { /* visualize */ })
- For large datasets, page through with limit/offset to avoid large payloads.
- Use /api/data/schema to discover columns and dtypes at runtime.

Extending
- Swap CSV loader with a DB loader (pd.read_sql) inside DataFrameStore._load_df.
- Add caching headers/ETags if your data is static or updates on a schedule.
- Add authentication/permissions via DRF if needed.
------------------------------------------------------------

PROMPT 484:
"Generate Python code that uses the rasterio library to read a GeoTIFF file, convert it to a numpy array, and then use the matplotlib pyplot library to create a contour plot of the data."
RESPONSE:
import numpy as np
import rasterio
import matplotlib.pyplot as plt

# Path to your GeoTIFF
tif_path = "path/to/your/file.tif"

with rasterio.open(tif_path) as src:
    # Read the first band as a masked array (nodata handled automatically)
    data = src.read(1, masked=True)
    transform = src.transform

# Build 2D coordinate grids (pixel-center coordinates), works with rotated rasters too
rows = np.arange(data.shape[0])[:, None]   # shape (H, 1)
cols = np.arange(data.shape[1])[None, :]   # shape (1, W)
xs, ys = rasterio.transform.xy(transform, rows, cols, offset="center")
xs = np.asarray(xs)
ys = np.asarray(ys)

# Prepare data for plotting (replace masked values with NaN)
z = np.ma.filled(data, np.nan)

# Create contour plot
fig, ax = plt.subplots(figsize=(8, 6))
levels = 15  # or e.g., np.linspace(np.nanmin(z), np.nanmax(z), 20)

cf = ax.contourf(xs, ys, z, levels=levels, cmap="viridis")
ax.contour(xs, ys, z, levels=levels, colors="k", linewidths=0.4)

cbar = fig.colorbar(cf, ax=ax, label="Raster value")
ax.set_title("Contour plot of GeoTIFF (Band 1)")
ax.set_xlabel("X (map units)")
ax.set_ylabel("Y (map units)")
ax.set_aspect("equal")

plt.show()
------------------------------------------------------------

PROMPT 485:
"Generate Python code that imports the llama.cpp library and provides a simple wrapper around its functions, allowing users to interact with the library's functionality in a Pythonic way."
RESPONSE:
```python
"""
Simple Pythonic wrapper around llama.cpp via the llama-cpp-python package.

Requires:
  pip install llama-cpp-python

This wrapper covers:
- Text completion (generate)
- Chat completion (chat)
- Embeddings (embed)
- Tokenization utilities (tokenize / detokenize)
- Context/session control (reset)
"""

from __future__ import annotations

from typing import Generator, Iterable, List, Optional, Union, Dict, Any
from threading import Lock

try:
    # Package name: llama-cpp-python
    # Docs: https://github.com/abetlen/llama-cpp-python
    from llama_cpp import Llama
except Exception as e:
    raise ImportError(
        "Could not import llama_cpp. Install it with: pip install llama-cpp-python"
    ) from e


StrOrStrList = Union[str, List[str]]
MessagesType = List[Dict[str, str]]


def _ensure_stop_list(stop: Optional[StrOrStrList]) -> Optional[List[str]]:
    if stop is None:
        return None
    if isinstance(stop, str):
        return [stop]
    return list(stop)


def _normalize_messages(
    messages: Union[str, MessagesType],
    system: Optional[str] = None,
) -> MessagesType:
    if isinstance(messages, str):
        msgs: MessagesType = [{"role": "user", "content": messages}]
    else:
        msgs = messages

    if system:
        # Prepend a system message if not already present
        if not msgs or msgs[0].get("role") != "system":
            msgs = [{"role": "system", "content": system}] + msgs
    return msgs


class LlamaCppClient:
    """
    A lightweight, Pythonic wrapper around llama.cpp (via llama-cpp-python).

    Example:
        client = LlamaCppClient("path/to/gguf/model.gguf", n_ctx=4096, n_gpu_layers=35)

        # Completion
        text = client.generate("Write a haiku about autumn:", max_tokens=64)

        # Chat
        reply = client.chat(
            [{"role": "user", "content": "Explain gravity simply."}],
            max_tokens=128
        )

        # Embeddings
        vector = client.embed("The quick brown fox")

        # Tokenization
        toks = client.tokenize("Hello")
        s = client.detokenize(toks)
    """

    def __init__(
        self,
        model_path: str,
        n_ctx: int = 4096,
        n_gpu_layers: int = 0,
        n_threads: Optional[int] = None,
        seed: int = 0,
        embedding: bool = False,
        logits_all: bool = False,
        vocab_only: bool = False,
        use_mmap: bool = True,
        use_mlock: bool = False,
        chat_format: Optional[str] = None,
        draft_model: Optional[str] = None,
        verbose: bool = False,
        **kwargs: Any,
    ):
        """
        Initialize the underlying Llama model.

        Common args:
          - model_path: Path to .gguf model file
          - n_ctx: Context window size
          - n_gpu_layers: Number of layers to offload to GPU (requires GPU build)
          - n_threads: CPU threads to use
          - seed: RNG seed
          - embedding: Enable embedding model usage
          - chat_format: Optional chat template override (e.g. "llama-2", "chatml", etc.)
          - draft_model: Optional draft model path for speculative decoding
          - verbose: Verbose llama.cpp logs

        kwargs are forwarded to llama_cpp.Llama for advanced configuration.
        """
        self._lock = Lock()
        self._llm = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            n_threads=n_threads,
            seed=seed,
            embedding=embedding,
            logits_all=logits_all,
            vocab_only=vocab_only,
            use_mmap=use_mmap,
            use_mlock=use_mlock,
            chat_format=chat_format,
            draft_model=draft_model,
            verbose=verbose,
            **kwargs,
        )

    # ---------- Core APIs ----------

    def generate(
        self,
        prompt: str,
        max_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.95,
        top_k: int = 40,
        repeat_penalty: float = 1.1,
        stop: Optional[StrOrStrList] = None,
        stream: bool = False,
        echo: bool = False,
        seed: Optional[int] = None,
    ) -> Union[str, Generator[str, None, None]]:
        """
        Text completion for a raw prompt. Returns the generated text or a generator when stream=True.
        """
        stop_list = _ensure_stop_list(stop)

        with self._lock:
            result = self._llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repeat_penalty=repeat_penalty,
                stop=stop_list,
                stream=stream,
                echo=echo,
                seed=seed,
            )

        if stream:
            def _gen() -> Generator[str, None, None]:
                for part in result:
                    # Each part is an incremental update
                    delta = part["choices"][0].get("text", "")
                    if delta:
                        yield delta
            return _gen()

        # Non-streaming: return the full text
        return result["choices"][0]["text"]

    def chat(
        self,
        messages: Union[str, MessagesType],
        max_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.95,
        top_k: int = 40,
        repeat_penalty: float = 1.1,
        stop: Optional[StrOrStrList] = None,
        stream: bool = False,
        system: Optional[str] = None,
        seed: Optional[int] = None,
    ) -> Union[str, Generator[str, None, None]]:
        """
        Chat completion with role-based messages.
        Accepts either:
          - A single user string
          - A list of dicts: [{"role": "system"|"user"|"assistant", "content": "..."}]
        """
        msgs = _normalize_messages(messages, system=system)
        stop_list = _ensure_stop_list(stop)

        with self._lock:
            result = self._llm.create_chat_completion(
                messages=msgs,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repeat_penalty=repeat_penalty,
                stop=stop_list,
                stream=stream,
                seed=seed,
            )

        if stream:
            def _gen() -> Generator[str, None, None]:
                for part in result:
                    delta = part["choices"][0].get("delta", {}).get("content", "")
                    if delta:
                        yield delta
            return _gen()

        return result["choices"][0]["message"]["content"]

    def embed(self, inputs: StrOrStrList) -> Union[List[float], List[List[float]]]:
        """
        Create embeddings for a single string or a list of strings.
        Returns:
          - list[float] for single string
          - list[list[float]] for multiple strings
        """
        is_single = isinstance(inputs, str)
        payload = [inputs] if is_single else list(inputs)

        with self._lock:
            out = self._llm.create_embedding(input=payload)

        embeddings = [d["embedding"] for d in out["data"]]
        return embeddings[0] if is_single else embeddings

    # ---------- Utilities ----------

    def tokenize(self, text: str, add_bos: bool = False) -> List[int]:
        with self._lock:
            return self._llm.tokenize(text.encode("utf-8"), add_bos=add_bos)

    def detokenize(self, tokens: Iterable[int]) -> str:
        with self._lock:
            b = self._llm.detokenize(list(tokens))
        return b.decode("utf-8", errors="ignore")

    def reset(self) -> None:
        """
        Reset the model's internal state (e.g., KV cache). Useful between unrelated prompts.
        """
        with self._lock:
            self._llm.reset()

    def close(self) -> None:
        """
        Free resources early. The object will also be cleaned up by GC.
        """
        with self._lock:
            self._llm = None  # Allow GC to free native resources

    # Context manager support
    def __enter__(self) -> "LlamaCppClient":
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        self.close()

    def __repr__(self) -> str:
        return f"<LlamaCppClient model_loaded={self._llm is not None}>"


# -------------------------
# Minimal example usage
# -------------------------
if __name__ == "__main__":
    # Replace with your local GGUF model path
    MODEL = "path/to/your/model.gguf"

    client = LlamaCppClient(MODEL, n_ctx=2048, n_gpu_layers=0, n_threads=4, verbose=False)

    # Completion
    print("== Completion ==")
    out = client.generate("Write a one-sentence definition of machine learning:", max_tokens=48)
    print(out.strip())

    # Streaming completion
    print("\n== Streaming Completion ==")
    stream = client.generate("List three colors:", max_tokens=32, stream=True)
    for chunk in stream:
        print(chunk, end="", flush=True)
    print()

    # Chat
    print("\n== Chat ==")
    reply = client.chat(
        [{"role": "user", "content": "Explain what a black hole is to a 10-year-old."}],
        max_tokens=96
    )
    print(reply.strip())

    # Embeddings
    print("\n== Embedding ==")
    vec = client.embed("The quick brown fox jumps over the lazy dog.")
    print(f"Vector length: {len(vec)}")

    # Tokenization
    print("\n== Tokenization ==")
    toks = client.tokenize("Hello world")
    print("Tokens:", toks)
    print("Back:", client.detokenize(toks))
```
------------------------------------------------------------

PROMPT 486:
"Generate Python code that imports the necessary libraries and creates a Kubernetes client object to interact with a cluster, such as `import kubernetes as k8s; client = k8s.KubeClient()`"
RESPONSE:
# pip install kubernetes
from kubernetes import client, config
from kubernetes.config import ConfigException

# Load config: prefer in-cluster (Pod), fall back to local kubeconfig (~/.kube/config)
try:
    config.load_incluster_config()
except ConfigException:
    config.load_kube_config()

# Create a Kubernetes API client (CoreV1 for pods, services, namespaces, etc.)
core_v1 = client.CoreV1Api()

# Example: list namespaces (optional)
# for ns in core_v1.list_namespace().items:
#     print(ns.metadata.name)
------------------------------------------------------------

PROMPT 487:
"Generate Python code that imports the requests library and creates a Braze API client class with methods for logging in, creating and sending messages, and retrieving message statistics."
RESPONSE:
import requests
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone


class BrazeClient:
    """
    Simple Braze REST API client using the requests library.

    Notes:
    - Braze authenticates with a REST API key using the Authorization: Bearer <key> header.
    - Your base_url must match your Braze REST cluster (e.g., https://rest.iad-01.braze.com).
    """

    def __init__(self, base_url: str, api_key: Optional[str] = None, timeout: int = 30):
        """
        Initialize the client.

        :param base_url: Braze REST endpoint base URL (no trailing slash), e.g. "https://rest.iad-01.braze.com"
        :param api_key: Optional Braze REST API Key
        :param timeout: Default timeout (seconds) for HTTP calls
        """
        self.base_url = base_url.rstrip("/")
        self.api_key = api_key
        self.timeout = timeout
        self.session = requests.Session()

    def login(self, api_key: str) -> None:
        """
        "Login" by setting the API key used for Authorization headers.
        Braze does not support username/password login for the REST API.
        """
        self.api_key = api_key

    def _headers(self, extra: Optional[Dict[str, str]] = None) -> Dict[str, str]:
        if not self.api_key:
            raise RuntimeError("API key not set. Call login(api_key) first.")
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Accept": "application/json",
            "Content-Type": "application/json",
        }
        if extra:
            headers.update(extra)
        return headers

    def _request(
        self,
        method: str,
        path: str,
        *,
        params: Optional[Dict[str, Any]] = None,
        json: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
    ) -> Dict[str, Any]:
        url = f"{self.base_url}{path}"
        resp = self.session.request(
            method=method.upper(),
            url=url,
            params=params,
            json=json,
            headers=self._headers(headers),
            timeout=self.timeout,
        )
        # Raise for non-2xx and surface Braze error payload if possible
        try:
            data = resp.json()
        except ValueError:
            resp.raise_for_status()
            # If we get here, it's 2xx but not JSON, return raw text
            return {"status_code": resp.status_code, "text": resp.text}
        if not resp.ok:
            # Attach HTTP status to the error payload for easier debugging
            raise requests.HTTPError(
                f"{resp.status_code} error: {data}",
                response=resp,
            )
        return data

    def create_message(
        self,
        *,
        external_user_ids: Optional[List[str]] = None,
        user_aliases: Optional[List[Dict[str, str]]] = None,
        audience: Optional[Dict[str, Any]] = None,
        messages: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Build a payload for /messages/send.

        Provide one of:
          - external_user_ids (list of strings)
          - user_aliases (list of {"alias_name": "...", "alias_label": "..."})
          - audience (Braze audience object)

        The messages object should follow Braze's message schema, e.g.:
          messages = {
            "email": {
              "app_id": "<your-app-id>",
              "from": "sender@example.com",
              "from_name": "Your Brand",
              "reply_to": "support@example.com",
              "subject": "Hello!",
              "body": "<html>Welcome</html>"
            }
          }
        """
        if not any([external_user_ids, user_aliases, audience]):
            raise ValueError("You must provide external_user_ids, user_aliases, or audience.")
        if not messages:
            raise ValueError("messages payload is required.")

        payload: Dict[str, Any] = {"messages": messages}
        if external_user_ids:
            payload["external_user_ids"] = external_user_ids
        if user_aliases:
            payload["user_aliases"] = user_aliases
        if audience:
            payload["audience"] = audience
        return payload

    def send_message(
        self,
        message_payload: Dict[str, Any],
        *,
        idempotency_key: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Send a message with /messages/send.
        If provided, Idempotency-Key header can be used to prevent duplicate sends.
        """
        extra_headers = {"Idempotency-Key": idempotency_key} if idempotency_key else None
        return self._request(
            "POST",
            "/messages/send",
            json=message_payload,
            headers=extra_headers,
        )

    def get_message_stats(
        self,
        *,
        campaign_id: Optional[str] = None,
        message_variation_id: Optional[str] = None,
        ending_at: Optional[str] = None,
        length: int = 7,
        unit: str = "day",
    ) -> Dict[str, Any]:
        """
        Retrieve message send statistics via /sends/data_series.

        Provide either campaign_id or message_variation_id.
        - ending_at: ISO 8601 UTC timestamp (default now)
        - length: number of units to return (default 7)
        - unit: "day" or "hour" (Braze-supported units)

        Returns a time series of send metrics (e.g., sends, deliveries, opens, clicks) if available.
        """
        if not (campaign_id or message_variation_id):
            raise ValueError("Provide either campaign_id or message_variation_id.")

        if ending_at is None:
            ending_at = datetime.now(timezone.utc).isoformat()

        params: Dict[str, Any] = {
            "ending_at": ending_at,
            "length": length,
            "unit": unit,
        }
        if campaign_id:
            params["campaign_id"] = campaign_id
        if message_variation_id:
            params["message_variation_id"] = message_variation_id

        return self._request("GET", "/sends/data_series", params=params)
------------------------------------------------------------

PROMPT 488:
"Generate Python code that imports the necessary libraries and writes a SQL query to retrieve data from a database, using the `sqlite3` library to connect to a local SQLite database file."
RESPONSE:
import sqlite3

# Path to your local SQLite database file
db_path = "path/to/your_database.sqlite3"

# Example SQL query to retrieve data
# Adjust table and column names as needed
sql = """
SELECT id, name, email, created_at
FROM users
WHERE created_at >= ?
ORDER BY created_at DESC
LIMIT ?
"""

params = ("2024-01-01", 100)  # Example parameters: start date and row limit

conn = None
try:
    # Connect to the SQLite database
    conn = sqlite3.connect(db_path)
    # Return rows as dictionaries keyed by column name
    conn.row_factory = sqlite3.Row

    # Execute the query with parameters and fetch results
    rows = conn.execute(sql, params).fetchall()

    # Convert rows to plain dictionaries and print
    results = [dict(row) for row in rows]
    for r in results:
        print(r)

except sqlite3.Error as e:
    print(f"SQLite error: {e}")
finally:
    if conn is not None:
        conn.close()
------------------------------------------------------------

PROMPT 489:
"Generate Python code that creates a Django application with pagination functionality using Pagination Made Easy, allowing for easy integration of paginated data into templates and views."
RESPONSE:
Below is a small, reusable Django app called pagination_made_easy plus an example usage app. It provides:
- a paginate_queryset helper for function-based views
- a PaginationMadeEasyMixin for class-based views
- template tags to generate page URLs while preserving existing query parameters

You can drop the pagination_made_easy package into your project and use it in any app.

# file: pagination_made_easy/__init__.py
default_app_config = "pagination_made_easy.apps.PaginationMadeEasyConfig"

# file: pagination_made_easy/apps.py
from django.apps import AppConfig

class PaginationMadeEasyConfig(AppConfig):
    name = "pagination_made_easy"
    verbose_name = "Pagination Made Easy"

# file: pagination_made_easy/pagination.py
from django.core.paginator import Paginator, EmptyPage
from django.utils.datastructures import MultiValueDictKeyError

def elided_page_range(number, num_pages, on_each_side=2, on_ends=1):
    """
    Returns a list of page numbers with None as ellipses.
    Example: [1, 2, None, 5, 6, 7, None, 99, 100]
    """
    number = int(number)
    if num_pages <= (on_each_side + on_ends) * 2 + 3:
        return list(range(1, num_pages + 1))
    result = []
    prev = 0
    for i in range(1, num_pages + 1):
        in_leading = i <= on_ends
        in_trailing = i > num_pages - on_ends
        in_window = (number - on_each_side) <= i <= (number + on_each_side)
        if in_leading or in_trailing or in_window:
            if prev and i - prev > 1:
                result.append(None)
            result.append(i)
            prev = i
    return result

def _extract_page(request, page_param):
    raw = request.GET.get(page_param, "1")
    try:
        page_num = int(raw)
        return page_num if page_num > 0 else 1
    except (ValueError, TypeError, MultiValueDictKeyError):
        return 1

def _other_params(request, page_param):
    params = request.GET.copy()
    if page_param in params:
        del params[page_param]
    return params

def paginate_queryset(request, queryset, per_page=10, page_param="page", on_each_side=2, on_ends=1):
    """
    Returns (page_obj, paginator). page_obj includes:
      - pme_numbers: list of page numbers with None as ellipsis
      - pme_other_params: QueryDict of preserved GET parameters (excluding page)
      - pme_page_param: the page query param name
    """
    paginator = Paginator(queryset, per_page)
    page_num = _extract_page(request, page_param)
    try:
        page_obj = paginator.page(page_num)
    except EmptyPage:
        page_obj = paginator.page(paginator.num_pages)

    # Attach helper metadata
    page_obj.pme_numbers = elided_page_range(
        page_obj.number, paginator.num_pages, on_each_side=on_each_side, on_ends=on_ends
    )
    page_obj.pme_other_params = _other_params(request, page_param)
    page_obj.pme_page_param = page_param
    return page_obj, paginator

class PaginationMadeEasyMixin:
    """
    Mixin for Django ListView.
    Usage:
      class ItemListView(PaginationMadeEasyMixin, ListView):
          model = Item
          paginate_by = 10
          page_kwarg = "page"  # optional
          on_each_side = 2     # optional
          on_ends = 1          # optional
    """
    page_kwarg = "page"
    on_each_side = 2
    on_ends = 1

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        page_obj = context.get("page_obj")
        if page_obj is not None:
            # Attach helper metadata for tags/templates
            page_obj.pme_numbers = elided_page_range(
                page_obj.number,
                page_obj.paginator.num_pages,
                on_each_side=getattr(self, "on_each_side", 2),
                on_ends=getattr(self, "on_ends", 1),
            )
            params = self.request.GET.copy()
            if self.page_kwarg in params:
                del params[self.page_kwarg]
            page_obj.pme_other_params = params
            page_obj.pme_page_param = self.page_kwarg
        return context

# file: pagination_made_easy/templatetags/__init__.py
# (empty file, required so Django finds the templatetags package)

# file: pagination_made_easy/templatetags/pagination_made_easy.py
from django import template

register = template.Library()

def _build_url(other_params, page_param, n):
    qd = other_params.copy()
    qd[page_param] = str(n)
    return "?" + qd.urlencode()

@register.simple_tag
def pme_url(page_obj, n):
    """
    Returns a URL querystring for page n, preserving existing GET params.
    Usage: <a href="{% pme_url page_obj n %}">...</a>
    """
    other = getattr(page_obj, "pme_other_params", None)
    page_param = getattr(page_obj, "pme_page_param", "page")
    if other is None:
        return f"?{page_param}={int(n)}"
    return _build_url(other, page_param, int(n))

@register.simple_tag
def pme_prev_url(page_obj):
    if not page_obj.has_previous():
        return ""
    return pme_url(page_obj, page_obj.previous_page_number())

@register.simple_tag
def pme_next_url(page_obj):
    if not page_obj.has_next():
        return ""
    return pme_url(page_obj, page_obj.next_page_number())

@register.filter
def is_ellipsis(value):
    return value is None

@register.filter
def is_current(n, page_obj):
    try:
        return int(n) == int(page_obj.number)
    except Exception:
        return False

# -----------------------------
# Example usage app (items)
# -----------------------------

# file: items/models.py
from django.db import models

class Item(models.Model):
    name = models.CharField(max_length=200)

    def __str__(self):
        return self.name

# file: items/views.py
from django.shortcuts import render
from django.views.generic import ListView
from .models import Item
from pagination_made_easy.pagination import paginate_queryset, PaginationMadeEasyMixin

def item_list(request):
    qs = Item.objects.all().order_by("id")
    page_obj, paginator = paginate_queryset(
        request, qs, per_page=10, page_param="page", on_each_side=2, on_ends=1
    )
    return render(
        request,
        "items/item_list.html",
        {
            "items": page_obj.object_list,
            "page_obj": page_obj,
            "paginator": paginator,
        },
    )

class ItemListView(PaginationMadeEasyMixin, ListView):
    model = Item
    template_name = "items/item_list.html"
    context_object_name = "items"
    paginate_by = 10
    page_kwarg = "page"
    on_each_side = 2
    on_ends = 1
    ordering = "id"

# file: project/urls.py
from django.contrib import admin
from django.urls import path
from items.views import item_list, ItemListView

urlpatterns = [
    path("admin/", admin.site.urls),
    path("", item_list, name="items-fbv"),              # Function-based view
    path("cbv/", ItemListView.as_view(), name="items-cbv"),  # Class-based view
]

# ---------------------------------
# Minimal template example (HTML)
# ---------------------------------
# file: templates/items/item_list.html
# Note: This is not Python, but shows how to integrate the tags provided above.
# {% load pagination_made_easy %}
# <h1>Items</h1>
# <ul>
#   {% for item in items %}
#     <li>{{ item.name }}</li>
#   {% empty %}
#     <li>No items</li>
#   {% endfor %}
# </ul>
#
# <nav aria-label="Pagination">
#   <ul class="pagination">
#     <li class="page-item {% if not page_obj.has_previous %}disabled{% endif %}">
#       <a class="page-link" href="{% pme_prev_url page_obj %}">Previous</a>
#     </li>
#     {% for n in page_obj.pme_numbers %}
#       {% if n|is_ellipsis %}
#         <li class="page-item disabled"><span class="page-link">…</span></li>
#       {% else %}
#         <li class="page-item {% if n|is_current:page_obj %}active{% endif %}">
#           <a class="page-link" href="{% pme_url page_obj n %}">{{ n }}</a>
#         </li>
#       {% endif %}
#     {% endfor %}
#     <li class="page-item {% if not page_obj.has_next %}disabled{% endif %}">
#       <a class="page-link" href="{% pme_next_url page_obj %}">Next</a>
#     </li>
#   </ul>
# </nav>

Integration notes:
- Add "pagination_made_easy" and your app (e.g., "items") to INSTALLED_APPS.
- Ensure TEMPLATES["OPTIONS"]["context_processors"] and APP_DIRS are set so templates and templatetags load.
- The provided helper and mixin preserve existing GET parameters (like filters) when switching pages and expose a ready-to-loop page range with ellipses, making pagination integration easy in both templates and views.
------------------------------------------------------------

PROMPT 490:
"Generate Python code that imports the logging module and creates a basic logger, then defines a class that logs information about its methods and their arguments using the logging module."
RESPONSE:
import logging
from functools import wraps

# Create a basic logger configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s - %(message)s"
)
logger = logging.getLogger(__name__)

def log_calls(func):
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        logger.info(
            "Calling %s.%s args=%r kwargs=%r",
            self.__class__.__name__, func.__name__, args, kwargs
        )
        try:
            result = func(self, *args, **kwargs)
            logger.info(
                "Returned from %s.%s result=%r",
                self.__class__.__name__, func.__name__, result
            )
            return result
        except Exception:
            logger.exception("Exception in %s.%s", self.__class__.__name__, func.__name__)
            raise
    return wrapper

class ExampleService:
    def __init__(self, name):
        self.name = name
        logger.info("Initialized %s(name=%r)", self.__class__.__name__, name)

    @log_calls
    def greet(self, times=1):
        return [f"Hello from {self.name} #{i+1}" for i in range(times)]

    @log_calls
    def add(self, a, b):
        return a + b

    @log_calls
    def divide(self, a, b):
        return a / b

if __name__ == "__main__":
    service = ExampleService("Alice")
    service.greet(times=2)
    service.add(3, 5)
    try:
        service.divide(10, 0)
    except ZeroDivisionError:
        logger.info("Handled division by zero")
------------------------------------------------------------

PROMPT 491:
"Generate Python code that imports the necessary modules for developing a package, sets up a virtual environment, creates a setup script, defines dependencies, and generates documentation."
RESPONSE:
#!/usr/bin/env python3
"""
Bootstrap a Python package project:
- Imports necessary modules
- Sets up a virtual environment
- Creates a setup script and pyproject.toml
- Defines dependencies
- Generates Sphinx documentation

Run this script from the directory where you want the project folder created.
"""

import os
import sys
import subprocess
import venv
from pathlib import Path
import textwrap
import shutil


# ---------------- Configuration ----------------

PROJECT_NAME = "my_package"
PACKAGE_NAME = "my_package"  # import name under src/
VERSION = "0.1.0"
DESCRIPTION = "A sample Python package"
AUTHOR = "Your Name"
PYTHON_REQUIRES = ">=3.9"

# Runtime dependencies for your package
RUNTIME_DEPS = [
    # "requests>=2.31.0",
]

# Extra groups for dev and docs
DEV_DEPS = [
    "pytest>=7.0",
    "coverage[toml]>=7.2",
    "black>=24.0",
    "ruff>=0.6.0",
    "mypy>=1.10",
    "build>=1.2.1",
    "twine>=5.0.0",
]

DOCS_DEPS = [
    "sphinx>=7.0",
    "sphinx-rtd-theme>=2.0",
    "myst-parser>=3.0",
    "sphinx-autodoc-typehints>=2.0",
]


# ---------------- Utilities ----------------

def run(cmd, cwd=None, env=None, check=True):
    print(f"$ {' '.join(map(str, cmd))}")
    return subprocess.run(cmd, cwd=cwd, env=env, check=check)


def venv_python_path(venv_dir: Path) -> Path:
    if os.name == "nt":
        return venv_dir / "Scripts" / "python.exe"
    return venv_dir / "bin" / "python"


def write_file(path: Path, content: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(content, encoding="utf-8")
    print(f"Wrote {path}")


# ---------------- Scaffolding ----------------

def create_project_layout(root: Path):
    src_dir = root / "src" / PACKAGE_NAME
    tests_dir = root / "tests"
    docs_dir = root / "docs"

    # Basic package files
    write_file(
        src_dir / "__init__.py",
        textwrap.dedent(
            f'''\
            """{DESCRIPTION}"""
            __all__ = []
            __version__ = "{VERSION}"
            '''
        ),
    )

    write_file(
        src_dir / "example.py",
        textwrap.dedent(
            '''\
            def add(a: int, b: int) -> int:
                """Add two integers and return the result."""
                return a + b
            '''
        ),
    )

    # Tests
    write_file(
        tests_dir / "test_example.py",
        textwrap.dedent(
            f'''\
            from {PACKAGE_NAME}.example import add


            def test_add():
                assert add(2, 3) == 5
            '''
        ),
    )

    # Readme and License
    write_file(
        root / "README.md",
        textwrap.dedent(
            f'''\
            # {PROJECT_NAME}

            {DESCRIPTION}

            ## Development

            - Create venv: python -m venv .venv
            - Install: pip install -e ".[dev,docs]"
            - Test: pytest
            - Docs: sphinx-build -b html docs docs/_build/html
            '''
        ),
    )

    write_file(
        root / "LICENSE",
        textwrap.dedent(
            '''\
            MIT License

            Copyright (c) ...

            Permission is hereby granted, free of charge, to any person obtaining a copy
            of this software and associated documentation files (the "Software"), to deal
            in the Software without restriction, including without limitation the rights
            to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
            copies of the Software, and to permit persons to whom the Software is
            furnished to do so, subject to the following conditions:

            THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
            IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
            FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
            AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
            LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
            OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
            SOFTWARE.
            '''
        ),
    )

    # .gitignore
    write_file(
        root / ".gitignore",
        textwrap.dedent(
            '''\
            __pycache__/
            *.pyc
            .pytest_cache/
            .mypy_cache/
            .ruff_cache/
            .coverage
            htmlcov/
            dist/
            build/
            *.egg-info/
            .venv/
            docs/_build/
            '''
        ),
    )

    # pyproject.toml (PEP 517 build-system)
    write_file(
        root / "pyproject.toml",
        textwrap.dedent(
            '''\
            [build-system]
            requires = ["setuptools>=69", "wheel"]
            build-backend = "setuptools.build_meta"
            '''
        ),
    )

    # setup.py (setup script)
    setup_py = textwrap.dedent(
        f'''\
        from setuptools import setup, find_packages

        setup(
            name="{PROJECT_NAME}",
            version="{VERSION}",
            description="{DESCRIPTION}",
            author="{AUTHOR}",
            python_requires="{PYTHON_REQUIRES}",
            packages=find_packages("src"),
            package_dir={{"": "src"}},
            include_package_data=True,
            install_requires={RUNTIME_DEPS},
            extras_require={{
                "dev": {DEV_DEPS},
                "docs": {DOCS_DEPS},
            }},
            classifiers=[
                "Programming Language :: Python :: 3",
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
            ],
        )
        '''
    )
    write_file(root / "setup.py", setup_py)

    # Optional: setup.cfg for tool configs
    setup_cfg = textwrap.dedent(
        f'''\
        [metadata]
        name = {PROJECT_NAME}
        version = {VERSION}
        description = {DESCRIPTION}

        [tool:pytest]
        testpaths = tests
        addopts = -ra

        [coverage:run]
        source = src/{PACKAGE_NAME}

        [flake8]
        max-line-length = 100

        [mypy]
        python_version = 3.10
        pretty = True

        [tool.black]
        line-length = 100
        '''
    )
    write_file(root / "setup.cfg", setup_cfg)

    # Basic Sphinx docs
    write_file(
        docs_dir / "conf.py",
        textwrap.dedent(
            f'''\
            import os
            import sys
            from datetime import datetime
            sys.path.insert(0, os.path.abspath("../src"))

            project = "{PROJECT_NAME}"
            author = "{AUTHOR}"
            release = "{VERSION}"
            copyright = f"{{datetime.now().year}} {{author}}"

            extensions = [
                "sphinx.ext.autodoc",
                "sphinx.ext.napoleon",
                "sphinx.ext.viewcode",
                "sphinx_autodoc_typehints",
                "myst_parser",
            ]

            html_theme = "sphinx_rtd_theme"

            napoleon_google_docstring = True
            napoleon_numpy_docstring = True

            templates_path = ["_templates"]
            exclude_patterns = []

            html_static_path = ["_static"]
            '''
        ),
    )

    write_file(
        docs_dir / "index.rst",
        textwrap.dedent(
            f'''\
            {PROJECT_NAME} documentation
            {"=" * (len("{PROJECT_NAME}") + 13)}

            .. toctree::
               :maxdepth: 2
               :caption: Contents:

               api/modules

            Indices and tables
            ==================

            * :ref:`genindex`
            * :ref:`modindex`
            * :ref:`search`
            '''
        ),
    )


def create_virtualenv(root: Path) -> Path:
    venv_dir = root / ".venv"
    if venv_dir.exists():
        print(f"Virtual environment already exists at {venv_dir}")
    else:
        print(f"Creating virtual environment at {venv_dir} ...")
        builder = venv.EnvBuilder(with_pip=True, clear=False, upgrade=False, symlinks=True)
        builder.create(venv_dir)
    return venv_dir


def install_dependencies(venv_python: Path, project_root: Path):
    # Upgrade pip/setuptools/wheel first
    run([str(venv_python), "-m", "pip", "install", "--upgrade", "pip", "setuptools", "wheel"])

    # Editable install with dev and docs extras
    run([str(venv_python), "-m", "pip", "install", "-e", ".[dev,docs]"], cwd=project_root)


def generate_docs(venv_python: Path, project_root: Path):
    docs_dir = project_root / "docs"
    api_dir = docs_dir / "api"
    api_dir.mkdir(parents=True, exist_ok=True)

    # Generate API rst files
    run(
        [
            str(venv_python),
            "-m",
            "sphinx.ext.apidoc",
            "-f",
            "-e",
            "-o",
            str(api_dir),
            str(project_root / "src" / PACKAGE_NAME),
        ],
        cwd=project_root,
    )

    # Build HTML docs
    build_dir = docs_dir / "_build" / "html"
    run(
        [
            str(venv_python),
            "-m",
            "sphinx",
            "-b",
            "html",
            str(docs_dir),
            str(build_dir),
        ],
        cwd=project_root,
    )

    print(f"Docs built at: {build_dir}")


def main():
    root = Path.cwd() / PROJECT_NAME
    if root.exists():
        print(f"Using existing project directory: {root}")
    else:
        print(f"Creating project directory: {root}")
        root.mkdir(parents=True, exist_ok=True)

    create_project_layout(root)
    venv_dir = create_virtualenv(root)
    vpy = venv_python_path(venv_dir)
    install_dependencies(vpy, root)
    generate_docs(vpy, root)

    print("\nDone.")
    print(f"- Project root: {root}")
    print(f"- Activate venv: {venv_dir / ('Scripts/activate' if os.name == 'nt' else 'bin/activate')}")
    print("- Run tests: pytest")
    print("- Build package: python -m build")
    print(f"- Docs: open {root / 'docs' / '_build' / 'html' / 'index.html'} in your browser")


if __name__ == "__main__":
    try:
        main()
    except subprocess.CalledProcessError as e:
        print(f"Command failed with exit code {e.returncode}: {e.cmd}", file=sys.stderr)
        sys.exit(e.returncode)
------------------------------------------------------------

PROMPT 492:
"Generate Python code that uses the Marshmallow library to serialize and deserialize annotated data, allowing for easy handling of missing data and efficient data transfer between systems."
RESPONSE:
# Requires:
#   pip install marshmallow marshmallow-dataclass
# (marshmallow-dataclass lets Marshmallow use Python type annotations on dataclasses.)

from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Optional

import marshmallow as ma
from marshmallow import EXCLUDE
from marshmallow_dataclass import class_schema


# ---------- Domain models (annotated) ----------

@dataclass
class Address:
    street: str
    city: str
    # Missing data friendly: optional and defaults to None if absent
    postal_code: Optional[str] = None


@dataclass
class User:
    id: int
    email: str

    # Missing data friendly: provides defaults when fields are absent
    name: str = "Anonymous"
    is_active: bool = True
    tags: List[str] = field(default_factory=list)

    # Optional nested structure
    address: Optional[Address] = None


# ---------- Schemas generated from annotations ----------

# Generate Marshmallow Schemas from the annotated dataclasses
BaseAddressSchema = class_schema(Address)
BaseUserSchema = class_schema(User)


# Customize schema behavior:
# - unknown=EXCLUDE: ignore unknown input keys safely
# - post_dump: strip None/empty collections to minimize payload size (efficient transfer)
class AddressSchema(BaseAddressSchema):
    class Meta:
        unknown = EXCLUDE

    @ma.post_dump
    def _drop_empty(self, data, many, **kwargs):
        return {k: v for k, v in data.items() if v is not None and v != []}


class UserSchema(BaseUserSchema):
    class Meta:
        unknown = EXCLUDE

    @ma.post_dump
    def _drop_empty(self, data, many, **kwargs):
        return {k: v for k, v in data.items() if v is not None and v != []}


# ---------- Example usage ----------

if __name__ == "__main__":
    user_schema = UserSchema()
    users_schema = UserSchema(many=True)

    # Incoming payload with:
    # - Missing fields (name, tags, is_active, address.postal_code)
    # - Extra unknown field (extra_field) which will be ignored
    incoming = {
        "id": 123,
        "email": "alice@example.com",
        "extra_field": "ignored",
        "address": {"street": "42 Galaxy Way", "city": "Andromeda"},
    }

    # Deserialize to a typed dataclass instance; defaults fill missing fields
    user_obj = user_schema.load(incoming)
    print("Loaded dataclass object:", user_obj)

    # Serialize to a dict (None/empty values removed by post_dump)
    as_dict = user_schema.dump(user_obj)
    print("Serialized dict:", as_dict)

    # Serialize to compact JSON for efficient transfer (include only selected fields)
    compact_json = UserSchema(only=("id", "email")).dumps(user_obj)
    print("Compact JSON:", compact_json)

    # Batch serialize/deserialize
    batch_incoming = [
        {"id": 1, "email": "a@example.com"},
        {"id": 2, "email": "b@example.com", "tags": ["beta", "tester"], "unknown_x": 1},
    ]
    users = users_schema.load(batch_incoming)  # unknown_x ignored, defaults applied
    print("Batch loaded:", users)
    print("Batch JSON:", users_schema.dumps(users))

    # Round-trip JSON
    json_str = user_schema.dumps(user_obj)
    user_back = user_schema.loads(json_str)
    print("Round-trip OK:", user_back == user_obj)

    # Efficient field filtering for specific transfers
    minimal_schema = UserSchema(only=("id", "email"))
    print("Minimal payload dict:", minimal_schema.dump(user_obj))
------------------------------------------------------------

PROMPT 493:
"Generate Python code that uses the cProfile module to collect profiling data, then uses the kcachegrind and qcachegrind libraries to visualize the data in an interactive graphical format."
RESPONSE:
# Python 3 code: profile with cProfile, convert to Callgrind format, and open in KCachegrind/QCachegrind

import os
import sys
import io
import cProfile
import pstats
import tempfile
import shutil
import subprocess
import platform
from pathlib import Path


def sample_workload(n=4000):
    # Replace this with the code you want to profile.
    # This is just a CPU-heavy example.
    def is_prime(x):
        if x < 2:
            return False
        if x % 2 == 0:
            return x == 2
        i = 3
        while i * i <= x:
            if x % i == 0:
                return False
            i += 2
        return True

    return sum(i for i in range(n) if is_prime(i))


def profile_callable(fn, *args, sort_by="cumulative", profile_out=None, **kwargs):
    pr = cProfile.Profile()
    pr.enable()
    result = fn(*args, **kwargs)
    pr.disable()

    s = io.StringIO()
    stats = pstats.Stats(pr, stream=s).sort_stats(sort_by)
    stats.print_stats(50)  # show top 50 lines
    print("===== cProfile summary (top 50) =====")
    print(s.getvalue())

    if profile_out is None:
        tmpdir = Path(tempfile.mkdtemp(prefix="pyprof_"))
        profile_out = tmpdir / "profile.prof"
    else:
        profile_out = Path(profile_out)
        profile_out.parent.mkdir(parents=True, exist_ok=True)

    pr.dump_stats(str(profile_out))
    print(f"Saved raw cProfile stats to: {profile_out}")
    return result, stats, profile_out


def convert_to_callgrind(profile_path, callgrind_out=None):
    """
    Convert a cProfile .prof file to Callgrind format using pyprof2calltree.
    Returns the path to the callgrind file.
    """
    profile_path = Path(profile_path)
    if callgrind_out is None:
        callgrind_out = profile_path.with_name("callgrind.out." + profile_path.stem)
    else:
        callgrind_out = Path(callgrind_out)
        callgrind_out.parent.mkdir(parents=True, exist_ok=True)

    # Prefer using the module form to ensure we use the same interpreter/pip env
    cmd = [sys.executable, "-m", "pyprof2calltree", "-i", str(profile_path), "-o", str(callgrind_out)]
    try:
        print("Converting .prof -> Callgrind with:", " ".join(cmd))
        subprocess.run(cmd, check=True)
        print(f"Wrote Callgrind file to: {callgrind_out}")
        return callgrind_out
    except FileNotFoundError:
        pass
    except subprocess.CalledProcessError as e:
        print("pyprof2calltree failed:", e)

    # Fallback: try direct script if on PATH
    exe = shutil.which("pyprof2calltree")
    if exe:
        cmd = [exe, "-i", str(profile_path), "-o", str(callgrind_out)]
        subprocess.run(cmd, check=True)
        print(f"Wrote Callgrind file to: {callgrind_out}")
        return callgrind_out

    raise RuntimeError(
        "pyprof2calltree is not installed. Install it with: pip install pyprof2calltree"
    )


def find_q_or_kcachegrind():
    """
    Return a tuple (viewer_exe, use_open_a) where viewer_exe is a path or app name.
    If on macOS and not found on PATH, we return ('QCachegrind', True) to use 'open -a'.
    """
    # Prefer QCachegrind when available (cross-platform), then KCachegrind
    for name in ("qcachegrind", "kcachegrind"):
        exe = shutil.which(name)
        if exe:
            return exe, False

    # macOS app bundles
    if platform.system() == "Darwin":
        # Try QCachegrind or KCachegrind via 'open -a'
        return "QCachegrind", True  # will try open -a QCachegrind

    return None, False


def open_in_kcachegrind(callgrind_path):
    """
    Launch KCachegrind or QCachegrind with the given callgrind file.
    """
    viewer, use_open_a = find_q_or_kcachegrind()
    if viewer is None:
        # As a convenience, try pyprof2calltree's -k which attempts to launch a viewer
        cmd = [sys.executable, "-m", "pyprof2calltree", "-k", "-i", str(callgrind_path)]
        exe_on_path = shutil.which("pyprof2calltree")
        try:
            if exe_on_path:
                subprocess.Popen([exe_on_path, "-k", "-i", str(callgrind_path)])
                print("Tried launching viewer via: pyprof2calltree -k")
                return
            else:
                subprocess.Popen(cmd)
                print("Tried launching viewer via: python -m pyprof2calltree -k ...")
                return
        except Exception:
            pass

        raise RuntimeError(
            "Neither qcachegrind nor kcachegrind was found on PATH.\n"
            "Install one of them, e.g.:\n"
            "- Linux (Debian/Ubuntu): sudo apt-get install kcachegrind\n"
            "- macOS (Homebrew): brew install qcachegrind\n"
            "- Windows: install QCachegrind and add it to PATH."
        )

    try:
        if use_open_a:
            # macOS app bundle
            subprocess.Popen(["open", "-a", viewer, str(callgrind_path)])
        else:
            subprocess.Popen([viewer, str(callgrind_path)])
        print(f"Launched viewer: {viewer}")
    except Exception as e:
        raise RuntimeError(f"Failed to launch {viewer}: {e}")


def main():
    # 1) Profile your code with cProfile
    _, stats, prof_path = profile_callable(sample_workload, 6000)

    # 2) Convert to Callgrind format for KCachegrind/QCachegrind
    try:
        callgrind_path = convert_to_callgrind(prof_path)
    except RuntimeError as e:
        print(str(e))
        print("You can still inspect the text profile above, or install pyprof2calltree and re-run.")
        return

    # 3) Launch interactive visualization (KCachegrind/QCachegrind)
    try:
        open_in_kcachegrind(callgrind_path)
    except RuntimeError as e:
        print(str(e))
        print(f"You can manually open the Callgrind file: {callgrind_path}")


if __name__ == "__main__":
    main()
------------------------------------------------------------

PROMPT 494:
"Generate Python code that uses the `wadl` library to parse a WADL file and navigate the described HTTP resources, using the `requests` library to send HTTP requests and retrieve responses."
RESPONSE:
# Requires: pip install wadllib requests lxml
# Note: The commonly used Python WADL parser is the "wadllib" package.
# If you meant a different library named "wadl", let me know and I can adapt.

import os
from urllib.parse import urlencode
import requests

try:
    # wadllib is the de-facto WADL library for Python
    from wadllib.application import Application as WadlApplication
except Exception as exc:
    raise RuntimeError(
        "This example expects the 'wadllib' package. Install it with: pip install wadllib"
    ) from exc

from lxml import etree


WADL_NS = "http://wadl.dev.java.net/2009/02"
XSD_NS = "http://www.w3.org/2001/XMLSchema"
NS = {"wadl": WADL_NS, "xsd": XSD_NS}


def _join_url(base: str, path: str) -> str:
    if not base:
        return path
    return base.rstrip("/") + "/" + path.lstrip("/")


class WADLClient:
    """
    Minimal WADL-driven HTTP client.

    - Parses the WADL using wadllib.
    - Builds an index of resources/methods.
    - Uses 'requests' to invoke described endpoints.
    """

    def __init__(self, wadl_path_or_url: str, default_headers: dict | None = None):
        self.default_headers = default_headers or {}
        self._wadl_source = wadl_path_or_url

        # Use wadllib to parse the WADL
        # Wadllib's Application expects a file-like or URL and an optional base.
        # We pass the same string as the base to allow relative resolution.
        self._app = WadlApplication(wadl_path_or_url, wadl_path_or_url)

        # Grab underlying XML root (lxml element) from wadllib
        # wadllib exposes the WADL <application> element as .application
        if not hasattr(self._app, "application"):
            raise RuntimeError("Unexpected wadllib API: missing .application element")

        self._root = self._app.application
        self._endpoints = []  # list of dicts describing endpoints
        self._by_id = {}      # method-id -> endpoint dict

        self._index_endpoints()

    def _index_endpoints(self):
        # WADL structure:
        # <application>
        #   <resources base="...">
        #     <resource path="...">
        #       <param ... style="template|query|header|matrix"/>
        #       <method name="GET|POST|..."/>

        resources_blocks = self._root.findall("wadl:resources", namespaces=NS)
        for res_block in resources_blocks:
            base = res_block.get("base", "")

            for resource in res_block.findall("wadl:resource", namespaces=NS):
                self._walk_resource(resource, base, parent_path="", inherited_params=[])

    def _walk_resource(self, resource_elem, base, parent_path, inherited_params):
        # Build full path for this resource
        my_path = resource_elem.get("path", "")
        full_path = self._join_paths(parent_path, my_path)

        # Resource-level params (apply to descendants unless overridden)
        resource_params = self._collect_params(resource_elem)
        combined_params = inherited_params + resource_params

        # Methods on this resource
        for method_elem in resource_elem.findall("wadl:method", namespaces=NS):
            entry = self._build_endpoint_entry(base, full_path, method_elem, combined_params)
            self._endpoints.append(entry)
            if entry.get("id"):
                self._by_id[entry["id"]] = entry

        # Recurse into child resources
        for child in resource_elem.findall("wadl:resource", namespaces=NS):
            self._walk_resource(child, base, full_path, combined_params)

    @staticmethod
    def _join_paths(a, b):
        if not a:
            return b
        if not b:
            return a
        return a.rstrip("/") + "/" + b.lstrip("/")

    @staticmethod
    def _collect_params(elem):
        params = []
        for p in elem.findall("wadl:param", namespaces=NS):
            params.append(
                {
                    "name": p.get("name"),
                    "style": p.get("style"),  # template | query | header | matrix
                    "type": p.get("type"),
                    "required": p.get("required") in ("true", "1"),
                    "default": p.get("default"),
                }
            )
        return params

    def _build_endpoint_entry(self, base, path, method_elem, inherited_params):
        method_name = method_elem.get("name", "GET").upper()
        method_id = method_elem.get("id")

        # Method-level params add to resource-level params
        method_params = self._collect_params(method_elem)
        req = method_elem.find("wadl:request", namespaces=NS)
        if req is not None:
            method_params.extend(self._collect_params(req))

        # Representations (request/response bodies)
        request_reprs = []
        if req is not None:
            for rep in req.findall("wadl:representation", namespaces=NS):
                request_reprs.append(
                    {
                        "mediaType": rep.get("mediaType"),
                        "element": rep.get("element"),
                    }
                )

        resp = method_elem.find("wadl:response", namespaces=NS)
        response_reprs = []
        status = None
        if resp is not None:
            status = resp.get("status")
            for rep in resp.findall("wadl:representation", namespaces=NS):
                response_reprs.append(
                    {
                        "mediaType": rep.get("mediaType"),
                        "element": rep.get("element"),
                    }
                )

        all_params = inherited_params + method_params
        template_params = [p for p in all_params if p.get("style") == "template"]
        query_params = [p for p in all_params if p.get("style") == "query"]
        header_params = [p for p in all_params if p.get("style") == "header"]

        return {
            "id": method_id,
            "method": method_name,
            "base": base,
            "path": path,
            "template_params": template_params,
            "query_params": query_params,
            "header_params": header_params,
            "request_representations": request_reprs,
            "response_representations": response_reprs,
            "expected_status": status,
        }

    def list_endpoints(self):
        return list(self._endpoints)

    def get_endpoint_by_id(self, method_id: str):
        return self._by_id.get(method_id)

    def build_url(self, endpoint: dict, template_values: dict | None = None, query: dict | None = None) -> str:
        template_values = template_values or {}
        query = query or {}

        # Fill template params in path: e.g., /widgets/{id}
        path = endpoint["path"]
        for p in endpoint["template_params"]:
            name = p["name"]
            if name in template_values:
                path = path.replace("{" + name + "}", str(template_values[name]))
            elif p.get("required"):
                raise ValueError(f"Missing required template param: {name}")

        url = _join_url(endpoint["base"], path)

        # Append query params (include defaults if provided and not in query)
        qp = {}
        for p in endpoint["query_params"]:
            name = p["name"]
            if name in query:
                qp[name] = query[name]
            elif p.get("default") is not None:
                qp[name] = p["default"]

        if qp:
            sep = "&" if "?" in url else "?"
            url = f"{url}{sep}{urlencode(qp, doseq=True)}"

        return url

    def request(
        self,
        endpoint: dict,
        template_values: dict | None = None,
        query: dict | None = None,
        headers: dict | None = None,
        json=None,
        data=None,
        files=None,
        timeout: float | None = None,
    ) -> requests.Response:
        url = self.build_url(endpoint, template_values, query)
        merged_headers = {**self.default_headers, **(headers or {})}

        resp = requests.request(
            endpoint["method"],
            url,
            headers=merged_headers,
            json=json,
            data=data,
            files=files,
            timeout=timeout,
        )
        return resp

    def request_by_id(
        self,
        method_id: str,
        template_values: dict | None = None,
        query: dict | None = None,
        headers: dict | None = None,
        json=None,
        data=None,
        files=None,
        timeout: float | None = None,
    ) -> requests.Response:
        ep = self.get_endpoint_by_id(method_id)
        if not ep:
            raise KeyError(f"No method with id '{method_id}'")
        return self.request(
            ep,
            template_values=template_values,
            query=query,
            headers=headers,
            json=json,
            data=data,
            files=files,
            timeout=timeout,
        )


if __name__ == "__main__":
    # Example usage:
    # 1) Point to a WADL file or URL
    # wadl_source = "https://example.com/api.wadl"
    wadl_source = os.environ.get("WADL_SOURCE", "api.wadl")

    client = WADLClient(wadl_source, default_headers={"User-Agent": "wadl-client/1.0"})

    # List endpoints discovered from WADL
    for ep in client.list_endpoints():
        tid = f" id={ep['id']}" if ep.get("id") else ""
        print(f"{ep['method']} {ep['base']}{ep['path']}{tid}")

    # If your WADL defines a method with id="getWidget" and path like "/widgets/{id}"
    example_id = "getWidget"
    if client.get_endpoint_by_id(example_id):
        resp = client.request_by_id(example_id, template_values={"id": 123})
        print("Status:", resp.status_code)
        print("Body:", resp.text[:500])  # preview

    # Alternatively, pick the first GET endpoint and call it without template params
    get_eps = [e for e in client.list_endpoints() if e["method"] == "GET"]
    if get_eps:
        resp = client.request(get_eps[0])
        print("First GET response:", resp.status_code)
------------------------------------------------------------

PROMPT 495:
"Generate Python code that reads and writes PLINK BED format files using the pandas library, efficiently handling large datasets and providing simple API for data manipulation and analysis."
RESPONSE:
# plink_pandas.py
# Read and write PLINK BED datasets with a simple pandas-friendly API,
# optimized for large datasets via memory-mapping and chunked processing.

from __future__ import annotations

import io
import os
import math
from typing import Iterator, List, Optional, Sequence, Tuple, Union

import numpy as np
import pandas as pd


# =========================
# Low-level helpers
# =========================

BED_MAGIC = (0x6C, 0x1B)  # first 2 bytes
BED_MODE_SNP_MAJOR = 0x01
BED_MODE_INDIV_MAJOR = 0x00


def _read_fam(fam_path: str) -> pd.DataFrame:
    # FAM has 6 columns: FID IID PAT MAT SEX PHENOTYPE
    cols = ["fid", "iid", "father", "mother", "sex", "phenotype"]
    fam = pd.read_csv(
        fam_path,
        sep=r"\s+",
        header=None,
        names=cols,
        dtype={"fid": str, "iid": str, "father": str, "mother": str, "sex": np.int8, "phenotype": float},
        engine="c",
    )
    return fam


def _read_bim(bim_path: str) -> pd.DataFrame:
    # BIM has 6 columns: CHR SNP CM BP A1 A2
    cols = ["chrom", "snp", "cm", "bp", "a1", "a2"]
    bim = pd.read_csv(
        bim_path,
        sep=r"\s+",
        header=None,
        names=cols,
        dtype={"chrom": str, "snp": str, "cm": float, "bp": np.int64, "a1": str, "a2": str},
        engine="c",
    )
    return bim


def _bed_shape_and_mode(bed_path: str, n_samples: int) -> Tuple[int, int]:
    # Return (n_variants, mode) after verifying header and size
    size = os.path.getsize(bed_path)
    if size < 3:
        raise ValueError("BED file too small to contain header.")
    with open(bed_path, "rb") as f:
        header = f.read(3)
    if header[0] != BED_MAGIC[0] or header[1] != BED_MAGIC[1]:
        raise ValueError("Invalid BED magic bytes; not a PLINK BED file.")
    mode = header[2]
    bytes_per_variant = (n_samples + 3) // 4  # ceil(n_samples / 4)
    data_bytes = size - 3
    if bytes_per_variant == 0:
        if data_bytes != 0:
            raise ValueError("Inconsistent FAM/sample count vs BED size.")
        return 0, mode
    if data_bytes % bytes_per_variant != 0:
        raise ValueError("BED size not divisible by bytes-per-variant; corrupt or wrong FAM.")
    n_variants = data_bytes // bytes_per_variant
    return n_variants, mode


def _build_decode_lut() -> np.ndarray:
    # LUT: byte (0..255) -> 4 float32 genotype values (0,1,2, or NaN)
    # PLINK two-bit codes:
    # 00 -> 0 (homozygote A1)
    # 01 -> NaN (missing)
    # 10 -> 1 (heterozygote)
    # 11 -> 2 (homozygote A2)
    lut = np.empty((256, 4), dtype=np.float32)
    for b in range(256):
        row = []
        for i in range(4):
            code = (b >> (2 * i)) & 0b11
            if code == 0b00:
                row.append(0.0)
            elif code == 0b01:
                row.append(np.nan)
            elif code == 0b10:
                row.append(1.0)
            else:
                row.append(2.0)
        lut[b, :] = row
    return lut


_DECODE_LUT = _build_decode_lut()


def _compute_variant_chunk_size(n_samples: int, target_mb: float = 256.0) -> int:
    # Decoding memory approx = chunk * n_samples * 4 bytes (float32)
    # Keep under target_mb
    if n_samples <= 0:
        return 0
    target_bytes = int(target_mb * 1024 * 1024)
    chunk = max(1, target_bytes // (n_samples * 4))
    return int(chunk)


def _decode_bytes_to_genotypes(
    bytes_block: np.ndarray, n_samples: int
) -> np.ndarray:
    # bytes_block shape = (n_variants_chunk, bytes_per_variant) uint8
    # returns float32 array of shape (n_variants_chunk, n_samples)
    arr = _DECODE_LUT[bytes_block]  # shape (chunk, bytes_per_variant, 4), float32
    out = arr.reshape(arr.shape[0], -1)[:, :n_samples]
    return out


def _codes_from_genotypes(g: np.ndarray) -> np.ndarray:
    # Convert genotypes in 0/1/2/NaN to PLINK two-bit code values per sample:
    # 0 -> 0b00, 1 -> 0b10, 2 -> 0b11, NaN -> 0b01
    # Returns uint8 array of same shape as g with values in {0,1,2,3}.
    if g.dtype.kind == "f":
        v = np.rint(g).astype(np.int16, copy=False)
        codes = np.zeros(v.shape, dtype=np.uint8)
        miss = ~np.isfinite(g)
        # Fill non-missing
        ok = ~miss
        vv = v[ok]
        if vv.size:
            # Map 0->0, 1->2, 2->3
            map_arr = np.array([0, 2, 3], dtype=np.uint8)
            if (vv < 0).any() or (vv > 2).any():
                raise ValueError("Genotype values must be in {0,1,2} or NaN.")
            codes[ok] = map_arr[vv]
        codes[miss] = 1
        return codes
    else:
        v = g.astype(np.int16, copy=False)
        codes = np.zeros(v.shape, dtype=np.uint8)
        miss = (v < 0)  # use negative to denote missing? Not typical; handle none
        ok = ~miss
        if (v[ok] < 0).any() or (v[ok] > 2).any():
            raise ValueError("Genotype values must be in {0,1,2}.")
        map_arr = np.array([0, 2, 3], dtype=np.uint8)
        codes[ok] = map_arr[v[ok]]
        codes[miss] = 1
        return codes


def _pack_codes_to_bytes(codes_row: np.ndarray) -> np.ndarray:
    # codes_row shape (n_samples,), uint8 in {0,1,2,3} where
    # bits are the two-bit value for each sample.
    n = codes_row.shape[0]
    nbytes = (n + 3) // 4
    padded = np.full(nbytes * 4, 1, dtype=np.uint8)  # pad as missing
    padded[:n] = codes_row
    shaped = padded.reshape(nbytes, 4)
    # Pack: byte = c0 | (c1<<2) | (c2<<4) | (c3<<6)
    b = shaped[:, 0] | (shaped[:, 1] << 2) | (shaped[:, 2] << 4) | (shaped[:, 3] << 6)
    return b.astype(np.uint8, copy=False)


# =========================
# Public API
# =========================

class PlinkBED:
    """
    High-level interface for reading and analyzing PLINK BED data with pandas.

    Notes:
    - Genotypes are decoded to dosage of allele2 (A2) per BIM, in {0,1,2}, NaN for missing.
    - BED mode must be SNP-major (standard PLINK .bed). Individual-major is not supported.
    - For large datasets, prefer iter_variant_chunks() and analytics methods to avoid materializing full matrices.
    """

    def __init__(self, prefix: Optional[str] = None, bed: Optional[str] = None, bim: Optional[str] = None, fam: Optional[str] = None):
        if prefix is not None:
            if any([bed, bim, fam]):
                raise ValueError("Provide either prefix or explicit bed/bim/fam paths, not both.")
            bed = prefix + ".bed"
            bim = prefix + ".bim"
            fam = prefix + ".fam"
        if not (bed and bim and fam):
            raise ValueError("Must provide either prefix or bed/bim/fam paths.")
        if not (os.path.exists(bed) and os.path.exists(bim) and os.path.exists(fam)):
            raise FileNotFoundError("One or more PLINK files not found.")

        self.bed_path = bed
        self.bim = _read_bim(bim)
        self.fam = _read_fam(fam)

        n_samples = self.fam.shape[0]
        n_variants, mode = _bed_shape_and_mode(self.bed_path, n_samples)
        if n_variants != self.bim.shape[0]:
            raise ValueError(f"BIM variant count {self.bim.shape[0]} != BED variants {n_variants}")
        if mode != BED_MODE_SNP_MAJOR:
            raise NotImplementedError("Only SNP-major BED (mode=1) is supported.")

        self._n_samples = n_samples
        self._n_variants = n_variants
        self._bytes_per_variant = (n_samples + 3) // 4

        # Memory-map raw BED payload after header (3 bytes)
        self._mem = np.memmap(
            self.bed_path,
            mode="r",
            dtype=np.uint8,
            offset=3,
            shape=(self._n_variants, self._bytes_per_variant),
            order="C",
        )

    @property
    def n_samples(self) -> int:
        return self._n_samples

    @property
    def n_variants(self) -> int:
        return self._n_variants

    @property
    def samples(self) -> pd.Index:
        return self.fam["iid"]

    @property
    def variants(self) -> pd.Index:
        return self.bim["snp"]

    def _resolve_sample_indices(self, samples: Optional[Union[Sequence[str], Sequence[int], np.ndarray]]) -> np.ndarray:
        if samples is None:
            return np.arange(self._n_samples, dtype=np.int64)
        if isinstance(samples, (list, tuple, np.ndarray)) and len(samples) > 0 and isinstance(samples[0], (np.integer, int, np.bool_)):
            arr = np.asarray(samples)
            if arr.dtype == bool:
                if arr.size != self._n_samples:
                    raise ValueError("Boolean sample mask has wrong length.")
                return np.flatnonzero(arr)
            return arr.astype(np.int64, copy=False)
        # treat as sample IDs
        idx = pd.Index(self.fam["iid"])
        return idx.get_indexer_for(samples)

    def _resolve_variant_indices(self, variants: Optional[Union[Sequence[str], Sequence[int], np.ndarray]]) -> np.ndarray:
        if variants is None:
            return np.arange(self._n_variants, dtype=np.int64)
        if isinstance(variants, (list, tuple, np.ndarray)) and len(variants) > 0 and isinstance(variants[0], (np.integer, int, np.bool_)):
            arr = np.asarray(variants)
            if arr.dtype == bool:
                if arr.size != self._n_variants:
                    raise ValueError("Boolean variant mask has wrong length.")
                return np.flatnonzero(arr)
            return arr.astype(np.int64, copy=False)
        # treat as variant IDs
        idx = pd.Index(self.bim["snp"])
        return idx.get_indexer_for(variants)

    def iter_variant_chunks(
        self,
        chunk_size: Optional[int] = None,
        variants: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        samples: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        as_dataframe: bool = False,
        dtype: str = "float32",
    ) -> Iterator[Tuple[np.ndarray, Union[np.ndarray, pd.DataFrame]]]:
        """
        Iterate over variant chunks, yielding (variant_index_array, genotypes).

        - genotypes: shape (chunk_variants, n_selected_samples), float32 dosages in {0,1,2,NaN}
          or a pandas DataFrame if as_dataframe=True.
        """
        var_idx = self._resolve_variant_indices(variants)
        samp_idx = self._resolve_sample_indices(samples)

        if (var_idx < 0).any() or (samp_idx < 0).any():
            raise KeyError("Some requested sample or variant IDs are not present.")

        n_samp_sel = len(samp_idx)
        if chunk_size is None:
            chunk_size = _compute_variant_chunk_size(self._n_samples, target_mb=256.0)
            chunk_size = max(1, min(chunk_size, max(1, len(var_idx))))

        for start in range(0, len(var_idx), chunk_size):
            sl = var_idx[start : start + chunk_size]
            # Decode block
            bytes_block = self._mem[sl, :]
            G = _decode_bytes_to_genotypes(bytes_block, self._n_samples).astype(dtype, copy=False)
            if n_samp_sel != self._n_samples:
                G = G[:, samp_idx]
            if as_dataframe:
                df = pd.DataFrame(
                    G,
                    index=self.bim["snp"].iloc[sl].to_numpy(),
                    columns=self.fam["iid"].iloc[samp_idx].to_numpy(),
                )
                yield sl, df
            else:
                yield sl, G

    def read_genotypes(
        self,
        variants: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        samples: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        as_dataframe: bool = True,
        dtype: str = "float32",
        target_mb: float = 512.0,
    ) -> Union[np.ndarray, pd.DataFrame]:
        """
        Materialize genotypes into memory. For large datasets, prefer iter_variant_chunks.

        Returns an array of shape (n_variants_selected, n_samples_selected) or a DataFrame with
        variant IDs as index and sample IDs as columns.
        """
        var_idx = self._resolve_variant_indices(variants)
        samp_idx = self._resolve_sample_indices(samples)
        if (var_idx < 0).any() or (samp_idx < 0).any():
            raise KeyError("Some requested sample or variant IDs are not present.")

        n_var = len(var_idx)
        n_samp = len(samp_idx)
        # Choose chunk size for memory safety
        chunk_size = _compute_variant_chunk_size(self._n_samples, target_mb=max(128.0, target_mb / 2))
        chunk_size = max(1, min(chunk_size, n_var))

        out = np.empty((n_var, n_samp), dtype=np.float32)
        w = 0
        for _, G in self.iter_variant_chunks(chunk_size=chunk_size, variants=var_idx, samples=samp_idx, as_dataframe=False, dtype=dtype):
            h = G.shape[0]
            out[w : w + h, :] = G
            w += h

        if not as_dataframe:
            return out.astype(dtype, copy=False)

        df = pd.DataFrame(
            out.astype(dtype, copy=False),
            index=self.bim["snp"].iloc[var_idx].to_numpy(),
            columns=self.fam["iid"].iloc[samp_idx].to_numpy(),
        )
        return df

    # ---------- Basic analytics (chunked) ----------

    def variant_missing_rate(
        self,
        variants: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        samples: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        chunk_size: Optional[int] = None,
    ) -> pd.Series:
        """
        Fraction of missing genotypes per variant over selected samples.
        """
        var_idx = self._resolve_variant_indices(variants)
        samp_idx = self._resolve_sample_indices(samples)
        miss_counts = np.zeros(len(var_idx), dtype=np.int64)
        total = len(samp_idx)

        i = 0
        for sl, G in self.iter_variant_chunks(chunk_size=chunk_size, variants=var_idx, samples=samp_idx, as_dataframe=False):
            miss_counts[i : i + G.shape[0]] = np.isnan(G).sum(axis=1)
            i += G.shape[0]
        rates = miss_counts / total
        return pd.Series(rates, index=self.bim["snp"].iloc[var_idx].to_numpy(), name="missing_rate")

    def allele_frequency(
        self,
        variants: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        samples: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        chunk_size: Optional[int] = None,
    ) -> pd.Series:
        """
        Allele frequency of A2 per variant over selected samples.
        Computed as sum(dosage)/(2*non-missing count).
        """
        var_idx = self._resolve_variant_indices(variants)
        samp_idx = self._resolve_sample_indices(samples)
        sums = np.zeros(len(var_idx), dtype=np.float64)
        counts = np.zeros(len(var_idx), dtype=np.int64)

        i = 0
        for sl, G in self.iter_variant_chunks(chunk_size=chunk_size, variants=var_idx, samples=samp_idx, as_dataframe=False):
            valid = ~np.isnan(G)
            sums[i : i + G.shape[0]] = np.where(valid, G, 0.0).sum(axis=1)
            counts[i : i + G.shape[0]] = valid.sum(axis=1)
            i += G.shape[0]
        af = np.divide(sums, 2.0 * counts, out=np.full_like(sums, np.nan, dtype=np.float64), where=counts > 0)
        return pd.Series(af, index=self.bim["snp"].iloc[var_idx].to_numpy(), name="af_a2")

    def sample_missing_rate(
        self,
        variants: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        samples: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        chunk_size: Optional[int] = None,
    ) -> pd.Series:
        """
        Fraction of missing genotypes per sample over selected variants.
        """
        var_idx = self._resolve_variant_indices(variants)
        samp_idx = self._resolve_sample_indices(samples)
        miss_counts = np.zeros(len(samp_idx), dtype=np.int64)
        total = len(var_idx)

        for _, G in self.iter_variant_chunks(chunk_size=chunk_size, variants=var_idx, samples=samp_idx, as_dataframe=False):
            miss_counts += np.isnan(G).sum(axis=0)
        rates = miss_counts / total
        return pd.Series(rates, index=self.fam["iid"].iloc[samp_idx].to_numpy(), name="missing_rate")

    def filter_variants_by_maf(
        self,
        min_maf: float = 0.01,
        variants: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        samples: Optional[Union[Sequence[str], Sequence[int], np.ndarray]] = None,
        chunk_size: Optional[int] = None,
    ) -> pd.Index:
        """
        Return variant IDs passing MAF >= min_maf over selected samples.
        """
        af = self.allele_frequency(variants=variants, samples=samples, chunk_size=chunk_size)
        maf = np.minimum(af, 1 - af)
        return af.index[maf >= min_maf]

    # ---------- Writing ----------

    @staticmethod
    def write(
        out_prefix: str,
        genotypes: Union[pd.DataFrame, np.ndarray],
        bim: pd.DataFrame,
        fam: pd.DataFrame,
        sample_order: Optional[Sequence[str]] = None,
        chunk_size: Optional[int] = None,
    ) -> None:
        """
        Write PLINK BED/BIM/FAM. Genotypes must be in SNP-major orientation:
        shape (n_variants, n_samples), values {0,1,2} for A2 dosage, NaN for missing.

        - If genotypes is a DataFrame, its index should align to bim['snp'] and columns to fam['iid'].
          If sample_order is given, columns will be reindexed to that order.
        - BIM and FAM will be written as provided (no reordering done here).
        """
        os.makedirs(os.path.dirname(os.path.abspath(out_prefix)) or ".", exist_ok=True)
        n_variants = bim.shape[0]
        n_samples = fam.shape[0]

        # Prepare genotype accessor
        if isinstance(genotypes, pd.DataFrame):
            # Align DataFrame
            if genotypes.shape[0] != n_variants:
                # Try reindex by variant IDs if possible
                if "snp" in bim.columns:
                    genotypes = genotypes.reindex(index=bim["snp"].to_numpy())
                if genotypes.shape[0] != n_variants:
                    raise ValueError("Genotype DataFrame row count does not match BIM variants.")
            if sample_order is None:
                sample_order = fam["iid"].to_numpy()
            G_df = genotypes.reindex(columns=sample_order)
            if G_df.shape[1] != n_samples:
                raise ValueError("Genotype DataFrame columns do not match FAM samples.")
            col_get = lambda j: G_df.iloc[:, j].to_numpy()
            row_get = lambda i: G_df.iloc[i].to_numpy()
        else:
            G = np.asarray(genotypes)
            if G.shape != (n_variants, n_samples):
                raise ValueError("genotypes array must have shape (n_variants, n_samples).")
            row_get = lambda i: G[i]

        # Write BIM and FAM
        bim_path = out_prefix + ".bim"
        fam_path = out_prefix + ".fam"
        bed_path = out_prefix + ".bed"

        bim_cols = ["chrom", "snp", "cm", "bp", "a1", "a2"]
        if list(bim.columns) != bim_cols:
            # Try to coerce order if contains needed columns
            if set(bim_cols).issubset(set(bim.columns)):
                bim = bim[bim_cols]
            else:
                raise ValueError(f"BIM must have columns {bim_cols}")
        fam_cols = ["fid", "iid", "father", "mother", "sex", "phenotype"]
        if list(fam.columns) != fam_cols:
            if set(fam_cols).issubset(set(fam.columns)):
                fam = fam[fam_cols]
            else:
                raise ValueError(f"FAM must have columns {fam_cols}")

        bim.to_csv(bim_path, sep="\t", header=False, index=False, na_rep="NA")
        fam.to_csv(fam_path, sep=" ", header=False, index=False, na_rep="NA")

        # Write BED
        with open(bed_path, "wb") as f:
            f.write(bytes([BED_MAGIC[0], BED_MAGIC[1], BED_MODE_SNP_MAJOR]))
            # Chunk over variants to reduce memory
            if chunk_size is None:
                # Estimate chunk size so we keep encode buffers small
                chunk_size = 4096
                if n_samples > 0:
                    # Each variant requires ~n_samples bytes temp (uint8 codes), be conservative
                    target_bytes = 64 * 1024 * 1024  # 64 MB
                    chunk_size = max(1, min(chunk_size, target_bytes // max(1, n_samples)))
            for start in range(0, n_variants, chunk_size):
                end = min(n_variants, start + chunk_size)
                block = []
                for i in range(start, end):
                    row = row_get(i)
                    codes = _codes_from_genotypes(row)
                    packed = _pack_codes_to_bytes(codes)
                    block.append(packed)
                if block:
                    out_block = np.vstack(block).astype(np.uint8, copy=False)
                    out_block.tofile(f)


# =========================
# Convenience functions
# =========================

def read_plink(prefix: Optional[str] = None, bed: Optional[str] = None, bim: Optional[str] = None, fam: Optional[str] = None) -> PlinkBED:
    """
    Create a PlinkBED object from a prefix or explicit paths.
    """
    return PlinkBED(prefix=prefix, bed=bed, bim=bim, fam=fam)


def write_plink(out_prefix: str, genotypes: Union[pd.DataFrame, np.ndarray], bim: pd.DataFrame, fam: pd.DataFrame, sample_order: Optional[Sequence[str]] = None, chunk_size: Optional[int] = None) -> None:
    """
    Convenience function to write PLINK files.
    """
    PlinkBED.write(out_prefix=out_prefix, genotypes=genotypes, bim=bim, fam=fam, sample_order=sample_order, chunk_size=chunk_size)


# =========================
# Minimal usage examples
# =========================
if __name__ == "__main__":
    # Example (adjust 'data/mydata' to your prefix):
    # reader = read_plink(prefix="data/mydata")
    # Print basic info
    # print(reader.n_samples, reader.n_variants)
    # Compute variant missing rates
    # vmr = reader.variant_missing_rate()
    # Compute allele frequencies
    # af = reader.allele_frequency()
    # Read a subset as DataFrame
    # df = reader.read_genotypes(variants=reader.variants[:1000], samples=reader.samples[:500], as_dataframe=True)
    # Write back to a new PLINK dataset
    # write_plink("data/mydata_copy", df, reader.bim.iloc[:1000].reset_index(drop=True), reader.fam.iloc[:500].reset_index(drop=True))
    pass
------------------------------------------------------------

PROMPT 496:
"Generate Python code that imports the psycopg2 library and creates a connection to a PostgreSQL database, allowing for the execution of SQL queries and the manipulation of database objects."
RESPONSE:
import os
import psycopg2
from psycopg2 import sql, extras


def get_connection():
    """
    Create and return a new psycopg2 connection using environment variables:
      PGHOST, PGPORT, PGDATABASE, PGUSER, PGPASSWORD, PGSSLMODE
    """
    return psycopg2.connect(
        host=os.getenv("PGHOST", "localhost"),
        port=int(os.getenv("PGPORT", "5432")),
        dbname=os.getenv("PGDATABASE", "mydb"),
        user=os.getenv("PGUSER", "myuser"),
        password=os.getenv("PGPASSWORD", "mypassword"),
        sslmode=os.getenv("PGSSLMODE", "prefer"),
    )


def execute(query, params=None, fetch=None, many=False, cursor_factory=extras.RealDictCursor):
    """
    Execute a SQL statement safely with parameters.

    Args:
      query: str or psycopg2.sql.Composable
      params: tuple/list of params or list of tuples for executemany
      fetch: None | "one" | "all"
      many: if True, use executemany
      cursor_factory: cursor type (default returns dict-like rows)

    Returns:
      - None if no fetch
      - one row (dict) if fetch="one"
      - list of rows if fetch="all"
    """
    with get_connection() as conn:
        with conn.cursor(cursor_factory=cursor_factory) as cur:
            if many:
                cur.executemany(query, params or [])
            else:
                cur.execute(query, params)
            if fetch == "one":
                return cur.fetchone()
            if fetch == "all":
                return cur.fetchall()
            return None  # no fetch; commit happens on exiting the context


def create_table_if_not_exists(table_name):
    """
    Example DDL operation that manipulates a database object (table).
    Uses psycopg2.sql to safely compose identifiers.
    """
    ddl = sql.SQL("""
        CREATE TABLE IF NOT EXISTS {} (
            id SERIAL PRIMARY KEY,
            email TEXT UNIQUE NOT NULL,
            created_at TIMESTAMPTZ DEFAULT NOW()
        )
    """).format(sql.Identifier(table_name))
    execute(ddl)


def drop_table_if_exists(table_name):
    ddl = sql.SQL("DROP TABLE IF EXISTS {}").format(sql.Identifier(table_name))
    execute(ddl)


if __name__ == "__main__":
    # Create a table
    create_table_if_not_exists("users")

    # Insert a row and return it
    new_user = execute(
        "INSERT INTO users (email) VALUES (%s) RETURNING id, email, created_at",
        params=("alice@example.com",),
        fetch="one",
    )
    print("Inserted:", new_user)

    # Insert many rows
    execute(
        "INSERT INTO users (email) VALUES (%s) ON CONFLICT DO NOTHING",
        params=[("bob@example.com",), ("carol@example.com",)],
        many=True,
    )

    # Query rows
    users = execute("SELECT id, email, created_at FROM users ORDER BY id", fetch="all")
    print("All users:", users)

    # Update a row
    updated = execute(
        "UPDATE users SET email = %s WHERE id = %s RETURNING id, email",
        params=("alice+updated@example.com", new_user["id"]),
        fetch="one",
    )
    print("Updated:", updated)

    # Delete a row
    execute("DELETE FROM users WHERE email = %s", params=("carol@example.com",))
------------------------------------------------------------

PROMPT 497:
"Generate Python code that creates a command-line interface for interacting with a data lake, allowing users to list, create, delete, and manipulate files and directories within the data lake."
RESPONSE:
```python
#!/usr/bin/env python3
"""
datalake_cli.py

A simple, extensible command-line interface for interacting with a data lake.
Supports local filesystem and, optionally, Amazon S3 paths.

Examples:
  # Local
  python datalake_cli.py ls /tmp
  python datalake_cli.py mkdir /tmp/newdir
  python datalake_cli.py cp /tmp/file.txt /tmp/copy.txt
  python datalake_cli.py rm -r /tmp/newdir

  # S3 (requires boto3: pip install boto3)
  python datalake_cli.py ls s3://my-bucket/path/
  python datalake_cli.py mkdir s3://my-bucket/new-prefix/
  python datalake_cli.py put ./localfile s3://my-bucket/path/file
  python datalake_cli.py get s3://my-bucket/path/file ./localfile
  python datalake_cli.py cp -r s3://my-bucket/data/ s3://my-bucket/archive/
  python datalake_cli.py rm -r s3://my-bucket/path/to/delete/

Notes:
  - Directories on S3 are virtual; a trailing slash is recommended for "directory-like" prefixes.
  - Cross-backend copies (e.g., local <-> S3, S3 <-> local) are supported via streaming.
  - For AWS profile, supply --aws-profile or set AWS_PROFILE env var.
"""

import argparse
import concurrent.futures as futures
import datetime as dt
import functools
import os
import pathlib
import re
import shutil
import sys
import threading
from typing import Any, Dict, Iterable, List, Optional, Tuple

# ----------------------------
# Utilities
# ----------------------------

def is_s3_url(url: str) -> bool:
    return url.startswith("s3://")

def is_file_url(url: str) -> bool:
    return url.startswith("file://")

def strip_file_scheme(url: str) -> str:
    return url[7:] if is_file_url(url) else url

def parse_s3_url(url: str) -> Tuple[str, str]:
    """
    Returns (bucket, key) for s3://bucket/key
    Key may be empty string.
    """
    if not is_s3_url(url):
        raise ValueError(f"Not an S3 URL: {url}")
    no_scheme = url[5:]
    parts = no_scheme.split("/", 1)
    bucket = parts[0]
    key = "" if len(parts) == 1 else parts[1]
    return bucket, key

def ensure_trailing_slash(prefix: str) -> str:
    return prefix if prefix.endswith("/") else prefix + "/"

def human_size(num_bytes: int) -> str:
    if num_bytes is None:
        return "-"
    units = ["B", "KB", "MB", "GB", "TB", "PB"]
    size = float(num_bytes)
    for u in units:
        if size < 1024:
            return f"{size:.1f}{u}" if u != "B" else f"{int(size)}B"
        size /= 1024
    return f"{size:.1f}EB"

def utc_fmt(dt_obj: Optional[dt.datetime]) -> str:
    if not dt_obj:
        return "-"
    if dt_obj.tzinfo is None:
        dt_obj = dt_obj.replace(tzinfo=dt.timezone.utc)
    return dt_obj.astimezone(dt.timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")

def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

class CLIError(Exception):
    pass

# ----------------------------
# Backend base class
# ----------------------------

class StorageBackend:
    name = "base"

    def exists(self, url: str) -> bool:
        raise NotImplementedError

    def is_dir(self, url: str) -> bool:
        raise NotImplementedError

    def list(self, url: str) -> List[Dict[str, Any]]:
        """
        Returns a list of entries under url.
        Each entry is a dict:
          {
            "name": str,            # base name (or child component). For S3: immediate child.
            "url": str,             # full URL
            "is_dir": bool,
            "size": Optional[int],  # bytes
            "modified": Optional[datetime]
          }
        """
        raise NotImplementedError

    def mkdir(self, url: str, parents: bool = False, exist_ok: bool = True) -> None:
        raise NotImplementedError

    def rm(self, url: str, recursive: bool = False) -> None:
        raise NotImplementedError

    def stat(self, url: str) -> Dict[str, Any]:
        """
        Returns dict with:
          "url", "is_dir", "size", "modified", "etag" (optional), "storage_class" (optional)
        """
        raise NotImplementedError

    # Streaming
    def open_reader(self, url: str):
        """
        Returns a file-like object opened for reading in binary mode.
        """
        raise NotImplementedError

    def upload_from_fileobj(self, url: str, fileobj, content_type: Optional[str] = None) -> None:
        """
        Writes bytes from fileobj to url.
        """
        raise NotImplementedError

    def copy(self, src_url: str, dst_url: str) -> None:
        """
        Native server-side copy if supported (within same backend or same bucket).
        Otherwise raise NotImplementedError to allow streaming fallback.
        """
        raise NotImplementedError

# ----------------------------
# Local filesystem backend
# ----------------------------

class LocalBackend(StorageBackend):
    name = "local"

    def _path(self, url: str) -> pathlib.Path:
        if is_file_url(url):
            url = strip_file_scheme(url)
        return pathlib.Path(url)

    def exists(self, url: str) -> bool:
        return self._path(url).exists()

    def is_dir(self, url: str) -> bool:
        return self._path(url).is_dir()

    def list(self, url: str) -> List[Dict[str, Any]]:
        p = self._path(url)
        if not p.exists():
            raise CLIError(f"Path not found: {p}")
        if p.is_file():
            st = p.stat()
            return [{
                "name": p.name,
                "url": str(p),
                "is_dir": False,
                "size": st.st_size,
                "modified": dt.datetime.fromtimestamp(st.st_mtime, tz=dt.timezone.utc),
            }]
        entries = []
        for child in p.iterdir():
            is_dir = child.is_dir()
            size = None
            modified = None
            try:
                st = child.stat()
                size = st.st_size if not is_dir else None
                modified = dt.datetime.fromtimestamp(st.st_mtime, tz=dt.timezone.utc)
            except Exception:
                pass
            entries.append({
                "name": child.name + ("/" if is_dir else ""),
                "url": str(child),
                "is_dir": is_dir,
                "size": size,
                "modified": modified,
            })
        entries.sort(key=lambda e: (not e["is_dir"], e["name"].lower()))
        return entries

    def mkdir(self, url: str, parents: bool = False, exist_ok: bool = True) -> None:
        p = self._path(url)
        p.mkdir(parents=parents, exist_ok=exist_ok)

    def rm(self, url: str, recursive: bool = False) -> None:
        p = self._path(url)
        if not p.exists():
            return
        if p.is_dir():
            if recursive:
                shutil.rmtree(p)
            else:
                p.rmdir()
        else:
            p.unlink()

    def stat(self, url: str) -> Dict[str, Any]:
        p = self._path(url)
        if not p.exists():
            raise CLIError(f"Path not found: {p}")
        is_dir = p.is_dir()
        st = p.stat()
        return {
            "url": str(p),
            "is_dir": is_dir,
            "size": None if is_dir else st.st_size,
            "modified": dt.datetime.fromtimestamp(st.st_mtime, tz=dt.timezone.utc),
            "etag": None,
            "storage_class": None,
        }

    def open_reader(self, url: str):
        p = self._path(url)
        if p.is_dir():
            raise CLIError(f"Cannot read a directory: {p}")
        return open(p, "rb")

    def upload_from_fileobj(self, url: str, fileobj, content_type: Optional[str] = None) -> None:
        p = self._path(url)
        parent = p.parent
        parent.mkdir(parents=True, exist_ok=True)
        with open(p, "wb") as f:
            shutil.copyfileobj(fileobj, f, length=1024 * 1024)

    def copy(self, src_url: str, dst_url: str) -> None:
        src = self._path(src_url)
        dst = self._path(dst_url)
        if src.is_dir():
            raise NotImplementedError("Local native copy only supports files")
        dst.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(src, dst)

# ----------------------------
# S3 backend (optional, requires boto3)
# ----------------------------

class S3Backend(StorageBackend):
    name = "s3"

    def __init__(self, profile: Optional[str] = None, region: Optional[str] = None, endpoint_url: Optional[str] = None):
        try:
            import boto3  # noqa
        except Exception as exc:
            raise CLIError("boto3 is required for S3 support. Install with: pip install boto3") from exc

        import boto3
        session_kwargs = {}
        if profile:
            session_kwargs["profile_name"] = profile
        self._session = boto3.Session(**session_kwargs) if session_kwargs else boto3.Session()
        client_kwargs = {}
        if region:
            client_kwargs["region_name"] = region
        if endpoint_url:
            client_kwargs["endpoint_url"] = endpoint_url
        self._client = self._session.client("s3", **client_kwargs)
        self._resource = self._session.resource("s3", **client_kwargs)
        from boto3.s3.transfer import TransferConfig
        # Reasonable defaults; adjust multipart threshold as needed.
        self._transfer_cfg = TransferConfig(multipart_threshold=8 * 1024 * 1024, max_concurrency=8)

        # thread local to cache resource objects if needed
        self._tls = threading.local()

    def _c(self):
        return self._client

    def exists(self, url: str) -> bool:
        bucket, key = parse_s3_url(url)
        if key == "" or url.endswith("/"):
            # treat as "directory-like": does any object exist with this prefix?
            prefix = key if key.endswith("/") else key + "/"
            paginator = self._c().get_paginator("list_objects_v2")
            for page in paginator.paginate(Bucket=bucket, Prefix=prefix, MaxKeys=1):
                if page.get("KeyCount", 0) > 0:
                    return True
            return False
        # file-like
        try:
            self._c().head_object(Bucket=bucket, Key=key)
            return True
        except Exception:
            return False

    def is_dir(self, url: str) -> bool:
        bucket, key = parse_s3_url(url)
        if url.endswith("/") or key == "":
            return True if self.exists(url) else False
        # if not explicitly a dir, check if there are children
        prefix = key + "/"
        paginator = self._c().get_paginator("list_objects_v2")
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix, MaxKeys=1):
            if page.get("KeyCount", 0) > 0:
                return True
        return False

    def list(self, url: str) -> List[Dict[str, Any]]:
        bucket, key = parse_s3_url(url)
        prefix = key
        if prefix != "" and not prefix.endswith("/"):
            # Determine if it's a file
            try:
                obj = self._c().head_object(Bucket=bucket, Key=prefix)
                return [{
                    "name": prefix.split("/")[-1],
                    "url": url,
                    "is_dir": False,
                    "size": obj.get("ContentLength"),
                    "modified": obj.get("LastModified"),
                }]
            except Exception:
                # not a file; treat as directory
                pass
        if prefix != "" and not prefix.endswith("/"):
            prefix = prefix + "/"
        paginator = self._c().get_paginator("list_objects_v2")
        result = []
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix, Delimiter="/"):
            # CommonPrefixes are sub-"directories"
            for cp in page.get("CommonPrefixes", []):
                pfx = cp["Prefix"]
                name = pfx[len(prefix):]
                result.append({
                    "name": name,
                    "url": f"s3://{bucket}/{pfx}",
                    "is_dir": True,
                    "size": None,
                    "modified": None,
                })
            for obj in page.get("Contents", []):
                keyname = obj["Key"]
                if keyname == prefix:
                    # "directory marker" object; skip listing itself
                    continue
                name = keyname[len(prefix):]
                if "/" in name:
                    # deeper child handled by CommonPrefixes
                    continue
                result.append({
                    "name": name,
                    "url": f"s3://{bucket}/{keyname}",
                    "is_dir": False,
                    "size": obj.get("Size"),
                    "modified": obj.get("LastModified"),
                })
        result.sort(key=lambda e: (not e["is_dir"], e["name"].lower()))
        return result

    def mkdir(self, url: str, parents: bool = False, exist_ok: bool = True) -> None:
        bucket, key = parse_s3_url(url)
        if key == "":
            # Bucket itself; nothing to do (or could create bucket - not implemented)
            return
        prefix = key if key.endswith("/") else key + "/"
        # create a "directory marker" zero-byte object
        self._c().put_object(Bucket=bucket, Key=prefix, Body=b"")

    def rm(self, url: str, recursive: bool = False) -> None:
        bucket, key = parse_s3_url(url)
        if (key == "" or url.endswith("/")) or self.is_dir(url):
            prefix = key if key.endswith("/") else key + "/"
            if not recursive:
                # Ensure directory is empty
                listing = self.list(url)
                if listing:
                    raise CLIError(f"S3 prefix not empty (use -r): {url}")
                # try delete the directory marker if exists
                marker = prefix
                try:
                    self._c().delete_object(Bucket=bucket, Key=marker)
                except Exception:
                    pass
                return
            # recursive delete
            paginator = self._c().get_paginator("list_objects_v2")
            to_delete = []
            for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
                for obj in page.get("Contents", []):
                    to_delete.append({"Key": obj["Key"]})
                    if len(to_delete) == 1000:
                        self._c().delete_objects(Bucket=bucket, Delete={"Objects": to_delete})
                        to_delete = []
            if to_delete:
                self._c().delete_objects(Bucket=bucket, Delete={"Objects": to_delete})
        else:
            # file
            self._c().delete_object(Bucket=bucket, Key=key)

    def stat(self, url: str) -> Dict[str, Any]:
        bucket, key = parse_s3_url(url)
        if key == "" or self.is_dir(url):
            # "directory-like" object
            # Check if there's a directory marker
            dir_marker = ensure_trailing_slash(key)
            is_dir = True
            size = None
            modified = None
            etag = None
            try:
                head = self._c().head_object(Bucket=bucket, Key=dir_marker)
                size = head.get("ContentLength")
                modified = head.get("LastModified")
                etag = head.get("ETag")
            except Exception:
                pass
            return {
                "url": f"s3://{bucket}/{dir_marker}" if key else f"s3://{bucket}",
                "is_dir": True,
                "size": size,
                "modified": modified,
                "etag": etag,
                "storage_class": None,
            }
        head = self._c().head_object(Bucket=bucket, Key=key)
        return {
            "url": url,
            "is_dir": False,
            "size": head.get("ContentLength"),
            "modified": head.get("LastModified"),
            "etag": head.get("ETag"),
            "storage_class": head.get("StorageClass"),
        }

    def open_reader(self, url: str):
        bucket, key = parse_s3_url(url)
        if key.endswith("/") or self.is_dir(url):
            raise CLIError(f"Cannot read a directory/prefix: {url}")
        obj = self._c().get_object(Bucket=bucket, Key=key)
        return obj["Body"]

    def upload_from_fileobj(self, url: str, fileobj, content_type: Optional[str] = None) -> None:
        from boto3.s3.transfer import S3Transfer
        bucket, key = parse_s3_url(url)
        extra_args = {}
        if content_type:
            extra_args["ContentType"] = content_type
        transfer = S3Transfer(self._c(), config=self._transfer_cfg)
        transfer.upload_fileobj(fileobj, bucket, key, extra_args=extra_args)

    def copy(self, src_url: str, dst_url: str) -> None:
        src_bucket, src_key = parse_s3_url(src_url)
        dst_bucket, dst_key = parse_s3_url(dst_url)
        if dst_key.endswith("/"):
            # interpret as copying file under that prefix with same basename
            basename = src_key.split("/")[-1]
            dst_key = dst_key + basename
        if src_key.endswith("/") or dst_key.endswith("/"):
            raise NotImplementedError("S3 native copy only supports files")
        self._c().copy({"Bucket": src_bucket, "Key": src_key}, dst_bucket, dst_key)

# ----------------------------
# Backend selector
# ----------------------------

def get_backend(url: str, s3_opts: Dict[str, Optional[str]]) -> StorageBackend:
    if is_s3_url(url):
        return S3Backend(**s3_opts)
    else:
        return LocalBackend()

def infer_content_type(path: str) -> Optional[str]:
    # Simple heuristic based on extension
    import mimetypes
    ctype, _ = mimetypes.guess_type(path)
    return ctype

# ----------------------------
# High-level operations
# ----------------------------

def op_ls(url: str, long: bool, human: bool, s3_opts: Dict[str, Optional[str]]) -> int:
    backend = get_backend(url, s3_opts)
    try:
        entries = backend.list(url)
    except CLIError as e:
        eprint(f"Error: {e}")
        return 2
    if not long:
        for e in entries:
            print(e["name"])
        return 0
    for e in entries:
        size = human_size(e["size"]) if human else (e["size"] if e["size"] is not None else "-")
        mod = utc_fmt(e["modified"])
        typ = "DIR" if e["is_dir"] else "FILE"
        print(f"{typ:4} {size:>10} {mod:>22}  {e['name']}")
    return 0

def op_tree(url: str, s3_opts: Dict[str, Optional[str]]) -> int:
    backend = get_backend(url, s3_opts)
    def walk(u: str, prefix: str = ""):
        try:
            entries = backend.list(u)
        except CLIError as e:
            eprint(f"Error: {e}")
            return
        for e in entries:
            print(f"{prefix}{e['name']}")
            if e["is_dir"]:
                walk(e["url"], prefix + "  ")
    print(url)
    walk(url, "  ")
    return 0

def op_mkdir(url: str, parents: bool, s3_opts: Dict[str, Optional[str]]) -> int:
    backend = get_backend(url, s3_opts)
    try:
        backend.mkdir(url, parents=parents, exist_ok=True)
    except CLIError as e:
        eprint(f"Error: {e}")
        return 2
    return 0

def op_rm(url: str, recursive: bool, s3_opts: Dict[str, Optional[str]]) -> int:
    backend = get_backend(url, s3_opts)
    try:
        backend.rm(url, recursive=recursive)
    except CLIError as e:
        eprint(f"Error: {e}")
        return 2
    return 0

def op_stat(url: str, human: bool, s3_opts: Dict[str, Optional[str]]) -> int:
    backend = get_backend(url, s3_opts)
    try:
        st = backend.stat(url)
    except CLIError as e:
        eprint(f"Error: {e}")
        return 2
    print(f"URL: {st['url']}")
    print(f"Type: {'DIR' if st['is_dir'] else 'FILE'}")
    print(f"Size: {human_size(st['size']) if human else st['size']}")
    print(f"Modified: {utc_fmt(st['modified'])}")
    if st.get("etag"):
        print(f"ETag: {st['etag']}")
    if st.get("storage_class"):
        print(f"StorageClass: {st['storage_class']}")
    return 0

def stream_copy(src_backend: StorageBackend, src_url: str, dst_backend: StorageBackend, dst_url: str) -> None:
    # Attempt native copy if both are S3 and same backend type
    if isinstance(src_backend, S3Backend) and isinstance(dst_backend, S3Backend):
        try:
            src_backend.copy(src_url, dst_url)
            return
        except NotImplementedError:
            pass
    # General streaming copy
    ctype = infer_content_type(src_url)
    with src_backend.open_reader(src_url) as fsrc:
        dst_backend.upload_from_fileobj(dst_url, fsrc, content_type=ctype)

def list_recursive(backend: StorageBackend, url: str) -> List[Tuple[str, bool]]:
    """
    Returns list of (url, is_dir) recursively, including root if file.
    """
    try:
        if not backend.is_dir(url):
            return [(url, False)]
    except Exception:
        # If it's a file or non-existing, try list to confirm
        ls = backend.list(url)
        if len(ls) == 1 and not ls[0]["is_dir"]:
            return [(ls[0]["url"], False)]
        return []
    items = []
    stack = [url]
    while stack:
        cur = stack.pop()
        for e in backend.list(cur):
            items.append((e["url"], e["is_dir"]))
            if e["is_dir"]:
                stack.append(e["url"])
    return items

def relative_child_path(parent_url: str, child_url: str) -> str:
    """
    Compute relative path name of child_url under parent_url.
    Works for both local and S3 by comparing path components.
    """
    if is_s3_url(parent_url) and is_s3_url(child_url):
        pb, pk = parse_s3_url(parent_url)
        cb, ck = parse_s3_url(child_url)
        if pb != cb:
            raise CLIError("S3 relative path requires same bucket")
        base = pk
        if base != "" and not base.endswith("/"):
            base += "/"
        if not ck.startswith(base):
            raise CLIError("Child is not under parent")
        rel = ck[len(base):]
        return rel
    # local or mixed: fall back to string manipulation of normalized paths
    p1 = strip_file_scheme(parent_url)
    p2 = strip_file_scheme(child_url)
    p1 = os.path.abspath(p1)
    p2 = os.path.abspath(p2)
    rel = os.path.relpath(p2, start=p1)
    return rel

def op_cp(src: str, dst: str, recursive: bool, parallel: int, s3_opts: Dict[str, Optional[str]]) -> int:
    src_b = get_backend(src, s3_opts)
    dst_b = get_backend(dst, s3_opts)

    # Determine if src is dir
    src_is_dir = False
    try:
        src_is_dir = src_b.is_dir(src)
    except Exception:
        pass

    # If src is file and dst ends with slash (or is dir), resolve final dst
    if not src_is_dir:
        if (is_s3_url(dst) and dst.endswith("/")) or (not is_s3_url(dst) and dst_b.is_dir(dst)):
            # append basename
            name = src.split("/")[-1] if is_s3_url(src) else os.path.basename(strip_file_scheme(src))
            if is_s3_url(dst) and not dst.endswith("/"):
                dst = ensure_trailing_slash(dst)
            dst = dst + (name if is_s3_url(dst) else (name if dst.endswith(os.sep) else (os.sep.join([dst, name]) if not is_s3_url(dst) else name)))
        try:
            stream_copy(src_b, src, dst_b, dst)
            return 0
        except CLIError as e:
            eprint(f"Error: {e}")
            return 2

    # Recursive directory copy
    if not recursive:
        raise CLIError("Source is a directory. Use -r to copy recursively.")

    # Ensure destination prefix is treated as directory
    if is_s3_url(dst) and not dst.endswith("/"):
        dst = ensure_trailing_slash(dst)

    # Enumerate files under src
    to_copy: List[Tuple[str, str]] = []
    for url_item, is_dir_item in list_recursive(src_b, src):
        if is_dir_item:
            continue
        rel = relative_child_path(src, url_item)
        # Normalize separators for S3
        if is_s3_url(dst):
            rel = rel.replace("\\", "/")
            dst_url = dst + rel if dst.endswith("/") else ensure_trailing_slash(dst) + rel
        else:
            # local
            base = strip_file_scheme(dst)
            dst_url = os.path.join(base, rel)
        # Reattach file:// if user used file scheme for local
        if is_file_url(dst):
            dst_url = "file://" + dst_url
        to_copy.append((url_item, dst_url))

    # Copy in parallel
    def worker(src_u: str, dst_u: str):
        sb = get_backend(src_u, s3_opts)
        db = get_backend(dst_u, s3_opts)
        stream_copy(sb, src_u, db, dst_u)

    errors = 0
    with futures.ThreadPoolExecutor(max_workers=parallel) as ex:
        futs = [ex.submit(worker, s, d) for s, d in to_copy]
        for f in futures.as_completed(futs):
            try:
                f.result()
            except Exception as e:
                errors += 1
                eprint(f"Copy error: {e}")
    return 0 if errors == 0 else 3

def op_mv(src: str, dst: str, recursive: bool, parallel: int, s3_opts: Dict[str, Optional[str]]) -> int:
    # Move implemented as cp + rm
    rc = op_cp(src, dst, recursive=recursive, parallel=parallel, s3_opts=s3_opts)
    if rc != 0:
        return rc
    return op_rm(src, recursive=recursive, s3_opts=s3_opts)

def op_cat(url: str, s3_opts: Dict[str, Optional[str]]) -> int:
    backend = get_backend(url, s3_opts)
    try:
        with backend.open_reader(url) as f:
            # Stream to stdout.buffer
            shutil.copyfileobj(f, sys.stdout.buffer, length=1024 * 1024)
    except CLIError as e:
        eprint(f"Error: {e}")
        return 2
    return 0

def op_put(local_path: str, dst_url: str, s3_opts: Dict[str, Optional[str]]) -> int:
    lb = LocalBackend()
    db = get_backend(dst_url, s3_opts)
    # If dst is a directory-like, append basename
    if db.is_dir(dst_url) or (is_s3_url(dst_url) and dst_url.endswith("/")):
        name = os.path.basename(local_path.rstrip("/\\"))
        if is_s3_url(dst_url):
            dst_url = ensure_trailing_slash(dst_url) + name
        else:
            dst_url = os.path.join(dst_url, name)
    try:
        with lb.open_reader(local_path) as f:
            db.upload_from_fileobj(dst_url, f, content_type=infer_content_type(local_path))
    except CLIError as e:
        eprint(f"Error: {e}")
        return 2
    return 0

def op_get(src_url: str, local_path: str, s3_opts: Dict[str, Optional[str]]) -> int:
    sb = get_backend(src_url, s3_opts)
    lb = LocalBackend()
    # If local_path is a directory, append basename
    lp = pathlib.Path(local_path)
    if lp.exists() and lp.is_dir():
        name = src_url.rstrip("/").split("/")[-1]
        local_path = str(lp / name)
    try:
        with sb.open_reader(src_url) as f:
            lb.upload_from_fileobj(local_path, f, content_type=None)
    except CLIError as e:
        eprint(f"Error: {e}")
        return 2
    return 0

def op_find(url: str, name_pattern: Optional[str], s3_opts: Dict[str, Optional[str]]) -> int:
    backend = get_backend(url, s3_opts)
    regex = re.compile(name_pattern) if name_pattern else None

    def matches(name: str) -> bool:
        if regex is None:
            return True
        return bool(regex.search(name))

    # BFS
    q = [url]
    while q:
        cur = q.pop(0)
        try:
            entries = backend.list(cur)
        except CLIError as e:
            eprint(f"Error: {e}")
            return 2
        for e in entries:
            if matches(e["name"].rstrip("/")):
                print(e["url"])
            if e["is_dir"]:
                q.append(e["url"])
    return 0

# ----------------------------
# CLI parsing
# ----------------------------

def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="datalake", description="CLI for interacting with a data lake (local FS and S3)")
    p.add_argument("--aws-profile", default=os.getenv("AWS_PROFILE"), help="AWS profile name for S3")
    p.add_argument("--aws-region", default=os.getenv("AWS_REGION") or os.getenv("AWS_DEFAULT_REGION"), help="AWS region for S3")
    p.add_argument("--s3-endpoint", default=os.getenv("S3_ENDPOINT"), help="Custom S3 endpoint URL (optional)")
    p.add_argument("--parallel", type=int, default=8, help="Parallelism for recursive operations (default: 8)")

    sub = p.add_subparsers(dest="cmd", required=True)

    sp = sub.add_parser("ls", help="List files/directories")
    sp.add_argument("url", help="URL or path")
    sp.add_argument("-l", "--long", action="store_true", help="Long listing")
    sp.add_argument("-H", "--human", action="store_true", help="Human-readable sizes")

    sp = sub.add_parser("tree", help="Tree view")
    sp.add_argument("url", help="URL or path")

    sp = sub.add_parser("mkdir", help="Create directory/prefix")
    sp.add_argument("url", help="URL or path")
    sp.add_argument("-p", "--parents", action="store_true", help="Create parents as needed")

    sp = sub.add_parser("rm", help="Remove file or directory/prefix")
    sp.add_argument("url", help="URL or path")
    sp.add_argument("-r", "--recursive", action="store_true", help="Recursive")

    sp = sub.add_parser("stat", help="Show metadata")
    sp.add_argument("url", help="URL or path")
    sp.add_argument("-H", "--human", action="store_true", help="Human-readable sizes")

    sp = sub.add_parser("cp", help="Copy file or directory")
    sp.add_argument("src", help="Source URL or path")
    sp.add_argument("dst", help="Destination URL or path")
    sp.add_argument("-r", "--recursive", action="store_true", help="Recursive")

    sp = sub.add_parser("mv", help="Move file or directory")
    sp.add_argument("src", help="Source URL or path")
    sp.add_argument("dst", help="Destination URL or path")
    sp.add_argument("-r", "--recursive", action="store_true", help="Recursive")

    sp = sub.add_parser("cat", help="Print file to stdout")
    sp.add_argument("url", help="URL or path")

    sp = sub.add_parser("put", help="Upload local file to data lake")
    sp.add_argument("local_path", help="Local file path")
    sp.add_argument("dst", help="Destination URL (e.g., s3://bucket/key or local path)")

    sp = sub.add_parser("get", help="Download file from data lake to local")
    sp.add_argument("src", help="Source URL (e.g., s3://bucket/key or local path)")
    sp.add_argument("local_path", help="Local destination path")

    sp = sub.add_parser("find", help="Find entries recursively")
    sp.add_argument("url", help="URL or path to search")
    sp.add_argument("--name", help="Regex to match on base name")

    return p

def main(argv: Optional[List[str]] = None) -> int:
    args = build_parser().parse_args(argv)
    s3_opts = {
        "profile": args.aws_profile,
        "region": args.aws_region,
        "endpoint_url": args.s3_endpoint,
    }

    try:
        if args.cmd == "ls":
            return op_ls(args.url, long=args.long, human=args.human, s3_opts=s3_opts)
        elif args.cmd == "tree":
            return op_tree(args.url, s3_opts=s3_opts)
        elif args.cmd == "mkdir":
            return op_mkdir(args.url, parents=args.parents, s3_opts=s3_opts)
        elif args.cmd == "rm":
            return op_rm(args.url, recursive=args.recursive, s3_opts=s3_opts)
        elif args.cmd == "stat":
            return op_stat(args.url, human=args.human, s3_opts=s3_opts)
        elif args.cmd == "cp":
            return op_cp(args.src, args.dst, recursive=args.recursive, parallel=args.parallel, s3_opts=s3_opts)
        elif args.cmd == "mv":
            return op_mv(args.src, args.dst, recursive=args.recursive, parallel=args.parallel, s3_opts=s3_opts)
        elif args.cmd == "cat":
            return op_cat(args.url, s3_opts=s3_opts)
        elif args.cmd == "put":
            return op_put(args.local_path, args.dst, s3_opts=s3_opts)
        elif args.cmd == "get":
            return op_get(args.src, args.local_path, s3_opts=s3_opts)
        elif args.cmd == "find":
            return op_find(args.url, args.name, s3_opts=s3_opts)
        else:
            eprint("Unknown command")
            return 2
    except CLIError as e:
        eprint(f"Error: {e}")
        return 2

if __name__ == "__main__":
    sys.exit(main())
```
------------------------------------------------------------

PROMPT 498:
"Generate Python code that imports the MySQLdb extension for Flask and creates a Flask application that connects to a MySQL database, performs CRUD operations, and returns data in JSON format."
RESPONSE:
# app.py
# pip install Flask flask-mysqldb
# Ensure you have the MySQL client libs installed (mysqlclient) for your platform.

from flask import Flask, request, jsonify, abort
from flask_mysqldb import MySQL
import MySQLdb

app = Flask(__name__)

# Configure your MySQL connection
app.config["MYSQL_HOST"] = "localhost"
app.config["MYSQL_USER"] = "your_mysql_user"
app.config["MYSQL_PASSWORD"] = "your_mysql_password"
app.config["MYSQL_DB"] = "your_database_name"
app.config["MYSQL_CURSORCLASS"] = "DictCursor"  # Return rows as dicts for easy JSON

mysql = MySQL(app)


def init_db():
    cur = mysql.connection.cursor()
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS users (
            id INT PRIMARY KEY AUTO_INCREMENT,
            name VARCHAR(100) NOT NULL,
            email VARCHAR(255) NOT NULL UNIQUE,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
        """
    )
    mysql.connection.commit()
    cur.close()


@app.before_first_request
def setup():
    init_db()


# Helpers
def get_json_data(required_fields=None):
    if not request.is_json:
        abort(make_error(400, "Request must be application/json"))
    data = request.get_json(silent=True)
    if data is None:
        abort(make_error(400, "Invalid JSON payload"))

    if required_fields:
        missing = [f for f in required_fields if f not in data or data[f] in (None, "")]
        if missing:
            abort(make_error(400, f"Missing required fields: {', '.join(missing)}"))
    return data


def make_error(status_code, message):
    response = jsonify({"error": message, "status": status_code})
    response.status_code = status_code
    return response


# CRUD routes
@app.route("/users", methods=["GET"])
def list_users():
    cur = mysql.connection.cursor()
    cur.execute("SELECT id, name, email, created_at FROM users ORDER BY id")
    rows = cur.fetchall()
    cur.close()
    return jsonify(rows), 200


@app.route("/users/<int:user_id>", methods=["GET"])
def get_user(user_id):
    cur = mysql.connection.cursor()
    cur.execute("SELECT id, name, email, created_at FROM users WHERE id = %s", (user_id,))
    row = cur.fetchone()
    cur.close()
    if not row:
        return make_error(404, "User not found")
    return jsonify(row), 200


@app.route("/users", methods=["POST"])
def create_user():
    data = get_json_data(required_fields=["name", "email"])
    name = data["name"].strip()
    email = data["email"].strip()

    try:
        cur = mysql.connection.cursor()
        cur.execute(
            "INSERT INTO users (name, email) VALUES (%s, %s)",
            (name, email),
        )
        mysql.connection.commit()
        new_id = cur.lastrowid
        cur.close()
    except MySQLdb.IntegrityError as e:
        # Likely duplicate email
        return make_error(409, "Email already exists")
    except Exception as e:
        return make_error(500, "Failed to create user")

    # Return the created resource
    cur = mysql.connection.cursor()
    cur.execute("SELECT id, name, email, created_at FROM users WHERE id = %s", (new_id,))
    row = cur.fetchone()
    cur.close()
    return jsonify(row), 201


@app.route("/users/<int:user_id>", methods=["PUT"])
def update_user(user_id):
    data = get_json_data(required_fields=["name", "email"])
    name = data["name"].strip()
    email = data["email"].strip()

    try:
        cur = mysql.connection.cursor()
        cur.execute(
            "UPDATE users SET name = %s, email = %s WHERE id = %s",
            (name, email, user_id),
        )
        mysql.connection.commit()
        updated = cur.rowcount
        cur.close()
        if updated == 0:
            return make_error(404, "User not found")
    except MySQLdb.IntegrityError:
        return make_error(409, "Email already exists")
    except Exception:
        return make_error(500, "Failed to update user")

    cur = mysql.connection.cursor()
    cur.execute("SELECT id, name, email, created_at FROM users WHERE id = %s", (user_id,))
    row = cur.fetchone()
    cur.close()
    return jsonify(row), 200


@app.route("/users/<int:user_id>", methods=["DELETE"])
def delete_user(user_id):
    try:
        cur = mysql.connection.cursor()
        cur.execute("DELETE FROM users WHERE id = %s", (user_id,))
        mysql.connection.commit()
        deleted = cur.rowcount
        cur.close()
        if deleted == 0:
            return make_error(404, "User not found")
    except Exception:
        return make_error(500, "Failed to delete user")

    return jsonify({"status": 204, "message": "User deleted"}), 204


# Error handlers to ensure JSON responses
@app.errorhandler(400)
def handle_400(err):
    return make_error(400, getattr(err, "description", "Bad request"))


@app.errorhandler(404)
def handle_404(err):
    return make_error(404, getattr(err, "description", "Not found"))


@app.errorhandler(405)
def handle_405(err):
    return make_error(405, "Method not allowed")


@app.errorhandler(500)
def handle_500(err):
    return make_error(500, "Internal server error")


if __name__ == "__main__":
    app.run(debug=True)
------------------------------------------------------------

PROMPT 499:
"Generate Python code that uses the `rust_lint` library to perform linting and formatting on Python code, using the `pydantic` library to parse the code and apply the necessary transformations."
RESPONSE:
# lint_pipeline.py

from __future__ import annotations

import json
import sys
from typing import Any, Dict, List, Literal, Optional, Tuple

from pydantic import BaseModel, Field, ValidationError, field_validator


class FormatterOptions(BaseModel):
    line_length: Optional[int] = Field(default=None, ge=1, le=400)
    indent_style: Optional[Literal["space", "tab"]] = None
    quote_style: Optional[Literal["auto", "single", "double"]] = None
    line_ending: Optional[Literal["lf", "crlf", "native"]] = None

    def to_kwargs(self) -> Dict[str, Any]:
        # Only pass non-null options through to the underlying tool
        return {k: v for k, v in self.model_dump().items() if v is not None}


class LintOptions(BaseModel):
    select: Optional[List[str]] = None
    ignore: Optional[List[str]] = None
    fix: bool = True
    preview: bool = False

    def to_kwargs(self) -> Dict[str, Any]:
        out: Dict[str, Any] = {}
        if self.select:
            out["select"] = list(self.select)
        if self.ignore:
            out["ignore"] = list(self.ignore)
        out["fix"] = self.fix
        out["preview"] = self.preview
        return out


class PipelineConfig(BaseModel):
    formatter: FormatterOptions = Field(default_factory=FormatterOptions)
    lint: LintOptions = Field(default_factory=LintOptions)


class CodeDocument(BaseModel):
    code: str
    path: Optional[str] = None
    encoding: str = "utf-8"

    @field_validator("code")
    @classmethod
    def non_empty(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError("Code is empty.")
        return v


class LintDiagnostic(BaseModel):
    code: str
    message: str
    line: int
    column: int
    end_line: Optional[int] = None
    end_column: Optional[int] = None
    severity: Optional[Literal["error", "warning", "info"]] = None
    source: Optional[str] = None
    fix_applied: Optional[bool] = None


class LintResult(BaseModel):
    formatted_code: str
    diagnostics: List[LintDiagnostic] = Field(default_factory=list)


class RustLintUnavailable(RuntimeError):
    pass


class RustLintAdapter:
    """
    Thin, defensive adapter around a 'rust_lint' Python package.

    This adapter does not assume a specific API. It will try common function
    names dynamically and degrade gracefully. Adjust the call sites to match
    your actual rust_lint package API.
    """

    def __init__(self) -> None:
        try:
            import rust_lint  # type: ignore

            self._lib = rust_lint  # type: ignore[attr-defined]
        except Exception as exc:  # pragma: no cover
            self._lib = None
            self._import_error = exc

    @property
    def available(self) -> bool:
        return self._lib is not None

    def format_code(self, code: str, options: FormatterOptions) -> str:
        if not self.available:
            raise RustLintUnavailable(
                f"rust_lint is not importable: {getattr(self, '_import_error', 'unknown error')}"
            )

        # Try common function names that a formatter might expose.
        lib = self._lib
        kwargs = options.to_kwargs()

        # 1) rust_lint.format_code(code: str, **kwargs) -> str
        fn = getattr(lib, "format_code", None)
        if callable(fn):
            return fn(code, **kwargs)

        # 2) rust_lint.format(code: str, options: dict) -> str
        fn = getattr(lib, "format", None)
        if callable(fn):
            return fn(code, options=kwargs)

        # 3) rust_lint.formatter.format(code, **kwargs)
        formatter = getattr(lib, "formatter", None)
        if formatter:
            fn = getattr(formatter, "format", None)
            if callable(fn):
                return fn(code, **kwargs)

        raise RustLintUnavailable(
            "rust_lint does not expose a recognized formatting function. "
            "Expected one of: format_code(code, **opts), format(code, options=opts), formatter.format(code, **opts)."
        )

    def lint_and_maybe_fix(
        self, code: str, options: LintOptions, path: Optional[str]
    ) -> Tuple[str, List[LintDiagnostic]]:
        if not self.available:
            raise RustLintUnavailable(
                f"rust_lint is not importable: {getattr(self, '_import_error', 'unknown error')}"
            )

        lib = self._lib
        kwargs = options.to_kwargs()

        # Try a combined API first:
        # rust_lint.lint_and_fix(code: str, path: Optional[str], **kwargs) -> (str, List[Mapping])
        fn = getattr(lib, "lint_and_fix", None)
        if callable(fn):
            fixed_code, raw_diags = fn(code, path=path, **kwargs)
            return fixed_code, self._coerce_diags(raw_diags)

        # Try a two-step API:
        # rust_lint.lint(code, path=..., **kwargs) -> List[Mapping]
        lint_fn = getattr(lib, "lint", None)
        # rust_lint.apply_fixes(code, diags) -> str
        apply_fn = getattr(lib, "apply_fixes", None)

        if callable(lint_fn):
            raw_diags = lint_fn(code, path=path, **kwargs)
            fixed_code = code
            if options.fix and callable(apply_fn):
                fixed_code = apply_fn(code, raw_diags)
                # Mark fixes as applied where appropriate
                diags = []
                for d in raw_diags or []:
                    dd = self._coerce_diag(d)
                    dd.fix_applied = True
                    diags.append(dd)
                return fixed_code, diags
            return code, self._coerce_diags(raw_diags)

        raise RustLintUnavailable(
            "rust_lint does not expose recognized linting functions. "
            "Expected one of: lint_and_fix, lint (+ optional apply_fixes)."
        )

    @staticmethod
    def _coerce_diag(d: Any) -> LintDiagnostic:
        if isinstance(d, LintDiagnostic):
            return d
        if isinstance(d, dict):
            # Try common keys; fall back to safe defaults
            return LintDiagnostic(
                code=str(d.get("code") or d.get("rule") or "UNKNOWN"),
                message=str(d.get("message") or d.get("msg") or ""),
                line=int(d.get("line", d.get("row", 1))),
                column=int(d.get("column", d.get("col", 1))),
                end_line=d.get("end_line"),
                end_column=d.get("end_column"),
                severity=d.get("severity"),
                source=d.get("source"),
                fix_applied=d.get("fix_applied"),
            )
        # Best-effort string-like
        return LintDiagnostic(
            code="UNKNOWN",
            message=str(d),
            line=1,
            column=1,
        )

    def _coerce_diags(self, ds: Any) -> List[LintDiagnostic]:
        out: List[LintDiagnostic] = []
        if not ds:
            return out
        if isinstance(ds, list):
            for d in ds:
                out.append(self._coerce_diag(d))
        else:
            out.append(self._coerce_diag(ds))
        return out


class Pipeline:
    def __init__(self, adapter: Optional[RustLintAdapter] = None) -> None:
        self.adapter = adapter or RustLintAdapter()

    def run(self, doc: CodeDocument, cfg: PipelineConfig) -> LintResult:
        # 1) Format
        try:
            formatted = self.adapter.format_code(doc.code, cfg.formatter)
        except RustLintUnavailable as e:
            # Fall back: if rust_lint isn't available, do a no-op format and proceed.
            # You can plug in a different formatter here if desired.
            formatted = doc.code

        # 2) Lint (and optionally fix)
        try:
            fixed_code, diagnostics = self.adapter.lint_and_maybe_fix(
                formatted, cfg.lint, path=doc.path
            )
        except RustLintUnavailable:
            fixed_code, diagnostics = formatted, []

        return LintResult(formatted_code=fixed_code, diagnostics=diagnostics)


# Example usage
if __name__ == "__main__":
    sample_code = """
import os, sys

def add(a,b):
    return  a+  b
"""

    # Configure via Pydantic models
    cfg = PipelineConfig(
        formatter=FormatterOptions(line_length=88, indent_style="space", quote_style="auto"),
        lint=LintOptions(select=["E", "F"], ignore=[], fix=True, preview=False),
    )

    try:
        doc = CodeDocument(code=sample_code, path="example.py")
    except ValidationError as ve:
        print(f"Invalid document: {ve}", file=sys.stderr)
        sys.exit(2)

    pipeline = Pipeline()
    result = pipeline.run(doc, cfg)

    print("=== Formatted Code ===")
    print(result.formatted_code)
    print("\n=== Diagnostics (JSON) ===")
    print(json.dumps([d.model_dump() for d in result.diagnostics], indent=2))
------------------------------------------------------------

PROMPT 500:
"Generate Python code that uses SQLAlchemy to create a database model from a given dataset, allowing for the automatic generation of SQL schema and Python classes for interacting with the data."
RESPONSE:
#!/usr/bin/env python3
"""
Auto-model: Generate SQLAlchemy ORM models and schema from tabular datasets.

- Infers column types from a CSV (or any pandas DataFrame)
- Builds SQLAlchemy ORM classes dynamically
- Creates the database schema automatically
- Optionally loads the data into the database
- Demonstrates how to query the generated models

Requirements:
  pip install sqlalchemy pandas python-dateutil

Usage examples:
  python auto_model.py --csv users.csv --db sqlite:///example.db --load
  python auto_model.py --folder ./data --db postgresql+psycopg://user:pass@localhost/dbname --load
"""

import argparse
import json
import os
import re
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import pandas as pd
from pandas.api.types import (
    is_bool_dtype,
    is_datetime64_any_dtype,
    is_float_dtype,
    is_integer_dtype,
    is_object_dtype,
    is_string_dtype,
)

from sqlalchemy import (
    Boolean,
    Column,
    Date,
    DateTime,
    Float,
    Integer,
    JSON,
    String,
    Text,
    Time,
    create_engine,
    inspect as sa_inspect,
)
from sqlalchemy.orm import DeclarativeBase, sessionmaker


# --------------------------- SQLAlchemy Base ---------------------------

class Base(DeclarativeBase):
    pass


# --------------------------- Utilities ---------------------------

RESERVED_WORDS = {
    # common SQL and Python reserved words
    "select", "from", "where", "join", "table", "group", "order", "by",
    "limit", "offset", "create", "delete", "update", "insert",
    "and", "or", "not", "in", "is", "null",
    "class", "def", "lambda", "return", "if", "else", "elif", "for", "while",
    "try", "except", "finally", "with", "as", "pass", "break", "continue",
    "global", "nonlocal", "assert", "yield", "import", "from", "raise",
    "True", "False", "None"
}

def snake_case(name: str) -> str:
    name = re.sub(r"[^\w]+", "_", name.strip())
    name = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", name)
    name = re.sub(r"_+", "_", name).strip("_").lower()
    if not name:
        name = "column"
    if name[0].isdigit():
        name = f"c_{name}"
    if name in RESERVED_WORDS:
        name = f"{name}_field"
    return name

def pascal_case(name: str) -> str:
    tokens = re.split(r"[^\w]+", name)
    tokens = [t for t in tokens if t]
    if not tokens:
        return "AutoModel"
    pc = "".join(t.capitalize() for t in tokens)
    if pc[0].isdigit():
        pc = f"C{pc}"
    if pc.lower() in RESERVED_WORDS:
        pc = f"{pc}Model"
    return pc

def unique_name(base: str, existing: set) -> str:
    name = base
    i = 1
    while name in existing:
        i += 1
        name = f"{base}_{i}"
    existing.add(name)
    return name

def looks_like_json(val: str) -> bool:
    if not isinstance(val, str):
        return False
    val = val.strip()
    if not val or (val[0] not in "{[" or val[-1] not in "}]"):
        return False
    try:
        json.loads(val)
        return True
    except Exception:
        return False

def is_time_string(val: str) -> bool:
    # Simple HH:MM[:SS] detection
    if not isinstance(val, str):
        return False
    return bool(re.match(r"^\d{1,2}:\d{2}(:\d{2})?$", val.strip()))

def infer_date_or_datetime(series: pd.Series) -> Tuple[bool, Optional[bool]]:
    """
    Returns (is_date_or_datetime, is_datetime_with_time)
    - If False, second value is None.
    - If True, second value True means DateTime (has time), False means Date-only.
    """
    if is_datetime64_any_dtype(series):
        # Check if any has non-midnight time component
        try:
            s = pd.to_datetime(series.dropna(), errors="coerce")
            if s.empty:
                return True, False
            has_time = (s.dt.time != pd.to_datetime("00:00:00").time()).any()
            return True, bool(has_time)
        except Exception:
            return True, True

    # Try parse object/string as date/time
    if is_object_dtype(series) or is_string_dtype(series):
        sample = series.dropna().astype(str).head(200)
        if sample.empty:
            return False, None
        # Try parse as datetime
        parsed = pd.to_datetime(sample, errors="coerce", infer_datetime_format=True)
        if parsed.notna().mean() >= 0.9:
            has_time = (parsed.dt.time != pd.to_datetime("00:00:00").time()).any()
            return True, bool(has_time)
        # Try time-only
        if sample.apply(is_time_string).mean() >= 0.9:
            return True, True  # treat as DateTime/Time; will map to Time later
    return False, None

def infer_sqlalchemy_type(series: pd.Series):
    """
    Map a pandas Series to a SQLAlchemy Column type with reasonable defaults.
    """
    non_null = series.dropna()

    # Booleans
    if is_bool_dtype(series):
        return Boolean()

    # Integers
    if is_integer_dtype(series):
        return Integer()

    # Floats
    if is_float_dtype(series):
        return Float()

    # Date/Datetime/Time
    is_dt, has_time = infer_date_or_datetime(series)
    if is_dt:
        # Detect time-only
        if is_object_dtype(series) or is_string_dtype(series):
            sample = non_null.astype(str).head(200)
            if sample.apply(is_time_string).mean() >= 0.9:
                return Time()
        if has_time:
            return DateTime()
        else:
            return Date()

    # Object/string-like
    if is_object_dtype(series) or is_string_dtype(series):
        # JSON?
        sample = non_null.astype(str).head(200)
        if not sample.empty and sample.apply(looks_like_json).mean() >= 0.8:
            return JSON()

        # Determine max length to choose String vs Text
        if non_null.empty:
            return String(255)
        max_len = int(non_null.astype(str).map(len).max())
        if max_len <= 255:
            return String(max_len if max_len > 0 else 50)
        else:
            return Text()

    # Fallback
    return Text()


@dataclass
class ModelSpec:
    class_obj: type
    table_name: str
    original_to_attr: Dict[str, str]
    added_pk: Optional[str]  # name of synthetic PK if added


def generate_model_from_dataframe(table_name: str, df: pd.DataFrame, base=Base) -> ModelSpec:
    """
    Generate a SQLAlchemy ORM model class from a pandas DataFrame,
    inferring types and constraints.
    """
    # Sanitize names
    snake_table = snake_case(table_name)
    class_name = pascal_case(table_name)

    # Deduplicate/sanitize column names
    existing = set()
    col_map: Dict[str, str] = {}
    for col in df.columns:
        s = unique_name(snake_case(col), existing)
        col_map[col] = s

    # Figure out primary key
    lower_cols = {c.lower(): c for c in df.columns}
    pk_attr: Optional[str] = None
    # Prefer 'id' or '<table>_id' or 'uuid' if present
    for candidate in ("id", f"{snake_table}_id", "uuid"):
        if candidate in lower_cols:
            pk_attr = col_map[lower_cols[candidate]]
            break

    attrs = {
        "__tablename__": snake_table,
        "__table_args__": {"extend_existing": True},
    }

    # Build columns
    for original, attr_name in col_map.items():
        series = df[original]
        col_type = infer_sqlalchemy_type(series)
        nullable = series.isna().any()
        unique = False
        try:
            # Only set unique if the column is indeed unique (ignoring NaNs)
            unique = series.dropna().is_unique and not series.dropna().empty
        except Exception:
            unique = False

        # Primary key handling
        is_pk = (pk_attr == attr_name)
        kwargs = {"nullable": nullable}
        if unique:
            kwargs["unique"] = True

        # If PK is integer, allow autoincrement
        if is_pk:
            if isinstance(col_type, Integer):
                col = Column(attr_name, Integer, primary_key=True, autoincrement=True)
            else:
                col = Column(attr_name, col_type.__class__(*getattr(col_type, "args", ())), primary_key=True)
        else:
            col = Column(attr_name, col_type, **kwargs)

        attrs[attr_name] = col

    # If no primary key found, add synthetic integer id
    added_pk = None
    if not any(getattr(c, "primary_key", False) for c in attrs.values() if isinstance(c, Column)):
        attrs["id"] = Column(Integer, primary_key=True, autoincrement=True)
        added_pk = "id"

    # Create model class dynamically
    model_cls = type(class_name, (base,), attrs)
    return ModelSpec(class_obj=model_cls, table_name=snake_table, original_to_attr=col_map, added_pk=added_pk)


# --------------------------- Data loading ---------------------------

def load_dataframe_with_inference(csv_path: str, sample_rows: int = 5000) -> pd.DataFrame:
    """
    Load CSV with dtype inference. Reads a sample first to help pandas guess better.
    """
    # First pass: small sample to detect
    try:
        sample = pd.read_csv(csv_path, nrows=sample_rows, low_memory=False)
        # Second pass with columns from sample to stabilize types
        df = pd.read_csv(csv_path, dtype=sample.dtypes.to_dict(), low_memory=False)
        return df
    except Exception:
        # Fallback single pass
        return pd.read_csv(csv_path, low_memory=False)

def insert_dataframe(session, model_spec: ModelSpec, df: pd.DataFrame, chunksize: int = 1000):
    """
    Insert dataframe rows into DB using bulk operations. Automatically maps original
    column names to ORM attribute names.
    """
    Model = model_spec.class_obj
    # Rename columns to sanitized attribute names
    df2 = df.rename(columns=model_spec.original_to_attr).copy()

    # Remove synthetic PK if present
    if model_spec.added_pk and model_spec.added_pk in df2.columns:
        df2 = df2.drop(columns=[model_spec.added_pk])

    # Convert NaN to None for SQLAlchemy
    df2 = df2.where(pd.notna(df2), None)

    # Insert in chunks
    records_iter = (df2[i:i + chunksize] for i in range(0, len(df2), chunksize))
    for chunk in records_iter:
        mappings = chunk.to_dict(orient="records")
        if not mappings:
            continue
        session.bulk_insert_mappings(Model, mappings)
    session.commit()


# --------------------------- CLI / Main ---------------------------

def generate_models_for_folder(folder: str) -> List[Tuple[ModelSpec, pd.DataFrame]]:
    result = []
    for name in os.listdir(folder):
        path = os.path.join(folder, name)
        if os.path.isfile(path) and name.lower().endswith(".csv"):
            df = load_dataframe_with_inference(path)
            table_name = os.path.splitext(os.path.basename(path))[0]
            spec = generate_model_from_dataframe(table_name, df)
            result.append((spec, df))
    return result

def ensure_schema(engine, base=Base):
    inspector = sa_inspect(engine)
    # This just creates all tables; existence is handled by extend_existing
    base.metadata.create_all(engine)

def demo_query(session, model_spec: ModelSpec, limit: int = 5):
    Model = model_spec.class_obj
    rows = session.query(Model).limit(limit).all()
    print(f"Sample rows from {model_spec.table_name}:")
    for r in rows:
        print({c.name: getattr(r, c.name) for c in r.__table__.columns})

def build_arg_parser():
    p = argparse.ArgumentParser(description="Generate SQLAlchemy models and schema from dataset(s).")
    g = p.add_mutually_exclusive_group(required=True)
    g.add_argument("--csv", help="Path to a CSV file.")
    g.add_argument("--folder", help="Path to a folder containing CSV files.")
    p.add_argument("--db", default="sqlite:///auto_model.db", help="SQLAlchemy database URL.")
    p.add_argument("--load", action="store_true", help="Load the data into the database after creating schema.")
    p.add_argument("--show", action="store_true", help="Print a few rows after loading.")
    p.add_argument("--echo", action="store_true", help="Enable SQL echo.")
    return p

def main():
    args = build_arg_parser().parse_args()

    engine = create_engine(args.db, echo=args.echo, future=True)
    Session = sessionmaker(bind=engine, autoflush=False)

    model_df_pairs: List[Tuple[ModelSpec, pd.DataFrame]] = []

    if args.csv:
        if not os.path.exists(args.csv):
            raise FileNotFoundError(args.csv)
        df = load_dataframe_with_inference(args.csv)
        table_name = os.path.splitext(os.path.basename(args.csv))[0]
        spec = generate_model_from_dataframe(table_name, df)
        model_df_pairs.append((spec, df))

    elif args.folder:
        if not os.path.isdir(args.folder):
            raise NotADirectoryError(args.folder)
        model_df_pairs = generate_models_for_folder(args.folder)
        if not model_df_pairs:
            print("No CSV files found in folder.")
            return

    # Create schema for all generated models
    ensure_schema(engine, Base)

    # Optionally load data
    if args.load:
        with Session() as session:
            for spec, df in model_df_pairs:
                print(f"Loading data into '{spec.table_name}'...")
                insert_dataframe(session, spec, df)

    # Optionally show sample rows
    if args.show:
        with Session() as session:
            for spec, _ in model_df_pairs:
                demo_query(session, spec, limit=5)

    # Print a summary of generated models
    print("\nGenerated models:")
    for spec, df in model_df_pairs:
        print(f"- Class: {spec.class_obj.__name__}, Table: {spec.table_name}, Columns: {[c.name for c in spec.class_obj.__table__.columns]}")

if __name__ == "__main__":
    main()
------------------------------------------------------------

